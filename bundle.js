!function(e){var n={};function t(o){if(n[o])return n[o].exports;var i=n[o]={i:o,l:!1,exports:{}};return e[o].call(i.exports,i,i.exports,t),i.l=!0,i.exports}t.m=e,t.c=n,t.d=function(e,n,o){t.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:o})},t.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},t.t=function(e,n){if(1&n&&(e=t(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var o=Object.create(null);if(t.r(o),Object.defineProperty(o,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var i in e)t.d(o,i,function(n){return e[n]}.bind(null,i));return o},t.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return t.d(n,"a",n),n},t.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},t.p="",t(t.s=15)}([function(module,exports,__webpack_require__){eval("(function (global, factory) {\n     true ? factory(exports, __webpack_require__(7), __webpack_require__(13), __webpack_require__(14)) :\n    undefined;\n}(this, (function (exports, _slicedToArray, _classCallCheck, _createClass) { 'use strict';\n\n    function _interopDefaultLegacy (e) { return e && typeof e === 'object' && 'default' in e ? e : { 'default': e }; }\n\n    var _slicedToArray__default = /*#__PURE__*/_interopDefaultLegacy(_slicedToArray);\n    var _classCallCheck__default = /*#__PURE__*/_interopDefaultLegacy(_classCallCheck);\n    var _createClass__default = /*#__PURE__*/_interopDefaultLegacy(_createClass);\n\n    var createExtendedExponentialRampToValueAutomationEvent = function createExtendedExponentialRampToValueAutomationEvent(value, endTime, insertTime) {\n      return {\n        endTime: endTime,\n        insertTime: insertTime,\n        type: 'exponentialRampToValue',\n        value: value\n      };\n    };\n\n    var createExtendedLinearRampToValueAutomationEvent = function createExtendedLinearRampToValueAutomationEvent(value, endTime, insertTime) {\n      return {\n        endTime: endTime,\n        insertTime: insertTime,\n        type: 'linearRampToValue',\n        value: value\n      };\n    };\n\n    var createSetValueAutomationEvent = function createSetValueAutomationEvent(value, startTime) {\n      return {\n        startTime: startTime,\n        type: 'setValue',\n        value: value\n      };\n    };\n\n    var createSetValueCurveAutomationEvent = function createSetValueCurveAutomationEvent(values, startTime, duration) {\n      return {\n        duration: duration,\n        startTime: startTime,\n        type: 'setValueCurve',\n        values: values\n      };\n    };\n\n    var getTargetValueAtTime = function getTargetValueAtTime(time, valueAtStartTime, _ref) {\n      var startTime = _ref.startTime,\n          target = _ref.target,\n          timeConstant = _ref.timeConstant;\n      return target + (valueAtStartTime - target) * Math.exp((startTime - time) / timeConstant);\n    };\n\n    var isExponentialRampToValueAutomationEvent = function isExponentialRampToValueAutomationEvent(automationEvent) {\n      return automationEvent.type === 'exponentialRampToValue';\n    };\n\n    var isLinearRampToValueAutomationEvent = function isLinearRampToValueAutomationEvent(automationEvent) {\n      return automationEvent.type === 'linearRampToValue';\n    };\n\n    var isAnyRampToValueAutomationEvent = function isAnyRampToValueAutomationEvent(automationEvent) {\n      return isExponentialRampToValueAutomationEvent(automationEvent) || isLinearRampToValueAutomationEvent(automationEvent);\n    };\n\n    var isSetValueAutomationEvent = function isSetValueAutomationEvent(automationEvent) {\n      return automationEvent.type === 'setValue';\n    };\n\n    var isSetValueCurveAutomationEvent = function isSetValueCurveAutomationEvent(automationEvent) {\n      return automationEvent.type === 'setValueCurve';\n    };\n\n    var getValueOfAutomationEventAtIndexAtTime = function getValueOfAutomationEventAtIndexAtTime(automationEvents, index, time, defaultValue) {\n      var automationEvent = automationEvents[index];\n      return automationEvent === undefined ? defaultValue : isAnyRampToValueAutomationEvent(automationEvent) || isSetValueAutomationEvent(automationEvent) ? automationEvent.value : isSetValueCurveAutomationEvent(automationEvent) ? automationEvent.values[automationEvent.values.length - 1] : getTargetValueAtTime(time, getValueOfAutomationEventAtIndexAtTime(automationEvents, index - 1, automationEvent.startTime, defaultValue), automationEvent);\n    };\n\n    var getEndTimeAndValueOfPreviousAutomationEvent = function getEndTimeAndValueOfPreviousAutomationEvent(automationEvents, index, currentAutomationEvent, nextAutomationEvent, defaultValue) {\n      return currentAutomationEvent === undefined ? [nextAutomationEvent.insertTime, defaultValue] : isAnyRampToValueAutomationEvent(currentAutomationEvent) ? [currentAutomationEvent.endTime, currentAutomationEvent.value] : isSetValueAutomationEvent(currentAutomationEvent) ? [currentAutomationEvent.startTime, currentAutomationEvent.value] : isSetValueCurveAutomationEvent(currentAutomationEvent) ? [currentAutomationEvent.startTime + currentAutomationEvent.duration, currentAutomationEvent.values[currentAutomationEvent.values.length - 1]] : [currentAutomationEvent.startTime, getValueOfAutomationEventAtIndexAtTime(automationEvents, index - 1, currentAutomationEvent.startTime, defaultValue)];\n    };\n\n    var isCancelAndHoldAutomationEvent = function isCancelAndHoldAutomationEvent(automationEvent) {\n      return automationEvent.type === 'cancelAndHold';\n    };\n\n    var isCancelScheduledValuesAutomationEvent = function isCancelScheduledValuesAutomationEvent(automationEvent) {\n      return automationEvent.type === 'cancelScheduledValues';\n    };\n\n    var getEventTime = function getEventTime(automationEvent) {\n      if (isCancelAndHoldAutomationEvent(automationEvent) || isCancelScheduledValuesAutomationEvent(automationEvent)) {\n        return automationEvent.cancelTime;\n      }\n\n      if (isExponentialRampToValueAutomationEvent(automationEvent) || isLinearRampToValueAutomationEvent(automationEvent)) {\n        return automationEvent.endTime;\n      }\n\n      return automationEvent.startTime;\n    };\n\n    var getExponentialRampValueAtTime = function getExponentialRampValueAtTime(time, startTime, valueAtStartTime, _ref) {\n      var endTime = _ref.endTime,\n          value = _ref.value;\n\n      if (valueAtStartTime === value) {\n        return value;\n      }\n\n      if (0 < valueAtStartTime && 0 < value || valueAtStartTime < 0 && value < 0) {\n        return valueAtStartTime * Math.pow(value / valueAtStartTime, (time - startTime) / (endTime - startTime));\n      }\n\n      return 0;\n    };\n\n    var getLinearRampValueAtTime = function getLinearRampValueAtTime(time, startTime, valueAtStartTime, _ref) {\n      var endTime = _ref.endTime,\n          value = _ref.value;\n      return valueAtStartTime + (time - startTime) / (endTime - startTime) * (value - valueAtStartTime);\n    };\n\n    var interpolateValue = function interpolateValue(values, theoreticIndex) {\n      var lowerIndex = Math.floor(theoreticIndex);\n      var upperIndex = Math.ceil(theoreticIndex);\n\n      if (lowerIndex === upperIndex) {\n        return values[lowerIndex];\n      }\n\n      return (1 - (theoreticIndex - lowerIndex)) * values[lowerIndex] + (1 - (upperIndex - theoreticIndex)) * values[upperIndex];\n    };\n\n    var getValueCurveValueAtTime = function getValueCurveValueAtTime(time, _ref) {\n      var duration = _ref.duration,\n          startTime = _ref.startTime,\n          values = _ref.values;\n      var theoreticIndex = (time - startTime) / duration * (values.length - 1);\n      return interpolateValue(values, theoreticIndex);\n    };\n\n    var isSetTargetAutomationEvent = function isSetTargetAutomationEvent(automationEvent) {\n      return automationEvent.type === 'setTarget';\n    };\n\n    var AutomationEventList = /*#__PURE__*/function () {\n      function AutomationEventList(defaultValue) {\n        _classCallCheck__default['default'](this, AutomationEventList);\n\n        this._automationEvents = [];\n        this._currenTime = 0;\n        this._defaultValue = defaultValue;\n      }\n\n      _createClass__default['default'](AutomationEventList, [{\n        key: Symbol.iterator,\n        value: function value() {\n          return this._automationEvents[Symbol.iterator]();\n        }\n      }, {\n        key: \"add\",\n        value: function add(automationEvent) {\n          var eventTime = getEventTime(automationEvent);\n\n          if (isCancelAndHoldAutomationEvent(automationEvent) || isCancelScheduledValuesAutomationEvent(automationEvent)) {\n            var index = this._automationEvents.findIndex(function (currentAutomationEvent) {\n              if (isCancelScheduledValuesAutomationEvent(automationEvent) && isSetValueCurveAutomationEvent(currentAutomationEvent)) {\n                return currentAutomationEvent.startTime + currentAutomationEvent.duration >= eventTime;\n              }\n\n              return getEventTime(currentAutomationEvent) >= eventTime;\n            });\n\n            var removedAutomationEvent = this._automationEvents[index];\n\n            if (index !== -1) {\n              this._automationEvents = this._automationEvents.slice(0, index);\n            }\n\n            if (isCancelAndHoldAutomationEvent(automationEvent)) {\n              var lastAutomationEvent = this._automationEvents[this._automationEvents.length - 1];\n\n              if (removedAutomationEvent !== undefined && isAnyRampToValueAutomationEvent(removedAutomationEvent)) {\n                if (isSetTargetAutomationEvent(lastAutomationEvent)) {\n                  throw new Error('The internal list is malformed.');\n                }\n\n                var startTime = isSetValueCurveAutomationEvent(lastAutomationEvent) ? lastAutomationEvent.startTime + lastAutomationEvent.duration : getEventTime(lastAutomationEvent);\n                var startValue = isSetValueCurveAutomationEvent(lastAutomationEvent) ? lastAutomationEvent.values[lastAutomationEvent.values.length - 1] : lastAutomationEvent.value;\n                var value = isExponentialRampToValueAutomationEvent(removedAutomationEvent) ? getExponentialRampValueAtTime(eventTime, startTime, startValue, removedAutomationEvent) : getLinearRampValueAtTime(eventTime, startTime, startValue, removedAutomationEvent);\n                var truncatedAutomationEvent = isExponentialRampToValueAutomationEvent(removedAutomationEvent) ? createExtendedExponentialRampToValueAutomationEvent(value, eventTime, this._currenTime) : createExtendedLinearRampToValueAutomationEvent(value, eventTime, this._currenTime);\n\n                this._automationEvents.push(truncatedAutomationEvent);\n              }\n\n              if (lastAutomationEvent !== undefined && isSetTargetAutomationEvent(lastAutomationEvent)) {\n                this._automationEvents.push(createSetValueAutomationEvent(this.getValue(eventTime), eventTime));\n              }\n\n              if (lastAutomationEvent !== undefined && isSetValueCurveAutomationEvent(lastAutomationEvent) && lastAutomationEvent.startTime + lastAutomationEvent.duration > eventTime) {\n                this._automationEvents[this._automationEvents.length - 1] = createSetValueCurveAutomationEvent(new Float32Array([6, 7]), lastAutomationEvent.startTime, eventTime - lastAutomationEvent.startTime);\n              }\n            }\n          } else {\n            var _index = this._automationEvents.findIndex(function (currentAutomationEvent) {\n              return getEventTime(currentAutomationEvent) > eventTime;\n            });\n\n            var previousAutomationEvent = _index === -1 ? this._automationEvents[this._automationEvents.length - 1] : this._automationEvents[_index - 1];\n\n            if (previousAutomationEvent !== undefined && isSetValueCurveAutomationEvent(previousAutomationEvent) && getEventTime(previousAutomationEvent) + previousAutomationEvent.duration > eventTime) {\n              return false;\n            }\n\n            var persistentAutomationEvent = isExponentialRampToValueAutomationEvent(automationEvent) ? createExtendedExponentialRampToValueAutomationEvent(automationEvent.value, automationEvent.endTime, this._currenTime) : isLinearRampToValueAutomationEvent(automationEvent) ? createExtendedLinearRampToValueAutomationEvent(automationEvent.value, eventTime, this._currenTime) : automationEvent;\n\n            if (_index === -1) {\n              this._automationEvents.push(persistentAutomationEvent);\n            } else {\n              if (isSetValueCurveAutomationEvent(automationEvent) && eventTime + automationEvent.duration > getEventTime(this._automationEvents[_index])) {\n                return false;\n              }\n\n              this._automationEvents.splice(_index, 0, persistentAutomationEvent);\n            }\n          }\n\n          return true;\n        }\n      }, {\n        key: \"flush\",\n        value: function flush(time) {\n          var index = this._automationEvents.findIndex(function (currentAutomationEvent) {\n            return getEventTime(currentAutomationEvent) > time;\n          });\n\n          if (index > 1) {\n            var remainingAutomationEvents = this._automationEvents.slice(index - 1);\n\n            var firstRemainingAutomationEvent = remainingAutomationEvents[0];\n\n            if (isSetTargetAutomationEvent(firstRemainingAutomationEvent)) {\n              remainingAutomationEvents.unshift(createSetValueAutomationEvent(getValueOfAutomationEventAtIndexAtTime(this._automationEvents, index - 2, firstRemainingAutomationEvent.startTime, this._defaultValue), firstRemainingAutomationEvent.startTime));\n            }\n\n            this._automationEvents = remainingAutomationEvents;\n          }\n        }\n      }, {\n        key: \"getValue\",\n        value: function getValue(time) {\n          if (this._automationEvents.length === 0) {\n            return this._defaultValue;\n          }\n\n          var lastAutomationEvent = this._automationEvents[this._automationEvents.length - 1];\n\n          var index = this._automationEvents.findIndex(function (automationEvent) {\n            return getEventTime(automationEvent) > time;\n          });\n\n          var nextAutomationEvent = this._automationEvents[index];\n          var currentAutomationEvent = getEventTime(lastAutomationEvent) <= time ? lastAutomationEvent : this._automationEvents[index - 1];\n\n          if (currentAutomationEvent !== undefined && isSetTargetAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === undefined || !isAnyRampToValueAutomationEvent(nextAutomationEvent) || nextAutomationEvent.insertTime > time)) {\n            return getTargetValueAtTime(time, getValueOfAutomationEventAtIndexAtTime(this._automationEvents, index - 2, currentAutomationEvent.startTime, this._defaultValue), currentAutomationEvent);\n          }\n\n          if (currentAutomationEvent !== undefined && isSetValueAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === undefined || !isAnyRampToValueAutomationEvent(nextAutomationEvent))) {\n            return currentAutomationEvent.value;\n          }\n\n          if (currentAutomationEvent !== undefined && isSetValueCurveAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === undefined || !isAnyRampToValueAutomationEvent(nextAutomationEvent) || currentAutomationEvent.startTime + currentAutomationEvent.duration > time)) {\n            if (time < currentAutomationEvent.startTime + currentAutomationEvent.duration) {\n              return getValueCurveValueAtTime(time, currentAutomationEvent);\n            }\n\n            return currentAutomationEvent.values[currentAutomationEvent.values.length - 1];\n          }\n\n          if (currentAutomationEvent !== undefined && isAnyRampToValueAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === undefined || !isAnyRampToValueAutomationEvent(nextAutomationEvent))) {\n            return currentAutomationEvent.value;\n          }\n\n          if (nextAutomationEvent !== undefined && isExponentialRampToValueAutomationEvent(nextAutomationEvent)) {\n            var _getEndTimeAndValueOf = getEndTimeAndValueOfPreviousAutomationEvent(this._automationEvents, index - 1, currentAutomationEvent, nextAutomationEvent, this._defaultValue),\n                _getEndTimeAndValueOf2 = _slicedToArray__default['default'](_getEndTimeAndValueOf, 2),\n                startTime = _getEndTimeAndValueOf2[0],\n                value = _getEndTimeAndValueOf2[1];\n\n            return getExponentialRampValueAtTime(time, startTime, value, nextAutomationEvent);\n          }\n\n          if (nextAutomationEvent !== undefined && isLinearRampToValueAutomationEvent(nextAutomationEvent)) {\n            var _getEndTimeAndValueOf3 = getEndTimeAndValueOfPreviousAutomationEvent(this._automationEvents, index - 1, currentAutomationEvent, nextAutomationEvent, this._defaultValue),\n                _getEndTimeAndValueOf4 = _slicedToArray__default['default'](_getEndTimeAndValueOf3, 2),\n                _startTime = _getEndTimeAndValueOf4[0],\n                _value = _getEndTimeAndValueOf4[1];\n\n            return getLinearRampValueAtTime(time, _startTime, _value, nextAutomationEvent);\n          }\n\n          return this._defaultValue;\n        }\n      }]);\n\n      return AutomationEventList;\n    }();\n\n    var createCancelAndHoldAutomationEvent = function createCancelAndHoldAutomationEvent(cancelTime) {\n      return {\n        cancelTime: cancelTime,\n        type: 'cancelAndHold'\n      };\n    };\n\n    var createCancelScheduledValuesAutomationEvent = function createCancelScheduledValuesAutomationEvent(cancelTime) {\n      return {\n        cancelTime: cancelTime,\n        type: 'cancelScheduledValues'\n      };\n    };\n\n    var createExponentialRampToValueAutomationEvent = function createExponentialRampToValueAutomationEvent(value, endTime) {\n      return {\n        endTime: endTime,\n        type: 'exponentialRampToValue',\n        value: value\n      };\n    };\n\n    var createLinearRampToValueAutomationEvent = function createLinearRampToValueAutomationEvent(value, endTime) {\n      return {\n        endTime: endTime,\n        type: 'linearRampToValue',\n        value: value\n      };\n    };\n\n    var createSetTargetAutomationEvent = function createSetTargetAutomationEvent(target, startTime, timeConstant) {\n      return {\n        startTime: startTime,\n        target: target,\n        timeConstant: timeConstant,\n        type: 'setTarget'\n      };\n    };\n\n    exports.AutomationEventList = AutomationEventList;\n    exports.createCancelAndHoldAutomationEvent = createCancelAndHoldAutomationEvent;\n    exports.createCancelScheduledValuesAutomationEvent = createCancelScheduledValuesAutomationEvent;\n    exports.createExponentialRampToValueAutomationEvent = createExponentialRampToValueAutomationEvent;\n    exports.createLinearRampToValueAutomationEvent = createLinearRampToValueAutomationEvent;\n    exports.createSetTargetAutomationEvent = createSetTargetAutomationEvent;\n    exports.createSetValueAutomationEvent = createSetValueAutomationEvent;\n    exports.createSetValueCurveAutomationEvent = createSetValueCurveAutomationEvent;\n\n    Object.defineProperty(exports, '__esModule', { value: true });\n\n})));\n\n\n//# sourceURL=webpack:///./node_modules/automation-events/build/es5/bundle.js?")},function(module,exports,__webpack_require__){eval('var __WEBPACK_AMD_DEFINE_ARRAY__, __WEBPACK_AMD_DEFINE_RESULT__;/*\n\nWebMidi v2.5.2\n\nWebMidi.js helps you tame the Web MIDI API. Send and receive MIDI messages with ease. Control instruments with user-friendly functions (playNote, sendPitchBend, etc.). React to MIDI input with simple event listeners (noteon, pitchbend, controlchange, etc.).\nhttps://github.com/djipco/webmidi\n\n\nThe MIT License (MIT)\n\nCopyright (c) 2015-2019, Jean-Philippe Côté\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the "Software"), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial\nportions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES\nOR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n*/\n\n\n!function(scope){"use strict";function WebMidi(){if(WebMidi.prototype._singleton)throw new Error("WebMidi is a singleton, it cannot be instantiated directly.");(WebMidi.prototype._singleton=this)._inputs=[],this._outputs=[],this._userHandlers={},this._stateChangeQueue=[],this._processingStateChange=!1,this._midiInterfaceEvents=["connected","disconnected"],this._nrpnBuffer=[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]],this._nrpnEventsEnabled=!0,this._nrpnTypes=["entry","increment","decrement"],this._notes=["C","C#","D","D#","E","F","F#","G","G#","A","A#","B"],this._semitones={C:0,D:2,E:4,F:5,G:7,A:9,B:11},Object.defineProperties(this,{MIDI_SYSTEM_MESSAGES:{value:{sysex:240,timecode:241,songposition:242,songselect:243,tuningrequest:246,sysexend:247,clock:248,start:250,continue:251,stop:252,activesensing:254,reset:255,midimessage:0,unknownsystemmessage:-1},writable:!1,enumerable:!0,configurable:!1},MIDI_CHANNEL_MESSAGES:{value:{noteoff:8,noteon:9,keyaftertouch:10,controlchange:11,channelmode:11,nrpn:11,programchange:12,channelaftertouch:13,pitchbend:14},writable:!1,enumerable:!0,configurable:!1},MIDI_REGISTERED_PARAMETER:{value:{pitchbendrange:[0,0],channelfinetuning:[0,1],channelcoarsetuning:[0,2],tuningprogram:[0,3],tuningbank:[0,4],modulationrange:[0,5],azimuthangle:[61,0],elevationangle:[61,1],gain:[61,2],distanceratio:[61,3],maximumdistance:[61,4],maximumdistancegain:[61,5],referencedistanceratio:[61,6],panspreadangle:[61,7],rollangle:[61,8]},writable:!1,enumerable:!0,configurable:!1},MIDI_CONTROL_CHANGE_MESSAGES:{value:{bankselectcoarse:0,modulationwheelcoarse:1,breathcontrollercoarse:2,footcontrollercoarse:4,portamentotimecoarse:5,dataentrycoarse:6,volumecoarse:7,balancecoarse:8,pancoarse:10,expressioncoarse:11,effectcontrol1coarse:12,effectcontrol2coarse:13,generalpurposeslider1:16,generalpurposeslider2:17,generalpurposeslider3:18,generalpurposeslider4:19,bankselectfine:32,modulationwheelfine:33,breathcontrollerfine:34,footcontrollerfine:36,portamentotimefine:37,dataentryfine:38,volumefine:39,balancefine:40,panfine:42,expressionfine:43,effectcontrol1fine:44,effectcontrol2fine:45,holdpedal:64,portamento:65,sustenutopedal:66,softpedal:67,legatopedal:68,hold2pedal:69,soundvariation:70,resonance:71,soundreleasetime:72,soundattacktime:73,brightness:74,soundcontrol6:75,soundcontrol7:76,soundcontrol8:77,soundcontrol9:78,soundcontrol10:79,generalpurposebutton1:80,generalpurposebutton2:81,generalpurposebutton3:82,generalpurposebutton4:83,reverblevel:91,tremololevel:92,choruslevel:93,celestelevel:94,phaserlevel:95,databuttonincrement:96,databuttondecrement:97,nonregisteredparametercoarse:98,nonregisteredparameterfine:99,registeredparametercoarse:100,registeredparameterfine:101},writable:!1,enumerable:!0,configurable:!1},MIDI_NRPN_MESSAGES:{value:{entrymsb:6,entrylsb:38,increment:96,decrement:97,paramlsb:98,parammsb:99,nullactiveparameter:127},writable:!1,enumerable:!0,configurable:!1},MIDI_CHANNEL_MODE_MESSAGES:{value:{allsoundoff:120,resetallcontrollers:121,localcontrol:122,allnotesoff:123,omnimodeoff:124,omnimodeon:125,monomodeon:126,polymodeon:127},writable:!1,enumerable:!0,configurable:!1},octaveOffset:{value:0,writable:!0,enumerable:!0,configurable:!1}}),Object.defineProperties(this,{supported:{enumerable:!0,get:function(){return"requestMIDIAccess"in navigator}},enabled:{enumerable:!0,get:function(){return void 0!==this.interface}.bind(this)},inputs:{enumerable:!0,get:function(){return this._inputs}.bind(this)},outputs:{enumerable:!0,get:function(){return this._outputs}.bind(this)},sysexEnabled:{enumerable:!0,get:function(){return!(!this.interface||!this.interface.sysexEnabled)}.bind(this)},nrpnEventsEnabled:{enumerable:!0,get:function(){return!!this._nrpnEventsEnabled}.bind(this),set:function(enabled){return this._nrpnEventsEnabled=enabled,this._nrpnEventsEnabled}},nrpnTypes:{enumerable:!0,get:function(){return this._nrpnTypes}.bind(this)},time:{enumerable:!0,get:function(){return performance.now()}}})}var wm=new WebMidi;function Input(midiInput){var that=this;this._userHandlers={channel:{},system:{}},this._midiInput=midiInput,Object.defineProperties(this,{connection:{enumerable:!0,get:function(){return that._midiInput.connection}},id:{enumerable:!0,get:function(){return that._midiInput.id}},manufacturer:{enumerable:!0,get:function(){return that._midiInput.manufacturer}},name:{enumerable:!0,get:function(){return that._midiInput.name}},state:{enumerable:!0,get:function(){return that._midiInput.state}},type:{enumerable:!0,get:function(){return that._midiInput.type}}}),this._initializeUserHandlers(),this._midiInput.onmidimessage=this._onMidiMessage.bind(this)}function Output(midiOutput){var that=this;this._midiOutput=midiOutput,Object.defineProperties(this,{connection:{enumerable:!0,get:function(){return that._midiOutput.connection}},id:{enumerable:!0,get:function(){return that._midiOutput.id}},manufacturer:{enumerable:!0,get:function(){return that._midiOutput.manufacturer}},name:{enumerable:!0,get:function(){return that._midiOutput.name}},state:{enumerable:!0,get:function(){return that._midiOutput.state}},type:{enumerable:!0,get:function(){return that._midiOutput.type}}})}WebMidi.prototype.enable=function(callback,sysex){this.enabled||(this.supported?navigator.requestMIDIAccess({sysex:sysex}).then(function(midiAccess){var promiseTimeout,events=[],promises=[];this.interface=midiAccess,this._resetInterfaceUserHandlers(),this.interface.onstatechange=function(e){events.push(e)};for(var inputs=midiAccess.inputs.values(),input=inputs.next();input&&!input.done;input=inputs.next())promises.push(input.value.open());for(var outputs=midiAccess.outputs.values(),output=outputs.next();output&&!output.done;output=outputs.next())promises.push(output.value.open());function onPortsOpen(){clearTimeout(promiseTimeout),this._updateInputsAndOutputs(),this.interface.onstatechange=this._onInterfaceStateChange.bind(this),"function"==typeof callback&&callback.call(this),events.forEach(function(event){this._onInterfaceStateChange(event)}.bind(this))}promiseTimeout=setTimeout(onPortsOpen.bind(this),200),Promise&&Promise.all(promises).catch(function(err){}).then(onPortsOpen.bind(this))}.bind(this),function(err){"function"==typeof callback&&callback.call(this,err)}.bind(this)):"function"==typeof callback&&callback(new Error("The Web MIDI API is not supported by your browser.")))},WebMidi.prototype.disable=function(){if(!this.supported)throw new Error("The Web MIDI API is not supported by your browser.");this.enabled&&(this.removeListener(),this.inputs.forEach(function(input){input.removeListener()})),this.interface&&(this.interface.onstatechange=void 0),this.interface=void 0,this._inputs=[],this._outputs=[],this._nrpnEventsEnabled=!0,this._resetInterfaceUserHandlers()},WebMidi.prototype.addListener=function(type,listener){if(!this.enabled)throw new Error("WebMidi must be enabled before adding event listeners.");if("function"!=typeof listener)throw new TypeError("The \'listener\' parameter must be a function.");if(!(0<=this._midiInterfaceEvents.indexOf(type)))throw new TypeError("The specified event type is not supported.");return this._userHandlers[type].push(listener),this},WebMidi.prototype.hasListener=function(type,listener){if(!this.enabled)throw new Error("WebMidi must be enabled before checking event listeners.");if("function"!=typeof listener)throw new TypeError("The \'listener\' parameter must be a function.");if(!(0<=this._midiInterfaceEvents.indexOf(type)))throw new TypeError("The specified event type is not supported.");for(var o=0;o<this._userHandlers[type].length;o++)if(this._userHandlers[type][o]===listener)return!0;return!1},WebMidi.prototype.removeListener=function(type,listener){if(!this.enabled)throw new Error("WebMidi must be enabled before removing event listeners.");if(void 0!==listener&&"function"!=typeof listener)throw new TypeError("The \'listener\' parameter must be a function.");if(0<=this._midiInterfaceEvents.indexOf(type))if(listener)for(var o=0;o<this._userHandlers[type].length;o++)this._userHandlers[type][o]===listener&&this._userHandlers[type].splice(o,1);else this._userHandlers[type]=[];else{if(void 0!==type)throw new TypeError("The specified event type is not supported.");this._resetInterfaceUserHandlers()}return this},WebMidi.prototype.toMIDIChannels=function(channel){var channels;if("all"===channel||void 0===channel)channels=["all"];else{if("none"===channel)return channels=[];channels=Array.isArray(channel)?channel:[channel]}return-1<channels.indexOf("all")&&(channels=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]),channels.map(function(ch){return parseInt(ch)}).filter(function(ch){return 1<=ch&&ch<=16})},WebMidi.prototype.getInputById=function(id){if(!this.enabled)throw new Error("WebMidi is not enabled.");id=String(id);for(var i=0;i<this.inputs.length;i++)if(this.inputs[i].id===id)return this.inputs[i];return!1},WebMidi.prototype.getOutputById=function(id){if(!this.enabled)throw new Error("WebMidi is not enabled.");id=String(id);for(var i=0;i<this.outputs.length;i++)if(this.outputs[i].id===id)return this.outputs[i];return!1},WebMidi.prototype.getInputByName=function(name){if(!this.enabled)throw new Error("WebMidi is not enabled.");for(var i=0;i<this.inputs.length;i++)if(~this.inputs[i].name.indexOf(name))return this.inputs[i];return!1},WebMidi.prototype.getOctave=function(number){if(null!=number&&0<=number&&number<=127)return Math.floor(Math.floor(number)/12-1)+Math.floor(wm.octaveOffset)},WebMidi.prototype.getOutputByName=function(name){if(!this.enabled)throw new Error("WebMidi is not enabled.");for(var i=0;i<this.outputs.length;i++)if(~this.outputs[i].name.indexOf(name))return this.outputs[i];return!1},WebMidi.prototype.guessNoteNumber=function(input){var output=!1;if(input&&input.toFixed&&0<=input&&input<=127?output=Math.round(input):0<=parseInt(input)&&parseInt(input)<=127?output=parseInt(input):("string"==typeof input||input instanceof String)&&(output=this.noteNameToNumber(input)),!1===output)throw new Error("Invalid input value ("+input+").");return output},WebMidi.prototype.noteNameToNumber=function(name){"string"!=typeof name&&(name="");var matches=name.match(/([CDEFGAB])(#{0,2}|b{0,2})(-?\\d+)/i);if(!matches)throw new RangeError("Invalid note name.");var semitones=wm._semitones[matches[1].toUpperCase()],result=12*(parseInt(matches[3])+1-Math.floor(wm.octaveOffset))+semitones;if(-1<matches[2].toLowerCase().indexOf("b")?result-=matches[2].length:-1<matches[2].toLowerCase().indexOf("#")&&(result+=matches[2].length),result<0||127<result)throw new RangeError("Invalid note name or note outside valid range.");return result},WebMidi.prototype._updateInputsAndOutputs=function(){this._updateInputs(),this._updateOutputs()},WebMidi.prototype._updateInputs=function(){for(var i=0;i<this._inputs.length;i++){for(var remove=!0,updated=this.interface.inputs.values(),input=updated.next();input&&!input.done;input=updated.next())if(this._inputs[i]._midiInput===input.value){remove=!1;break}remove&&this._inputs.splice(i,1)}this.interface&&this.interface.inputs.forEach(function(nInput){for(var add=!0,j=0;j<this._inputs.length;j++)this._inputs[j]._midiInput===nInput&&(add=!1);add&&this._inputs.push(new Input(nInput))}.bind(this))},WebMidi.prototype._updateOutputs=function(){for(var i=0;i<this._outputs.length;i++){for(var remove=!0,updated=this.interface.outputs.values(),output=updated.next();output&&!output.done;output=updated.next())if(this._outputs[i]._midiOutput===output.value){remove=!1;break}remove&&this._outputs.splice(i,1)}this.interface&&this.interface.outputs.forEach(function(nOutput){for(var add=!0,j=0;j<this._outputs.length;j++)this._outputs[j]._midiOutput===nOutput&&(add=!1);add&&this._outputs.push(new Output(nOutput))}.bind(this))},WebMidi.prototype._onInterfaceStateChange=function(e){this._updateInputsAndOutputs();var event={timestamp:e.timeStamp,type:e.port.state};this.interface&&"connected"===e.port.state?"output"===e.port.type?event.port=this.getOutputById(e.port.id):"input"===e.port.type&&(event.port=this.getInputById(e.port.id)):event.port={connection:"closed",id:e.port.id,manufacturer:e.port.manufacturer,name:e.port.name,state:e.port.state,type:e.port.type},this._userHandlers[e.port.state].forEach(function(handler){handler(event)})},WebMidi.prototype._resetInterfaceUserHandlers=function(){for(var i=0;i<this._midiInterfaceEvents.length;i++)this._userHandlers[this._midiInterfaceEvents[i]]=[]},Input.prototype.on=Input.prototype.addListener=function(type,channel,listener){var that=this;if(void 0===channel&&(channel="all"),Array.isArray(channel)||(channel=[channel]),channel.forEach(function(item){if("all"!==item&&!(1<=item&&item<=16))throw new RangeError("The \'channel\' parameter is invalid.")}),"function"!=typeof listener)throw new TypeError("The \'listener\' parameter must be a function.");if(void 0!==wm.MIDI_SYSTEM_MESSAGES[type])this._userHandlers.system[type]||(this._userHandlers.system[type]=[]),this._userHandlers.system[type].push(listener);else{if(void 0===wm.MIDI_CHANNEL_MESSAGES[type])throw new TypeError("The specified event type is not supported.");if(-1<channel.indexOf("all")){channel=[];for(var j=1;j<=16;j++)channel.push(j)}this._userHandlers.channel[type]||(this._userHandlers.channel[type]=[]),channel.forEach(function(ch){that._userHandlers.channel[type][ch]||(that._userHandlers.channel[type][ch]=[]),that._userHandlers.channel[type][ch].push(listener)})}return this},Input.prototype.hasListener=function(type,channel,listener){var that=this;if("function"!=typeof listener)throw new TypeError("The \'listener\' parameter must be a function.");if(void 0===channel&&(channel="all"),channel.constructor!==Array&&(channel=[channel]),void 0!==wm.MIDI_SYSTEM_MESSAGES[type]){for(var o=0;o<this._userHandlers.system[type].length;o++)if(this._userHandlers.system[type][o]===listener)return!0}else if(void 0!==wm.MIDI_CHANNEL_MESSAGES[type]){if(-1<channel.indexOf("all")){channel=[];for(var j=1;j<=16;j++)channel.push(j)}return!!this._userHandlers.channel[type]&&channel.every(function(chNum){var listeners=that._userHandlers.channel[type][chNum];return listeners&&-1<listeners.indexOf(listener)})}return!1},Input.prototype.removeListener=function(type,channel,listener){var that=this;if(void 0!==listener&&"function"!=typeof listener)throw new TypeError("The \'listener\' parameter must be a function.");if(void 0===channel&&(channel="all"),channel.constructor!==Array&&(channel=[channel]),void 0!==wm.MIDI_SYSTEM_MESSAGES[type])if(void 0===listener)this._userHandlers.system[type]=[];else for(var o=0;o<this._userHandlers.system[type].length;o++)this._userHandlers.system[type][o]===listener&&this._userHandlers.system[type].splice(o,1);else if(void 0!==wm.MIDI_CHANNEL_MESSAGES[type]){if(-1<channel.indexOf("all")){channel=[];for(var j=1;j<=16;j++)channel.push(j)}if(!this._userHandlers.channel[type])return this;channel.forEach(function(chNum){var listeners=that._userHandlers.channel[type][chNum];if(listeners)if(void 0===listener)that._userHandlers.channel[type][chNum]=[];else for(var l=0;l<listeners.length;l++)listeners[l]===listener&&listeners.splice(l,1)})}else{if(void 0!==type)throw new TypeError("The specified event type is not supported.");this._initializeUserHandlers()}return this},Input.prototype._initializeUserHandlers=function(){for(var prop1 in wm.MIDI_CHANNEL_MESSAGES)Object.prototype.hasOwnProperty.call(wm.MIDI_CHANNEL_MESSAGES,prop1)&&(this._userHandlers.channel[prop1]={});for(var prop2 in wm.MIDI_SYSTEM_MESSAGES)Object.prototype.hasOwnProperty.call(wm.MIDI_SYSTEM_MESSAGES,prop2)&&(this._userHandlers.system[prop2]=[])},Input.prototype._onMidiMessage=function(e){if(0<this._userHandlers.system.midimessage.length){var event={target:this,data:e.data,timestamp:e.timeStamp,type:"midimessage"};this._userHandlers.system.midimessage.forEach(function(callback){callback(event)})}e.data[0]<240?(this._parseChannelEvent(e),this._parseNrpnEvent(e)):e.data[0]<=255&&this._parseSystemEvent(e)},Input.prototype._parseNrpnEvent=function(e){var data1,data2,command=e.data[0]>>4,channelBufferIndex=15&e.data[0],channel=1+channelBufferIndex;if(1<e.data.length&&(data1=e.data[1],data2=2<e.data.length?e.data[2]:void 0),wm.nrpnEventsEnabled&&command===wm.MIDI_CHANNEL_MESSAGES.controlchange&&(data1>=wm.MIDI_NRPN_MESSAGES.increment&&data1<=wm.MIDI_NRPN_MESSAGES.parammsb||data1===wm.MIDI_NRPN_MESSAGES.entrymsb||data1===wm.MIDI_NRPN_MESSAGES.entrylsb)){var ccEvent={target:this,type:"controlchange",data:e.data,timestamp:e.timeStamp,channel:channel,controller:{number:data1,name:this.getCcNameByNumber(data1)},value:data2};if(ccEvent.controller.number===wm.MIDI_NRPN_MESSAGES.parammsb&&ccEvent.value!=wm.MIDI_NRPN_MESSAGES.nullactiveparameter)wm._nrpnBuffer[channelBufferIndex]=[],wm._nrpnBuffer[channelBufferIndex][0]=ccEvent;else if(1===wm._nrpnBuffer[channelBufferIndex].length&&ccEvent.controller.number===wm.MIDI_NRPN_MESSAGES.paramlsb)wm._nrpnBuffer[channelBufferIndex].push(ccEvent);else if(2!==wm._nrpnBuffer[channelBufferIndex].length||ccEvent.controller.number!==wm.MIDI_NRPN_MESSAGES.increment&&ccEvent.controller.number!==wm.MIDI_NRPN_MESSAGES.decrement&&ccEvent.controller.number!==wm.MIDI_NRPN_MESSAGES.entrymsb)if(3===wm._nrpnBuffer[channelBufferIndex].length&&wm._nrpnBuffer[channelBufferIndex][2].number===wm.MIDI_NRPN_MESSAGES.entrymsb&&ccEvent.controller.number===wm.MIDI_NRPN_MESSAGES.entrylsb)wm._nrpnBuffer[channelBufferIndex].push(ccEvent);else if(3<=wm._nrpnBuffer[channelBufferIndex].length&&wm._nrpnBuffer[channelBufferIndex].length<=4&&ccEvent.controller.number===wm.MIDI_NRPN_MESSAGES.parammsb&&ccEvent.value===wm.MIDI_NRPN_MESSAGES.nullactiveparameter)wm._nrpnBuffer[channelBufferIndex].push(ccEvent);else if(4<=wm._nrpnBuffer[channelBufferIndex].length&&wm._nrpnBuffer[channelBufferIndex].length<=5&&ccEvent.controller.number===wm.MIDI_NRPN_MESSAGES.paramlsb&&ccEvent.value===wm.MIDI_NRPN_MESSAGES.nullactiveparameter){wm._nrpnBuffer[channelBufferIndex].push(ccEvent);var rawData=[];wm._nrpnBuffer[channelBufferIndex].forEach(function(ev){rawData.push(ev.data)});var nrpnNumber=wm._nrpnBuffer[channelBufferIndex][0].value<<7|wm._nrpnBuffer[channelBufferIndex][1].value,nrpnValue=wm._nrpnBuffer[channelBufferIndex][2].value;6===wm._nrpnBuffer[channelBufferIndex].length&&(nrpnValue=wm._nrpnBuffer[channelBufferIndex][2].value<<7|wm._nrpnBuffer[channelBufferIndex][3].value);var nrpnControllerType="";switch(wm._nrpnBuffer[channelBufferIndex][2].controller.number){case wm.MIDI_NRPN_MESSAGES.entrymsb:nrpnControllerType=wm._nrpnTypes[0];break;case wm.MIDI_NRPN_MESSAGES.increment:nrpnControllerType=wm._nrpnTypes[1];break;case wm.MIDI_NRPN_MESSAGES.decrement:nrpnControllerType=wm._nrpnTypes[2];break;default:throw new Error("The NPRN type was unidentifiable.")}var nrpnEvent={timestamp:ccEvent.timestamp,channel:ccEvent.channel,type:"nrpn",data:rawData,controller:{number:nrpnNumber,type:nrpnControllerType,name:"Non-Registered Parameter "+nrpnNumber},value:nrpnValue};wm._nrpnBuffer[channelBufferIndex]=[],this._userHandlers.channel[nrpnEvent.type]&&this._userHandlers.channel[nrpnEvent.type][nrpnEvent.channel]&&this._userHandlers.channel[nrpnEvent.type][nrpnEvent.channel].forEach(function(callback){callback(nrpnEvent)})}else wm._nrpnBuffer[channelBufferIndex]=[];else wm._nrpnBuffer[channelBufferIndex].push(ccEvent)}},Input.prototype._parseChannelEvent=function(e){var data1,data2,command=e.data[0]>>4,channel=1+(15&e.data[0]);1<e.data.length&&(data1=e.data[1],data2=2<e.data.length?e.data[2]:void 0);var event={target:this,data:e.data,timestamp:e.timeStamp,channel:channel};command===wm.MIDI_CHANNEL_MESSAGES.noteoff||command===wm.MIDI_CHANNEL_MESSAGES.noteon&&0===data2?(event.type="noteoff",event.note={number:data1,name:wm._notes[data1%12],octave:wm.getOctave(data1)},event.velocity=data2/127,event.rawVelocity=data2):command===wm.MIDI_CHANNEL_MESSAGES.noteon?(event.type="noteon",event.note={number:data1,name:wm._notes[data1%12],octave:wm.getOctave(data1)},event.velocity=data2/127,event.rawVelocity=data2):command===wm.MIDI_CHANNEL_MESSAGES.keyaftertouch?(event.type="keyaftertouch",event.note={number:data1,name:wm._notes[data1%12],octave:wm.getOctave(data1)},event.value=data2/127):command===wm.MIDI_CHANNEL_MESSAGES.controlchange&&0<=data1&&data1<=119?(event.type="controlchange",event.controller={number:data1,name:this.getCcNameByNumber(data1)},event.value=data2):command===wm.MIDI_CHANNEL_MESSAGES.channelmode&&120<=data1&&data1<=127?(event.type="channelmode",event.controller={number:data1,name:this.getChannelModeByNumber(data1)},event.value=data2):command===wm.MIDI_CHANNEL_MESSAGES.programchange?(event.type="programchange",event.value=data1):command===wm.MIDI_CHANNEL_MESSAGES.channelaftertouch?(event.type="channelaftertouch",event.value=data1/127):command===wm.MIDI_CHANNEL_MESSAGES.pitchbend?(event.type="pitchbend",event.value=((data2<<7)+data1-8192)/8192):event.type="unknownchannelmessage",this._userHandlers.channel[event.type]&&this._userHandlers.channel[event.type][channel]&&this._userHandlers.channel[event.type][channel].forEach(function(callback){callback(event)})},Input.prototype.getCcNameByNumber=function(number){if(!(0<=(number=Math.floor(number))&&number<=119))throw new RangeError("The control change number must be between 0 and 119.");for(var cc in wm.MIDI_CONTROL_CHANGE_MESSAGES)if(Object.prototype.hasOwnProperty.call(wm.MIDI_CONTROL_CHANGE_MESSAGES,cc)&&number===wm.MIDI_CONTROL_CHANGE_MESSAGES[cc])return cc},Input.prototype.getChannelModeByNumber=function(number){if(!(120<=(number=Math.floor(number))&&status<=127))throw new RangeError("The control change number must be between 120 and 127.");for(var cm in wm.MIDI_CHANNEL_MODE_MESSAGES)if(Object.prototype.hasOwnProperty.call(wm.MIDI_CHANNEL_MODE_MESSAGES,cm)&&number===wm.MIDI_CHANNEL_MODE_MESSAGES[cm])return cm},Input.prototype._parseSystemEvent=function(e){var command=e.data[0],event={target:this,data:e.data,timestamp:e.timeStamp};command===wm.MIDI_SYSTEM_MESSAGES.sysex?event.type="sysex":command===wm.MIDI_SYSTEM_MESSAGES.timecode?event.type="timecode":command===wm.MIDI_SYSTEM_MESSAGES.songposition?event.type="songposition":command===wm.MIDI_SYSTEM_MESSAGES.songselect?(event.type="songselect",event.song=e.data[1]):command===wm.MIDI_SYSTEM_MESSAGES.tuningrequest?event.type="tuningrequest":command===wm.MIDI_SYSTEM_MESSAGES.clock?event.type="clock":command===wm.MIDI_SYSTEM_MESSAGES.start?event.type="start":command===wm.MIDI_SYSTEM_MESSAGES.continue?event.type="continue":command===wm.MIDI_SYSTEM_MESSAGES.stop?event.type="stop":command===wm.MIDI_SYSTEM_MESSAGES.activesensing?event.type="activesensing":command===wm.MIDI_SYSTEM_MESSAGES.reset?event.type="reset":event.type="unknownsystemmessage",this._userHandlers.system[event.type]&&this._userHandlers.system[event.type].forEach(function(callback){callback(event)})},Output.prototype.send=function(status,data,timestamp){if(!(128<=status&&status<=255))throw new RangeError("The status byte must be an integer between 128 (0x80) and 255 (0xFF).");void 0===data&&(data=[]),Array.isArray(data)||(data=[data]);var message=[];return data.forEach(function(item){var parsed=Math.floor(item);if(!(0<=parsed&&parsed<=255))throw new RangeError("Data bytes must be integers between 0 (0x00) and 255 (0xFF).");message.push(parsed)}),this._midiOutput.send([status].concat(message),parseFloat(timestamp)||0),this},Output.prototype.sendSysex=function(manufacturer,data,options){if(!wm.sysexEnabled)throw new Error("Sysex message support must first be activated.");return options=options||{},manufacturer=[].concat(manufacturer),data.forEach(function(item){if(item<0||127<item)throw new RangeError("The data bytes of a sysex message must be integers between 0 (0x00) and 127 (0x7F).")}),data=manufacturer.concat(data,wm.MIDI_SYSTEM_MESSAGES.sysexend),this.send(wm.MIDI_SYSTEM_MESSAGES.sysex,data,this._parseTimeParameter(options.time)),this},Output.prototype.sendTimecodeQuarterFrame=function(value,options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.timecode,value,this._parseTimeParameter(options.time)),this},Output.prototype.sendSongPosition=function(value,options){options=options||{};var msb=(value=Math.floor(value)||0)>>7&127,lsb=127&value;return this.send(wm.MIDI_SYSTEM_MESSAGES.songposition,[msb,lsb],this._parseTimeParameter(options.time)),this},Output.prototype.sendSongSelect=function(value,options){if(options=options||{},!(0<=(value=Math.floor(value))&&value<=127))throw new RangeError("The song number must be between 0 and 127.");return this.send(wm.MIDI_SYSTEM_MESSAGES.songselect,[value],this._parseTimeParameter(options.time)),this},Output.prototype.sendTuningRequest=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.tuningrequest,void 0,this._parseTimeParameter(options.time)),this},Output.prototype.sendClock=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.clock,void 0,this._parseTimeParameter(options.time)),this},Output.prototype.sendStart=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.start,void 0,this._parseTimeParameter(options.time)),this},Output.prototype.sendContinue=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.continue,void 0,this._parseTimeParameter(options.time)),this},Output.prototype.sendStop=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.stop,void 0,this._parseTimeParameter(options.time)),this},Output.prototype.sendActiveSensing=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.activesensing,[],this._parseTimeParameter(options.time)),this},Output.prototype.sendReset=function(options){return options=options||{},this.send(wm.MIDI_SYSTEM_MESSAGES.reset,void 0,this._parseTimeParameter(options.time)),this},Output.prototype.stopNote=function(note,channel,options){if("all"===note)return this.sendChannelMode("allnotesoff",0,channel,options);var nVelocity=64;return(options=options||{}).rawVelocity?!isNaN(options.velocity)&&0<=options.velocity&&options.velocity<=127&&(nVelocity=options.velocity):!isNaN(options.velocity)&&0<=options.velocity&&options.velocity<=1&&(nVelocity=127*options.velocity),this._convertNoteToArray(note).forEach(function(item){wm.toMIDIChannels(channel).forEach(function(ch){this.send((wm.MIDI_CHANNEL_MESSAGES.noteoff<<4)+(ch-1),[item,Math.round(nVelocity)],this._parseTimeParameter(options.time))}.bind(this))}.bind(this)),this},Output.prototype.playNote=function(note,channel,options){var time,nVelocity=64;if((options=options||{}).rawVelocity?!isNaN(options.velocity)&&0<=options.velocity&&options.velocity<=127&&(nVelocity=options.velocity):!isNaN(options.velocity)&&0<=options.velocity&&options.velocity<=1&&(nVelocity=127*options.velocity),time=this._parseTimeParameter(options.time),this._convertNoteToArray(note).forEach(function(item){wm.toMIDIChannels(channel).forEach(function(ch){this.send((wm.MIDI_CHANNEL_MESSAGES.noteon<<4)+(ch-1),[item,Math.round(nVelocity)],time)}.bind(this))}.bind(this)),!isNaN(options.duration)){options.duration<=0&&(options.duration=0);var nRelease=64;options.rawVelocity?!isNaN(options.release)&&0<=options.release&&options.release<=127&&(nRelease=options.release):!isNaN(options.release)&&0<=options.release&&options.release<=1&&(nRelease=127*options.release),this._convertNoteToArray(note).forEach(function(item){wm.toMIDIChannels(channel).forEach(function(ch){this.send((wm.MIDI_CHANNEL_MESSAGES.noteoff<<4)+(ch-1),[item,Math.round(nRelease)],(time||wm.time)+options.duration)}.bind(this))}.bind(this))}return this},Output.prototype.sendKeyAftertouch=function(note,channel,pressure,options){var that=this;if(options=options||{},channel<1||16<channel)throw new RangeError("The channel must be between 1 and 16.");(isNaN(pressure)||pressure<0||1<pressure)&&(pressure=.5);var nPressure=Math.round(127*pressure);return this._convertNoteToArray(note).forEach(function(item){wm.toMIDIChannels(channel).forEach(function(ch){that.send((wm.MIDI_CHANNEL_MESSAGES.keyaftertouch<<4)+(ch-1),[item,nPressure],that._parseTimeParameter(options.time))})}),this},Output.prototype.sendControlChange=function(controller,value,channel,options){if(options=options||{},"string"==typeof controller){if(void 0===(controller=wm.MIDI_CONTROL_CHANGE_MESSAGES[controller]))throw new TypeError("Invalid controller name.")}else if(!(0<=(controller=Math.floor(controller))&&controller<=119))throw new RangeError("Controller numbers must be between 0 and 119.");if(!(0<=(value=Math.floor(value)||0)&&value<=127))throw new RangeError("Controller value must be between 0 and 127.");return wm.toMIDIChannels(channel).forEach(function(ch){this.send((wm.MIDI_CHANNEL_MESSAGES.controlchange<<4)+(ch-1),[controller,value],this._parseTimeParameter(options.time))}.bind(this)),this},Output.prototype._selectRegisteredParameter=function(parameter,channel,time){var that=this;if(parameter[0]=Math.floor(parameter[0]),!(0<=parameter[0]&&parameter[0]<=127))throw new RangeError("The control65 value must be between 0 and 127");if(parameter[1]=Math.floor(parameter[1]),!(0<=parameter[1]&&parameter[1]<=127))throw new RangeError("The control64 value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.sendControlChange(101,parameter[0],channel,{time:time}),that.sendControlChange(100,parameter[1],channel,{time:time})}),this},Output.prototype._selectNonRegisteredParameter=function(parameter,channel,time){var that=this;if(parameter[0]=Math.floor(parameter[0]),!(0<=parameter[0]&&parameter[0]<=127))throw new RangeError("The control63 value must be between 0 and 127");if(parameter[1]=Math.floor(parameter[1]),!(0<=parameter[1]&&parameter[1]<=127))throw new RangeError("The control62 value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.sendControlChange(99,parameter[0],channel,{time:time}),that.sendControlChange(98,parameter[1],channel,{time:time})}),this},Output.prototype._setCurrentRegisteredParameter=function(data,channel,time){var that=this;if((data=[].concat(data))[0]=Math.floor(data[0]),!(0<=data[0]&&data[0]<=127))throw new RangeError("The msb value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.sendControlChange(6,data[0],channel,{time:time})}),data[1]=Math.floor(data[1]),0<=data[1]&&data[1]<=127&&wm.toMIDIChannels(channel).forEach(function(){that.sendControlChange(38,data[1],channel,{time:time})}),this},Output.prototype._deselectRegisteredParameter=function(channel,time){var that=this;return wm.toMIDIChannels(channel).forEach(function(){that.sendControlChange(101,127,channel,{time:time}),that.sendControlChange(100,127,channel,{time:time})}),this},Output.prototype.setRegisteredParameter=function(parameter,data,channel,options){var that=this;if(options=options||{},!Array.isArray(parameter)){if(!wm.MIDI_REGISTERED_PARAMETER[parameter])throw new Error("The specified parameter is not available.");parameter=wm.MIDI_REGISTERED_PARAMETER[parameter]}return wm.toMIDIChannels(channel).forEach(function(){that._selectRegisteredParameter(parameter,channel,options.time),that._setCurrentRegisteredParameter(data,channel,options.time),that._deselectRegisteredParameter(channel,options.time)}),this},Output.prototype.setNonRegisteredParameter=function(parameter,data,channel,options){var that=this;if(options=options||{},!(0<=parameter[0]&&parameter[0]<=127&&0<=parameter[1]&&parameter[1]<=127))throw new Error("Position 0 and 1 of the 2-position parameter array must both be between 0 and 127.");return data=[].concat(data),wm.toMIDIChannels(channel).forEach(function(){that._selectNonRegisteredParameter(parameter,channel,options.time),that._setCurrentRegisteredParameter(data,channel,options.time),that._deselectRegisteredParameter(channel,options.time)}),this},Output.prototype.incrementRegisteredParameter=function(parameter,channel,options){var that=this;if(options=options||{},!Array.isArray(parameter)){if(!wm.MIDI_REGISTERED_PARAMETER[parameter])throw new Error("The specified parameter is not available.");parameter=wm.MIDI_REGISTERED_PARAMETER[parameter]}return wm.toMIDIChannels(channel).forEach(function(){that._selectRegisteredParameter(parameter,channel,options.time),that.sendControlChange(96,0,channel,{time:options.time}),that._deselectRegisteredParameter(channel,options.time)}),this},Output.prototype.decrementRegisteredParameter=function(parameter,channel,options){if(options=options||{},!Array.isArray(parameter)){if(!wm.MIDI_REGISTERED_PARAMETER[parameter])throw new TypeError("The specified parameter is not available.");parameter=wm.MIDI_REGISTERED_PARAMETER[parameter]}return wm.toMIDIChannels(channel).forEach(function(){this._selectRegisteredParameter(parameter,channel,options.time),this.sendControlChange(97,0,channel,{time:options.time}),this._deselectRegisteredParameter(channel,options.time)}.bind(this)),this},Output.prototype.setPitchBendRange=function(semitones,cents,channel,options){var that=this;if(options=options||{},!(0<=(semitones=Math.floor(semitones)||0)&&semitones<=127))throw new RangeError("The semitones value must be between 0 and 127");if(!(0<=(cents=Math.floor(cents)||0)&&cents<=127))throw new RangeError("The cents value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.setRegisteredParameter("pitchbendrange",[semitones,cents],channel,{time:options.time})}),this},Output.prototype.setModulationRange=function(semitones,cents,channel,options){var that=this;if(options=options||{},!(0<=(semitones=Math.floor(semitones)||0)&&semitones<=127))throw new RangeError("The semitones value must be between 0 and 127");if(!(0<=(cents=Math.floor(cents)||0)&&cents<=127))throw new RangeError("The cents value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.setRegisteredParameter("modulationrange",[semitones,cents],channel,{time:options.time})}),this},Output.prototype.setMasterTuning=function(value,channel,options){var that=this;if(options=options||{},(value=parseFloat(value)||0)<=-65||64<=value)throw new RangeError("The value must be a decimal number larger than -65 and smaller than 64.");var coarse=Math.floor(value)+64,fine=value-Math.floor(value),msb=(fine=Math.round((fine+1)/2*16383))>>7&127,lsb=127&fine;return wm.toMIDIChannels(channel).forEach(function(){that.setRegisteredParameter("channelcoarsetuning",coarse,channel,{time:options.time}),that.setRegisteredParameter("channelfinetuning",[msb,lsb],channel,{time:options.time})}),this},Output.prototype.setTuningProgram=function(value,channel,options){var that=this;if(options=options||{},!(0<=(value=Math.floor(value))&&value<=127))throw new RangeError("The program value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.setRegisteredParameter("tuningprogram",value,channel,{time:options.time})}),this},Output.prototype.setTuningBank=function(value,channel,options){var that=this;if(options=options||{},!(0<=(value=Math.floor(value)||0)&&value<=127))throw new RangeError("The bank value must be between 0 and 127");return wm.toMIDIChannels(channel).forEach(function(){that.setRegisteredParameter("tuningbank",value,channel,{time:options.time})}),this},Output.prototype.sendChannelMode=function(command,value,channel,options){if(options=options||{},"string"==typeof command){if(!(command=wm.MIDI_CHANNEL_MODE_MESSAGES[command]))throw new TypeError("Invalid channel mode message name.")}else if(!(120<=(command=Math.floor(command))&&command<=127))throw new RangeError("Channel mode numerical identifiers must be between 120 and 127.");if((value=Math.floor(value)||0)<0||127<value)throw new RangeError("Value must be an integer between 0 and 127.");return wm.toMIDIChannels(channel).forEach(function(ch){this.send((wm.MIDI_CHANNEL_MESSAGES.channelmode<<4)+(ch-1),[command,value],this._parseTimeParameter(options.time))}.bind(this)),this},Output.prototype.sendProgramChange=function(program,channel,options){var that=this;if(options=options||{},program=Math.floor(program),isNaN(program)||program<0||127<program)throw new RangeError("Program numbers must be between 0 and 127.");return wm.toMIDIChannels(channel).forEach(function(ch){that.send((wm.MIDI_CHANNEL_MESSAGES.programchange<<4)+(ch-1),[program],that._parseTimeParameter(options.time))}),this},Output.prototype.sendChannelAftertouch=function(pressure,channel,options){var that=this;options=options||{},pressure=parseFloat(pressure),(isNaN(pressure)||pressure<0||1<pressure)&&(pressure=.5);var nPressure=Math.round(127*pressure);return wm.toMIDIChannels(channel).forEach(function(ch){that.send((wm.MIDI_CHANNEL_MESSAGES.channelaftertouch<<4)+(ch-1),[nPressure],that._parseTimeParameter(options.time))}),this},Output.prototype.sendPitchBend=function(bend,channel,options){var that=this;if(options=options||{},isNaN(bend)||bend<-1||1<bend)throw new RangeError("Pitch bend value must be between -1 and 1.");var nLevel=Math.round((bend+1)/2*16383),msb=nLevel>>7&127,lsb=127&nLevel;return wm.toMIDIChannels(channel).forEach(function(ch){that.send((wm.MIDI_CHANNEL_MESSAGES.pitchbend<<4)+(ch-1),[lsb,msb],that._parseTimeParameter(options.time))}),this},Output.prototype._parseTimeParameter=function(time){var value,parsed=parseFloat(time);return"string"==typeof time&&"+"===time.substring(0,1)?parsed&&0<parsed&&(value=wm.time+parsed):parsed>wm.time&&(value=parsed),value},Output.prototype._convertNoteToArray=function(note){var notes=[];return Array.isArray(note)||(note=[note]),note.forEach(function(item){notes.push(wm.guessNoteNumber(item))}),notes}, true?!(__WEBPACK_AMD_DEFINE_ARRAY__ = [], __WEBPACK_AMD_DEFINE_RESULT__ = (function(){return wm}).apply(exports, __WEBPACK_AMD_DEFINE_ARRAY__),\n\t\t\t\t__WEBPACK_AMD_DEFINE_RESULT__ !== undefined && (module.exports = __WEBPACK_AMD_DEFINE_RESULT__)):undefined}(this);\n\n//# sourceURL=webpack:///./node_modules/webmidi/webmidi.min.js?')},function(module,__webpack_exports__,__webpack_require__){"use strict";eval('/* harmony import */ var _node_modules_css_loader_dist_runtime_api_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(4);\n/* harmony import */ var _node_modules_css_loader_dist_runtime_api_js__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_node_modules_css_loader_dist_runtime_api_js__WEBPACK_IMPORTED_MODULE_0__);\n// Imports\n\nvar ___CSS_LOADER_EXPORT___ = _node_modules_css_loader_dist_runtime_api_js__WEBPACK_IMPORTED_MODULE_0___default()(function(i){return i[1]});\n// Module\n___CSS_LOADER_EXPORT___.push([module.i, "body {\\r\\n    width: 100%;\\r\\n    height: auto;\\r\\n    max-width: 960px;\\r\\n    margin: auto;\\r\\n    margin-top: 80px;\\r\\n    font-family: monospace;\\r\\n    font-size: 2em;\\r\\n}\\r\\n\\r\\n#controls {\\r\\n    margin-left: 10vw;\\r\\n    margin-right: 10vw;\\r\\n}\\r\\n\\r\\n.form-row {\\r\\n    display: flex;\\r\\n    justify-content: flex-end;\\r\\n    padding: .5em;\\r\\n    height: 2.3em;\\r\\n}\\r\\n\\r\\n.form-row > label {\\r\\n    padding: .5em 1em .5em 0;\\r\\n    flex: 1;\\r\\n}\\r\\n.form-row > input, select {\\r\\n    flex: 2;\\r\\n    padding: .5em;\\r\\n    font-size: 1em;\\r\\n    font-family: monospace;\\r\\n}\\r\\n\\r\\n#container {\\r\\n    width: 100%;\\r\\n    display: flex;\\r\\n    justify-content: center;\\r\\n    margin-top: 2em;\\r\\n}\\r\\n\\r\\n#number {\\r\\n    font-size: 3vw;\\r\\n    width: 80vw;\\r\\n}\\r\\n\\r\\n#loading {\\r\\n    position: absolute;\\r\\n    top: 20px;\\r\\n    right: 20px;\\r\\n}\\r\\n\\r\\nbutton {\\r\\n    border: 3px solid black;\\r\\n    display: block;\\r\\n    position: relative;\\r\\n    text-align: center;\\r\\n    width: 30vw;\\r\\n    pointer-events: none;\\r\\n    opacity: 0.5;\\r\\n    margin-left: auto;\\r\\n    margin-right: auto;\\r\\n    background-color: white;\\r\\n    font-family: monospace;\\r\\n    font-size: 1em;\\r\\n}\\r\\n\\r\\nbutton:hover {\\r\\n    background-color: black;\\r\\n    color: white;\\r\\n}\\r\\n\\r\\nbutton.active {\\r\\n    pointer-events: initial;\\r\\n    opacity: 1;\\r\\n}\\r\\n\\r\\nh1 {\\r\\n    text-align: center;\\r\\n}\\r\\n", ""]);\n// Exports\n/* harmony default export */ __webpack_exports__["a"] = (___CSS_LOADER_EXPORT___);\n\n\n//# sourceURL=webpack:///./src/style.css?./node_modules/css-loader/dist/cjs.js')},function(module,exports,__webpack_require__){"use strict";eval("\n\nvar isOldIE = function isOldIE() {\n  var memo;\n  return function memorize() {\n    if (typeof memo === 'undefined') {\n      // Test for IE <= 9 as proposed by Browserhacks\n      // @see http://browserhacks.com/#hack-e71d8692f65334173fee715c222cb805\n      // Tests for existence of standard globals is to allow style-loader\n      // to operate correctly into non-standard environments\n      // @see https://github.com/webpack-contrib/style-loader/issues/177\n      memo = Boolean(window && document && document.all && !window.atob);\n    }\n\n    return memo;\n  };\n}();\n\nvar getTarget = function getTarget() {\n  var memo = {};\n  return function memorize(target) {\n    if (typeof memo[target] === 'undefined') {\n      var styleTarget = document.querySelector(target); // Special case to return head of iframe instead of iframe itself\n\n      if (window.HTMLIFrameElement && styleTarget instanceof window.HTMLIFrameElement) {\n        try {\n          // This will throw an exception if access to iframe is blocked\n          // due to cross-origin restrictions\n          styleTarget = styleTarget.contentDocument.head;\n        } catch (e) {\n          // istanbul ignore next\n          styleTarget = null;\n        }\n      }\n\n      memo[target] = styleTarget;\n    }\n\n    return memo[target];\n  };\n}();\n\nvar stylesInDom = [];\n\nfunction getIndexByIdentifier(identifier) {\n  var result = -1;\n\n  for (var i = 0; i < stylesInDom.length; i++) {\n    if (stylesInDom[i].identifier === identifier) {\n      result = i;\n      break;\n    }\n  }\n\n  return result;\n}\n\nfunction modulesToDom(list, options) {\n  var idCountMap = {};\n  var identifiers = [];\n\n  for (var i = 0; i < list.length; i++) {\n    var item = list[i];\n    var id = options.base ? item[0] + options.base : item[0];\n    var count = idCountMap[id] || 0;\n    var identifier = \"\".concat(id, \" \").concat(count);\n    idCountMap[id] = count + 1;\n    var index = getIndexByIdentifier(identifier);\n    var obj = {\n      css: item[1],\n      media: item[2],\n      sourceMap: item[3]\n    };\n\n    if (index !== -1) {\n      stylesInDom[index].references++;\n      stylesInDom[index].updater(obj);\n    } else {\n      stylesInDom.push({\n        identifier: identifier,\n        updater: addStyle(obj, options),\n        references: 1\n      });\n    }\n\n    identifiers.push(identifier);\n  }\n\n  return identifiers;\n}\n\nfunction insertStyleElement(options) {\n  var style = document.createElement('style');\n  var attributes = options.attributes || {};\n\n  if (typeof attributes.nonce === 'undefined') {\n    var nonce =  true ? __webpack_require__.nc : undefined;\n\n    if (nonce) {\n      attributes.nonce = nonce;\n    }\n  }\n\n  Object.keys(attributes).forEach(function (key) {\n    style.setAttribute(key, attributes[key]);\n  });\n\n  if (typeof options.insert === 'function') {\n    options.insert(style);\n  } else {\n    var target = getTarget(options.insert || 'head');\n\n    if (!target) {\n      throw new Error(\"Couldn't find a style target. This probably means that the value for the 'insert' parameter is invalid.\");\n    }\n\n    target.appendChild(style);\n  }\n\n  return style;\n}\n\nfunction removeStyleElement(style) {\n  // istanbul ignore if\n  if (style.parentNode === null) {\n    return false;\n  }\n\n  style.parentNode.removeChild(style);\n}\n/* istanbul ignore next  */\n\n\nvar replaceText = function replaceText() {\n  var textStore = [];\n  return function replace(index, replacement) {\n    textStore[index] = replacement;\n    return textStore.filter(Boolean).join('\\n');\n  };\n}();\n\nfunction applyToSingletonTag(style, index, remove, obj) {\n  var css = remove ? '' : obj.media ? \"@media \".concat(obj.media, \" {\").concat(obj.css, \"}\") : obj.css; // For old IE\n\n  /* istanbul ignore if  */\n\n  if (style.styleSheet) {\n    style.styleSheet.cssText = replaceText(index, css);\n  } else {\n    var cssNode = document.createTextNode(css);\n    var childNodes = style.childNodes;\n\n    if (childNodes[index]) {\n      style.removeChild(childNodes[index]);\n    }\n\n    if (childNodes.length) {\n      style.insertBefore(cssNode, childNodes[index]);\n    } else {\n      style.appendChild(cssNode);\n    }\n  }\n}\n\nfunction applyToTag(style, options, obj) {\n  var css = obj.css;\n  var media = obj.media;\n  var sourceMap = obj.sourceMap;\n\n  if (media) {\n    style.setAttribute('media', media);\n  } else {\n    style.removeAttribute('media');\n  }\n\n  if (sourceMap && typeof btoa !== 'undefined') {\n    css += \"\\n/*# sourceMappingURL=data:application/json;base64,\".concat(btoa(unescape(encodeURIComponent(JSON.stringify(sourceMap)))), \" */\");\n  } // For old IE\n\n  /* istanbul ignore if  */\n\n\n  if (style.styleSheet) {\n    style.styleSheet.cssText = css;\n  } else {\n    while (style.firstChild) {\n      style.removeChild(style.firstChild);\n    }\n\n    style.appendChild(document.createTextNode(css));\n  }\n}\n\nvar singleton = null;\nvar singletonCounter = 0;\n\nfunction addStyle(obj, options) {\n  var style;\n  var update;\n  var remove;\n\n  if (options.singleton) {\n    var styleIndex = singletonCounter++;\n    style = singleton || (singleton = insertStyleElement(options));\n    update = applyToSingletonTag.bind(null, style, styleIndex, false);\n    remove = applyToSingletonTag.bind(null, style, styleIndex, true);\n  } else {\n    style = insertStyleElement(options);\n    update = applyToTag.bind(null, style, options);\n\n    remove = function remove() {\n      removeStyleElement(style);\n    };\n  }\n\n  update(obj);\n  return function updateStyle(newObj) {\n    if (newObj) {\n      if (newObj.css === obj.css && newObj.media === obj.media && newObj.sourceMap === obj.sourceMap) {\n        return;\n      }\n\n      update(obj = newObj);\n    } else {\n      remove();\n    }\n  };\n}\n\nmodule.exports = function (list, options) {\n  options = options || {}; // Force single-tag solution on IE6-9, which has a hard limit on the # of <style>\n  // tags it will allow on a page\n\n  if (!options.singleton && typeof options.singleton !== 'boolean') {\n    options.singleton = isOldIE();\n  }\n\n  list = list || [];\n  var lastIdentifiers = modulesToDom(list, options);\n  return function update(newList) {\n    newList = newList || [];\n\n    if (Object.prototype.toString.call(newList) !== '[object Array]') {\n      return;\n    }\n\n    for (var i = 0; i < lastIdentifiers.length; i++) {\n      var identifier = lastIdentifiers[i];\n      var index = getIndexByIdentifier(identifier);\n      stylesInDom[index].references--;\n    }\n\n    var newLastIdentifiers = modulesToDom(newList, options);\n\n    for (var _i = 0; _i < lastIdentifiers.length; _i++) {\n      var _identifier = lastIdentifiers[_i];\n\n      var _index = getIndexByIdentifier(_identifier);\n\n      if (stylesInDom[_index].references === 0) {\n        stylesInDom[_index].updater();\n\n        stylesInDom.splice(_index, 1);\n      }\n    }\n\n    lastIdentifiers = newLastIdentifiers;\n  };\n};\n\n//# sourceURL=webpack:///./node_modules/style-loader/dist/runtime/injectStylesIntoStyleTag.js?")},function(module,exports,__webpack_require__){"use strict";eval('\n\n/*\n  MIT License http://www.opensource.org/licenses/mit-license.php\n  Author Tobias Koppers @sokra\n*/\n// css base code, injected by the css-loader\n// eslint-disable-next-line func-names\nmodule.exports = function (cssWithMappingToString) {\n  var list = []; // return the list of modules as css string\n\n  list.toString = function toString() {\n    return this.map(function (item) {\n      var content = cssWithMappingToString(item);\n\n      if (item[2]) {\n        return "@media ".concat(item[2], " {").concat(content, "}");\n      }\n\n      return content;\n    }).join("");\n  }; // import a list of modules into the list\n  // eslint-disable-next-line func-names\n\n\n  list.i = function (modules, mediaQuery, dedupe) {\n    if (typeof modules === "string") {\n      // eslint-disable-next-line no-param-reassign\n      modules = [[null, modules, ""]];\n    }\n\n    var alreadyImportedModules = {};\n\n    if (dedupe) {\n      for (var i = 0; i < this.length; i++) {\n        // eslint-disable-next-line prefer-destructuring\n        var id = this[i][0];\n\n        if (id != null) {\n          alreadyImportedModules[id] = true;\n        }\n      }\n    }\n\n    for (var _i = 0; _i < modules.length; _i++) {\n      var item = [].concat(modules[_i]);\n\n      if (dedupe && alreadyImportedModules[item[0]]) {\n        // eslint-disable-next-line no-continue\n        continue;\n      }\n\n      if (mediaQuery) {\n        if (!item[2]) {\n          item[2] = mediaQuery;\n        } else {\n          item[2] = "".concat(mediaQuery, " and ").concat(item[2]);\n        }\n      }\n\n      list.push(item);\n    }\n  };\n\n  return list;\n};\n\n//# sourceURL=webpack:///./node_modules/css-loader/dist/runtime/api.js?')},function(module,exports,__webpack_require__){"use strict";eval("// Copyright Joyent, Inc. and other Node contributors.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a\n// copy of this software and associated documentation files (the\n// \"Software\"), to deal in the Software without restriction, including\n// without limitation the rights to use, copy, modify, merge, publish,\n// distribute, sublicense, and/or sell copies of the Software, and to permit\n// persons to whom the Software is furnished to do so, subject to the\n// following conditions:\n//\n// The above copyright notice and this permission notice shall be included\n// in all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN\n// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE\n// USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nvar R = typeof Reflect === 'object' ? Reflect : null\nvar ReflectApply = R && typeof R.apply === 'function'\n  ? R.apply\n  : function ReflectApply(target, receiver, args) {\n    return Function.prototype.apply.call(target, receiver, args);\n  }\n\nvar ReflectOwnKeys\nif (R && typeof R.ownKeys === 'function') {\n  ReflectOwnKeys = R.ownKeys\n} else if (Object.getOwnPropertySymbols) {\n  ReflectOwnKeys = function ReflectOwnKeys(target) {\n    return Object.getOwnPropertyNames(target)\n      .concat(Object.getOwnPropertySymbols(target));\n  };\n} else {\n  ReflectOwnKeys = function ReflectOwnKeys(target) {\n    return Object.getOwnPropertyNames(target);\n  };\n}\n\nfunction ProcessEmitWarning(warning) {\n  if (console && console.warn) console.warn(warning);\n}\n\nvar NumberIsNaN = Number.isNaN || function NumberIsNaN(value) {\n  return value !== value;\n}\n\nfunction EventEmitter() {\n  EventEmitter.init.call(this);\n}\nmodule.exports = EventEmitter;\nmodule.exports.once = once;\n\n// Backwards-compat with node 0.10.x\nEventEmitter.EventEmitter = EventEmitter;\n\nEventEmitter.prototype._events = undefined;\nEventEmitter.prototype._eventsCount = 0;\nEventEmitter.prototype._maxListeners = undefined;\n\n// By default EventEmitters will print a warning if more than 10 listeners are\n// added to it. This is a useful default which helps finding memory leaks.\nvar defaultMaxListeners = 10;\n\nfunction checkListener(listener) {\n  if (typeof listener !== 'function') {\n    throw new TypeError('The \"listener\" argument must be of type Function. Received type ' + typeof listener);\n  }\n}\n\nObject.defineProperty(EventEmitter, 'defaultMaxListeners', {\n  enumerable: true,\n  get: function() {\n    return defaultMaxListeners;\n  },\n  set: function(arg) {\n    if (typeof arg !== 'number' || arg < 0 || NumberIsNaN(arg)) {\n      throw new RangeError('The value of \"defaultMaxListeners\" is out of range. It must be a non-negative number. Received ' + arg + '.');\n    }\n    defaultMaxListeners = arg;\n  }\n});\n\nEventEmitter.init = function() {\n\n  if (this._events === undefined ||\n      this._events === Object.getPrototypeOf(this)._events) {\n    this._events = Object.create(null);\n    this._eventsCount = 0;\n  }\n\n  this._maxListeners = this._maxListeners || undefined;\n};\n\n// Obviously not all Emitters should be limited to 10. This function allows\n// that to be increased. Set to zero for unlimited.\nEventEmitter.prototype.setMaxListeners = function setMaxListeners(n) {\n  if (typeof n !== 'number' || n < 0 || NumberIsNaN(n)) {\n    throw new RangeError('The value of \"n\" is out of range. It must be a non-negative number. Received ' + n + '.');\n  }\n  this._maxListeners = n;\n  return this;\n};\n\nfunction _getMaxListeners(that) {\n  if (that._maxListeners === undefined)\n    return EventEmitter.defaultMaxListeners;\n  return that._maxListeners;\n}\n\nEventEmitter.prototype.getMaxListeners = function getMaxListeners() {\n  return _getMaxListeners(this);\n};\n\nEventEmitter.prototype.emit = function emit(type) {\n  var args = [];\n  for (var i = 1; i < arguments.length; i++) args.push(arguments[i]);\n  var doError = (type === 'error');\n\n  var events = this._events;\n  if (events !== undefined)\n    doError = (doError && events.error === undefined);\n  else if (!doError)\n    return false;\n\n  // If there is no 'error' event listener then throw.\n  if (doError) {\n    var er;\n    if (args.length > 0)\n      er = args[0];\n    if (er instanceof Error) {\n      // Note: The comments on the `throw` lines are intentional, they show\n      // up in Node's output if this results in an unhandled exception.\n      throw er; // Unhandled 'error' event\n    }\n    // At least give some kind of context to the user\n    var err = new Error('Unhandled error.' + (er ? ' (' + er.message + ')' : ''));\n    err.context = er;\n    throw err; // Unhandled 'error' event\n  }\n\n  var handler = events[type];\n\n  if (handler === undefined)\n    return false;\n\n  if (typeof handler === 'function') {\n    ReflectApply(handler, this, args);\n  } else {\n    var len = handler.length;\n    var listeners = arrayClone(handler, len);\n    for (var i = 0; i < len; ++i)\n      ReflectApply(listeners[i], this, args);\n  }\n\n  return true;\n};\n\nfunction _addListener(target, type, listener, prepend) {\n  var m;\n  var events;\n  var existing;\n\n  checkListener(listener);\n\n  events = target._events;\n  if (events === undefined) {\n    events = target._events = Object.create(null);\n    target._eventsCount = 0;\n  } else {\n    // To avoid recursion in the case that type === \"newListener\"! Before\n    // adding it to the listeners, first emit \"newListener\".\n    if (events.newListener !== undefined) {\n      target.emit('newListener', type,\n                  listener.listener ? listener.listener : listener);\n\n      // Re-assign `events` because a newListener handler could have caused the\n      // this._events to be assigned to a new object\n      events = target._events;\n    }\n    existing = events[type];\n  }\n\n  if (existing === undefined) {\n    // Optimize the case of one listener. Don't need the extra array object.\n    existing = events[type] = listener;\n    ++target._eventsCount;\n  } else {\n    if (typeof existing === 'function') {\n      // Adding the second element, need to change to array.\n      existing = events[type] =\n        prepend ? [listener, existing] : [existing, listener];\n      // If we've already got an array, just append.\n    } else if (prepend) {\n      existing.unshift(listener);\n    } else {\n      existing.push(listener);\n    }\n\n    // Check for listener leak\n    m = _getMaxListeners(target);\n    if (m > 0 && existing.length > m && !existing.warned) {\n      existing.warned = true;\n      // No error code for this since it is a Warning\n      // eslint-disable-next-line no-restricted-syntax\n      var w = new Error('Possible EventEmitter memory leak detected. ' +\n                          existing.length + ' ' + String(type) + ' listeners ' +\n                          'added. Use emitter.setMaxListeners() to ' +\n                          'increase limit');\n      w.name = 'MaxListenersExceededWarning';\n      w.emitter = target;\n      w.type = type;\n      w.count = existing.length;\n      ProcessEmitWarning(w);\n    }\n  }\n\n  return target;\n}\n\nEventEmitter.prototype.addListener = function addListener(type, listener) {\n  return _addListener(this, type, listener, false);\n};\n\nEventEmitter.prototype.on = EventEmitter.prototype.addListener;\n\nEventEmitter.prototype.prependListener =\n    function prependListener(type, listener) {\n      return _addListener(this, type, listener, true);\n    };\n\nfunction onceWrapper() {\n  if (!this.fired) {\n    this.target.removeListener(this.type, this.wrapFn);\n    this.fired = true;\n    if (arguments.length === 0)\n      return this.listener.call(this.target);\n    return this.listener.apply(this.target, arguments);\n  }\n}\n\nfunction _onceWrap(target, type, listener) {\n  var state = { fired: false, wrapFn: undefined, target: target, type: type, listener: listener };\n  var wrapped = onceWrapper.bind(state);\n  wrapped.listener = listener;\n  state.wrapFn = wrapped;\n  return wrapped;\n}\n\nEventEmitter.prototype.once = function once(type, listener) {\n  checkListener(listener);\n  this.on(type, _onceWrap(this, type, listener));\n  return this;\n};\n\nEventEmitter.prototype.prependOnceListener =\n    function prependOnceListener(type, listener) {\n      checkListener(listener);\n      this.prependListener(type, _onceWrap(this, type, listener));\n      return this;\n    };\n\n// Emits a 'removeListener' event if and only if the listener was removed.\nEventEmitter.prototype.removeListener =\n    function removeListener(type, listener) {\n      var list, events, position, i, originalListener;\n\n      checkListener(listener);\n\n      events = this._events;\n      if (events === undefined)\n        return this;\n\n      list = events[type];\n      if (list === undefined)\n        return this;\n\n      if (list === listener || list.listener === listener) {\n        if (--this._eventsCount === 0)\n          this._events = Object.create(null);\n        else {\n          delete events[type];\n          if (events.removeListener)\n            this.emit('removeListener', type, list.listener || listener);\n        }\n      } else if (typeof list !== 'function') {\n        position = -1;\n\n        for (i = list.length - 1; i >= 0; i--) {\n          if (list[i] === listener || list[i].listener === listener) {\n            originalListener = list[i].listener;\n            position = i;\n            break;\n          }\n        }\n\n        if (position < 0)\n          return this;\n\n        if (position === 0)\n          list.shift();\n        else {\n          spliceOne(list, position);\n        }\n\n        if (list.length === 1)\n          events[type] = list[0];\n\n        if (events.removeListener !== undefined)\n          this.emit('removeListener', type, originalListener || listener);\n      }\n\n      return this;\n    };\n\nEventEmitter.prototype.off = EventEmitter.prototype.removeListener;\n\nEventEmitter.prototype.removeAllListeners =\n    function removeAllListeners(type) {\n      var listeners, events, i;\n\n      events = this._events;\n      if (events === undefined)\n        return this;\n\n      // not listening for removeListener, no need to emit\n      if (events.removeListener === undefined) {\n        if (arguments.length === 0) {\n          this._events = Object.create(null);\n          this._eventsCount = 0;\n        } else if (events[type] !== undefined) {\n          if (--this._eventsCount === 0)\n            this._events = Object.create(null);\n          else\n            delete events[type];\n        }\n        return this;\n      }\n\n      // emit removeListener for all listeners on all events\n      if (arguments.length === 0) {\n        var keys = Object.keys(events);\n        var key;\n        for (i = 0; i < keys.length; ++i) {\n          key = keys[i];\n          if (key === 'removeListener') continue;\n          this.removeAllListeners(key);\n        }\n        this.removeAllListeners('removeListener');\n        this._events = Object.create(null);\n        this._eventsCount = 0;\n        return this;\n      }\n\n      listeners = events[type];\n\n      if (typeof listeners === 'function') {\n        this.removeListener(type, listeners);\n      } else if (listeners !== undefined) {\n        // LIFO order\n        for (i = listeners.length - 1; i >= 0; i--) {\n          this.removeListener(type, listeners[i]);\n        }\n      }\n\n      return this;\n    };\n\nfunction _listeners(target, type, unwrap) {\n  var events = target._events;\n\n  if (events === undefined)\n    return [];\n\n  var evlistener = events[type];\n  if (evlistener === undefined)\n    return [];\n\n  if (typeof evlistener === 'function')\n    return unwrap ? [evlistener.listener || evlistener] : [evlistener];\n\n  return unwrap ?\n    unwrapListeners(evlistener) : arrayClone(evlistener, evlistener.length);\n}\n\nEventEmitter.prototype.listeners = function listeners(type) {\n  return _listeners(this, type, true);\n};\n\nEventEmitter.prototype.rawListeners = function rawListeners(type) {\n  return _listeners(this, type, false);\n};\n\nEventEmitter.listenerCount = function(emitter, type) {\n  if (typeof emitter.listenerCount === 'function') {\n    return emitter.listenerCount(type);\n  } else {\n    return listenerCount.call(emitter, type);\n  }\n};\n\nEventEmitter.prototype.listenerCount = listenerCount;\nfunction listenerCount(type) {\n  var events = this._events;\n\n  if (events !== undefined) {\n    var evlistener = events[type];\n\n    if (typeof evlistener === 'function') {\n      return 1;\n    } else if (evlistener !== undefined) {\n      return evlistener.length;\n    }\n  }\n\n  return 0;\n}\n\nEventEmitter.prototype.eventNames = function eventNames() {\n  return this._eventsCount > 0 ? ReflectOwnKeys(this._events) : [];\n};\n\nfunction arrayClone(arr, n) {\n  var copy = new Array(n);\n  for (var i = 0; i < n; ++i)\n    copy[i] = arr[i];\n  return copy;\n}\n\nfunction spliceOne(list, index) {\n  for (; index + 1 < list.length; index++)\n    list[index] = list[index + 1];\n  list.pop();\n}\n\nfunction unwrapListeners(arr) {\n  var ret = new Array(arr.length);\n  for (var i = 0; i < ret.length; ++i) {\n    ret[i] = arr[i].listener || arr[i];\n  }\n  return ret;\n}\n\nfunction once(emitter, name) {\n  return new Promise(function (resolve, reject) {\n    function errorListener(err) {\n      emitter.removeListener(name, resolver);\n      reject(err);\n    }\n\n    function resolver() {\n      if (typeof emitter.removeListener === 'function') {\n        emitter.removeListener('error', errorListener);\n      }\n      resolve([].slice.call(arguments));\n    };\n\n    eventTargetAgnosticAddListener(emitter, name, resolver, { once: true });\n    if (name !== 'error') {\n      addErrorHandlerIfEventEmitter(emitter, errorListener, { once: true });\n    }\n  });\n}\n\nfunction addErrorHandlerIfEventEmitter(emitter, handler, flags) {\n  if (typeof emitter.on === 'function') {\n    eventTargetAgnosticAddListener(emitter, 'error', handler, flags);\n  }\n}\n\nfunction eventTargetAgnosticAddListener(emitter, name, listener, flags) {\n  if (typeof emitter.on === 'function') {\n    if (flags.once) {\n      emitter.once(name, listener);\n    } else {\n      emitter.on(name, listener);\n    }\n  } else if (typeof emitter.addEventListener === 'function') {\n    // EventTarget does not have `error` event semantics like Node\n    // EventEmitters, we do not listen for `error` events here.\n    emitter.addEventListener(name, function wrapListener(arg) {\n      // IE does not have builtin `{ once: true }` support so we\n      // have to do it manually.\n      if (flags.once) {\n        emitter.removeEventListener(name, wrapListener);\n      }\n      listener(arg);\n    });\n  } else {\n    throw new TypeError('The \"emitter\" argument must be of type EventEmitter. Received type ' + typeof emitter);\n  }\n}\n\n\n//# sourceURL=webpack:///./node_modules/events/events.js?")},function(module,exports){eval("if ('serviceWorker' in navigator) {\r\n    window.addEventListener('load', () => {\r\n        navigator.serviceWorker.register('/pi-ano/service-worker.js').then(registration => {\r\n            console.log('SW registered: ', registration);\r\n        }).catch(registrationError => {\r\n            console.log('SW registration failed: ', registrationError);\r\n        });\r\n    });\r\n}\r\n\n\n//# sourceURL=webpack:///./src/register-sw.js?")},function(module,exports,__webpack_require__){eval('var arrayWithHoles = __webpack_require__(8);\n\nvar iterableToArrayLimit = __webpack_require__(9);\n\nvar unsupportedIterableToArray = __webpack_require__(10);\n\nvar nonIterableRest = __webpack_require__(12);\n\nfunction _slicedToArray(arr, i) {\n  return arrayWithHoles(arr) || iterableToArrayLimit(arr, i) || unsupportedIterableToArray(arr, i) || nonIterableRest();\n}\n\nmodule.exports = _slicedToArray;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/slicedToArray.js?')},function(module,exports){eval('function _arrayWithHoles(arr) {\n  if (Array.isArray(arr)) return arr;\n}\n\nmodule.exports = _arrayWithHoles;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/arrayWithHoles.js?')},function(module,exports){eval('function _iterableToArrayLimit(arr, i) {\n  if (typeof Symbol === "undefined" || !(Symbol.iterator in Object(arr))) return;\n  var _arr = [];\n  var _n = true;\n  var _d = false;\n  var _e = undefined;\n\n  try {\n    for (var _i = arr[Symbol.iterator](), _s; !(_n = (_s = _i.next()).done); _n = true) {\n      _arr.push(_s.value);\n\n      if (i && _arr.length === i) break;\n    }\n  } catch (err) {\n    _d = true;\n    _e = err;\n  } finally {\n    try {\n      if (!_n && _i["return"] != null) _i["return"]();\n    } finally {\n      if (_d) throw _e;\n    }\n  }\n\n  return _arr;\n}\n\nmodule.exports = _iterableToArrayLimit;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/iterableToArrayLimit.js?')},function(module,exports,__webpack_require__){eval('var arrayLikeToArray = __webpack_require__(11);\n\nfunction _unsupportedIterableToArray(o, minLen) {\n  if (!o) return;\n  if (typeof o === "string") return arrayLikeToArray(o, minLen);\n  var n = Object.prototype.toString.call(o).slice(8, -1);\n  if (n === "Object" && o.constructor) n = o.constructor.name;\n  if (n === "Map" || n === "Set") return Array.from(o);\n  if (n === "Arguments" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return arrayLikeToArray(o, minLen);\n}\n\nmodule.exports = _unsupportedIterableToArray;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/unsupportedIterableToArray.js?')},function(module,exports){eval('function _arrayLikeToArray(arr, len) {\n  if (len == null || len > arr.length) len = arr.length;\n\n  for (var i = 0, arr2 = new Array(len); i < len; i++) {\n    arr2[i] = arr[i];\n  }\n\n  return arr2;\n}\n\nmodule.exports = _arrayLikeToArray;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/arrayLikeToArray.js?')},function(module,exports){eval('function _nonIterableRest() {\n  throw new TypeError("Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.");\n}\n\nmodule.exports = _nonIterableRest;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/nonIterableRest.js?')},function(module,exports){eval('function _classCallCheck(instance, Constructor) {\n  if (!(instance instanceof Constructor)) {\n    throw new TypeError("Cannot call a class as a function");\n  }\n}\n\nmodule.exports = _classCallCheck;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/classCallCheck.js?')},function(module,exports){eval('function _defineProperties(target, props) {\n  for (var i = 0; i < props.length; i++) {\n    var descriptor = props[i];\n    descriptor.enumerable = descriptor.enumerable || false;\n    descriptor.configurable = true;\n    if ("value" in descriptor) descriptor.writable = true;\n    Object.defineProperty(target, descriptor.key, descriptor);\n  }\n}\n\nfunction _createClass(Constructor, protoProps, staticProps) {\n  if (protoProps) _defineProperties(Constructor.prototype, protoProps);\n  if (staticProps) _defineProperties(Constructor, staticProps);\n  return Constructor;\n}\n\nmodule.exports = _createClass;\nmodule.exports["default"] = module.exports, module.exports.__esModule = true;\n\n//# sourceURL=webpack:///./node_modules/@babel/runtime/helpers/createClass.js?')},function(module,__webpack_exports__,__webpack_require__){"use strict";eval('// ESM COMPAT FLAG\n__webpack_require__.r(__webpack_exports__);\n\n// NAMESPACE OBJECT: ./node_modules/@tonaljs/core/dist/index.es.js\nvar index_es_namespaceObject = {};\n__webpack_require__.r(index_es_namespaceObject);\n__webpack_require__.d(index_es_namespaceObject, "accToAlt", function() { return accToAlt; });\n__webpack_require__.d(index_es_namespaceObject, "altToAcc", function() { return altToAcc; });\n__webpack_require__.d(index_es_namespaceObject, "coordToInterval", function() { return coordToInterval; });\n__webpack_require__.d(index_es_namespaceObject, "coordToNote", function() { return coordToNote; });\n__webpack_require__.d(index_es_namespaceObject, "decode", function() { return decode; });\n__webpack_require__.d(index_es_namespaceObject, "deprecate", function() { return deprecate; });\n__webpack_require__.d(index_es_namespaceObject, "distance", function() { return distance; });\n__webpack_require__.d(index_es_namespaceObject, "encode", function() { return encode; });\n__webpack_require__.d(index_es_namespaceObject, "fillStr", function() { return fillStr; });\n__webpack_require__.d(index_es_namespaceObject, "interval", function() { return index_es_interval; });\n__webpack_require__.d(index_es_namespaceObject, "isNamed", function() { return isNamed; });\n__webpack_require__.d(index_es_namespaceObject, "isPitch", function() { return isPitch; });\n__webpack_require__.d(index_es_namespaceObject, "note", function() { return index_es_note; });\n__webpack_require__.d(index_es_namespaceObject, "stepToLetter", function() { return stepToLetter; });\n__webpack_require__.d(index_es_namespaceObject, "tokenizeInterval", function() { return tokenizeInterval; });\n__webpack_require__.d(index_es_namespaceObject, "tokenizeNote", function() { return tokenizeNote; });\n__webpack_require__.d(index_es_namespaceObject, "transpose", function() { return transpose; });\n\n// EXTERNAL MODULE: ./node_modules/style-loader/dist/runtime/injectStylesIntoStyleTag.js\nvar injectStylesIntoStyleTag = __webpack_require__(3);\nvar injectStylesIntoStyleTag_default = /*#__PURE__*/__webpack_require__.n(injectStylesIntoStyleTag);\n\n// EXTERNAL MODULE: ./node_modules/css-loader/dist/cjs.js!./src/style.css\nvar style = __webpack_require__(2);\n\n// CONCATENATED MODULE: ./src/style.css\n\n            \n\nvar style_options = {};\n\nstyle_options.insert = "head";\nstyle_options.singleton = false;\n\nvar update = injectStylesIntoStyleTag_default()(style["a" /* default */], style_options);\n\n\n\n/* harmony default export */ var src_style = (style["a" /* default */].locals || {});\n// EXTERNAL MODULE: ./src/register-sw.js\nvar register_sw = __webpack_require__(6);\n\n// CONCATENATED MODULE: ./src/pi.js\nfunction getPiDigits(factor) {\r\n    let i = 1n;\r\n    let x = 3n * (10n ** factor);\r\n    let pi = x;\r\n    while (x > 0) {\r\n        x = x * i / ((i + 1n) * 4n);\r\n        pi += x / (i + 2n);\r\n        i += 2n;\r\n    }\r\n    return pi / (10n ** 20n);\r\n}\r\n\r\nconst getPiDigits10k = () => getPiDigits(10020n);\r\n\r\n// [3, 1, 4, 1, 5, 9, 2, 6, ...]\r\nfunction getPiDigitArray() {\r\n    return (\'\' + getPiDigits10k()).split(\'\');\r\n}\r\n\n// CONCATENATED MODULE: ./src/controls-view.js\nclass ControlsView {\r\n\r\n    constructor(\r\n        scalesByLength,\r\n        noteDurations,\r\n        pianoNotes)\r\n    {\r\n        this.button = document.querySelector(\'button\');\r\n        this.scaleSelect = document.querySelector(\'#scale\');\r\n        this.bpmRange = document.querySelector(\'#bpm\');\r\n        this.bpmNumber = document.querySelector(\'#bpm-number\');\r\n        this.tonicSelect = document.querySelector(\'#tonic\');\r\n        this.noteDurationSelect = document.querySelector(\'#note-duration\');\r\n        this.loadingMessage = document.querySelector(\'#loading\');\r\n\r\n        this.initControls(scalesByLength, noteDurations, pianoNotes);\r\n    }\r\n\r\n    initControls(scalesByLength, noteDurations, pianoNotes) {\r\n        this.bpmRange.addEventListener(\'input\', () => {\r\n            this.bpmNumber.value = this.bpmRange.value;\r\n        });\r\n        this.bpmNumber.addEventListener(\'input\', () => {\r\n            this.bpmRange.value = this.bpmNumber.value;\r\n        });\r\n\r\n        this.initScales(scalesByLength);\r\n\r\n        this.initTonic(pianoNotes);\r\n\r\n        this.initNoteDurations(noteDurations);\r\n    }\r\n\r\n    initScales(scalesByLength) {\r\n        scalesByLength.forEach(([length, scales]) => {\r\n            const scaleOptionGroup = document.createElement(\'optgroup\');\r\n            scaleOptionGroup.label = `${length} note scales`;\r\n            scales\r\n                .sort((s1, s2) => s1.localeCompare(s2))\r\n                .forEach(scaleName => {\r\n                    const scaleOption = document.createElement(\'option\');\r\n                    scaleOption.innerText = scaleName;\r\n                    scaleOption.value = scaleName;\r\n                    if (scaleName === \'major\') {\r\n                        scaleOption.selected = true;\r\n                    }\r\n                    scaleOptionGroup.appendChild(scaleOption);\r\n                });\r\n\r\n            this.scaleSelect.appendChild(scaleOptionGroup);\r\n        });\r\n    }\r\n\r\n    initTonic(pianoNotes) {\r\n        for (let i = 0; i < pianoNotes.length; i++) {\r\n            const tonicOption = document.createElement(\'option\');\r\n            tonicOption.value = i;\r\n            tonicOption.innerText = pianoNotes[i];\r\n            if (pianoNotes[i] === \'C4\') {\r\n                tonicOption.selected = true;\r\n            }\r\n            this.tonicSelect.appendChild(tonicOption);\r\n        }\r\n    }\r\n\r\n    initNoteDurations(noteDurations) {\r\n        for (let i = 0; i <= 5; i++) {\r\n            const noteDurationOption = document.createElement(\'option\');\r\n            noteDurationOption.value = noteDurations[i].duration;\r\n            const text = noteDurations[i].name;\r\n            noteDurationOption.innerText = text;\r\n            if (text.startsWith(\'whole\')) {\r\n                noteDurationOption.selected = true;\r\n            }\r\n            this.noteDurationSelect.appendChild(noteDurationOption);\r\n        }\r\n    }\r\n\r\n    getScale() {\r\n        return this.scaleSelect.value;\r\n    }\r\n\r\n    getBpm() {\r\n        return this.bpmRange.value;\r\n    }\r\n\r\n    getTonic() {\r\n        return this.tonicSelect.value;\r\n    }\r\n\r\n    getNoteDuration() {\r\n        return this.noteDurationSelect.value;\r\n    }\r\n\r\n    activateButton() {\r\n        this.button.classList.add(\'active\');\r\n    }\r\n\r\n    setButtonLabel(text) {\r\n        this.button.textContent = text;\r\n    }\r\n\r\n    addButtonClickListener(listener) {\r\n        this.button.addEventListener(\'click\', listener);\r\n    }\r\n\r\n    setLoadingComplete() {\r\n        this.loadingMessage.remove();\r\n    }\r\n}\r\n\n// CONCATENATED MODULE: ./src/visualization.js\nclass Visualization {\r\n\r\n    constructor()\r\n    {\r\n        this.display = document.querySelector(\'#number\');\r\n    }\r\n\r\n    append(text) {\r\n        this.display.innerHTML += text;\r\n    }\r\n\r\n    clear() {\r\n        this.display.innerHTML = \'\';\r\n    }\r\n\r\n    space() {\r\n        this.append(\'&nbsp;\');\r\n    }\r\n\r\n    newline() {\r\n        this.append(\'<br>&nbsp;&nbsp;\');\r\n    }\r\n}\r\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/core/dist/index.es.js\n/**\r\n * Fill a string with a repeated character\r\n *\r\n * @param character\r\n * @param repetition\r\n */\r\nconst fillStr = (s, n) => Array(Math.abs(n) + 1).join(s);\r\nfunction deprecate(original, alternative, fn) {\r\n    return function (...args) {\r\n        // tslint:disable-next-line\r\n        console.warn(`${original} is deprecated. Use ${alternative}.`);\r\n        return fn.apply(this, args);\r\n    };\r\n}\n\nfunction isNamed(src) {\r\n    return src !== null && typeof src === "object" && typeof src.name === "string"\r\n        ? true\r\n        : false;\r\n}\n\nfunction isPitch(pitch) {\r\n    return pitch !== null &&\r\n        typeof pitch === "object" &&\r\n        typeof pitch.step === "number" &&\r\n        typeof pitch.alt === "number"\r\n        ? true\r\n        : false;\r\n}\r\n// The number of fifths of [C, D, E, F, G, A, B]\r\nconst FIFTHS = [0, 2, 4, -1, 1, 3, 5];\r\n// The number of octaves it span each step\r\nconst STEPS_TO_OCTS = FIFTHS.map((fifths) => Math.floor((fifths * 7) / 12));\r\nfunction encode(pitch) {\r\n    const { step, alt, oct, dir = 1 } = pitch;\r\n    const f = FIFTHS[step] + 7 * alt;\r\n    if (oct === undefined) {\r\n        return [dir * f];\r\n    }\r\n    const o = oct - STEPS_TO_OCTS[step] - 4 * alt;\r\n    return [dir * f, dir * o];\r\n}\r\n// We need to get the steps from fifths\r\n// Fifths for CDEFGAB are [ 0, 2, 4, -1, 1, 3, 5 ]\r\n// We add 1 to fifths to avoid negative numbers, so:\r\n// for ["F", "C", "G", "D", "A", "E", "B"] we have:\r\nconst FIFTHS_TO_STEPS = [3, 0, 4, 1, 5, 2, 6];\r\nfunction decode(coord) {\r\n    const [f, o, dir] = coord;\r\n    const step = FIFTHS_TO_STEPS[unaltered(f)];\r\n    const alt = Math.floor((f + 1) / 7);\r\n    if (o === undefined) {\r\n        return { step, alt, dir };\r\n    }\r\n    const oct = o + 4 * alt + STEPS_TO_OCTS[step];\r\n    return { step, alt, oct, dir };\r\n}\r\n// Return the number of fifths as if it were unaltered\r\nfunction unaltered(f) {\r\n    const i = (f + 1) % 7;\r\n    return i < 0 ? 7 + i : i;\r\n}\n\nconst NoNote = { empty: true, name: "", pc: "", acc: "" };\r\nconst index_es_cache = new Map();\r\nconst stepToLetter = (step) => "CDEFGAB".charAt(step);\r\nconst altToAcc = (alt) => alt < 0 ? fillStr("b", -alt) : fillStr("#", alt);\r\nconst accToAlt = (acc) => acc[0] === "b" ? -acc.length : acc.length;\r\n/**\r\n * Given a note literal (a note name or a note object), returns the Note object\r\n * @example\r\n * note(\'Bb4\') // => { name: "Bb4", midi: 70, chroma: 10, ... }\r\n */\r\nfunction index_es_note(src) {\r\n    const cached = index_es_cache.get(src);\r\n    if (cached) {\r\n        return cached;\r\n    }\r\n    const value = typeof src === "string"\r\n        ? parse(src)\r\n        : isPitch(src)\r\n            ? index_es_note(pitchName(src))\r\n            : isNamed(src)\r\n                ? index_es_note(src.name)\r\n                : NoNote;\r\n    index_es_cache.set(src, value);\r\n    return value;\r\n}\r\nconst REGEX = /^([a-gA-G]?)(#{1,}|b{1,}|x{1,}|)(-?\\d*)\\s*(.*)$/;\r\n/**\r\n * @private\r\n */\r\nfunction tokenizeNote(str) {\r\n    const m = REGEX.exec(str);\r\n    return [m[1].toUpperCase(), m[2].replace(/x/g, "##"), m[3], m[4]];\r\n}\r\n/**\r\n * @private\r\n */\r\nfunction coordToNote(noteCoord) {\r\n    return index_es_note(decode(noteCoord));\r\n}\r\nconst index_es_mod = (n, m) => ((n % m) + m) % m;\r\nconst SEMI = [0, 2, 4, 5, 7, 9, 11];\r\nfunction parse(noteName) {\r\n    const tokens = tokenizeNote(noteName);\r\n    if (tokens[0] === "" || tokens[3] !== "") {\r\n        return NoNote;\r\n    }\r\n    const letter = tokens[0];\r\n    const acc = tokens[1];\r\n    const octStr = tokens[2];\r\n    const step = (letter.charCodeAt(0) + 3) % 7;\r\n    const alt = accToAlt(acc);\r\n    const oct = octStr.length ? +octStr : undefined;\r\n    const coord = encode({ step, alt, oct });\r\n    const name = letter + acc + octStr;\r\n    const pc = letter + acc;\r\n    const chroma = (SEMI[step] + alt + 120) % 12;\r\n    const height = oct === undefined\r\n        ? index_es_mod(SEMI[step] + alt, 12) - 12 * 99\r\n        : SEMI[step] + alt + 12 * (oct + 1);\r\n    const midi = height >= 0 && height <= 127 ? height : null;\r\n    const freq = oct === undefined ? null : Math.pow(2, (height - 69) / 12) * 440;\r\n    return {\r\n        empty: false,\r\n        acc,\r\n        alt,\r\n        chroma,\r\n        coord,\r\n        freq,\r\n        height,\r\n        letter,\r\n        midi,\r\n        name,\r\n        oct,\r\n        pc,\r\n        step,\r\n    };\r\n}\r\nfunction pitchName(props) {\r\n    const { step, alt, oct } = props;\r\n    const letter = stepToLetter(step);\r\n    if (!letter) {\r\n        return "";\r\n    }\r\n    const pc = letter + altToAcc(alt);\r\n    return oct || oct === 0 ? pc + oct : pc;\r\n}\n\nconst NoInterval = { empty: true, name: "", acc: "" };\r\n// shorthand tonal notation (with quality after number)\r\nconst INTERVAL_TONAL_REGEX = "([-+]?\\\\d+)(d{1,4}|m|M|P|A{1,4})";\r\n// standard shorthand notation (with quality before number)\r\nconst INTERVAL_SHORTHAND_REGEX = "(AA|A|P|M|m|d|dd)([-+]?\\\\d+)";\r\nconst REGEX$1 = new RegExp("^" + INTERVAL_TONAL_REGEX + "|" + INTERVAL_SHORTHAND_REGEX + "$");\r\n/**\r\n * @private\r\n */\r\nfunction tokenizeInterval(str) {\r\n    const m = REGEX$1.exec(`${str}`);\r\n    if (m === null) {\r\n        return ["", ""];\r\n    }\r\n    return m[1] ? [m[1], m[2]] : [m[4], m[3]];\r\n}\r\nconst cache$1 = {};\r\n/**\r\n * Get interval properties. It returns an object with:\r\n *\r\n * - name: the interval name\r\n * - num: the interval number\r\n * - type: \'perfectable\' or \'majorable\'\r\n * - q: the interval quality (d, m, M, A)\r\n * - dir: interval direction (1 ascending, -1 descending)\r\n * - simple: the simplified number\r\n * - semitones: the size in semitones\r\n * - chroma: the interval chroma\r\n *\r\n * @param {string} interval - the interval name\r\n * @return {Object} the interval properties\r\n *\r\n * @example\r\n * import { interval } from \'@tonaljs/core\'\r\n * interval(\'P5\').semitones // => 7\r\n * interval(\'m3\').type // => \'majorable\'\r\n */\r\nfunction index_es_interval(src) {\r\n    return typeof src === "string"\r\n        ? cache$1[src] || (cache$1[src] = parse$1(src))\r\n        : isPitch(src)\r\n            ? index_es_interval(pitchName$1(src))\r\n            : isNamed(src)\r\n                ? index_es_interval(src.name)\r\n                : NoInterval;\r\n}\r\nconst SIZES = [0, 2, 4, 5, 7, 9, 11];\r\nconst TYPES = "PMMPPMM";\r\nfunction parse$1(str) {\r\n    const tokens = tokenizeInterval(str);\r\n    if (tokens[0] === "") {\r\n        return NoInterval;\r\n    }\r\n    const num = +tokens[0];\r\n    const q = tokens[1];\r\n    const step = (Math.abs(num) - 1) % 7;\r\n    const t = TYPES[step];\r\n    if (t === "M" && q === "P") {\r\n        return NoInterval;\r\n    }\r\n    const type = t === "M" ? "majorable" : "perfectable";\r\n    const name = "" + num + q;\r\n    const dir = num < 0 ? -1 : 1;\r\n    const simple = num === 8 || num === -8 ? num : dir * (step + 1);\r\n    const alt = qToAlt(type, q);\r\n    const oct = Math.floor((Math.abs(num) - 1) / 7);\r\n    const semitones = dir * (SIZES[step] + alt + 12 * oct);\r\n    const chroma = (((dir * (SIZES[step] + alt)) % 12) + 12) % 12;\r\n    const coord = encode({ step, alt, oct, dir });\r\n    return {\r\n        empty: false,\r\n        name,\r\n        num,\r\n        q,\r\n        step,\r\n        alt,\r\n        dir,\r\n        type,\r\n        simple,\r\n        semitones,\r\n        chroma,\r\n        coord,\r\n        oct,\r\n    };\r\n}\r\n/**\r\n * @private\r\n */\r\nfunction coordToInterval(coord) {\r\n    const [f, o = 0] = coord;\r\n    const isDescending = f * 7 + o * 12 < 0;\r\n    const ivl = isDescending ? [-f, -o, -1] : [f, o, 1];\r\n    return index_es_interval(decode(ivl));\r\n}\r\nfunction qToAlt(type, q) {\r\n    return (q === "M" && type === "majorable") ||\r\n        (q === "P" && type === "perfectable")\r\n        ? 0\r\n        : q === "m" && type === "majorable"\r\n            ? -1\r\n            : /^A+$/.test(q)\r\n                ? q.length\r\n                : /^d+$/.test(q)\r\n                    ? -1 * (type === "perfectable" ? q.length : q.length + 1)\r\n                    : 0;\r\n}\r\n// return the interval name of a pitch\r\nfunction pitchName$1(props) {\r\n    const { step, alt, oct = 0, dir } = props;\r\n    if (!dir) {\r\n        return "";\r\n    }\r\n    const num = step + 1 + 7 * oct;\r\n    const d = dir < 0 ? "-" : "";\r\n    const type = TYPES[step] === "M" ? "majorable" : "perfectable";\r\n    const name = d + num + altToQ(type, alt);\r\n    return name;\r\n}\r\nfunction altToQ(type, alt) {\r\n    if (alt === 0) {\r\n        return type === "majorable" ? "M" : "P";\r\n    }\r\n    else if (alt === -1 && type === "majorable") {\r\n        return "m";\r\n    }\r\n    else if (alt > 0) {\r\n        return fillStr("A", alt);\r\n    }\r\n    else {\r\n        return fillStr("d", type === "perfectable" ? alt : alt + 1);\r\n    }\r\n}\n\n/**\r\n * Transpose a note by an interval.\r\n *\r\n * @param {string} note - the note or note name\r\n * @param {string} interval - the interval or interval name\r\n * @return {string} the transposed note name or empty string if not valid notes\r\n * @example\r\n * import { tranpose } from "@tonaljs/core"\r\n * transpose("d3", "3M") // => "F#3"\r\n * transpose("D", "3M") // => "F#"\r\n * ["C", "D", "E", "F", "G"].map(pc => transpose(pc, "M3)) // => ["E", "F#", "G#", "A", "B"]\r\n */\r\nfunction transpose(noteName, intervalName) {\r\n    const note$1 = index_es_note(noteName);\r\n    const interval$1 = index_es_interval(intervalName);\r\n    if (note$1.empty || interval$1.empty) {\r\n        return "";\r\n    }\r\n    const noteCoord = note$1.coord;\r\n    const intervalCoord = interval$1.coord;\r\n    const tr = noteCoord.length === 1\r\n        ? [noteCoord[0] + intervalCoord[0]]\r\n        : [noteCoord[0] + intervalCoord[0], noteCoord[1] + intervalCoord[1]];\r\n    return coordToNote(tr).name;\r\n}\r\n/**\r\n * Find the interval distance between two notes or coord classes.\r\n *\r\n * To find distance between coord classes, both notes must be coord classes and\r\n * the interval is always ascending\r\n *\r\n * @param {Note|string} from - the note or note name to calculate distance from\r\n * @param {Note|string} to - the note or note name to calculate distance to\r\n * @return {string} the interval name or empty string if not valid notes\r\n *\r\n */\r\nfunction distance(fromNote, toNote) {\r\n    const from = index_es_note(fromNote);\r\n    const to = index_es_note(toNote);\r\n    if (from.empty || to.empty) {\r\n        return "";\r\n    }\r\n    const fcoord = from.coord;\r\n    const tcoord = to.coord;\r\n    const fifths = tcoord[0] - fcoord[0];\r\n    const octs = fcoord.length === 2 && tcoord.length === 2\r\n        ? tcoord[1] - fcoord[1]\r\n        : -Math.floor((fifths * 7) / 12);\r\n    return coordToInterval([fifths, octs]).name;\r\n}\n\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/abc-notation/dist/index.es.js\n\n\nconst index_es_fillStr = (character, times) => Array(times + 1).join(character);\r\nconst index_es_REGEX = /^(_{1,}|=|\\^{1,}|)([abcdefgABCDEFG])([,\']*)$/;\r\nfunction tokenize(str) {\r\n    const m = index_es_REGEX.exec(str);\r\n    if (!m) {\r\n        return ["", "", ""];\r\n    }\r\n    return [m[1], m[2], m[3]];\r\n}\r\n/**\r\n * Convert a (string) note in ABC notation into a (string) note in scientific notation\r\n *\r\n * @example\r\n * abcToScientificNotation("c") // => "C5"\r\n */\r\nfunction abcToScientificNotation(str) {\r\n    const [acc, letter, oct] = tokenize(str);\r\n    if (letter === "") {\r\n        return "";\r\n    }\r\n    let o = 4;\r\n    for (let i = 0; i < oct.length; i++) {\r\n        o += oct.charAt(i) === "," ? -1 : 1;\r\n    }\r\n    const a = acc[0] === "_"\r\n        ? acc.replace(/_/g, "b")\r\n        : acc[0] === "^"\r\n            ? acc.replace(/\\^/g, "#")\r\n            : "";\r\n    return letter.charCodeAt(0) > 96\r\n        ? letter.toUpperCase() + a + (o + 1)\r\n        : letter + a + o;\r\n}\r\n/**\r\n * Convert a (string) note in scientific notation into a (string) note in ABC notation\r\n *\r\n * @example\r\n * scientificToAbcNotation("C#4") // => "^C"\r\n */\r\nfunction scientificToAbcNotation(str) {\r\n    const n = index_es_note(str);\r\n    if (n.empty || !n.oct) {\r\n        return "";\r\n    }\r\n    const { letter, acc, oct } = n;\r\n    const a = acc[0] === "b" ? acc.replace(/b/g, "_") : acc.replace(/#/g, "^");\r\n    const l = oct > 4 ? letter.toLowerCase() : letter;\r\n    const o = oct === 5 ? "" : oct > 4 ? index_es_fillStr("\'", oct - 5) : index_es_fillStr(",", 4 - oct);\r\n    return a + l + o;\r\n}\r\nfunction index_es_transpose(note, interval) {\r\n    return scientificToAbcNotation(transpose(abcToScientificNotation(note), interval));\r\n}\r\nfunction index_es_distance(from, to) {\r\n    return distance(abcToScientificNotation(from), abcToScientificNotation(to));\r\n}\r\nvar index_es_index = {\r\n    abcToScientificNotation,\r\n    scientificToAbcNotation,\r\n    tokenize,\r\n    transpose: index_es_transpose,\r\n    distance: index_es_distance,\r\n};\n\n/* harmony default export */ var index_es = (index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/array/dist/index.es.js\n\n\n// ascending range\r\nfunction ascR(b, n) {\r\n    const a = [];\r\n    // tslint:disable-next-line:curly\r\n    for (; n--; a[n] = n + b)\r\n        ;\r\n    return a;\r\n}\r\n// descending range\r\nfunction descR(b, n) {\r\n    const a = [];\r\n    // tslint:disable-next-line:curly\r\n    for (; n--; a[n] = b - n)\r\n        ;\r\n    return a;\r\n}\r\n/**\r\n * Creates a numeric range\r\n *\r\n * @param {number} from\r\n * @param {number} to\r\n * @return {Array<number>}\r\n *\r\n * @example\r\n * range(-2, 2) // => [-2, -1, 0, 1, 2]\r\n * range(2, -2) // => [2, 1, 0, -1, -2]\r\n */\r\nfunction range(from, to) {\r\n    return from < to ? ascR(from, to - from + 1) : descR(from, from - to + 1);\r\n}\r\n/**\r\n * Rotates a list a number of times. It"s completly agnostic about the\r\n * contents of the list.\r\n *\r\n * @param {Integer} times - the number of rotations\r\n * @param {Array} array\r\n * @return {Array} the rotated array\r\n *\r\n * @example\r\n * rotate(1, [1, 2, 3]) // => [2, 3, 1]\r\n */\r\nfunction rotate(times, arr) {\r\n    const len = arr.length;\r\n    const n = ((times % len) + len) % len;\r\n    return arr.slice(n, len).concat(arr.slice(0, n));\r\n}\r\n/**\r\n * Return a copy of the array with the null values removed\r\n * @function\r\n * @param {Array} array\r\n * @return {Array}\r\n *\r\n * @example\r\n * compact(["a", "b", null, "c"]) // => ["a", "b", "c"]\r\n */\r\nfunction compact(arr) {\r\n    return arr.filter((n) => n === 0 || n);\r\n}\r\n/**\r\n * Sort an array of notes in ascending order. Pitch classes are listed\r\n * before notes. Any string that is not a note is removed.\r\n *\r\n * @param {string[]} notes\r\n * @return {string[]} sorted array of notes\r\n *\r\n * @example\r\n * sortedNoteNames([\'c2\', \'c5\', \'c1\', \'c0\', \'c6\', \'c\'])\r\n * // => [\'C\', \'C0\', \'C1\', \'C2\', \'C5\', \'C6\']\r\n * sortedNoteNames([\'c\', \'F\', \'G\', \'a\', \'b\', \'h\', \'J\'])\r\n * // => [\'C\', \'F\', \'G\', \'A\', \'B\']\r\n */\r\nfunction sortedNoteNames(notes) {\r\n    const valid = notes.map((n) => index_es_note(n)).filter((n) => !n.empty);\r\n    return valid.sort((a, b) => a.height - b.height).map((n) => n.name);\r\n}\r\n/**\r\n * Get sorted notes with duplicates removed. Pitch classes are listed\r\n * before notes.\r\n *\r\n * @function\r\n * @param {string[]} array\r\n * @return {string[]} unique sorted notes\r\n *\r\n * @example\r\n * Array.sortedUniqNoteNames([\'a\', \'b\', \'c2\', \'1p\', \'p2\', \'c2\', \'b\', \'c\', \'c3\' ])\r\n * // => [ \'C\', \'A\', \'B\', \'C2\', \'C3\' ]\r\n */\r\nfunction sortedUniqNoteNames(arr) {\r\n    return sortedNoteNames(arr).filter((n, i, a) => i === 0 || n !== a[i - 1]);\r\n}\r\n/**\r\n * Randomizes the order of the specified array in-place, using the Fisher–Yates shuffle.\r\n *\r\n * @function\r\n * @param {Array} array\r\n * @return {Array} the array shuffled\r\n *\r\n * @example\r\n * shuffle(["C", "D", "E", "F"]) // => [...]\r\n */\r\nfunction shuffle(arr, rnd = Math.random) {\r\n    let i;\r\n    let t;\r\n    let m = arr.length;\r\n    while (m) {\r\n        i = Math.floor(rnd() * m--);\r\n        t = arr[m];\r\n        arr[m] = arr[i];\r\n        arr[i] = t;\r\n    }\r\n    return arr;\r\n}\r\n/**\r\n * Get all permutations of an array\r\n *\r\n * @param {Array} array - the array\r\n * @return {Array<Array>} an array with all the permutations\r\n * @example\r\n * permutations(["a", "b", "c"])) // =>\r\n * [\r\n *   ["a", "b", "c"],\r\n *   ["b", "a", "c"],\r\n *   ["b", "c", "a"],\r\n *   ["a", "c", "b"],\r\n *   ["c", "a", "b"],\r\n *   ["c", "b", "a"]\r\n * ]\r\n */\r\nfunction permutations(arr) {\r\n    if (arr.length === 0) {\r\n        return [[]];\r\n    }\r\n    return permutations(arr.slice(1)).reduce((acc, perm) => {\r\n        return acc.concat(arr.map((e, pos) => {\r\n            const newPerm = perm.slice();\r\n            newPerm.splice(pos, 0, arr[0]);\r\n            return newPerm;\r\n        }));\r\n    }, []);\r\n}\n\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/collection/dist/index.es.js\n// ascending range\r\nfunction index_es_ascR(b, n) {\r\n    const a = [];\r\n    // tslint:disable-next-line:curly\r\n    for (; n--; a[n] = n + b)\r\n        ;\r\n    return a;\r\n}\r\n// descending range\r\nfunction index_es_descR(b, n) {\r\n    const a = [];\r\n    // tslint:disable-next-line:curly\r\n    for (; n--; a[n] = b - n)\r\n        ;\r\n    return a;\r\n}\r\n/**\r\n * Creates a numeric range\r\n *\r\n * @param {number} from\r\n * @param {number} to\r\n * @return {Array<number>}\r\n *\r\n * @example\r\n * range(-2, 2) // => [-2, -1, 0, 1, 2]\r\n * range(2, -2) // => [2, 1, 0, -1, -2]\r\n */\r\nfunction index_es_range(from, to) {\r\n    return from < to ? index_es_ascR(from, to - from + 1) : index_es_descR(from, from - to + 1);\r\n}\r\n/**\r\n * Rotates a list a number of times. It"s completly agnostic about the\r\n * contents of the list.\r\n *\r\n * @param {Integer} times - the number of rotations\r\n * @param {Array} collection\r\n * @return {Array} the rotated collection\r\n *\r\n * @example\r\n * rotate(1, [1, 2, 3]) // => [2, 3, 1]\r\n */\r\nfunction index_es_rotate(times, arr) {\r\n    const len = arr.length;\r\n    const n = ((times % len) + len) % len;\r\n    return arr.slice(n, len).concat(arr.slice(0, n));\r\n}\r\n/**\r\n * Return a copy of the collection with the null values removed\r\n * @function\r\n * @param {Array} collection\r\n * @return {Array}\r\n *\r\n * @example\r\n * compact(["a", "b", null, "c"]) // => ["a", "b", "c"]\r\n */\r\nfunction index_es_compact(arr) {\r\n    return arr.filter((n) => n === 0 || n);\r\n}\r\n/**\r\n * Randomizes the order of the specified collection in-place, using the Fisher–Yates shuffle.\r\n *\r\n * @function\r\n * @param {Array} collection\r\n * @return {Array} the collection shuffled\r\n *\r\n * @example\r\n * shuffle(["C", "D", "E", "F"]) // => [...]\r\n */\r\nfunction index_es_shuffle(arr, rnd = Math.random) {\r\n    let i;\r\n    let t;\r\n    let m = arr.length;\r\n    while (m) {\r\n        i = Math.floor(rnd() * m--);\r\n        t = arr[m];\r\n        arr[m] = arr[i];\r\n        arr[i] = t;\r\n    }\r\n    return arr;\r\n}\r\n/**\r\n * Get all permutations of an collection\r\n *\r\n * @param {Array} collection - the collection\r\n * @return {Array<Array>} an collection with all the permutations\r\n * @example\r\n * permutations(["a", "b", "c"])) // =>\r\n * [\r\n *   ["a", "b", "c"],\r\n *   ["b", "a", "c"],\r\n *   ["b", "c", "a"],\r\n *   ["a", "c", "b"],\r\n *   ["c", "a", "b"],\r\n *   ["c", "b", "a"]\r\n * ]\r\n */\r\nfunction index_es_permutations(arr) {\r\n    if (arr.length === 0) {\r\n        return [[]];\r\n    }\r\n    return index_es_permutations(arr.slice(1)).reduce((acc, perm) => {\r\n        return acc.concat(arr.map((e, pos) => {\r\n            const newPerm = perm.slice();\r\n            newPerm.splice(pos, 0, arr[0]);\r\n            return newPerm;\r\n        }));\r\n    }, []);\r\n}\r\nvar dist_index_es_index = {\r\n    compact: index_es_compact,\r\n    permutations: index_es_permutations,\r\n    range: index_es_range,\r\n    rotate: index_es_rotate,\r\n    shuffle: index_es_shuffle,\r\n};\n\n/* harmony default export */ var dist_index_es = (dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/pcset/dist/index.es.js\n\n\n\nconst EmptyPcset = {\r\n    empty: true,\r\n    name: "",\r\n    setNum: 0,\r\n    chroma: "000000000000",\r\n    normalized: "000000000000",\r\n    intervals: [],\r\n};\r\n// UTILITIES\r\nconst setNumToChroma = (num) => Number(num).toString(2);\r\nconst chromaToNumber = (chroma) => parseInt(chroma, 2);\r\nconst dist_index_es_REGEX = /^[01]{12}$/;\r\nfunction isChroma(set) {\r\n    return dist_index_es_REGEX.test(set);\r\n}\r\nconst isPcsetNum = (set) => typeof set === "number" && set >= 0 && set <= 4095;\r\nconst isPcset = (set) => set && isChroma(set.chroma);\r\nconst dist_index_es_cache = { [EmptyPcset.chroma]: EmptyPcset };\r\n/**\r\n * Get the pitch class set of a collection of notes or set number or chroma\r\n */\r\nfunction get(src) {\r\n    const chroma = isChroma(src)\r\n        ? src\r\n        : isPcsetNum(src)\r\n            ? setNumToChroma(src)\r\n            : Array.isArray(src)\r\n                ? listToChroma(src)\r\n                : isPcset(src)\r\n                    ? src.chroma\r\n                    : EmptyPcset.chroma;\r\n    return (dist_index_es_cache[chroma] = dist_index_es_cache[chroma] || chromaToPcset(chroma));\r\n}\r\n/**\r\n * Use Pcset.properties\r\n * @function\r\n * @deprecated\r\n */\r\nconst index_es_pcset = deprecate("Pcset.pcset", "Pcset.get", get);\r\n/**\r\n * Get pitch class set chroma\r\n * @function\r\n * @example\r\n * Pcset.chroma(["c", "d", "e"]); //=> "101010000000"\r\n */\r\nconst index_es_chroma = (set) => get(set).chroma;\r\n/**\r\n * Get intervals (from C) of a set\r\n * @function\r\n * @example\r\n * Pcset.intervals(["c", "d", "e"]); //=>\r\n */\r\nconst index_es_intervals = (set) => get(set).intervals;\r\n/**\r\n * Get pitch class set number\r\n * @function\r\n * @example\r\n * Pcset.num(["c", "d", "e"]); //=> 2192\r\n */\r\nconst num = (set) => get(set).setNum;\r\nconst IVLS = [\r\n    "1P",\r\n    "2m",\r\n    "2M",\r\n    "3m",\r\n    "3M",\r\n    "4P",\r\n    "5d",\r\n    "5P",\r\n    "6m",\r\n    "6M",\r\n    "7m",\r\n    "7M",\r\n];\r\n/**\r\n * @private\r\n * Get the intervals of a pcset *starting from C*\r\n * @param {Set} set - the pitch class set\r\n * @return {IntervalName[]} an array of interval names or an empty array\r\n * if not a valid pitch class set\r\n */\r\nfunction chromaToIntervals(chroma) {\r\n    const intervals = [];\r\n    for (let i = 0; i < 12; i++) {\r\n        // tslint:disable-next-line:curly\r\n        if (chroma.charAt(i) === "1")\r\n            intervals.push(IVLS[i]);\r\n    }\r\n    return intervals;\r\n}\r\n/**\r\n * Get a list of all possible pitch class sets (all possible chromas) *having\r\n * C as root*. There are 2048 different chromas. If you want them with another\r\n * note you have to transpose it\r\n *\r\n * @see http://allthescales.org/\r\n * @return {Array<PcsetChroma>} an array of possible chromas from \'10000000000\' to \'11111111111\'\r\n */\r\nfunction index_es_chromas() {\r\n    return index_es_range(2048, 4095).map(setNumToChroma);\r\n}\r\n/**\r\n * Given a a list of notes or a pcset chroma, produce the rotations\r\n * of the chroma discarding the ones that starts with "0"\r\n *\r\n * This is used, for example, to get all the modes of a scale.\r\n *\r\n * @param {Array|string} set - the list of notes or pitchChr of the set\r\n * @param {boolean} normalize - (Optional, true by default) remove all\r\n * the rotations that starts with "0"\r\n * @return {Array<string>} an array with all the modes of the chroma\r\n *\r\n * @example\r\n * Pcset.modes(["C", "D", "E"]).map(Pcset.intervals)\r\n */\r\nfunction modes(set, normalize = true) {\r\n    const pcs = get(set);\r\n    const binary = pcs.chroma.split("");\r\n    return index_es_compact(binary.map((_, i) => {\r\n        const r = index_es_rotate(i, binary);\r\n        return normalize && r[0] === "0" ? null : r.join("");\r\n    }));\r\n}\r\n/**\r\n * Test if two pitch class sets are numentical\r\n *\r\n * @param {Array|string} set1 - one of the pitch class sets\r\n * @param {Array|string} set2 - the other pitch class set\r\n * @return {boolean} true if they are equal\r\n * @example\r\n * Pcset.isEqual(["c2", "d3"], ["c5", "d2"]) // => true\r\n */\r\nfunction isEqual(s1, s2) {\r\n    return get(s1).setNum === get(s2).setNum;\r\n}\r\n/**\r\n * Create a function that test if a collection of notes is a\r\n * subset of a given set\r\n *\r\n * The function is curryfied.\r\n *\r\n * @param {PcsetChroma|NoteName[]} set - the superset to test against (chroma or\r\n * list of notes)\r\n * @return{function(PcsetChroma|NoteNames[]): boolean} a function accepting a set\r\n * to test against (chroma or list of notes)\r\n * @example\r\n * const inCMajor = Pcset.isSubsetOf(["C", "E", "G"])\r\n * inCMajor(["e6", "c4"]) // => true\r\n * inCMajor(["e6", "c4", "d3"]) // => false\r\n */\r\nfunction isSubsetOf(set) {\r\n    const s = get(set).setNum;\r\n    return (notes) => {\r\n        const o = get(notes).setNum;\r\n        // tslint:disable-next-line: no-bitwise\r\n        return s && s !== o && (o & s) === o;\r\n    };\r\n}\r\n/**\r\n * Create a function that test if a collection of notes is a\r\n * superset of a given set (it contains all notes and at least one more)\r\n *\r\n * @param {Set} set - an array of notes or a chroma set string to test against\r\n * @return {(subset: Set): boolean} a function that given a set\r\n * returns true if is a subset of the first one\r\n * @example\r\n * const extendsCMajor = Pcset.isSupersetOf(["C", "E", "G"])\r\n * extendsCMajor(["e6", "a", "c4", "g2"]) // => true\r\n * extendsCMajor(["c6", "e4", "g3"]) // => false\r\n */\r\nfunction isSupersetOf(set) {\r\n    const s = get(set).setNum;\r\n    return (notes) => {\r\n        const o = get(notes).setNum;\r\n        // tslint:disable-next-line: no-bitwise\r\n        return s && s !== o && (o | s) === o;\r\n    };\r\n}\r\n/**\r\n * Test if a given pitch class set includes a note\r\n *\r\n * @param {Array<string>} set - the base set to test against\r\n * @param {string} note - the note to test\r\n * @return {boolean} true if the note is included in the pcset\r\n *\r\n * Can be partially applied\r\n *\r\n * @example\r\n * const isNoteInCMajor = isNoteIncludedIn([\'C\', \'E\', \'G\'])\r\n * isNoteInCMajor(\'C4\') // => true\r\n * isNoteInCMajor(\'C#4\') // => false\r\n */\r\nfunction isNoteIncludedIn(set) {\r\n    const s = get(set);\r\n    return (noteName) => {\r\n        const n = index_es_note(noteName);\r\n        return s && !n.empty && s.chroma.charAt(n.chroma) === "1";\r\n    };\r\n}\r\n/** @deprecated use: isNoteIncludedIn */\r\nconst includes = isNoteIncludedIn;\r\n/**\r\n * Filter a list with a pitch class set\r\n *\r\n * @param {Array|string} set - the pitch class set notes\r\n * @param {Array|string} notes - the note list to be filtered\r\n * @return {Array} the filtered notes\r\n *\r\n * @example\r\n * Pcset.filter(["C", "D", "E"], ["c2", "c#2", "d2", "c3", "c#3", "d3"]) // => [ "c2", "d2", "c3", "d3" ])\r\n * Pcset.filter(["C2"], ["c2", "c#2", "d2", "c3", "c#3", "d3"]) // => [ "c2", "c3" ])\r\n */\r\nfunction index_es_filter(set) {\r\n    const isIncluded = isNoteIncludedIn(set);\r\n    return (notes) => {\r\n        return notes.filter(isIncluded);\r\n    };\r\n}\r\nvar pcset_dist_index_es_index = {\r\n    get,\r\n    chroma: index_es_chroma,\r\n    num,\r\n    intervals: index_es_intervals,\r\n    chromas: index_es_chromas,\r\n    isSupersetOf,\r\n    isSubsetOf,\r\n    isNoteIncludedIn,\r\n    isEqual,\r\n    filter: index_es_filter,\r\n    modes,\r\n    // deprecated\r\n    pcset: index_es_pcset,\r\n};\r\n//// PRIVATE ////\r\nfunction chromaRotations(chroma) {\r\n    const binary = chroma.split("");\r\n    return binary.map((_, i) => index_es_rotate(i, binary).join(""));\r\n}\r\nfunction chromaToPcset(chroma) {\r\n    const setNum = chromaToNumber(chroma);\r\n    const normalizedNum = chromaRotations(chroma)\r\n        .map(chromaToNumber)\r\n        .filter((n) => n >= 2048)\r\n        .sort()[0];\r\n    const normalized = setNumToChroma(normalizedNum);\r\n    const intervals = chromaToIntervals(chroma);\r\n    return {\r\n        empty: false,\r\n        name: "",\r\n        setNum,\r\n        chroma,\r\n        normalized,\r\n        intervals,\r\n    };\r\n}\r\nfunction listToChroma(set) {\r\n    if (set.length === 0) {\r\n        return EmptyPcset.chroma;\r\n    }\r\n    let pitch;\r\n    const binary = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];\r\n    // tslint:disable-next-line:prefer-for-of\r\n    for (let i = 0; i < set.length; i++) {\r\n        pitch = index_es_note(set[i]);\r\n        // tslint:disable-next-line: curly\r\n        if (pitch.empty)\r\n            pitch = index_es_interval(set[i]);\r\n        // tslint:disable-next-line: curly\r\n        if (!pitch.empty)\r\n            binary[pitch.chroma] = 1;\r\n    }\r\n    return binary.join("");\r\n}\n\n/* harmony default export */ var pcset_dist_index_es = (pcset_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/chord-type/dist/index.es.js\n\n\n\n/**\r\n * @private\r\n * Chord List\r\n * Source: https://en.wikibooks.org/wiki/Music_Theory/Complete_List_of_Chord_Patterns\r\n * Format: ["intervals", "full name", "abrv1 abrv2"]\r\n */\r\nconst CHORDS = [\r\n    // ==Major==\r\n    ["1P 3M 5P", "major", "M ^ "],\r\n    ["1P 3M 5P 7M", "major seventh", "maj7 Δ ma7 M7 Maj7 ^7"],\r\n    ["1P 3M 5P 7M 9M", "major ninth", "maj9 Δ9 ^9"],\r\n    ["1P 3M 5P 7M 9M 13M", "major thirteenth", "maj13 Maj13 ^13"],\r\n    ["1P 3M 5P 6M", "sixth", "6 add6 add13 M6"],\r\n    ["1P 3M 5P 6M 9M", "sixth/ninth", "6/9 69 M69"],\r\n    ["1P 3M 6m 7M", "major seventh flat sixth", "M7b6 ^7b6"],\r\n    [\r\n        "1P 3M 5P 7M 11A",\r\n        "major seventh sharp eleventh",\r\n        "maj#4 Δ#4 Δ#11 M7#11 ^7#11 maj7#11",\r\n    ],\r\n    // ==Minor==\r\n    // \'\'\'Normal\'\'\'\r\n    ["1P 3m 5P", "minor", "m min -"],\r\n    ["1P 3m 5P 7m", "minor seventh", "m7 min7 mi7 -7"],\r\n    [\r\n        "1P 3m 5P 7M",\r\n        "minor/major seventh",\r\n        "m/ma7 m/maj7 mM7 mMaj7 m/M7 -Δ7 mΔ -^7",\r\n    ],\r\n    ["1P 3m 5P 6M", "minor sixth", "m6 -6"],\r\n    ["1P 3m 5P 7m 9M", "minor ninth", "m9 -9"],\r\n    ["1P 3m 5P 7M 9M", "minor/major ninth", "mM9 mMaj9 -^9"],\r\n    ["1P 3m 5P 7m 9M 11P", "minor eleventh", "m11 -11"],\r\n    ["1P 3m 5P 7m 9M 13M", "minor thirteenth", "m13 -13"],\r\n    // \'\'\'Diminished\'\'\'\r\n    ["1P 3m 5d", "diminished", "dim ° o"],\r\n    ["1P 3m 5d 7d", "diminished seventh", "dim7 °7 o7"],\r\n    ["1P 3m 5d 7m", "half-diminished", "m7b5 ø -7b5 h7 h"],\r\n    // ==Dominant/Seventh==\r\n    // \'\'\'Normal\'\'\'\r\n    ["1P 3M 5P 7m", "dominant seventh", "7 dom"],\r\n    ["1P 3M 5P 7m 9M", "dominant ninth", "9"],\r\n    ["1P 3M 5P 7m 9M 13M", "dominant thirteenth", "13"],\r\n    ["1P 3M 5P 7m 11A", "lydian dominant seventh", "7#11 7#4"],\r\n    // \'\'\'Altered\'\'\'\r\n    ["1P 3M 5P 7m 9m", "dominant flat ninth", "7b9"],\r\n    ["1P 3M 5P 7m 9A", "dominant sharp ninth", "7#9"],\r\n    ["1P 3M 7m 9m", "altered", "alt7"],\r\n    // \'\'\'Suspended\'\'\'\r\n    ["1P 4P 5P", "suspended fourth", "sus4 sus"],\r\n    ["1P 2M 5P", "suspended second", "sus2"],\r\n    ["1P 4P 5P 7m", "suspended fourth seventh", "7sus4 7sus"],\r\n    ["1P 5P 7m 9M 11P", "eleventh", "11"],\r\n    [\r\n        "1P 4P 5P 7m 9m",\r\n        "suspended fourth flat ninth",\r\n        "b9sus phryg 7b9sus 7b9sus4",\r\n    ],\r\n    // ==Other==\r\n    ["1P 5P", "fifth", "5"],\r\n    ["1P 3M 5A", "augmented", "aug + +5 ^#5"],\r\n    ["1P 3m 5A", "minor augmented", "m#5 -#5 m+"],\r\n    ["1P 3M 5A 7M", "augmented seventh", "maj7#5 maj7+5 +maj7 ^7#5"],\r\n    [\r\n        "1P 3M 5P 7M 9M 11A",\r\n        "major sharp eleventh (lydian)",\r\n        "maj9#11 Δ9#11 ^9#11",\r\n    ],\r\n    // ==Legacy==\r\n    ["1P 2M 4P 5P", "", "sus24 sus4add9"],\r\n    ["1P 3M 5A 7M 9M", "", "maj9#5 Maj9#5"],\r\n    ["1P 3M 5A 7m", "", "7#5 +7 7+ 7aug aug7"],\r\n    ["1P 3M 5A 7m 9A", "", "7#5#9 7#9#5 7alt"],\r\n    ["1P 3M 5A 7m 9M", "", "9#5 9+"],\r\n    ["1P 3M 5A 7m 9M 11A", "", "9#5#11"],\r\n    ["1P 3M 5A 7m 9m", "", "7#5b9 7b9#5"],\r\n    ["1P 3M 5A 7m 9m 11A", "", "7#5b9#11"],\r\n    ["1P 3M 5A 9A", "", "+add#9"],\r\n    ["1P 3M 5A 9M", "", "M#5add9 +add9"],\r\n    ["1P 3M 5P 6M 11A", "", "M6#11 M6b5 6#11 6b5"],\r\n    ["1P 3M 5P 6M 7M 9M", "", "M7add13"],\r\n    ["1P 3M 5P 6M 9M 11A", "", "69#11"],\r\n    ["1P 3m 5P 6M 9M", "", "m69 -69"],\r\n    ["1P 3M 5P 6m 7m", "", "7b6"],\r\n    ["1P 3M 5P 7M 9A 11A", "", "maj7#9#11"],\r\n    ["1P 3M 5P 7M 9M 11A 13M", "", "M13#11 maj13#11 M13+4 M13#4"],\r\n    ["1P 3M 5P 7M 9m", "", "M7b9"],\r\n    ["1P 3M 5P 7m 11A 13m", "", "7#11b13 7b5b13"],\r\n    ["1P 3M 5P 7m 13M", "", "7add6 67 7add13"],\r\n    ["1P 3M 5P 7m 9A 11A", "", "7#9#11 7b5#9 7#9b5"],\r\n    ["1P 3M 5P 7m 9A 11A 13M", "", "13#9#11"],\r\n    ["1P 3M 5P 7m 9A 11A 13m", "", "7#9#11b13"],\r\n    ["1P 3M 5P 7m 9A 13M", "", "13#9"],\r\n    ["1P 3M 5P 7m 9A 13m", "", "7#9b13"],\r\n    ["1P 3M 5P 7m 9M 11A", "", "9#11 9+4 9#4"],\r\n    ["1P 3M 5P 7m 9M 11A 13M", "", "13#11 13+4 13#4"],\r\n    ["1P 3M 5P 7m 9M 11A 13m", "", "9#11b13 9b5b13"],\r\n    ["1P 3M 5P 7m 9m 11A", "", "7b9#11 7b5b9 7b9b5"],\r\n    ["1P 3M 5P 7m 9m 11A 13M", "", "13b9#11"],\r\n    ["1P 3M 5P 7m 9m 11A 13m", "", "7b9b13#11 7b9#11b13 7b5b9b13"],\r\n    ["1P 3M 5P 7m 9m 13M", "", "13b9"],\r\n    ["1P 3M 5P 7m 9m 13m", "", "7b9b13"],\r\n    ["1P 3M 5P 7m 9m 9A", "", "7b9#9"],\r\n    ["1P 3M 5P 9M", "", "Madd9 2 add9 add2"],\r\n    ["1P 3M 5P 9m", "", "Maddb9"],\r\n    ["1P 3M 5d", "", "Mb5"],\r\n    ["1P 3M 5d 6M 7m 9M", "", "13b5"],\r\n    ["1P 3M 5d 7M", "", "M7b5"],\r\n    ["1P 3M 5d 7M 9M", "", "M9b5"],\r\n    ["1P 3M 5d 7m", "", "7b5"],\r\n    ["1P 3M 5d 7m 9M", "", "9b5"],\r\n    ["1P 3M 7m", "", "7no5"],\r\n    ["1P 3M 7m 13m", "", "7b13"],\r\n    ["1P 3M 7m 9M", "", "9no5"],\r\n    ["1P 3M 7m 9M 13M", "", "13no5"],\r\n    ["1P 3M 7m 9M 13m", "", "9b13"],\r\n    ["1P 3m 4P 5P", "", "madd4"],\r\n    ["1P 3m 5P 6m 7M", "", "mMaj7b6"],\r\n    ["1P 3m 5P 6m 7M 9M", "", "mMaj9b6"],\r\n    ["1P 3m 5P 7m 11P", "", "m7add11 m7add4"],\r\n    ["1P 3m 5P 9M", "", "madd9"],\r\n    ["1P 3m 5d 6M 7M", "", "o7M7"],\r\n    ["1P 3m 5d 7M", "", "oM7"],\r\n    ["1P 3m 6m 7M", "", "mb6M7"],\r\n    ["1P 3m 6m 7m", "", "m7#5"],\r\n    ["1P 3m 6m 7m 9M", "", "m9#5"],\r\n    ["1P 3m 5A 7m 9M 11P", "", "m11A"],\r\n    ["1P 3m 6m 9m", "", "mb6b9"],\r\n    ["1P 2M 3m 5d 7m", "", "m9b5"],\r\n    ["1P 4P 5A 7M", "", "M7#5sus4"],\r\n    ["1P 4P 5A 7M 9M", "", "M9#5sus4"],\r\n    ["1P 4P 5A 7m", "", "7#5sus4"],\r\n    ["1P 4P 5P 7M", "", "M7sus4"],\r\n    ["1P 4P 5P 7M 9M", "", "M9sus4"],\r\n    ["1P 4P 5P 7m 9M", "", "9sus4 9sus"],\r\n    ["1P 4P 5P 7m 9M 13M", "", "13sus4 13sus"],\r\n    ["1P 4P 5P 7m 9m 13m", "", "7sus4b9b13 7b9b13sus4"],\r\n    ["1P 4P 7m 10m", "", "4 quartal"],\r\n    ["1P 5P 7m 9m 11P", "", "11b9"],\r\n];\n\nconst NoChordType = {\r\n    ...EmptyPcset,\r\n    name: "",\r\n    quality: "Unknown",\r\n    intervals: [],\r\n    aliases: [],\r\n};\r\nlet dictionary = [];\r\nlet chord_type_dist_index_es_index = {};\r\n/**\r\n * Given a chord name or chroma, return the chord properties\r\n * @param {string} source - chord name or pitch class set chroma\r\n * @example\r\n * import { get } from \'tonaljs/chord-type\'\r\n * get(\'major\') // => { name: \'major\', ... }\r\n */\r\nfunction index_es_get(type) {\r\n    return chord_type_dist_index_es_index[type] || NoChordType;\r\n}\r\nconst index_es_chordType = deprecate("ChordType.chordType", "ChordType.get", index_es_get);\r\n/**\r\n * Get all chord (long) names\r\n */\r\nfunction index_es_names() {\r\n    return dictionary.map((chord) => chord.name).filter((x) => x);\r\n}\r\n/**\r\n * Get all chord symbols\r\n */\r\nfunction symbols() {\r\n    return dictionary.map((chord) => chord.aliases[0]).filter((x) => x);\r\n}\r\n/**\r\n * Keys used to reference chord types\r\n */\r\nfunction index_es_keys() {\r\n    return Object.keys(chord_type_dist_index_es_index);\r\n}\r\n/**\r\n * Return a list of all chord types\r\n */\r\nfunction index_es_all() {\r\n    return dictionary.slice();\r\n}\r\nconst entries = deprecate("ChordType.entries", "ChordType.all", index_es_all);\r\n/**\r\n * Clear the dictionary\r\n */\r\nfunction removeAll() {\r\n    dictionary = [];\r\n    chord_type_dist_index_es_index = {};\r\n}\r\n/**\r\n * Add a chord to the dictionary.\r\n * @param intervals\r\n * @param aliases\r\n * @param [fullName]\r\n */\r\nfunction add(intervals, aliases, fullName) {\r\n    const quality = getQuality(intervals);\r\n    const chord = {\r\n        ...get(intervals),\r\n        name: fullName || "",\r\n        quality,\r\n        intervals,\r\n        aliases,\r\n    };\r\n    dictionary.push(chord);\r\n    if (chord.name) {\r\n        chord_type_dist_index_es_index[chord.name] = chord;\r\n    }\r\n    chord_type_dist_index_es_index[chord.setNum] = chord;\r\n    chord_type_dist_index_es_index[chord.chroma] = chord;\r\n    chord.aliases.forEach((alias) => addAlias(chord, alias));\r\n}\r\nfunction addAlias(chord, alias) {\r\n    chord_type_dist_index_es_index[alias] = chord;\r\n}\r\nfunction getQuality(intervals) {\r\n    const has = (interval) => intervals.indexOf(interval) !== -1;\r\n    return has("5A")\r\n        ? "Augmented"\r\n        : has("3M")\r\n            ? "Major"\r\n            : has("5d")\r\n                ? "Diminished"\r\n                : has("3m")\r\n                    ? "Minor"\r\n                    : "Unknown";\r\n}\r\nCHORDS.forEach(([ivls, fullName, names]) => add(ivls.split(" "), names.split(" "), fullName));\r\ndictionary.sort((a, b) => a.setNum - b.setNum);\r\nvar index$1 = {\r\n    names: index_es_names,\r\n    symbols,\r\n    get: index_es_get,\r\n    all: index_es_all,\r\n    add,\r\n    removeAll,\r\n    keys: index_es_keys,\r\n    // deprecated\r\n    entries,\r\n    chordType: index_es_chordType,\r\n};\n\n/* harmony default export */ var chord_type_dist_index_es = (index$1);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/chord-detect/dist/index.es.js\n\n\n\n\nconst namedSet = (notes) => {\r\n    const pcToName = notes.reduce((record, n) => {\r\n        const chroma = index_es_note(n).chroma;\r\n        if (chroma !== undefined) {\r\n            record[chroma] = record[chroma] || index_es_note(n).name;\r\n        }\r\n        return record;\r\n    }, {});\r\n    return (chroma) => pcToName[chroma];\r\n};\r\nfunction detect(source) {\r\n    const notes = source.map((n) => index_es_note(n).pc).filter((x) => x);\r\n    if (index_es_note.length === 0) {\r\n        return [];\r\n    }\r\n    const found = findExactMatches(notes, 1);\r\n    return found\r\n        .filter((chord) => chord.weight)\r\n        .sort((a, b) => b.weight - a.weight)\r\n        .map((chord) => chord.name);\r\n}\r\nfunction findExactMatches(notes, weight) {\r\n    const tonic = notes[0];\r\n    const tonicChroma = index_es_note(tonic).chroma;\r\n    const noteName = namedSet(notes);\r\n    // we need to test all chormas to get the correct baseNote\r\n    const allModes = modes(notes, false);\r\n    const found = [];\r\n    allModes.forEach((mode, index) => {\r\n        // some chords could have the same chroma but different interval spelling\r\n        const chordTypes = index_es_all().filter((chordType) => chordType.chroma === mode);\r\n        chordTypes.forEach((chordType) => {\r\n            const chordName = chordType.aliases[0];\r\n            const baseNote = noteName(index);\r\n            const isInversion = index !== tonicChroma;\r\n            if (isInversion) {\r\n                found.push({\r\n                    weight: 0.5 * weight,\r\n                    name: `${baseNote}${chordName}/${tonic}`,\r\n                });\r\n            }\r\n            else {\r\n                found.push({ weight: 1 * weight, name: `${baseNote}${chordName}` });\r\n            }\r\n        });\r\n    });\r\n    return found;\r\n}\r\nvar chord_detect_dist_index_es_index = { detect };\n\n/* harmony default export */ var chord_detect_dist_index_es = (chord_detect_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/scale-type/dist/index.es.js\n\n\n\n// SCALES\r\n// Format: ["intervals", "name", "alias1", "alias2", ...]\r\nconst SCALES = [\r\n    // 5-note scales\r\n    ["1P 2M 3M 5P 6M", "major pentatonic", "pentatonic"],\r\n    ["1P 3M 4P 5P 7M", "ionian pentatonic"],\r\n    ["1P 3M 4P 5P 7m", "mixolydian pentatonic", "indian"],\r\n    ["1P 2M 4P 5P 6M", "ritusen"],\r\n    ["1P 2M 4P 5P 7m", "egyptian"],\r\n    ["1P 3M 4P 5d 7m", "neopolitan major pentatonic"],\r\n    ["1P 3m 4P 5P 6m", "vietnamese 1"],\r\n    ["1P 2m 3m 5P 6m", "pelog"],\r\n    ["1P 2m 4P 5P 6m", "kumoijoshi"],\r\n    ["1P 2M 3m 5P 6m", "hirajoshi"],\r\n    ["1P 2m 4P 5d 7m", "iwato"],\r\n    ["1P 2m 4P 5P 7m", "in-sen"],\r\n    ["1P 3M 4A 5P 7M", "lydian pentatonic", "chinese"],\r\n    ["1P 3m 4P 6m 7m", "malkos raga"],\r\n    ["1P 3m 4P 5d 7m", "locrian pentatonic", "minor seven flat five pentatonic"],\r\n    ["1P 3m 4P 5P 7m", "minor pentatonic", "vietnamese 2"],\r\n    ["1P 3m 4P 5P 6M", "minor six pentatonic"],\r\n    ["1P 2M 3m 5P 6M", "flat three pentatonic", "kumoi"],\r\n    ["1P 2M 3M 5P 6m", "flat six pentatonic"],\r\n    ["1P 2m 3M 5P 6M", "scriabin"],\r\n    ["1P 3M 5d 6m 7m", "whole tone pentatonic"],\r\n    ["1P 3M 4A 5A 7M", "lydian #5P pentatonic"],\r\n    ["1P 3M 4A 5P 7m", "lydian dominant pentatonic"],\r\n    ["1P 3m 4P 5P 7M", "minor #7M pentatonic"],\r\n    ["1P 3m 4d 5d 7m", "super locrian pentatonic"],\r\n    // 6-note scales\r\n    ["1P 2M 3m 4P 5P 7M", "minor hexatonic"],\r\n    ["1P 2A 3M 5P 5A 7M", "augmented"],\r\n    ["1P 2M 3m 3M 5P 6M", "major blues"],\r\n    ["1P 2M 4P 5P 6M 7m", "piongio"],\r\n    ["1P 2m 3M 4A 6M 7m", "prometheus neopolitan"],\r\n    ["1P 2M 3M 4A 6M 7m", "prometheus"],\r\n    ["1P 2m 3M 5d 6m 7m", "mystery #1"],\r\n    ["1P 2m 3M 4P 5A 6M", "six tone symmetric"],\r\n    ["1P 2M 3M 4A 5A 7m", "whole tone", "messiaen\'s mode #1"],\r\n    ["1P 2m 4P 4A 5P 7M", "messiaen\'s mode #5"],\r\n    ["1P 3m 4P 5d 5P 7m", "minor blues", "blues"],\r\n    // 7-note scales\r\n    ["1P 2M 3M 4P 5d 6m 7m", "locrian major", "arabian"],\r\n    ["1P 2m 3M 4A 5P 6m 7M", "double harmonic lydian"],\r\n    ["1P 2M 3m 4P 5P 6m 7M", "harmonic minor"],\r\n    [\r\n        "1P 2m 3m 4d 5d 6m 7m",\r\n        "altered",\r\n        "super locrian",\r\n        "diminished whole tone",\r\n        "pomeroy",\r\n    ],\r\n    ["1P 2M 3m 4P 5d 6m 7m", "locrian #2", "half-diminished", "aeolian b5"],\r\n    [\r\n        "1P 2M 3M 4P 5P 6m 7m",\r\n        "mixolydian b6",\r\n        "melodic minor fifth mode",\r\n        "hindu",\r\n    ],\r\n    ["1P 2M 3M 4A 5P 6M 7m", "lydian dominant", "lydian b7", "overtone"],\r\n    ["1P 2M 3M 4A 5P 6M 7M", "lydian"],\r\n    ["1P 2M 3M 4A 5A 6M 7M", "lydian augmented"],\r\n    [\r\n        "1P 2m 3m 4P 5P 6M 7m",\r\n        "dorian b2",\r\n        "phrygian #6",\r\n        "melodic minor second mode",\r\n    ],\r\n    ["1P 2M 3m 4P 5P 6M 7M", "melodic minor"],\r\n    ["1P 2m 3m 4P 5d 6m 7m", "locrian"],\r\n    [\r\n        "1P 2m 3m 4d 5d 6m 7d",\r\n        "ultralocrian",\r\n        "superlocrian bb7",\r\n        "·superlocrian diminished",\r\n    ],\r\n    ["1P 2m 3m 4P 5d 6M 7m", "locrian 6", "locrian natural 6", "locrian sharp 6"],\r\n    ["1P 2A 3M 4P 5P 5A 7M", "augmented heptatonic"],\r\n    ["1P 2M 3m 5d 5P 6M 7m", "romanian minor"],\r\n    ["1P 2M 3m 4A 5P 6M 7m", "dorian #4"],\r\n    ["1P 2M 3m 4A 5P 6M 7M", "lydian diminished"],\r\n    ["1P 2m 3m 4P 5P 6m 7m", "phrygian"],\r\n    ["1P 2M 3M 4A 5A 7m 7M", "leading whole tone"],\r\n    ["1P 2M 3M 4A 5P 6m 7m", "lydian minor"],\r\n    ["1P 2m 3M 4P 5P 6m 7m", "phrygian dominant", "spanish", "phrygian major"],\r\n    ["1P 2m 3m 4P 5P 6m 7M", "balinese"],\r\n    ["1P 2m 3m 4P 5P 6M 7M", "neopolitan major"],\r\n    ["1P 2M 3m 4P 5P 6m 7m", "aeolian", "minor"],\r\n    ["1P 2M 3M 4P 5P 6m 7M", "harmonic major"],\r\n    ["1P 2m 3M 4P 5P 6m 7M", "double harmonic major", "gypsy"],\r\n    ["1P 2M 3m 4P 5P 6M 7m", "dorian"],\r\n    ["1P 2M 3m 4A 5P 6m 7M", "hungarian minor"],\r\n    ["1P 2A 3M 4A 5P 6M 7m", "hungarian major"],\r\n    ["1P 2m 3M 4P 5d 6M 7m", "oriental"],\r\n    ["1P 2m 3m 3M 4A 5P 7m", "flamenco"],\r\n    ["1P 2m 3m 4A 5P 6m 7M", "todi raga"],\r\n    ["1P 2M 3M 4P 5P 6M 7m", "mixolydian", "dominant"],\r\n    ["1P 2m 3M 4P 5d 6m 7M", "persian"],\r\n    ["1P 2M 3M 4P 5P 6M 7M", "major", "ionian"],\r\n    ["1P 2m 3M 5d 6m 7m 7M", "enigmatic"],\r\n    [\r\n        "1P 2M 3M 4P 5A 6M 7M",\r\n        "major augmented",\r\n        "major #5",\r\n        "ionian augmented",\r\n        "ionian #5",\r\n    ],\r\n    ["1P 2A 3M 4A 5P 6M 7M", "lydian #9"],\r\n    // 8-note scales\r\n    ["1P 2m 2M 4P 4A 5P 6m 7M", "messiaen\'s mode #4"],\r\n    ["1P 2m 3M 4P 4A 5P 6m 7M", "purvi raga"],\r\n    ["1P 2m 3m 3M 4P 5P 6m 7m", "spanish heptatonic"],\r\n    ["1P 2M 3M 4P 5P 6M 7m 7M", "bebop"],\r\n    ["1P 2M 3m 3M 4P 5P 6M 7m", "bebop minor"],\r\n    ["1P 2M 3M 4P 5P 5A 6M 7M", "bebop major"],\r\n    ["1P 2m 3m 4P 5d 5P 6m 7m", "bebop locrian"],\r\n    ["1P 2M 3m 4P 5P 6m 7m 7M", "minor bebop"],\r\n    ["1P 2M 3m 4P 5d 6m 6M 7M", "diminished", "whole-half diminished"],\r\n    ["1P 2M 3M 4P 5d 5P 6M 7M", "ichikosucho"],\r\n    ["1P 2M 3m 4P 5P 6m 6M 7M", "minor six diminished"],\r\n    [\r\n        "1P 2m 3m 3M 4A 5P 6M 7m",\r\n        "half-whole diminished",\r\n        "dominant diminished",\r\n        "messiaen\'s mode #2",\r\n    ],\r\n    ["1P 3m 3M 4P 5P 6M 7m 7M", "kafi raga"],\r\n    ["1P 2M 3M 4P 4A 5A 6A 7M", "messiaen\'s mode #6"],\r\n    // 9-note scales\r\n    ["1P 2M 3m 3M 4P 5d 5P 6M 7m", "composite blues"],\r\n    ["1P 2M 3m 3M 4A 5P 6m 7m 7M", "messiaen\'s mode #3"],\r\n    // 10-note scales\r\n    ["1P 2m 2M 3m 4P 4A 5P 6m 6M 7M", "messiaen\'s mode #7"],\r\n    // 12-note scales\r\n    ["1P 2m 2M 3m 3M 4P 5d 5P 6m 6M 7m 7M", "chromatic"],\r\n];\n\nconst NoScaleType = {\r\n    ...EmptyPcset,\r\n    intervals: [],\r\n    aliases: [],\r\n};\r\nlet index_es_dictionary = [];\r\nlet scale_type_dist_index_es_index = {};\r\nfunction dist_index_es_names() {\r\n    return index_es_dictionary.map((scale) => scale.name);\r\n}\r\n/**\r\n * Given a scale name or chroma, return the scale properties\r\n *\r\n * @param {string} type - scale name or pitch class set chroma\r\n * @example\r\n * import { get } from \'tonaljs/scale-type\'\r\n * get(\'major\') // => { name: \'major\', ... }\r\n */\r\nfunction dist_index_es_get(type) {\r\n    return scale_type_dist_index_es_index[type] || NoScaleType;\r\n}\r\nconst scaleType = deprecate("ScaleDictionary.scaleType", "ScaleType.get", dist_index_es_get);\r\n/**\r\n * Return a list of all scale types\r\n */\r\nfunction dist_index_es_all() {\r\n    return index_es_dictionary.slice();\r\n}\r\nconst index_es_entries = deprecate("ScaleDictionary.entries", "ScaleType.all", dist_index_es_all);\r\n/**\r\n * Keys used to reference scale types\r\n */\r\nfunction dist_index_es_keys() {\r\n    return Object.keys(scale_type_dist_index_es_index);\r\n}\r\n/**\r\n * Clear the dictionary\r\n */\r\nfunction index_es_removeAll() {\r\n    index_es_dictionary = [];\r\n    scale_type_dist_index_es_index = {};\r\n}\r\n/**\r\n * Add a scale into dictionary\r\n * @param intervals\r\n * @param name\r\n * @param aliases\r\n */\r\nfunction index_es_add(intervals, name, aliases = []) {\r\n    const scale = { ...get(intervals), name, intervals, aliases };\r\n    index_es_dictionary.push(scale);\r\n    scale_type_dist_index_es_index[scale.name] = scale;\r\n    scale_type_dist_index_es_index[scale.setNum] = scale;\r\n    scale_type_dist_index_es_index[scale.chroma] = scale;\r\n    scale.aliases.forEach((alias) => index_es_addAlias(scale, alias));\r\n    return scale;\r\n}\r\nfunction index_es_addAlias(scale, alias) {\r\n    scale_type_dist_index_es_index[alias] = scale;\r\n}\r\nSCALES.forEach(([ivls, name, ...aliases]) => index_es_add(ivls.split(" "), name, aliases));\r\nvar index_es_index$1 = {\r\n    names: dist_index_es_names,\r\n    get: dist_index_es_get,\r\n    all: dist_index_es_all,\r\n    add: index_es_add,\r\n    removeAll: index_es_removeAll,\r\n    keys: dist_index_es_keys,\r\n    // deprecated\r\n    entries: index_es_entries,\r\n    scaleType,\r\n};\n\n/* harmony default export */ var scale_type_dist_index_es = (index_es_index$1);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/chord/dist/index.es.js\n\n\n\n\n\n\n\nconst NoChord = {\r\n    empty: true,\r\n    name: "",\r\n    symbol: "",\r\n    root: "",\r\n    rootDegree: 0,\r\n    type: "",\r\n    tonic: null,\r\n    setNum: NaN,\r\n    quality: "Unknown",\r\n    chroma: "",\r\n    normalized: "",\r\n    aliases: [],\r\n    notes: [],\r\n    intervals: [],\r\n};\r\n// 6, 64, 7, 9, 11 and 13 are consider part of the chord\r\n// (see https://github.com/danigb/tonal/issues/55)\r\nconst NUM_TYPES = /^(6|64|7|9|11|13)$/;\r\n/**\r\n * Tokenize a chord name. It returns an array with the tonic and chord type\r\n * If not tonic is found, all the name is considered the chord name.\r\n *\r\n * This function does NOT check if the chord type exists or not. It only tries\r\n * to split the tonic and chord type.\r\n *\r\n * @function\r\n * @param {string} name - the chord name\r\n * @return {Array} an array with [tonic, type]\r\n * @example\r\n * tokenize("Cmaj7") // => [ "C", "maj7" ]\r\n * tokenize("C7") // => [ "C", "7" ]\r\n * tokenize("mMaj7") // => [ null, "mMaj7" ]\r\n * tokenize("Cnonsense") // => [ null, "nonsense" ]\r\n */\r\nfunction index_es_tokenize(name) {\r\n    const [letter, acc, oct, type] = tokenizeNote(name);\r\n    if (letter === "") {\r\n        return ["", name];\r\n    }\r\n    // aug is augmented (see https://github.com/danigb/tonal/issues/55)\r\n    if (letter === "A" && type === "ug") {\r\n        return ["", "aug"];\r\n    }\r\n    // see: https://github.com/tonaljs/tonal/issues/70\r\n    if (!type && (oct === "4" || oct === "5")) {\r\n        return [letter + acc, oct];\r\n    }\r\n    if (NUM_TYPES.test(oct)) {\r\n        return [letter + acc, oct + type];\r\n    }\r\n    else {\r\n        return [letter + acc + oct, type];\r\n    }\r\n}\r\n/**\r\n * Get a Chord from a chord name.\r\n */\r\nfunction chord_dist_index_es_get(src) {\r\n    if (src === "") {\r\n        return NoChord;\r\n    }\r\n    if (Array.isArray(src) && src.length === 2) {\r\n        return getChord(src[1], src[0]);\r\n    }\r\n    else {\r\n        const [tonic, type] = index_es_tokenize(src);\r\n        const chord = getChord(type, tonic);\r\n        return chord.empty ? getChord(src) : chord;\r\n    }\r\n}\r\n/**\r\n * Get chord properties\r\n *\r\n * @param typeName - the chord type name\r\n * @param [tonic] - Optional tonic\r\n * @param [root]  - Optional root (requires a tonic)\r\n */\r\nfunction getChord(typeName, optionalTonic, optionalRoot) {\r\n    const type = index_es_get(typeName);\r\n    const tonic = index_es_note(optionalTonic || "");\r\n    const root = index_es_note(optionalRoot || "");\r\n    if (type.empty ||\r\n        (optionalTonic && tonic.empty) ||\r\n        (optionalRoot && root.empty)) {\r\n        return NoChord;\r\n    }\r\n    const rootInterval = distance(tonic.pc, root.pc);\r\n    const rootDegree = type.intervals.indexOf(rootInterval) + 1;\r\n    if (!root.empty && !rootDegree) {\r\n        return NoChord;\r\n    }\r\n    const intervals = Array.from(type.intervals);\r\n    for (let i = 1; i < rootDegree; i++) {\r\n        const num = intervals[0][0];\r\n        const quality = intervals[0][1];\r\n        const newNum = parseInt(num, 10) + 7;\r\n        intervals.push(`${newNum}${quality}`);\r\n        intervals.shift();\r\n    }\r\n    const notes = tonic.empty\r\n        ? []\r\n        : intervals.map((i) => transpose(tonic, i));\r\n    typeName = type.aliases.indexOf(typeName) !== -1 ? typeName : type.aliases[0];\r\n    const symbol = `${tonic.empty ? "" : tonic.pc}${typeName}${root.empty || rootDegree <= 1 ? "" : "/" + root.pc}`;\r\n    const name = `${optionalTonic ? tonic.pc + " " : ""}${type.name}${rootDegree > 1 && optionalRoot ? " over " + root.pc : ""}`;\r\n    return {\r\n        ...type,\r\n        name,\r\n        symbol,\r\n        type: type.name,\r\n        root: root.name,\r\n        intervals,\r\n        rootDegree,\r\n        tonic: tonic.name,\r\n        notes,\r\n    };\r\n}\r\nconst index_es_chord = deprecate("Chord.chord", "Chord.get", chord_dist_index_es_get);\r\n/**\r\n * Transpose a chord name\r\n *\r\n * @param {string} chordName - the chord name\r\n * @return {string} the transposed chord\r\n *\r\n * @example\r\n * transpose(\'Dm7\', \'P4\') // => \'Gm7\r\n */\r\nfunction dist_index_es_transpose(chordName, interval) {\r\n    const [tonic, type] = index_es_tokenize(chordName);\r\n    if (!tonic) {\r\n        return chordName;\r\n    }\r\n    return transpose(tonic, interval) + type;\r\n}\r\n/**\r\n * Get all scales where the given chord fits\r\n *\r\n * @example\r\n * chordScales(\'C7b9\')\r\n * // => ["phrygian dominant", "flamenco", "spanish heptatonic", "half-whole diminished", "chromatic"]\r\n */\r\nfunction index_es_chordScales(name) {\r\n    const s = chord_dist_index_es_get(name);\r\n    const isChordIncluded = isSupersetOf(s.chroma);\r\n    return dist_index_es_all()\r\n        .filter((scale) => isChordIncluded(scale.chroma))\r\n        .map((scale) => scale.name);\r\n}\r\n/**\r\n * Get all chords names that are a superset of the given one\r\n * (has the same notes and at least one more)\r\n *\r\n * @function\r\n * @example\r\n * extended("CMaj7")\r\n * // => [ \'Cmaj#4\', \'Cmaj7#9#11\', \'Cmaj9\', \'CM7add13\', \'Cmaj13\', \'Cmaj9#11\', \'CM13#11\', \'CM7b9\' ]\r\n */\r\nfunction extended(chordName) {\r\n    const s = chord_dist_index_es_get(chordName);\r\n    const isSuperset = isSupersetOf(s.chroma);\r\n    return index_es_all()\r\n        .filter((chord) => isSuperset(chord.chroma))\r\n        .map((chord) => s.tonic + chord.aliases[0]);\r\n}\r\n/**\r\n * Find all chords names that are a subset of the given one\r\n * (has less notes but all from the given chord)\r\n *\r\n * @example\r\n */\r\nfunction reduced(chordName) {\r\n    const s = chord_dist_index_es_get(chordName);\r\n    const isSubset = isSubsetOf(s.chroma);\r\n    return index_es_all()\r\n        .filter((chord) => isSubset(chord.chroma))\r\n        .map((chord) => s.tonic + chord.aliases[0]);\r\n}\r\nvar chord_dist_index_es_index = {\r\n    getChord,\r\n    get: chord_dist_index_es_get,\r\n    detect: detect,\r\n    chordScales: index_es_chordScales,\r\n    extended,\r\n    reduced,\r\n    tokenize: index_es_tokenize,\r\n    transpose: dist_index_es_transpose,\r\n    // deprecate\r\n    chord: index_es_chord,\r\n};\n\n/* harmony default export */ var chord_dist_index_es = (chord_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/duration-value/dist/index.es.js\n// source: https://en.wikipedia.org/wiki/Note_value\r\nconst DATA = [\r\n    [\r\n        0.125,\r\n        "dl",\r\n        ["large", "duplex longa", "maxima", "octuple", "octuple whole"],\r\n    ],\r\n    [0.25, "l", ["long", "longa"]],\r\n    [0.5, "d", ["double whole", "double", "breve"]],\r\n    [1, "w", ["whole", "semibreve"]],\r\n    [2, "h", ["half", "minim"]],\r\n    [4, "q", ["quarter", "crotchet"]],\r\n    [8, "e", ["eighth", "quaver"]],\r\n    [16, "s", ["sixteenth", "semiquaver"]],\r\n    [32, "t", ["thirty-second", "demisemiquaver"]],\r\n    [64, "sf", ["sixty-fourth", "hemidemisemiquaver"]],\r\n    [128, "h", ["hundred twenty-eighth"]],\r\n    [256, "th", ["two hundred fifty-sixth"]],\r\n];\n\nconst VALUES = [];\r\nDATA.forEach(([denominator, shorthand, names]) => dist_index_es_add(denominator, shorthand, names));\r\nconst NoDuration = {\r\n    empty: true,\r\n    name: "",\r\n    value: 0,\r\n    fraction: [0, 0],\r\n    shorthand: "",\r\n    dots: "",\r\n    names: [],\r\n};\r\nfunction duration_value_dist_index_es_names() {\r\n    return VALUES.reduce((names, duration) => {\r\n        duration.names.forEach((name) => names.push(name));\r\n        return names;\r\n    }, []);\r\n}\r\nfunction shorthands() {\r\n    return VALUES.map((dur) => dur.shorthand);\r\n}\r\nconst duration_value_dist_index_es_REGEX = /^([^.]+)(\\.*)$/;\r\nfunction duration_value_dist_index_es_get(name) {\r\n    const [_, simple, dots] = duration_value_dist_index_es_REGEX.exec(name) || [];\r\n    const base = VALUES.find((dur) => dur.shorthand === simple || dur.names.includes(simple));\r\n    if (!base) {\r\n        return NoDuration;\r\n    }\r\n    const fraction = calcDots(base.fraction, dots.length);\r\n    const value = fraction[0] / fraction[1];\r\n    return { ...base, name, dots, value, fraction };\r\n}\r\nconst index_es_value = (name) => duration_value_dist_index_es_get(name).value;\r\nconst fraction = (name) => duration_value_dist_index_es_get(name).fraction;\r\nvar duration_value_dist_index_es_index = { names: duration_value_dist_index_es_names, shorthands, get: duration_value_dist_index_es_get, value: index_es_value, fraction };\r\n//// PRIVATE ////\r\nfunction dist_index_es_add(denominator, shorthand, names) {\r\n    VALUES.push({\r\n        empty: false,\r\n        dots: "",\r\n        name: "",\r\n        value: 1 / denominator,\r\n        fraction: denominator < 1 ? [1 / denominator, 1] : [1, denominator],\r\n        shorthand,\r\n        names,\r\n    });\r\n}\r\nfunction calcDots(fraction, dots) {\r\n    const pow = Math.pow(2, dots);\r\n    let numerator = fraction[0] * pow;\r\n    let denominator = fraction[1] * pow;\r\n    const base = numerator;\r\n    // add fractions\r\n    for (let i = 0; i < dots; i++) {\r\n        numerator += base / Math.pow(2, i + 1);\r\n    }\r\n    // simplify\r\n    while (numerator % 2 === 0 && denominator % 2 === 0) {\r\n        numerator /= 2;\r\n        denominator /= 2;\r\n    }\r\n    return [numerator, denominator];\r\n}\n\n/* harmony default export */ var duration_value_dist_index_es = (duration_value_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/interval/dist/index.es.js\n\n\n/**\r\n * Get the natural list of names\r\n */\r\nfunction interval_dist_index_es_names() {\r\n    return "1P 2M 3M 4P 5P 6m 7m".split(" ");\r\n}\r\n/**\r\n * Get properties of an interval\r\n *\r\n * @function\r\n * @example\r\n * Interval.get(\'P4\') // => {"alt": 0,  "dir": 1,  "name": "4P", "num": 4, "oct": 0, "q": "P", "semitones": 5, "simple": 4, "step": 3, "type": "perfectable"}\r\n */\r\nconst interval_dist_index_es_get = index_es_interval;\r\n/**\r\n * Get name of an interval\r\n *\r\n * @function\r\n * @example\r\n * Interval.name(\'4P\') // => "4P"\r\n * Interval.name(\'P4\') // => "4P"\r\n * Interval.name(\'C4\') // => ""\r\n */\r\nconst index_es_name = (name) => index_es_interval(name).name;\r\n/**\r\n * Get semitones of an interval\r\n * @function\r\n * @example\r\n * Interval.semitones(\'P4\') // => 5\r\n */\r\nconst semitones = (name) => index_es_interval(name).semitones;\r\n/**\r\n * Get quality of an interval\r\n * @function\r\n * @example\r\n * Interval.quality(\'P4\') // => "P"\r\n */\r\nconst index_es_quality = (name) => index_es_interval(name).q;\r\n/**\r\n * Get number of an interval\r\n * @function\r\n * @example\r\n * Interval.num(\'P4\') // => 4\r\n */\r\nconst index_es_num = (name) => index_es_interval(name).num;\r\n/**\r\n * Get the simplified version of an interval.\r\n *\r\n * @function\r\n * @param {string} interval - the interval to simplify\r\n * @return {string} the simplified interval\r\n *\r\n * @example\r\n * Interval.simplify("9M") // => "2M"\r\n * Interval.simplify("2M") // => "2M"\r\n * Interval.simplify("-2M") // => "7m"\r\n * ["8P", "9M", "10M", "11P", "12P", "13M", "14M", "15P"].map(Interval.simplify)\r\n * // => [ "8P", "2M", "3M", "4P", "5P", "6M", "7M", "8P" ]\r\n */\r\nfunction simplify(name) {\r\n    const i = index_es_interval(name);\r\n    return i.empty ? "" : i.simple + i.q;\r\n}\r\n/**\r\n * Get the inversion (https://en.wikipedia.org/wiki/Inversion_(music)#Intervals)\r\n * of an interval.\r\n *\r\n * @function\r\n * @param {string} interval - the interval to invert in interval shorthand\r\n * notation or interval array notation\r\n * @return {string} the inverted interval\r\n *\r\n * @example\r\n * Interval.invert("3m") // => "6M"\r\n * Interval.invert("2M") // => "7m"\r\n */\r\nfunction invert(name) {\r\n    const i = index_es_interval(name);\r\n    if (i.empty) {\r\n        return "";\r\n    }\r\n    const step = (7 - i.step) % 7;\r\n    const alt = i.type === "perfectable" ? -i.alt : -(i.alt + 1);\r\n    return index_es_interval({ step, alt, oct: i.oct, dir: i.dir }).name;\r\n}\r\n// interval numbers\r\nconst IN = [1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 7, 7];\r\n// interval qualities\r\nconst IQ = "P m M m M P d P m M m M".split(" ");\r\n/**\r\n * Get interval name from semitones number. Since there are several interval\r\n * names for the same number, the name it\'s arbitrary, but deterministic.\r\n *\r\n * @param {Integer} num - the number of semitones (can be negative)\r\n * @return {string} the interval name\r\n * @example\r\n * Interval.fromSemitones(7) // => "5P"\r\n * Interval.fromSemitones(-7) // => "-5P"\r\n */\r\nfunction fromSemitones(semitones) {\r\n    const d = semitones < 0 ? -1 : 1;\r\n    const n = Math.abs(semitones);\r\n    const c = n % 12;\r\n    const o = Math.floor(n / 12);\r\n    return d * (IN[c] + 7 * o) + IQ[c];\r\n}\r\n/**\r\n * Find interval between two notes\r\n *\r\n * @example\r\n * Interval.distance("C4", "G4"); // => "5P"\r\n */\r\nconst dist_index_es_distance = distance;\r\n/**\r\n * Adds two intervals\r\n *\r\n * @function\r\n * @param {string} interval1\r\n * @param {string} interval2\r\n * @return {string} the added interval name\r\n * @example\r\n * Interval.add("3m", "5P") // => "7m"\r\n */\r\nconst interval_dist_index_es_add = combinator((a, b) => [a[0] + b[0], a[1] + b[1]]);\r\n/**\r\n * Returns a function that adds an interval\r\n *\r\n * @function\r\n * @example\r\n * [\'1P\', \'2M\', \'3M\'].map(Interval.addTo(\'5P\')) // => ["5P", "6M", "7M"]\r\n */\r\nconst addTo = (interval) => (other) => interval_dist_index_es_add(interval, other);\r\n/**\r\n * Subtracts two intervals\r\n *\r\n * @function\r\n * @param {string} minuendInterval\r\n * @param {string} subtrahendInterval\r\n * @return {string} the substracted interval name\r\n * @example\r\n * Interval.substract(\'5P\', \'3M\') // => \'3m\'\r\n * Interval.substract(\'3M\', \'5P\') // => \'-3m\'\r\n */\r\nconst substract = combinator((a, b) => [a[0] - b[0], a[1] - b[1]]);\r\nfunction transposeFifths(interval, fifths) {\r\n    const ivl = interval_dist_index_es_get(interval);\r\n    if (ivl.empty)\r\n        return "";\r\n    const [nFifths, nOcts, dir] = ivl.coord;\r\n    return coordToInterval([nFifths + fifths, nOcts, dir]).name;\r\n}\r\nvar interval_dist_index_es_index = {\r\n    names: interval_dist_index_es_names,\r\n    get: interval_dist_index_es_get,\r\n    name: index_es_name,\r\n    num: index_es_num,\r\n    semitones,\r\n    quality: index_es_quality,\r\n    fromSemitones,\r\n    distance: dist_index_es_distance,\r\n    invert,\r\n    simplify,\r\n    add: interval_dist_index_es_add,\r\n    addTo,\r\n    substract,\r\n    transposeFifths,\r\n};\r\nfunction combinator(fn) {\r\n    return (a, b) => {\r\n        const coordA = index_es_interval(a).coord;\r\n        const coordB = index_es_interval(b).coord;\r\n        if (coordA && coordB) {\r\n            const coord = fn(coordA, coordB);\r\n            return coordToInterval(coord).name;\r\n        }\r\n    };\r\n}\n\n/* harmony default export */ var interval_dist_index_es = (interval_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/midi/dist/index.es.js\n\n\nfunction isMidi(arg) {\r\n    return +arg >= 0 && +arg <= 127;\r\n}\r\n/**\r\n * Get the note midi number (a number between 0 and 127)\r\n *\r\n * It returns undefined if not valid note name\r\n *\r\n * @function\r\n * @param {string|number} note - the note name or midi number\r\n * @return {Integer} the midi number or undefined if not valid note\r\n * @example\r\n * import { toMidi } from \'@tonaljs/midi\'\r\n * toMidi("C4") // => 60\r\n * toMidi(60) // => 60\r\n * toMidi(\'60\') // => 60\r\n */\r\nfunction toMidi(note$1) {\r\n    if (isMidi(note$1)) {\r\n        return +note$1;\r\n    }\r\n    const n = index_es_note(note$1);\r\n    return n.empty ? null : n.midi;\r\n}\r\n/**\r\n * Get the frequency in hertzs from midi number\r\n *\r\n * @param {number} midi - the note midi number\r\n * @param {number} [tuning = 440] - A4 tuning frequency in Hz (440 by default)\r\n * @return {number} the frequency or null if not valid note midi\r\n * @example\r\n * import { midiToFreq} from \'@tonaljs/midi\'\r\n * midiToFreq(69) // => 440\r\n */\r\nfunction midiToFreq(midi, tuning = 440) {\r\n    return Math.pow(2, (midi - 69) / 12) * tuning;\r\n}\r\nconst L2 = Math.log(2);\r\nconst L440 = Math.log(440);\r\n/**\r\n * Get the midi number from a frequency in hertz. The midi number can\r\n * contain decimals (with two digits precission)\r\n *\r\n * @param {number} frequency\r\n * @return {number}\r\n * @example\r\n * import { freqToMidi} from \'@tonaljs/midi\'\r\n * freqToMidi(220)); //=> 57\r\n * freqToMidi(261.62)); //=> 60\r\n * freqToMidi(261)); //=> 59.96\r\n */\r\nfunction freqToMidi(freq) {\r\n    const v = (12 * (Math.log(freq) - L440)) / L2 + 69;\r\n    return Math.round(v * 100) / 100;\r\n}\r\nconst SHARPS = "C C# D D# E F F# G G# A A# B".split(" ");\r\nconst FLATS = "C Db D Eb E F Gb G Ab A Bb B".split(" ");\r\n/**\r\n * Given a midi number, returns a note name. The altered notes will have\r\n * flats unless explicitly set with the optional `useSharps` parameter.\r\n *\r\n * @function\r\n * @param {number} midi - the midi note number\r\n * @param {Object} options = default: `{ sharps: false, pitchClass: false }`\r\n * @param {boolean} useSharps - (Optional) set to true to use sharps instead of flats\r\n * @return {string} the note name\r\n * @example\r\n * import { midiToNoteName } from \'@tonaljs/midi\'\r\n * midiToNoteName(61) // => "Db4"\r\n * midiToNoteName(61, { pitchClass: true }) // => "Db"\r\n * midiToNoteName(61, { sharps: true }) // => "C#4"\r\n * midiToNoteName(61, { pitchClass: true, sharps: true }) // => "C#"\r\n * // it rounds to nearest note\r\n * midiToNoteName(61.7) // => "D4"\r\n */\r\nfunction midiToNoteName(midi, options = {}) {\r\n    if (isNaN(midi) || midi === -Infinity || midi === Infinity)\r\n        return "";\r\n    midi = Math.round(midi);\r\n    const pcs = options.sharps === true ? SHARPS : FLATS;\r\n    const pc = pcs[midi % 12];\r\n    if (options.pitchClass) {\r\n        return pc;\r\n    }\r\n    const o = Math.floor(midi / 12) - 1;\r\n    return pc + o;\r\n}\r\nvar midi_dist_index_es_index = { isMidi, toMidi, midiToFreq, midiToNoteName, freqToMidi };\n\n/* harmony default export */ var midi_dist_index_es = (midi_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/note/dist/index.es.js\n\n\n\nconst NAMES = ["C", "D", "E", "F", "G", "A", "B"];\r\nconst toName = (n) => n.name;\r\nconst onlyNotes = (array) => array.map(index_es_note).filter((n) => !n.empty);\r\n/**\r\n * Return the natural note names without octave\r\n * @function\r\n * @example\r\n * Note.names(); // => ["C", "D", "E", "F", "G", "A", "B"]\r\n */\r\nfunction note_dist_index_es_names(array) {\r\n    if (array === undefined) {\r\n        return NAMES.slice();\r\n    }\r\n    else if (!Array.isArray(array)) {\r\n        return [];\r\n    }\r\n    else {\r\n        return onlyNotes(array).map(toName);\r\n    }\r\n}\r\n/**\r\n * Get a note from a note name\r\n *\r\n * @function\r\n * @example\r\n * Note.get(\'Bb4\') // => { name: "Bb4", midi: 70, chroma: 10, ... }\r\n */\r\nconst note_dist_index_es_get = index_es_note;\r\n/**\r\n * Get the note name\r\n * @function\r\n */\r\nconst dist_index_es_name = (note) => note_dist_index_es_get(note).name;\r\n/**\r\n * Get the note pitch class name\r\n * @function\r\n */\r\nconst pitchClass = (note) => note_dist_index_es_get(note).pc;\r\n/**\r\n * Get the note accidentals\r\n * @function\r\n */\r\nconst accidentals = (note) => note_dist_index_es_get(note).acc;\r\n/**\r\n * Get the note octave\r\n * @function\r\n */\r\nconst index_es_octave = (note) => note_dist_index_es_get(note).oct;\r\n/**\r\n * Get the note midi\r\n * @function\r\n */\r\nconst index_es_midi = (note) => note_dist_index_es_get(note).midi;\r\n/**\r\n * Get the note midi\r\n * @function\r\n */\r\nconst index_es_freq = (note) => note_dist_index_es_get(note).freq;\r\n/**\r\n * Get the note chroma\r\n * @function\r\n */\r\nconst dist_index_es_chroma = (note) => note_dist_index_es_get(note).chroma;\r\n/**\r\n * Given a midi number, returns a note name. Uses flats for altered notes.\r\n *\r\n * @function\r\n * @param {number} midi - the midi note number\r\n * @return {string} the note name\r\n * @example\r\n * Note.fromMidi(61) // => "Db4"\r\n * Note.fromMidi(61.7) // => "D4"\r\n */\r\nfunction fromMidi(midi) {\r\n    return midiToNoteName(midi);\r\n}\r\n/**\r\n * Given a midi number, returns a note name. Uses flats for altered notes.\r\n */\r\nfunction fromFreq(freq) {\r\n    return midiToNoteName(freqToMidi(freq));\r\n}\r\n/**\r\n * Given a midi number, returns a note name. Uses flats for altered notes.\r\n */\r\nfunction fromFreqSharps(freq) {\r\n    return midiToNoteName(freqToMidi(freq), { sharps: true });\r\n}\r\n/**\r\n * Given a midi number, returns a note name. Uses flats for altered notes.\r\n *\r\n * @function\r\n * @param {number} midi - the midi note number\r\n * @return {string} the note name\r\n * @example\r\n * Note.fromMidiSharps(61) // => "C#4"\r\n */\r\nfunction fromMidiSharps(midi) {\r\n    return midiToNoteName(midi, { sharps: true });\r\n}\r\n/**\r\n * Transpose a note by an interval\r\n */\r\nconst note_dist_index_es_transpose = transpose;\r\nconst tr = transpose;\r\n/**\r\n * Transpose by an interval.\r\n * @function\r\n * @param {string} interval\r\n * @return {function} a function that transposes by the given interval\r\n * @example\r\n * ["C", "D", "E"].map(Note.transposeBy("5P"));\r\n * // => ["G", "A", "B"]\r\n */\r\nconst transposeBy = (interval) => (note) => note_dist_index_es_transpose(note, interval);\r\nconst trBy = transposeBy;\r\n/**\r\n * Transpose from a note\r\n * @function\r\n * @param {string} note\r\n * @return {function}  a function that transposes the the note by an interval\r\n * ["1P", "3M", "5P"].map(Note.transposeFrom("C"));\r\n * // => ["C", "E", "G"]\r\n */\r\nconst transposeFrom = (note) => (interval) => note_dist_index_es_transpose(note, interval);\r\nconst trFrom = transposeFrom;\r\n/**\r\n * Transpose a note by a number of perfect fifths.\r\n *\r\n * @function\r\n * @param {string} note - the note name\r\n * @param {number} fifhts - the number of fifths\r\n * @return {string} the transposed note name\r\n *\r\n * @example\r\n * import { transposeFifths } from "@tonaljs/note"\r\n * transposeFifths("G4", 1) // => "D"\r\n * [0, 1, 2, 3, 4].map(fifths => transposeFifths("C", fifths)) // => ["C", "G", "D", "A", "E"]\r\n */\r\nfunction index_es_transposeFifths(noteName, fifths) {\r\n    const note = note_dist_index_es_get(noteName);\r\n    if (note.empty) {\r\n        return "";\r\n    }\r\n    const [nFifths, nOcts] = note.coord;\r\n    const transposed = nOcts === undefined\r\n        ? coordToNote([nFifths + fifths])\r\n        : coordToNote([nFifths + fifths, nOcts]);\r\n    return transposed.name;\r\n}\r\nconst trFifths = index_es_transposeFifths;\r\nconst ascending = (a, b) => a.height - b.height;\r\nconst descending = (a, b) => b.height - a.height;\r\nfunction sortedNames(notes, comparator) {\r\n    comparator = comparator || ascending;\r\n    return onlyNotes(notes).sort(comparator).map(toName);\r\n}\r\nfunction sortedUniqNames(notes) {\r\n    return sortedNames(notes, ascending).filter((n, i, a) => i === 0 || n !== a[i - 1]);\r\n}\r\n/**\r\n * Simplify a note\r\n *\r\n * @function\r\n * @param {string} note - the note to be simplified\r\n * - sameAccType: default true. Use same kind of accidentals that source\r\n * @return {string} the simplified note or \'\' if not valid note\r\n * @example\r\n * simplify("C##") // => "D"\r\n * simplify("C###") // => "D#"\r\n * simplify("C###")\r\n * simplify("B#4") // => "C5"\r\n */\r\nconst index_es_simplify = (noteName) => {\r\n    const note = note_dist_index_es_get(noteName);\r\n    if (note.empty) {\r\n        return "";\r\n    }\r\n    return midiToNoteName(note.midi || note.chroma, {\r\n        sharps: note.alt > 0,\r\n        pitchClass: note.midi === null,\r\n    });\r\n};\r\n/**\r\n * Get enharmonic of a note\r\n *\r\n * @function\r\n * @param {string} note\r\n * @param [string] - [optional] Destination pitch class\r\n * @return {string} the enharmonic note name or \'\' if not valid note\r\n * @example\r\n * Note.enharmonic("Db") // => "C#"\r\n * Note.enharmonic("C") // => "C"\r\n * Note.enharmonic("F2","E#") // => "E#2"\r\n */\r\nfunction enharmonic(noteName, destName) {\r\n    const src = note_dist_index_es_get(noteName);\r\n    if (src.empty) {\r\n        return "";\r\n    }\r\n    // destination: use given or generate one\r\n    const dest = note_dist_index_es_get(destName ||\r\n        midiToNoteName(src.midi || src.chroma, {\r\n            sharps: src.alt < 0,\r\n            pitchClass: true,\r\n        }));\r\n    // ensure destination is valid\r\n    if (dest.empty || dest.chroma !== src.chroma) {\r\n        return "";\r\n    }\r\n    // if src has no octave, no need to calculate anything else\r\n    if (src.oct === undefined) {\r\n        return dest.pc;\r\n    }\r\n    // detect any octave overflow\r\n    const srcChroma = src.chroma - src.alt;\r\n    const destChroma = dest.chroma - dest.alt;\r\n    const destOctOffset = srcChroma > 11 || destChroma < 0\r\n        ? -1\r\n        : srcChroma < 0 || destChroma > 11\r\n            ? +1\r\n            : 0;\r\n    // calculate the new octave\r\n    const destOct = src.oct + destOctOffset;\r\n    return dest.pc + destOct;\r\n}\r\nvar note_dist_index_es_index = {\r\n    names: note_dist_index_es_names,\r\n    get: note_dist_index_es_get,\r\n    name: dist_index_es_name,\r\n    pitchClass,\r\n    accidentals,\r\n    octave: index_es_octave,\r\n    midi: index_es_midi,\r\n    ascending,\r\n    descending,\r\n    sortedNames,\r\n    sortedUniqNames,\r\n    fromMidi,\r\n    fromMidiSharps,\r\n    freq: index_es_freq,\r\n    fromFreq,\r\n    fromFreqSharps,\r\n    chroma: dist_index_es_chroma,\r\n    transpose: note_dist_index_es_transpose,\r\n    tr,\r\n    transposeBy,\r\n    trBy,\r\n    transposeFrom,\r\n    trFrom,\r\n    transposeFifths: index_es_transposeFifths,\r\n    trFifths,\r\n    simplify: index_es_simplify,\r\n    enharmonic,\r\n};\n\n/* harmony default export */ var note_dist_index_es = (note_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/roman-numeral/dist/index.es.js\n\n\nconst NoRomanNumeral = { empty: true, name: "", chordType: "" };\r\nconst roman_numeral_dist_index_es_cache = {};\r\n/**\r\n * Get properties of a roman numeral string\r\n *\r\n * @function\r\n * @param {string} - the roman numeral string (can have type, like: Imaj7)\r\n * @return {Object} - the roman numeral properties\r\n * @param {string} name - the roman numeral (tonic)\r\n * @param {string} type - the chord type\r\n * @param {string} num - the number (1 = I, 2 = II...)\r\n * @param {boolean} major - major or not\r\n *\r\n * @example\r\n * romanNumeral("VIIb5") // => { name: "VII", type: "b5", num: 7, major: true }\r\n */\r\nfunction roman_numeral_dist_index_es_get(src) {\r\n    return typeof src === "string"\r\n        ? roman_numeral_dist_index_es_cache[src] || (roman_numeral_dist_index_es_cache[src] = index_es_parse(src))\r\n        : typeof src === "number"\r\n            ? roman_numeral_dist_index_es_get(index_es_NAMES[src] || "")\r\n            : isPitch(src)\r\n                ? fromPitch(src)\r\n                : isNamed(src)\r\n                    ? roman_numeral_dist_index_es_get(src.name)\r\n                    : NoRomanNumeral;\r\n}\r\nconst romanNumeral = deprecate("RomanNumeral.romanNumeral", "RomanNumeral.get", roman_numeral_dist_index_es_get);\r\n/**\r\n * Get roman numeral names\r\n *\r\n * @function\r\n * @param {boolean} [isMajor=true]\r\n * @return {Array<String>}\r\n *\r\n * @example\r\n * names() // => ["I", "II", "III", "IV", "V", "VI", "VII"]\r\n */\r\nfunction roman_numeral_dist_index_es_names(major = true) {\r\n    return (major ? index_es_NAMES : NAMES_MINOR).slice();\r\n}\r\nfunction fromPitch(pitch) {\r\n    return roman_numeral_dist_index_es_get(altToAcc(pitch.alt) + index_es_NAMES[pitch.step]);\r\n}\r\nconst roman_numeral_dist_index_es_REGEX = /^(#{1,}|b{1,}|x{1,}|)(IV|I{1,3}|VI{0,2}|iv|i{1,3}|vi{0,2})([^IViv]*)$/;\r\nfunction dist_index_es_tokenize(str) {\r\n    return (roman_numeral_dist_index_es_REGEX.exec(str) || ["", "", "", ""]);\r\n}\r\nconst ROMANS = "I II III IV V VI VII";\r\nconst index_es_NAMES = ROMANS.split(" ");\r\nconst NAMES_MINOR = ROMANS.toLowerCase().split(" ");\r\nfunction index_es_parse(src) {\r\n    const [name, acc, roman, chordType] = dist_index_es_tokenize(src);\r\n    if (!roman) {\r\n        return NoRomanNumeral;\r\n    }\r\n    const upperRoman = roman.toUpperCase();\r\n    const step = index_es_NAMES.indexOf(upperRoman);\r\n    const alt = accToAlt(acc);\r\n    const dir = 1;\r\n    return {\r\n        empty: false,\r\n        name,\r\n        roman,\r\n        interval: index_es_interval({ step, alt, dir }).name,\r\n        acc,\r\n        chordType,\r\n        alt,\r\n        step,\r\n        major: roman === upperRoman,\r\n        oct: 0,\r\n        dir,\r\n    };\r\n}\r\nvar roman_numeral_dist_index_es_index = {\r\n    names: roman_numeral_dist_index_es_names,\r\n    get: roman_numeral_dist_index_es_get,\r\n    // deprecated\r\n    romanNumeral,\r\n};\n\n/* harmony default export */ var roman_numeral_dist_index_es = (roman_numeral_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/key/dist/index.es.js\n\n\n\n\nconst Empty = Object.freeze([]);\r\nconst NoKey = {\r\n    type: "major",\r\n    tonic: "",\r\n    alteration: 0,\r\n    keySignature: "",\r\n};\r\nconst NoKeyScale = {\r\n    tonic: "",\r\n    grades: Empty,\r\n    intervals: Empty,\r\n    scale: Empty,\r\n    chords: Empty,\r\n    chordsHarmonicFunction: Empty,\r\n    chordScales: Empty,\r\n};\r\nconst NoMajorKey = {\r\n    ...NoKey,\r\n    ...NoKeyScale,\r\n    type: "major",\r\n    minorRelative: "",\r\n    scale: Empty,\r\n    secondaryDominants: Empty,\r\n    secondaryDominantsMinorRelative: Empty,\r\n    substituteDominants: Empty,\r\n    substituteDominantsMinorRelative: Empty,\r\n};\r\nconst NoMinorKey = {\r\n    ...NoKey,\r\n    type: "minor",\r\n    relativeMajor: "",\r\n    natural: NoKeyScale,\r\n    harmonic: NoKeyScale,\r\n    melodic: NoKeyScale,\r\n};\r\nconst mapScaleToType = (scale, list, sep = "") => list.map((type, i) => `${scale[i]}${sep}${type}`);\r\nfunction index_es_keyScale(grades, chords, harmonicFunctions, chordScales) {\r\n    return (tonic) => {\r\n        const intervals = grades.map((gr) => roman_numeral_dist_index_es_get(gr).interval || "");\r\n        const scale = intervals.map((interval) => transpose(tonic, interval));\r\n        return {\r\n            tonic,\r\n            grades,\r\n            intervals,\r\n            scale,\r\n            chords: mapScaleToType(scale, chords),\r\n            chordsHarmonicFunction: harmonicFunctions.slice(),\r\n            chordScales: mapScaleToType(scale, chordScales, " "),\r\n        };\r\n    };\r\n}\r\nconst distInFifths = (from, to) => {\r\n    const f = index_es_note(from);\r\n    const t = index_es_note(to);\r\n    return f.empty || t.empty ? 0 : t.coord[0] - f.coord[0];\r\n};\r\nconst MajorScale = index_es_keyScale("I II III IV V VI VII".split(" "), "maj7 m7 m7 maj7 7 m7 m7b5".split(" "), "T SD T SD D T D".split(" "), "major,dorian,phrygian,lydian,mixolydian,minor,locrian".split(","));\r\nconst NaturalScale = index_es_keyScale("I II bIII IV V bVI bVII".split(" "), "m7 m7b5 maj7 m7 m7 maj7 7".split(" "), "T SD T SD D SD SD".split(" "), "minor,locrian,major,dorian,phrygian,lydian,mixolydian".split(","));\r\nconst HarmonicScale = index_es_keyScale("I II bIII IV V bVI VII".split(" "), "mMaj7 m7b5 +maj7 m7 7 maj7 o7".split(" "), "T SD T SD D SD D".split(" "), "harmonic minor,locrian 6,major augmented,lydian diminished,phrygian dominant,lydian #9,ultralocrian".split(","));\r\nconst MelodicScale = index_es_keyScale("I II bIII IV V VI VII".split(" "), "m6 m7 +maj7 7 7 m7b5 m7b5".split(" "), "T SD T SD D  ".split(" "), "melodic minor,dorian b2,lydian augmented,lydian dominant,mixolydian b6,locrian #2,altered".split(","));\r\n/**\r\n * Get a major key properties in a given tonic\r\n * @param tonic\r\n */\r\nfunction majorKey(tonic) {\r\n    const pc = index_es_note(tonic).pc;\r\n    if (!pc)\r\n        return NoMajorKey;\r\n    const keyScale = MajorScale(pc);\r\n    const alteration = distInFifths("C", pc);\r\n    const romanInTonic = (src) => {\r\n        const r = roman_numeral_dist_index_es_get(src);\r\n        if (r.empty)\r\n            return "";\r\n        return transpose(tonic, r.interval) + r.chordType;\r\n    };\r\n    return {\r\n        ...keyScale,\r\n        type: "major",\r\n        minorRelative: transpose(pc, "-3m"),\r\n        alteration,\r\n        keySignature: altToAcc(alteration),\r\n        secondaryDominants: "- VI7 VII7 I7 II7 III7 -".split(" ").map(romanInTonic),\r\n        secondaryDominantsMinorRelative: "- IIIm7b5 IV#m7 Vm7 VIm7 VIIm7b5 -"\r\n            .split(" ")\r\n            .map(romanInTonic),\r\n        substituteDominants: "- bIII7 IV7 bV7 bVI7 bVII7 -"\r\n            .split(" ")\r\n            .map(romanInTonic),\r\n        substituteDominantsMinorRelative: "- IIIm7 Im7 IIbm7 VIm7 IVm7 -"\r\n            .split(" ")\r\n            .map(romanInTonic),\r\n    };\r\n}\r\n/**\r\n * Get minor key properties in a given tonic\r\n * @param tonic\r\n */\r\nfunction minorKey(tnc) {\r\n    const pc = index_es_note(tnc).pc;\r\n    if (!pc)\r\n        return NoMinorKey;\r\n    const alteration = distInFifths("C", pc) - 3;\r\n    return {\r\n        type: "minor",\r\n        tonic: pc,\r\n        relativeMajor: transpose(pc, "3m"),\r\n        alteration,\r\n        keySignature: altToAcc(alteration),\r\n        natural: NaturalScale(pc),\r\n        harmonic: HarmonicScale(pc),\r\n        melodic: MelodicScale(pc),\r\n    };\r\n}\r\n/**\r\n * Given a key signature, returns the tonic of the major key\r\n * @param sigature\r\n * @example\r\n * majorTonicFromKeySignature(\'###\') // => \'A\'\r\n */\r\nfunction majorTonicFromKeySignature(sig) {\r\n    if (typeof sig === "number") {\r\n        return index_es_transposeFifths("C", sig);\r\n    }\r\n    else if (typeof sig === "string" && /^b+|#+$/.test(sig)) {\r\n        return index_es_transposeFifths("C", accToAlt(sig));\r\n    }\r\n    return null;\r\n}\r\nvar key_dist_index_es_index = { majorKey, majorTonicFromKeySignature, minorKey };\n\n/* harmony default export */ var key_dist_index_es = (key_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/mode/dist/index.es.js\n\n\n\n\n\nconst MODES = [\r\n    [0, 2773, 0, "ionian", "", "Maj7", "major"],\r\n    [1, 2902, 2, "dorian", "m", "m7"],\r\n    [2, 3418, 4, "phrygian", "m", "m7"],\r\n    [3, 2741, -1, "lydian", "", "Maj7"],\r\n    [4, 2774, 1, "mixolydian", "", "7"],\r\n    [5, 2906, 3, "aeolian", "m", "m7", "minor"],\r\n    [6, 3434, 5, "locrian", "dim", "m7b5"],\r\n];\r\nconst NoMode = {\r\n    ...EmptyPcset,\r\n    name: "",\r\n    alt: 0,\r\n    modeNum: NaN,\r\n    triad: "",\r\n    seventh: "",\r\n    aliases: [],\r\n};\r\nconst index_es_modes = MODES.map(toMode);\r\nconst mode_dist_index_es_index = {};\r\nindex_es_modes.forEach((mode) => {\r\n    mode_dist_index_es_index[mode.name] = mode;\r\n    mode.aliases.forEach((alias) => {\r\n        mode_dist_index_es_index[alias] = mode;\r\n    });\r\n});\r\n/**\r\n * Get a Mode by it\'s name\r\n *\r\n * @example\r\n * get(\'dorian\')\r\n * // =>\r\n * // {\r\n * //   intervals: [ \'1P\', \'2M\', \'3m\', \'4P\', \'5P\', \'6M\', \'7m\' ],\r\n * //   modeNum: 1,\r\n * //   chroma: \'101101010110\',\r\n * //   normalized: \'101101010110\',\r\n * //   name: \'dorian\',\r\n * //   setNum: 2902,\r\n * //   alt: 2,\r\n * //   triad: \'m\',\r\n * //   seventh: \'m7\',\r\n * //   aliases: []\r\n * // }\r\n */\r\nfunction mode_dist_index_es_get(name) {\r\n    return typeof name === "string"\r\n        ? mode_dist_index_es_index[name.toLowerCase()] || NoMode\r\n        : name && name.name\r\n            ? mode_dist_index_es_get(name.name)\r\n            : NoMode;\r\n}\r\nconst index_es_mode = deprecate("Mode.mode", "Mode.get", mode_dist_index_es_get);\r\n/**\r\n * Get a list of all modes\r\n */\r\nfunction mode_dist_index_es_all() {\r\n    return index_es_modes.slice();\r\n}\r\nconst dist_index_es_entries = deprecate("Mode.mode", "Mode.all", mode_dist_index_es_all);\r\n/**\r\n * Get a list of all mode names\r\n */\r\nfunction mode_dist_index_es_names() {\r\n    return index_es_modes.map((mode) => mode.name);\r\n}\r\nfunction toMode(mode) {\r\n    const [modeNum, setNum, alt, name, triad, seventh, alias] = mode;\r\n    const aliases = alias ? [alias] : [];\r\n    const chroma = Number(setNum).toString(2);\r\n    const intervals = chromaToIntervals(chroma);\r\n    return {\r\n        empty: false,\r\n        intervals,\r\n        modeNum,\r\n        chroma,\r\n        normalized: chroma,\r\n        name,\r\n        setNum,\r\n        alt,\r\n        triad,\r\n        seventh,\r\n        aliases,\r\n    };\r\n}\r\nfunction index_es_notes(modeName, tonic) {\r\n    return mode_dist_index_es_get(modeName).intervals.map((ivl) => transpose(tonic, ivl));\r\n}\r\nfunction index_es_chords(chords) {\r\n    return (modeName, tonic) => {\r\n        const mode = mode_dist_index_es_get(modeName);\r\n        if (mode.empty)\r\n            return [];\r\n        const triads = index_es_rotate(mode.modeNum, chords);\r\n        const tonics = mode.intervals.map((i) => transpose(tonic, i));\r\n        return triads.map((triad, i) => tonics[i] + triad);\r\n    };\r\n}\r\nconst index_es_triads = index_es_chords(MODES.map((x) => x[4]));\r\nconst seventhChords = index_es_chords(MODES.map((x) => x[5]));\r\nfunction mode_dist_index_es_distance(destination, source) {\r\n    const from = mode_dist_index_es_get(source);\r\n    const to = mode_dist_index_es_get(destination);\r\n    if (from.empty || to.empty)\r\n        return "";\r\n    return simplify(transposeFifths("1P", to.alt - from.alt));\r\n}\r\nfunction relativeTonic(destination, source, tonic) {\r\n    return transpose(tonic, mode_dist_index_es_distance(destination, source));\r\n}\r\nvar dist_index_es_index$1 = {\r\n    get: mode_dist_index_es_get,\r\n    names: mode_dist_index_es_names,\r\n    all: mode_dist_index_es_all,\r\n    distance: mode_dist_index_es_distance,\r\n    relativeTonic,\r\n    notes: index_es_notes,\r\n    triads: index_es_triads,\r\n    seventhChords,\r\n    // deprecated\r\n    entries: dist_index_es_entries,\r\n    mode: index_es_mode,\r\n};\n\n/* harmony default export */ var mode_dist_index_es = (dist_index_es_index$1);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/progression/dist/index.es.js\n\n\n\n\n/**\r\n * Given a tonic and a chord list expressed with roman numeral notation\r\n * returns the progression expressed with leadsheet chords symbols notation\r\n * @example\r\n * fromRomanNumerals("C", ["I", "IIm7", "V7"]);\r\n * // => ["C", "Dm7", "G7"]\r\n */\r\nfunction fromRomanNumerals(tonic, chords) {\r\n    const romanNumerals = chords.map(roman_numeral_dist_index_es_get);\r\n    return romanNumerals.map((rn) => transpose(tonic, index_es_interval(rn)) + rn.chordType);\r\n}\r\n/**\r\n * Given a tonic and a chord list with leadsheet symbols notation,\r\n * return the chord list with roman numeral notation\r\n * @example\r\n * toRomanNumerals("C", ["CMaj7", "Dm7", "G7"]);\r\n * // => ["IMaj7", "IIm7", "V7"]\r\n */\r\nfunction toRomanNumerals(tonic, chords) {\r\n    return chords.map((chord) => {\r\n        const [note, chordType] = index_es_tokenize(chord);\r\n        const intervalName = distance(tonic, note);\r\n        const roman = roman_numeral_dist_index_es_get(index_es_interval(intervalName));\r\n        return roman.name + chordType;\r\n    });\r\n}\r\nvar progression_dist_index_es_index = { fromRomanNumerals, toRomanNumerals };\n\n/* harmony default export */ var progression_dist_index_es = (progression_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/range/dist/index.es.js\n\n\n\n/**\r\n * Create a numeric range. You supply a list of notes or numbers and it will\r\n * be connected to create complex ranges.\r\n *\r\n * @param {Array} notes - the list of notes or midi numbers used\r\n * @return {Array} an array of numbers or empty array if not valid parameters\r\n *\r\n * @example\r\n * numeric(["C5", "C4"]) // => [ 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60 ]\r\n * // it works midi notes\r\n * numeric([10, 5]) // => [ 10, 9, 8, 7, 6, 5 ]\r\n * // complex range\r\n * numeric(["C4", "E4", "Bb3"]) // => [60, 61, 62, 63, 64, 63, 62, 61, 60, 59, 58]\r\n */\r\nfunction numeric(notes) {\r\n    const midi = index_es_compact(notes.map(toMidi));\r\n    if (!notes.length || midi.length !== notes.length) {\r\n        // there is no valid notes\r\n        return [];\r\n    }\r\n    return midi.reduce((result, note) => {\r\n        const last = result[result.length - 1];\r\n        return result.concat(index_es_range(last, note).slice(1));\r\n    }, [midi[0]]);\r\n}\r\n/**\r\n * Create a range of chromatic notes. The altered notes will use flats.\r\n *\r\n * @function\r\n * @param {Array} notes - the list of notes or midi note numbers to create a range from\r\n * @param {Object} options - The same as `midiToNoteName` (`{ sharps: boolean, pitchClass: boolean }`)\r\n * @return {Array} an array of note names\r\n *\r\n * @example\r\n * Range.chromatic(["C2, "E2", "D2"]) // => ["C2", "Db2", "D2", "Eb2", "E2", "Eb2", "D2"]\r\n * // with sharps\r\n * Range.chromatic(["C2", "C3"], { sharps: true }) // => [ "C2", "C#2", "D2", "D#2", "E2", "F2", "F#2", "G2", "G#2", "A2", "A#2", "B2", "C3" ]\r\n */\r\nfunction chromatic(notes, options) {\r\n    return numeric(notes).map((midi) => midiToNoteName(midi, options));\r\n}\r\nvar range_dist_index_es_index = { numeric, chromatic };\n\n/* harmony default export */ var range_dist_index_es = (range_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/scale/dist/index.es.js\n\n\n\n\n\n\n\n/**\r\n * References:\r\n * - https://www.researchgate.net/publication/327567188_An_Algorithm_for_Spelling_the_Pitches_of_Any_Musical_Scale\r\n * @module scale\r\n */\r\nconst NoScale = {\r\n    empty: true,\r\n    name: "",\r\n    type: "",\r\n    tonic: null,\r\n    setNum: NaN,\r\n    chroma: "",\r\n    normalized: "",\r\n    aliases: [],\r\n    notes: [],\r\n    intervals: [],\r\n};\r\n/**\r\n * Given a string with a scale name and (optionally) a tonic, split\r\n * that components.\r\n *\r\n * It retuns an array with the form [ name, tonic ] where tonic can be a\r\n * note name or null and name can be any arbitrary string\r\n * (this function doesn"t check if that scale name exists)\r\n *\r\n * @function\r\n * @param {string} name - the scale name\r\n * @return {Array} an array [tonic, name]\r\n * @example\r\n * tokenize("C mixolydean") // => ["C", "mixolydean"]\r\n * tokenize("anything is valid") // => ["", "anything is valid"]\r\n * tokenize() // => ["", ""]\r\n */\r\nfunction scale_dist_index_es_tokenize(name) {\r\n    if (typeof name !== "string") {\r\n        return ["", ""];\r\n    }\r\n    const i = name.indexOf(" ");\r\n    const tonic = index_es_note(name.substring(0, i));\r\n    if (tonic.empty) {\r\n        const n = index_es_note(name);\r\n        return n.empty ? ["", name] : [n.name, ""];\r\n    }\r\n    const type = name.substring(tonic.name.length + 1);\r\n    return [tonic.name, type.length ? type : ""];\r\n}\r\n/**\r\n * Get all scale names\r\n * @function\r\n */\r\nconst scale_dist_index_es_names = dist_index_es_names;\r\n/**\r\n * Get a Scale from a scale name.\r\n */\r\nfunction scale_dist_index_es_get(src) {\r\n    const tokens = Array.isArray(src) ? src : scale_dist_index_es_tokenize(src);\r\n    const tonic = index_es_note(tokens[0]).name;\r\n    const st = dist_index_es_get(tokens[1]);\r\n    if (st.empty) {\r\n        return NoScale;\r\n    }\r\n    const type = st.name;\r\n    const notes = tonic\r\n        ? st.intervals.map((i) => transpose(tonic, i))\r\n        : [];\r\n    const name = tonic ? tonic + " " + type : type;\r\n    return { ...st, name, type, tonic, notes };\r\n}\r\nconst index_es_scale = deprecate("Scale.scale", "Scale.get", scale_dist_index_es_get);\r\n/**\r\n * Get all chords that fits a given scale\r\n *\r\n * @function\r\n * @param {string} name - the scale name\r\n * @return {Array<string>} - the chord names\r\n *\r\n * @example\r\n * scaleChords("pentatonic") // => ["5", "64", "M", "M6", "Madd9", "Msus2"]\r\n */\r\nfunction scaleChords(name) {\r\n    const s = scale_dist_index_es_get(name);\r\n    const inScale = isSubsetOf(s.chroma);\r\n    return index_es_all()\r\n        .filter((chord) => inScale(chord.chroma))\r\n        .map((chord) => chord.aliases[0]);\r\n}\r\n/**\r\n * Get all scales names that are a superset of the given one\r\n * (has the same notes and at least one more)\r\n *\r\n * @function\r\n * @param {string} name\r\n * @return {Array} a list of scale names\r\n * @example\r\n * extended("major") // => ["bebop", "bebop dominant", "bebop major", "chromatic", "ichikosucho"]\r\n */\r\nfunction index_es_extended(name) {\r\n    const s = scale_dist_index_es_get(name);\r\n    const isSuperset = isSupersetOf(s.chroma);\r\n    return dist_index_es_all()\r\n        .filter((scale) => isSuperset(scale.chroma))\r\n        .map((scale) => scale.name);\r\n}\r\n/**\r\n * Find all scales names that are a subset of the given one\r\n * (has less notes but all from the given scale)\r\n *\r\n * @function\r\n * @param {string} name\r\n * @return {Array} a list of scale names\r\n *\r\n * @example\r\n * reduced("major") // => ["ionian pentatonic", "major pentatonic", "ritusen"]\r\n */\r\nfunction index_es_reduced(name) {\r\n    const isSubset = isSubsetOf(scale_dist_index_es_get(name).chroma);\r\n    return dist_index_es_all()\r\n        .filter((scale) => isSubset(scale.chroma))\r\n        .map((scale) => scale.name);\r\n}\r\n/**\r\n * Given an array of notes, return the scale: a pitch class set starting from\r\n * the first note of the array\r\n *\r\n * @function\r\n * @param {string[]} notes\r\n * @return {string[]} pitch classes with same tonic\r\n * @example\r\n * scaleNotes([\'C4\', \'c3\', \'C5\', \'C4\', \'c4\']) // => ["C"]\r\n * scaleNotes([\'D4\', \'c#5\', \'A5\', \'F#6\']) // => ["D", "F#", "A", "C#"]\r\n */\r\nfunction index_es_scaleNotes(notes) {\r\n    const pcset = notes.map((n) => index_es_note(n).pc).filter((x) => x);\r\n    const tonic = pcset[0];\r\n    const scale = sortedUniqNames(pcset);\r\n    return index_es_rotate(scale.indexOf(tonic), scale);\r\n}\r\n/**\r\n * Find mode names of a scale\r\n *\r\n * @function\r\n * @param {string} name - scale name\r\n * @example\r\n * modeNames("C pentatonic") // => [\r\n *   ["C", "major pentatonic"],\r\n *   ["D", "egyptian"],\r\n *   ["E", "malkos raga"],\r\n *   ["G", "ritusen"],\r\n *   ["A", "minor pentatonic"]\r\n * ]\r\n */\r\nfunction modeNames(name) {\r\n    const s = scale_dist_index_es_get(name);\r\n    if (s.empty) {\r\n        return [];\r\n    }\r\n    const tonics = s.tonic ? s.notes : s.intervals;\r\n    return modes(s.chroma)\r\n        .map((chroma, i) => {\r\n        const modeName = scale_dist_index_es_get(chroma).name;\r\n        return modeName ? [tonics[i], modeName] : ["", ""];\r\n    })\r\n        .filter((x) => x[0]);\r\n}\r\nfunction getNoteNameOf(scale) {\r\n    const names = Array.isArray(scale) ? index_es_scaleNotes(scale) : scale_dist_index_es_get(scale).notes;\r\n    const chromas = names.map((name) => index_es_note(name).chroma);\r\n    return (noteOrMidi) => {\r\n        const height = typeof noteOrMidi === "number" ? noteOrMidi : index_es_note(noteOrMidi).height;\r\n        if (height === undefined)\r\n            return undefined;\r\n        const chroma = height % 12;\r\n        const oct = Math.floor(height / 12) - 1;\r\n        const position = chromas.indexOf(chroma);\r\n        if (position === -1)\r\n            return undefined;\r\n        return names[position] + oct;\r\n    };\r\n}\r\nfunction rangeOf(scale) {\r\n    const getName = getNoteNameOf(scale);\r\n    return (fromNote, toNote) => {\r\n        const from = index_es_note(fromNote).height;\r\n        const to = index_es_note(toNote).height;\r\n        if (from === undefined || to === undefined)\r\n            return [];\r\n        return index_es_range(from, to)\r\n            .map(getName)\r\n            .filter((x) => x);\r\n    };\r\n}\r\nvar scale_dist_index_es_index = {\r\n    get: scale_dist_index_es_get,\r\n    names: scale_dist_index_es_names,\r\n    extended: index_es_extended,\r\n    modeNames,\r\n    reduced: index_es_reduced,\r\n    scaleChords,\r\n    scaleNotes: index_es_scaleNotes,\r\n    tokenize: scale_dist_index_es_tokenize,\r\n    rangeOf,\r\n    // deprecated\r\n    scale: index_es_scale,\r\n};\n\n/* harmony default export */ var scale_dist_index_es = (scale_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/time-signature/dist/index.es.js\n// CONSTANTS\r\nconst NONE = {\r\n    empty: true,\r\n    name: "",\r\n    upper: undefined,\r\n    lower: undefined,\r\n    type: undefined,\r\n    additive: [],\r\n};\r\nconst dist_index_es_NAMES = ["4/4", "3/4", "2/4", "2/2", "12/8", "9/8", "6/8", "3/8"];\r\n// PUBLIC API\r\nfunction time_signature_dist_index_es_names() {\r\n    return dist_index_es_NAMES.slice();\r\n}\r\nconst time_signature_dist_index_es_REGEX = /^(\\d?\\d(?:\\+\\d)*)\\/(\\d)$/;\r\nconst CACHE = new Map();\r\nfunction time_signature_dist_index_es_get(literal) {\r\n    const cached = CACHE.get(literal);\r\n    if (cached) {\r\n        return cached;\r\n    }\r\n    const ts = build(dist_index_es_parse(literal));\r\n    CACHE.set(literal, ts);\r\n    return ts;\r\n}\r\nfunction dist_index_es_parse(literal) {\r\n    if (typeof literal === "string") {\r\n        const [_, up, low] = time_signature_dist_index_es_REGEX.exec(literal) || [];\r\n        return dist_index_es_parse([up, low]);\r\n    }\r\n    const [up, down] = literal;\r\n    const denominator = +down;\r\n    if (typeof up === "number") {\r\n        return [up, denominator];\r\n    }\r\n    const list = up.split("+").map((n) => +n);\r\n    return list.length === 1 ? [list[0], denominator] : [list, denominator];\r\n}\r\nvar time_signature_dist_index_es_index = { names: time_signature_dist_index_es_names, parse: dist_index_es_parse, get: time_signature_dist_index_es_get };\r\n// PRIVATE\r\nfunction build([up, down]) {\r\n    const upper = Array.isArray(up) ? up.reduce((a, b) => a + b, 0) : up;\r\n    const lower = down;\r\n    if (upper === 0 || lower === 0) {\r\n        return NONE;\r\n    }\r\n    const name = Array.isArray(up) ? `${up.join("+")}/${down}` : `${up}/${down}`;\r\n    const additive = Array.isArray(up) ? up : [];\r\n    const type = lower === 4 || lower === 2\r\n        ? "simple"\r\n        : lower === 8 && upper % 3 === 0\r\n            ? "compound"\r\n            : "irregular";\r\n    return {\r\n        empty: false,\r\n        name,\r\n        type,\r\n        upper,\r\n        lower,\r\n        additive,\r\n    };\r\n}\n\n/* harmony default export */ var time_signature_dist_index_es = (time_signature_dist_index_es_index);\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./node_modules/@tonaljs/tonal/dist/index.es.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// deprecated (backwards compatibility)\r\nconst Tonal = index_es_namespaceObject;\r\nconst PcSet = pcset_dist_index_es;\r\nconst ChordDictionary = chord_type_dist_index_es;\r\nconst ScaleDictionary = scale_type_dist_index_es;\n\n\n//# sourceMappingURL=index.es.js.map\n\n// CONCATENATED MODULE: ./src/music.js\n\r\n\r\nclass music_Music {\r\n    constructor(digits) {\r\n        this.digits = digits;\r\n    }\r\n\r\n    getNoteDurations() {\r\n        return [\r\n            {name: \'sixteenth (1/16)\', duration: 1/16},\r\n            {name: \'eighth (1/8)\', duration: 1/8},\r\n            {name: \'quarter (1/4)\', duration: 1/4},\r\n            {name: \'half (1/2)\', duration: 1/2},\r\n            {name: \'dotted half (3/4)\', duration: 3/4},\r\n            {name: \'whole (1)\', duration: 1}\r\n        ];\r\n    }\r\n\r\n    getPianoNotes() {\r\n        // most scales are 7 notes or fewer\r\n        // there are 10 digits in base 10\r\n        // for 7 note scale we\'ll have notes for 7 digits in one octave\r\n        // we need to go into the next octave for notes 8, 9, 10\r\n        // 88 key piano goes from A0 to C8\r\n        // we\'ll make C7 the last selectable tonic knowing we may go slightly out of range of 88 key piano\r\n        const octaves = [1, 2, 3, 4, 5, 6];\r\n        const chromaticScaleLetters = [\'C\', \'C#\', \'D\', \'D#\', \'E\', \'F\', \'F#\', \'G\', \'G#\', \'A\', \'A#\', \'B\']\r\n        const genNotesForOctave = (octave) => chromaticScaleLetters.map(l => {\r\n            const [letter, accidental] = l.split(\'\');\r\n            return letter + (accidental||\'\') + octave;\r\n        });\r\n        return [\r\n            \'A0\', \'A0#\', \'B0\',\r\n            ...octaves.map(o => genNotesForOctave(o)).flat(),\r\n            \'C7\'\r\n        ];\r\n    }\r\n\r\n    getScalesByLength() {\r\n        const scales = new Map();\r\n\r\n        scale_type_dist_index_es.all().forEach(scale => {\r\n            if (! scales.has(scale.intervals.length)) {\r\n                scales.set(scale.intervals.length, []);\r\n            }\r\n            scales.get(scale.intervals.length).push(scale.name);\r\n        });\r\n\r\n        return Array.from(scales.entries());\r\n    }\r\n\r\n    generateNotes(options, start, end) {\r\n        const { scale, tonic: tonic1, beatLengthSeconds, noteDuration } = options;\r\n        const [ t1, t2, t3 ] = tonic1.split(\'\');\r\n        const letter = t1;\r\n        const octave = parseInt(t3 || t2, 10);\r\n        const accidental = t3 ? t2 : \'\';\r\n        const tonic2 = letter + accidental + (octave + 1);\r\n        const scaleNotes = [\r\n            ...scale_dist_index_es.get(tonic1 + \' \' + scale).notes,\r\n            ...scale_dist_index_es.get(tonic2 + \' \' + scale).notes\r\n        ]\r\n            .map(note_dist_index_es.simplify)\r\n            .splice(0, 10);\r\n\r\n        const noteNames = this.digits\r\n            .slice(start, end)\r\n            .map(i => scaleNotes[i]);\r\n\r\n        return noteNames.map((note, i) => (\r\n            {\r\n                digit: this.digits[start + i],\r\n                note,\r\n                velocity: 1,\r\n                time: beatLengthSeconds * (start + i),\r\n                duration: (noteDuration * beatLengthSeconds)\r\n            }\r\n        ));\r\n    }\r\n\r\n}\r\n\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/version.js\nconst version = "14.7.77";\n//# sourceMappingURL=version.js.map\n// EXTERNAL MODULE: ./node_modules/automation-events/build/es5/bundle.js\nvar bundle = __webpack_require__(0);\n\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/abort-error.js\nconst createAbortError = () => new DOMException(\'\', \'AbortError\');\n//# sourceMappingURL=abort-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-active-input-connection-to-audio-node.js\nconst createAddActiveInputConnectionToAudioNode = (insertElementInSet) => {\n    return (activeInputs, source, [output, input, eventListener], ignoreDuplicates) => {\n        insertElementInSet(activeInputs[input], [source, output, eventListener], (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output, ignoreDuplicates);\n    };\n};\n//# sourceMappingURL=add-active-input-connection-to-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-audio-node-connections.js\nconst createAddAudioNodeConnections = (audioNodeConnectionsStore) => {\n    return (audioNode, audioNodeRenderer, nativeAudioNode) => {\n        const activeInputs = [];\n        for (let i = 0; i < nativeAudioNode.numberOfInputs; i += 1) {\n            activeInputs.push(new Set());\n        }\n        audioNodeConnectionsStore.set(audioNode, {\n            activeInputs,\n            outputs: new Set(),\n            passiveInputs: new WeakMap(),\n            renderer: audioNodeRenderer\n        });\n    };\n};\n//# sourceMappingURL=add-audio-node-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-audio-param-connections.js\nconst createAddAudioParamConnections = (audioParamConnectionsStore) => {\n    return (audioParam, audioParamRenderer) => {\n        audioParamConnectionsStore.set(audioParam, { activeInputs: new Set(), passiveInputs: new WeakMap(), renderer: audioParamRenderer });\n    };\n};\n//# sourceMappingURL=add-audio-param-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/globals.js\nconst ACTIVE_AUDIO_NODE_STORE = new WeakSet();\nconst AUDIO_NODE_CONNECTIONS_STORE = new WeakMap();\nconst AUDIO_NODE_STORE = new WeakMap();\nconst AUDIO_PARAM_CONNECTIONS_STORE = new WeakMap();\nconst AUDIO_PARAM_STORE = new WeakMap();\nconst CONTEXT_STORE = new WeakMap();\nconst EVENT_LISTENERS = new WeakMap();\nconst CYCLE_COUNTERS = new WeakMap();\n// This clunky name is borrowed from the spec. :-)\nconst NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS = new WeakMap();\nconst NODE_TO_PROCESSOR_MAPS = new WeakMap();\n//# sourceMappingURL=globals.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-constructible.js\nconst handler = {\n    construct() {\n        return handler;\n    }\n};\nconst isConstructible = (constructible) => {\n    try {\n        const proxy = new Proxy(constructible, handler);\n        new proxy(); // tslint:disable-line:no-unused-expression\n    }\n    catch {\n        return false;\n    }\n    return true;\n};\n//# sourceMappingURL=is-constructible.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/split-import-statements.js\n/*\n * This massive regex tries to cover all the following cases.\n *\n * import \'./path\';\n * import defaultImport from \'./path\';\n * import { namedImport } from \'./path\';\n * import { namedImport as renamendImport } from \'./path\';\n * import * as namespaceImport from \'./path\';\n * import defaultImport, { namedImport } from \'./path\';\n * import defaultImport, { namedImport as renamendImport } from \'./path\';\n * import defaultImport, * as namespaceImport from \'./path\';\n */\nconst IMPORT_STATEMENT_REGEX = /^import(?:(?:[\\s]+[\\w]+|(?:[\\s]+[\\w]+[\\s]*,)?[\\s]*\\{[\\s]*[\\w]+(?:[\\s]+as[\\s]+[\\w]+)?(?:[\\s]*,[\\s]*[\\w]+(?:[\\s]+as[\\s]+[\\w]+)?)*[\\s]*}|(?:[\\s]+[\\w]+[\\s]*,)?[\\s]*\\*[\\s]+as[\\s]+[\\w]+)[\\s]+from)?(?:[\\s]*)("([^"\\\\]|\\\\.)+"|\'([^\'\\\\]|\\\\.)+\')(?:[\\s]*);?/; // tslint:disable-line:max-line-length\nconst splitImportStatements = (source, url) => {\n    const importStatements = [];\n    let sourceWithoutImportStatements = source.replace(/^[\\s]+/, \'\');\n    let result = sourceWithoutImportStatements.match(IMPORT_STATEMENT_REGEX);\n    while (result !== null) {\n        const unresolvedUrl = result[1].slice(1, -1);\n        const importStatementWithResolvedUrl = result[0]\n            .replace(/([\\s]+)?;?$/, \'\')\n            .replace(unresolvedUrl, new URL(unresolvedUrl, url).toString());\n        importStatements.push(importStatementWithResolvedUrl);\n        sourceWithoutImportStatements = sourceWithoutImportStatements.slice(result[0].length).replace(/^[\\s]+/, \'\');\n        result = sourceWithoutImportStatements.match(IMPORT_STATEMENT_REGEX);\n    }\n    return [importStatements.join(\';\'), sourceWithoutImportStatements];\n};\n//# sourceMappingURL=split-import-statements.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-audio-worklet-module.js\n\n\n\nconst verifyParameterDescriptors = (parameterDescriptors) => {\n    if (parameterDescriptors !== undefined && !Array.isArray(parameterDescriptors)) {\n        throw new TypeError(\'The parameterDescriptors property of given value for processorCtor is not an array.\');\n    }\n};\nconst verifyProcessorCtor = (processorCtor) => {\n    if (!isConstructible(processorCtor)) {\n        throw new TypeError(\'The given value for processorCtor should be a constructor.\');\n    }\n    if (processorCtor.prototype === null || typeof processorCtor.prototype !== \'object\') {\n        throw new TypeError(\'The given value for processorCtor should have a prototype.\');\n    }\n};\nconst createAddAudioWorkletModule = (cacheTestResult, createNotSupportedError, evaluateSource, exposeCurrentFrameAndCurrentTime, fetchSource, getNativeContext, getOrCreateBackupOfflineAudioContext, isNativeOfflineAudioContext, ongoingRequests, resolvedRequests, testAudioWorkletProcessorPostMessageSupport, window) => {\n    return (context, moduleURL, options = { credentials: \'omit\' }) => {\n        const nativeContext = getNativeContext(context);\n        // Bug #59: Safari does not implement the audioWorklet property.\n        if (nativeContext.audioWorklet !== undefined) {\n            return Promise.all([\n                fetchSource(moduleURL),\n                Promise.resolve(cacheTestResult(testAudioWorkletProcessorPostMessageSupport, testAudioWorkletProcessorPostMessageSupport))\n            ]).then(([[source, absoluteUrl], isSupportingPostMessage]) => {\n                const [importStatements, sourceWithoutImportStatements] = splitImportStatements(source, absoluteUrl);\n                /*\n                 * Bug #179: Firefox does not allow to transfer any buffer which has been passed to the process() method as an argument.\n                 *\n                 * This is the unminified version of the code used below.\n                 *\n                 * ```js\n                 * class extends AudioWorkletProcessor {\n                 *\n                 *     __buffers = new WeakSet();\n                 *\n                 *     constructor () {\n                 *         super();\n                 *\n                 *         this.port.postMessage = ((postMessage) => {\n                 *             return (message, transferables) => {\n                 *                 const filteredTransferables = (transferables)\n                 *                     ? transferables.filter((transferable) => !this.__buffers.has(transferable))\n                 *                     : transferables;\n                 *\n                 *                 return postMessage.call(this.port, message, filteredTransferables);\n                 *              };\n                 *         })(this.port.postMessage);\n                 *     }\n                 * }\n                 * ```\n                 */\n                const patchedSourceWithoutImportStatements = isSupportingPostMessage\n                    ? sourceWithoutImportStatements\n                    : sourceWithoutImportStatements.replace(/\\s+extends\\s+AudioWorkletProcessor\\s*{/, ` extends (class extends AudioWorkletProcessor {__b=new WeakSet();constructor(){super();(p=>p.postMessage=(q=>(m,t)=>q.call(p,m,t?t.filter(u=>!this.__b.has(u)):t))(p.postMessage))(this.port)}}){`);\n                /*\n                 * Bug #170: Chrome and Edge do call process() with an array with empty channelData for each input if no input is connected.\n                 *\n                 * Bug #179: Firefox does not allow to transfer any buffer which has been passed to the process() method as an argument.\n                 *\n                 * This is the unminified version of the code used below:\n                 *\n                 * ```js\n                 * `${ importStatements };\n                 * ((registerProcessor) => {${ sourceWithoutImportStatements }\n                 * })((name, processorCtor) => registerProcessor(name, class extends processorCtor {\n                 *\n                 *     __collectBuffers = (array) => {\n                 *         array.forEach((element) => this.__buffers.add(element.buffer));\n                 *     };\n                 *\n                 *     process (inputs, outputs, parameters) {\n                 *         inputs.forEach(this.__collectBuffers);\n                 *         outputs.forEach(this.__collectBuffers);\n                 *         this.__collectBuffers(Object.values(parameters));\n                 *\n                 *         return super.process(\n                 *             (inputs.map((input) => input.some((channelData) => channelData.length === 0)) ? [ ] : input),\n                 *             outputs,\n                 *             parameters\n                 *         );\n                 *     }\n                 *\n                 * }))`\n                 * ```\n                 */\n                const memberDefinition = isSupportingPostMessage ? \'\' : \'__c = (a) => a.forEach(e=>this.__b.add(e.buffer));\';\n                const bufferRegistration = isSupportingPostMessage\n                    ? \'\'\n                    : \'i.forEach(this.__c);o.forEach(this.__c);this.__c(Object.values(p));\';\n                const wrappedSource = `${importStatements};(registerProcessor=>{${patchedSourceWithoutImportStatements}\n})((n,p)=>registerProcessor(n,class extends p{${memberDefinition}process(i,o,p){${bufferRegistration}return super.process(i.map(j=>j.some(k=>k.length===0)?[]:j),o,p)}}))`;\n                const blob = new Blob([wrappedSource], { type: \'application/javascript; charset=utf-8\' });\n                const url = URL.createObjectURL(blob);\n                return nativeContext.audioWorklet\n                    .addModule(url, options)\n                    .then(() => {\n                    if (isNativeOfflineAudioContext(nativeContext)) {\n                        return;\n                    }\n                    // Bug #186: Chrome, Edge and Opera do not allow to create an AudioWorkletNode on a closed AudioContext.\n                    const backupOfflineAudioContext = getOrCreateBackupOfflineAudioContext(nativeContext);\n                    return backupOfflineAudioContext.audioWorklet.addModule(url, options);\n                })\n                    .finally(() => URL.revokeObjectURL(url));\n            });\n        }\n        const resolvedRequestsOfContext = resolvedRequests.get(context);\n        if (resolvedRequestsOfContext !== undefined && resolvedRequestsOfContext.has(moduleURL)) {\n            return Promise.resolve();\n        }\n        const ongoingRequestsOfContext = ongoingRequests.get(context);\n        if (ongoingRequestsOfContext !== undefined) {\n            const promiseOfOngoingRequest = ongoingRequestsOfContext.get(moduleURL);\n            if (promiseOfOngoingRequest !== undefined) {\n                return promiseOfOngoingRequest;\n            }\n        }\n        const promise = fetchSource(moduleURL)\n            .then(([source, absoluteUrl]) => {\n            const [importStatements, sourceWithoutImportStatements] = splitImportStatements(source, absoluteUrl);\n            /*\n             * This is the unminified version of the code used below:\n             *\n             * ```js\n             * ${ importStatements };\n             * ((a, b) => {\n             *     (a[b] = a[b] || [ ]).push(\n             *         (AudioWorkletProcessor, global, registerProcessor, sampleRate, self, window) => {\n             *             ${ sourceWithoutImportStatements }\n             *         }\n             *     );\n             * })(window, \'_AWGS\');\n             * ```\n             */\n            // tslint:disable-next-line:max-line-length\n            const wrappedSource = `${importStatements};((a,b)=>{(a[b]=a[b]||[]).push((AudioWorkletProcessor,global,registerProcessor,sampleRate,self,window)=>{${sourceWithoutImportStatements}\n})})(window,\'_AWGS\')`;\n            // @todo Evaluating the given source code is a possible security problem.\n            return evaluateSource(wrappedSource);\n        })\n            .then(() => {\n            const evaluateAudioWorkletGlobalScope = window._AWGS.pop();\n            if (evaluateAudioWorkletGlobalScope === undefined) {\n                // Bug #182 Chrome, Edge and Opera do throw an instance of a SyntaxError instead of a DOMException.\n                throw new SyntaxError();\n            }\n            exposeCurrentFrameAndCurrentTime(nativeContext.currentTime, nativeContext.sampleRate, () => evaluateAudioWorkletGlobalScope(class AudioWorkletProcessor {\n            }, undefined, (name, processorCtor) => {\n                if (name.trim() === \'\') {\n                    throw createNotSupportedError();\n                }\n                const nodeNameToProcessorConstructorMap = NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS.get(nativeContext);\n                if (nodeNameToProcessorConstructorMap !== undefined) {\n                    if (nodeNameToProcessorConstructorMap.has(name)) {\n                        throw createNotSupportedError();\n                    }\n                    verifyProcessorCtor(processorCtor);\n                    verifyParameterDescriptors(processorCtor.parameterDescriptors);\n                    nodeNameToProcessorConstructorMap.set(name, processorCtor);\n                }\n                else {\n                    verifyProcessorCtor(processorCtor);\n                    verifyParameterDescriptors(processorCtor.parameterDescriptors);\n                    NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS.set(nativeContext, new Map([[name, processorCtor]]));\n                }\n            }, nativeContext.sampleRate, undefined, undefined));\n        });\n        if (ongoingRequestsOfContext === undefined) {\n            ongoingRequests.set(context, new Map([[moduleURL, promise]]));\n        }\n        else {\n            ongoingRequestsOfContext.set(moduleURL, promise);\n        }\n        promise\n            .then(() => {\n            const rslvdRqstsFCntxt = resolvedRequests.get(context);\n            if (rslvdRqstsFCntxt === undefined) {\n                resolvedRequests.set(context, new Set([moduleURL]));\n            }\n            else {\n                rslvdRqstsFCntxt.add(moduleURL);\n            }\n        })\n            .finally(() => {\n            const ngngRqstsFCntxt = ongoingRequests.get(context);\n            if (ngngRqstsFCntxt !== undefined) {\n                ngngRqstsFCntxt.delete(moduleURL);\n            }\n        });\n        return promise;\n    };\n};\n//# sourceMappingURL=add-audio-worklet-module.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-value-for-key.js\nconst get_value_for_key_getValueForKey = (map, key) => {\n    const value = map.get(key);\n    if (value === undefined) {\n        throw new Error(\'A value with the given key could not be found.\');\n    }\n    return value;\n};\n//# sourceMappingURL=get-value-for-key.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/pick-element-from-set.js\nconst pickElementFromSet = (set, predicate) => {\n    const matchingElements = Array.from(set).filter(predicate);\n    if (matchingElements.length > 1) {\n        throw Error(\'More than one element was found.\');\n    }\n    if (matchingElements.length === 0) {\n        throw Error(\'No element was found.\');\n    }\n    const [matchingElement] = matchingElements;\n    set.delete(matchingElement);\n    return matchingElement;\n};\n//# sourceMappingURL=pick-element-from-set.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/delete-passive-input-connection-to-audio-node.js\n\n\nconst deletePassiveInputConnectionToAudioNode = (passiveInputs, source, output, input) => {\n    const passiveInputConnections = get_value_for_key_getValueForKey(passiveInputs, source);\n    const matchingConnection = pickElementFromSet(passiveInputConnections, (passiveInputConnection) => passiveInputConnection[0] === output && passiveInputConnection[1] === input);\n    if (passiveInputConnections.size === 0) {\n        passiveInputs.delete(source);\n    }\n    return matchingConnection;\n};\n//# sourceMappingURL=delete-passive-input-connection-to-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-event-listeners-of-audio-node.js\n\n\nconst get_event_listeners_of_audio_node_getEventListenersOfAudioNode = (audioNode) => {\n    return get_value_for_key_getValueForKey(EVENT_LISTENERS, audioNode);\n};\n//# sourceMappingURL=get-event-listeners-of-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/set-internal-state-to-active.js\n\n\nconst setInternalStateToActive = (audioNode) => {\n    if (ACTIVE_AUDIO_NODE_STORE.has(audioNode)) {\n        throw new Error(\'The AudioNode is already stored.\');\n    }\n    ACTIVE_AUDIO_NODE_STORE.add(audioNode);\n    get_event_listeners_of_audio_node_getEventListenersOfAudioNode(audioNode).forEach((eventListener) => eventListener(true));\n};\n//# sourceMappingURL=set-internal-state-to-active.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/audio-worklet-node.js\nconst isAudioWorkletNode = (audioNode) => {\n    return \'port\' in audioNode;\n};\n//# sourceMappingURL=audio-worklet-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/set-internal-state-to-passive.js\n\n\nconst setInternalStateToPassive = (audioNode) => {\n    if (!ACTIVE_AUDIO_NODE_STORE.has(audioNode)) {\n        throw new Error(\'The AudioNode is not stored.\');\n    }\n    ACTIVE_AUDIO_NODE_STORE.delete(audioNode);\n    get_event_listeners_of_audio_node_getEventListenersOfAudioNode(audioNode).forEach((eventListener) => eventListener(false));\n};\n//# sourceMappingURL=set-internal-state-to-passive.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/set-internal-state-to-passive-when-necessary.js\n\n\n// Set the internalState of the audioNode to \'passive\' if it is not an AudioWorkletNode and if it has no \'active\' input connections.\nconst setInternalStateToPassiveWhenNecessary = (audioNode, activeInputs) => {\n    if (!isAudioWorkletNode(audioNode) && activeInputs.every((connections) => connections.size === 0)) {\n        setInternalStateToPassive(audioNode);\n    }\n};\n//# sourceMappingURL=set-internal-state-to-passive-when-necessary.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-connection-to-audio-node.js\n\n\n\nconst createAddConnectionToAudioNode = (addActiveInputConnectionToAudioNode, addPassiveInputConnectionToAudioNode, connectNativeAudioNodeToNativeAudioNode, deleteActiveInputConnectionToAudioNode, disconnectNativeAudioNodeFromNativeAudioNode, getAudioNodeConnections, getAudioNodeTailTime, getEventListenersOfAudioNode, getNativeAudioNode, insertElementInSet, isActiveAudioNode, isPartOfACycle, isPassiveAudioNode) => {\n    const tailTimeTimeoutIds = new WeakMap();\n    return (source, destination, output, input, isOffline) => {\n        const { activeInputs, passiveInputs } = getAudioNodeConnections(destination);\n        const { outputs } = getAudioNodeConnections(source);\n        const eventListeners = getEventListenersOfAudioNode(source);\n        const eventListener = (isActive) => {\n            const nativeDestinationAudioNode = getNativeAudioNode(destination);\n            const nativeSourceAudioNode = getNativeAudioNode(source);\n            if (isActive) {\n                const partialConnection = deletePassiveInputConnectionToAudioNode(passiveInputs, source, output, input);\n                addActiveInputConnectionToAudioNode(activeInputs, source, partialConnection, false);\n                if (!isOffline && !isPartOfACycle(source)) {\n                    connectNativeAudioNodeToNativeAudioNode(nativeSourceAudioNode, nativeDestinationAudioNode, output, input);\n                }\n                if (isPassiveAudioNode(destination)) {\n                    setInternalStateToActive(destination);\n                }\n            }\n            else {\n                const partialConnection = deleteActiveInputConnectionToAudioNode(activeInputs, source, output, input);\n                addPassiveInputConnectionToAudioNode(passiveInputs, input, partialConnection, false);\n                if (!isOffline && !isPartOfACycle(source)) {\n                    disconnectNativeAudioNodeFromNativeAudioNode(nativeSourceAudioNode, nativeDestinationAudioNode, output, input);\n                }\n                const tailTime = getAudioNodeTailTime(destination);\n                if (tailTime === 0) {\n                    if (isActiveAudioNode(destination)) {\n                        setInternalStateToPassiveWhenNecessary(destination, activeInputs);\n                    }\n                }\n                else {\n                    const tailTimeTimeoutId = tailTimeTimeoutIds.get(destination);\n                    if (tailTimeTimeoutId !== undefined) {\n                        clearTimeout(tailTimeTimeoutId);\n                    }\n                    tailTimeTimeoutIds.set(destination, setTimeout(() => {\n                        if (isActiveAudioNode(destination)) {\n                            setInternalStateToPassiveWhenNecessary(destination, activeInputs);\n                        }\n                    }, tailTime * 1000));\n                }\n            }\n        };\n        if (insertElementInSet(outputs, [destination, output, input], (outputConnection) => outputConnection[0] === destination && outputConnection[1] === output && outputConnection[2] === input, true)) {\n            eventListeners.add(eventListener);\n            if (isActiveAudioNode(source)) {\n                addActiveInputConnectionToAudioNode(activeInputs, source, [output, input, eventListener], true);\n            }\n            else {\n                addPassiveInputConnectionToAudioNode(passiveInputs, input, [source, output, eventListener], true);\n            }\n            return true;\n        }\n        return false;\n    };\n};\n//# sourceMappingURL=add-connection-to-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-passive-input-connection-to-audio-node.js\nconst createAddPassiveInputConnectionToAudioNode = (insertElementInSet) => {\n    return (passiveInputs, input, [source, output, eventListener], ignoreDuplicates) => {\n        const passiveInputConnections = passiveInputs.get(source);\n        if (passiveInputConnections === undefined) {\n            passiveInputs.set(source, new Set([[output, input, eventListener]]));\n        }\n        else {\n            insertElementInSet(passiveInputConnections, [output, input, eventListener], (passiveInputConnection) => passiveInputConnection[0] === output && passiveInputConnection[1] === input, ignoreDuplicates);\n        }\n    };\n};\n//# sourceMappingURL=add-passive-input-connection-to-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-silent-connection.js\nconst createAddSilentConnection = (createNativeGainNode) => {\n    return (nativeContext, nativeAudioScheduledSourceNode) => {\n        const nativeGainNode = createNativeGainNode(nativeContext, {\n            channelCount: 1,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\',\n            gain: 0\n        });\n        nativeAudioScheduledSourceNode.connect(nativeGainNode).connect(nativeContext.destination);\n        const disconnect = () => {\n            nativeAudioScheduledSourceNode.removeEventListener(\'ended\', disconnect);\n            nativeAudioScheduledSourceNode.disconnect(nativeGainNode);\n            nativeGainNode.disconnect();\n        };\n        nativeAudioScheduledSourceNode.addEventListener(\'ended\', disconnect);\n    };\n};\n//# sourceMappingURL=add-silent-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/add-unrendered-audio-worklet-node.js\nconst createAddUnrenderedAudioWorkletNode = (getUnrenderedAudioWorkletNodes) => {\n    return (nativeContext, audioWorkletNode) => {\n        getUnrenderedAudioWorkletNodes(nativeContext).add(audioWorkletNode);\n    };\n};\n//# sourceMappingURL=add-unrendered-audio-worklet-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/analyser-node-constructor.js\nconst DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    fftSize: 2048,\n    maxDecibels: -30,\n    minDecibels: -100,\n    smoothingTimeConstant: 0.8\n};\nconst createAnalyserNodeConstructor = (audionNodeConstructor, createAnalyserNodeRenderer, createIndexSizeError, createNativeAnalyserNode, getNativeContext, isNativeOfflineAudioContext) => {\n    return class AnalyserNode extends audionNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...DEFAULT_OPTIONS, ...options };\n            const nativeAnalyserNode = createNativeAnalyserNode(nativeContext, mergedOptions);\n            const analyserNodeRenderer = ((isNativeOfflineAudioContext(nativeContext) ? createAnalyserNodeRenderer() : null));\n            super(context, false, nativeAnalyserNode, analyserNodeRenderer);\n            this._nativeAnalyserNode = nativeAnalyserNode;\n        }\n        get fftSize() {\n            return this._nativeAnalyserNode.fftSize;\n        }\n        set fftSize(value) {\n            this._nativeAnalyserNode.fftSize = value;\n        }\n        get frequencyBinCount() {\n            return this._nativeAnalyserNode.frequencyBinCount;\n        }\n        get maxDecibels() {\n            return this._nativeAnalyserNode.maxDecibels;\n        }\n        set maxDecibels(value) {\n            // Bug #118: Safari does not throw an error if maxDecibels is not more than minDecibels.\n            const maxDecibels = this._nativeAnalyserNode.maxDecibels;\n            this._nativeAnalyserNode.maxDecibels = value;\n            if (!(value > this._nativeAnalyserNode.minDecibels)) {\n                this._nativeAnalyserNode.maxDecibels = maxDecibels;\n                throw createIndexSizeError();\n            }\n        }\n        get minDecibels() {\n            return this._nativeAnalyserNode.minDecibels;\n        }\n        set minDecibels(value) {\n            // Bug #118: Safari does not throw an error if maxDecibels is not more than minDecibels.\n            const minDecibels = this._nativeAnalyserNode.minDecibels;\n            this._nativeAnalyserNode.minDecibels = value;\n            if (!(this._nativeAnalyserNode.maxDecibels > value)) {\n                this._nativeAnalyserNode.minDecibels = minDecibels;\n                throw createIndexSizeError();\n            }\n        }\n        get smoothingTimeConstant() {\n            return this._nativeAnalyserNode.smoothingTimeConstant;\n        }\n        set smoothingTimeConstant(value) {\n            this._nativeAnalyserNode.smoothingTimeConstant = value;\n        }\n        getByteFrequencyData(array) {\n            this._nativeAnalyserNode.getByteFrequencyData(array);\n        }\n        getByteTimeDomainData(array) {\n            this._nativeAnalyserNode.getByteTimeDomainData(array);\n        }\n        getFloatFrequencyData(array) {\n            this._nativeAnalyserNode.getFloatFrequencyData(array);\n        }\n        getFloatTimeDomainData(array) {\n            this._nativeAnalyserNode.getFloatTimeDomainData(array);\n        }\n    };\n};\n//# sourceMappingURL=analyser-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-owned-by-context.js\nconst isOwnedByContext = (nativeAudioNode, nativeContext) => {\n    return nativeAudioNode.context === nativeContext;\n};\n//# sourceMappingURL=is-owned-by-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/analyser-node-renderer-factory.js\n\nconst createAnalyserNodeRendererFactory = (createNativeAnalyserNode, getNativeAudioNode, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeAnalyserNodes = new WeakMap();\n        const createAnalyserNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeAnalyserNode = getNativeAudioNode(proxy);\n            // If the initially used nativeAnalyserNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeAnalyserNodeIsOwnedByContext = isOwnedByContext(nativeAnalyserNode, nativeOfflineAudioContext);\n            if (!nativeAnalyserNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeAnalyserNode.channelCount,\n                    channelCountMode: nativeAnalyserNode.channelCountMode,\n                    channelInterpretation: nativeAnalyserNode.channelInterpretation,\n                    fftSize: nativeAnalyserNode.fftSize,\n                    maxDecibels: nativeAnalyserNode.maxDecibels,\n                    minDecibels: nativeAnalyserNode.minDecibels,\n                    smoothingTimeConstant: nativeAnalyserNode.smoothingTimeConstant\n                };\n                nativeAnalyserNode = createNativeAnalyserNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeAnalyserNodes.set(nativeOfflineAudioContext, nativeAnalyserNode);\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAnalyserNode, trace);\n            return nativeAnalyserNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeAnalyserNode = renderedNativeAnalyserNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeAnalyserNode !== undefined) {\n                    return Promise.resolve(renderedNativeAnalyserNode);\n                }\n                return createAnalyserNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=analyser-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-copy-channel-methods-out-of-bounds-support.js\nconst test_audio_buffer_copy_channel_methods_out_of_bounds_support_testAudioBufferCopyChannelMethodsOutOfBoundsSupport = (nativeAudioBuffer) => {\n    try {\n        nativeAudioBuffer.copyToChannel(new Float32Array(1), 0, -1);\n    }\n    catch {\n        return false;\n    }\n    return true;\n};\n//# sourceMappingURL=test-audio-buffer-copy-channel-methods-out-of-bounds-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/index-size-error.js\nconst index_size_error_createIndexSizeError = () => new DOMException(\'\', \'IndexSizeError\');\n//# sourceMappingURL=index-size-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-buffer-get-channel-data-method.js\n\nconst wrapAudioBufferGetChannelDataMethod = (audioBuffer) => {\n    audioBuffer.getChannelData = ((getChannelData) => {\n        return (channel) => {\n            try {\n                return getChannelData.call(audioBuffer, channel);\n            }\n            catch (err) {\n                if (err.code === 12) {\n                    throw index_size_error_createIndexSizeError();\n                }\n                throw err;\n            }\n        };\n    })(audioBuffer.getChannelData);\n};\n//# sourceMappingURL=wrap-audio-buffer-get-channel-data-method.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-buffer-constructor.js\n\n\nconst audio_buffer_constructor_DEFAULT_OPTIONS = {\n    numberOfChannels: 1\n};\nconst createAudioBufferConstructor = (audioBufferStore, cacheTestResult, createNotSupportedError, nativeAudioBufferConstructor, nativeOfflineAudioContextConstructor, testNativeAudioBufferConstructorSupport, wrapAudioBufferCopyChannelMethods, wrapAudioBufferCopyChannelMethodsOutOfBounds) => {\n    let nativeOfflineAudioContext = null;\n    return class AudioBuffer {\n        constructor(options) {\n            if (nativeOfflineAudioContextConstructor === null) {\n                throw new Error(\'Missing the native OfflineAudioContext constructor.\');\n            }\n            const { length, numberOfChannels, sampleRate } = { ...audio_buffer_constructor_DEFAULT_OPTIONS, ...options };\n            if (nativeOfflineAudioContext === null) {\n                nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n            }\n            /*\n             * Bug #99: Firefox does not throw a NotSupportedError when the numberOfChannels is zero. But it only does it when using the\n             * factory function. But since Firefox also supports the constructor everything should be fine.\n             */\n            const audioBuffer = nativeAudioBufferConstructor !== null &&\n                cacheTestResult(testNativeAudioBufferConstructorSupport, testNativeAudioBufferConstructorSupport)\n                ? new nativeAudioBufferConstructor({ length, numberOfChannels, sampleRate })\n                : nativeOfflineAudioContext.createBuffer(numberOfChannels, length, sampleRate);\n            // Bug #99: Safari does not throw an error when the numberOfChannels is zero.\n            if (audioBuffer.numberOfChannels === 0) {\n                throw createNotSupportedError();\n            }\n            // Bug #5: Safari does not support copyFromChannel() and copyToChannel().\n            // Bug #100: Safari does throw a wrong error when calling getChannelData() with an out-of-bounds value.\n            if (typeof audioBuffer.copyFromChannel !== \'function\') {\n                wrapAudioBufferCopyChannelMethods(audioBuffer);\n                wrapAudioBufferGetChannelDataMethod(audioBuffer);\n                // Bug #157: Firefox does not allow the bufferOffset to be out-of-bounds.\n            }\n            else if (!cacheTestResult(test_audio_buffer_copy_channel_methods_out_of_bounds_support_testAudioBufferCopyChannelMethodsOutOfBoundsSupport, () => test_audio_buffer_copy_channel_methods_out_of_bounds_support_testAudioBufferCopyChannelMethodsOutOfBoundsSupport(audioBuffer))) {\n                wrapAudioBufferCopyChannelMethodsOutOfBounds(audioBuffer);\n            }\n            audioBufferStore.add(audioBuffer);\n            /*\n             * This does violate all good pratices but it is necessary to allow this AudioBuffer to be used with native\n             * (Offline)AudioContexts.\n             */\n            return audioBuffer;\n        }\n        static [Symbol.hasInstance](instance) {\n            return ((instance !== null && typeof instance === \'object\' && Object.getPrototypeOf(instance) === AudioBuffer.prototype) ||\n                audioBufferStore.has(instance));\n        }\n    };\n};\n//# sourceMappingURL=audio-buffer-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/constants.js\nconst MOST_NEGATIVE_SINGLE_FLOAT = -3.4028234663852886e38;\nconst MOST_POSITIVE_SINGLE_FLOAT = -MOST_NEGATIVE_SINGLE_FLOAT;\n//# sourceMappingURL=constants.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-active-audio-node.js\n\nconst is_active_audio_node_isActiveAudioNode = (audioNode) => ACTIVE_AUDIO_NODE_STORE.has(audioNode);\n//# sourceMappingURL=is-active-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-buffer-source-node-constructor.js\n\n\n\n\nconst audio_buffer_source_node_constructor_DEFAULT_OPTIONS = {\n    buffer: null,\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    // Bug #149: Safari does not yet support the detune AudioParam.\n    loop: false,\n    loopEnd: 0,\n    loopStart: 0,\n    playbackRate: 1\n};\nconst createAudioBufferSourceNodeConstructor = (audioNodeConstructor, createAudioBufferSourceNodeRenderer, createAudioParam, createInvalidStateError, createNativeAudioBufferSourceNode, getNativeContext, isNativeOfflineAudioContext, wrapEventListener) => {\n    return class AudioBufferSourceNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...audio_buffer_source_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const audioBufferSourceNodeRenderer = ((isOffline ? createAudioBufferSourceNodeRenderer() : null));\n            super(context, false, nativeAudioBufferSourceNode, audioBufferSourceNodeRenderer);\n            this._audioBufferSourceNodeRenderer = audioBufferSourceNodeRenderer;\n            this._isBufferNullified = false;\n            this._isBufferSet = mergedOptions.buffer !== null;\n            this._nativeAudioBufferSourceNode = nativeAudioBufferSourceNode;\n            this._onended = null;\n            // Bug #73: Safari does not export the correct values for maxValue and minValue.\n            this._playbackRate = createAudioParam(this, isOffline, nativeAudioBufferSourceNode.playbackRate, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n        }\n        get buffer() {\n            if (this._isBufferNullified) {\n                return null;\n            }\n            return this._nativeAudioBufferSourceNode.buffer;\n        }\n        set buffer(value) {\n            this._nativeAudioBufferSourceNode.buffer = value;\n            // Bug #72: Only Chrome, Edge & Opera do not allow to reassign the buffer yet.\n            if (value !== null) {\n                if (this._isBufferSet) {\n                    throw createInvalidStateError();\n                }\n                this._isBufferSet = true;\n            }\n        }\n        get loop() {\n            return this._nativeAudioBufferSourceNode.loop;\n        }\n        set loop(value) {\n            this._nativeAudioBufferSourceNode.loop = value;\n        }\n        get loopEnd() {\n            return this._nativeAudioBufferSourceNode.loopEnd;\n        }\n        set loopEnd(value) {\n            this._nativeAudioBufferSourceNode.loopEnd = value;\n        }\n        get loopStart() {\n            return this._nativeAudioBufferSourceNode.loopStart;\n        }\n        set loopStart(value) {\n            this._nativeAudioBufferSourceNode.loopStart = value;\n        }\n        get onended() {\n            return this._onended;\n        }\n        set onended(value) {\n            const wrappedListener = typeof value === \'function\' ? wrapEventListener(this, value) : null;\n            this._nativeAudioBufferSourceNode.onended = wrappedListener;\n            const nativeOnEnded = this._nativeAudioBufferSourceNode.onended;\n            this._onended = nativeOnEnded !== null && nativeOnEnded === wrappedListener ? value : nativeOnEnded;\n        }\n        get playbackRate() {\n            return this._playbackRate;\n        }\n        start(when = 0, offset = 0, duration) {\n            this._nativeAudioBufferSourceNode.start(when, offset, duration);\n            if (this._audioBufferSourceNodeRenderer !== null) {\n                this._audioBufferSourceNodeRenderer.start = duration === undefined ? [when, offset] : [when, offset, duration];\n            }\n            if (this.context.state !== \'closed\') {\n                setInternalStateToActive(this);\n                const resetInternalStateToPassive = () => {\n                    this._nativeAudioBufferSourceNode.removeEventListener(\'ended\', resetInternalStateToPassive);\n                    if (is_active_audio_node_isActiveAudioNode(this)) {\n                        setInternalStateToPassive(this);\n                    }\n                };\n                this._nativeAudioBufferSourceNode.addEventListener(\'ended\', resetInternalStateToPassive);\n            }\n        }\n        stop(when = 0) {\n            this._nativeAudioBufferSourceNode.stop(when);\n            if (this._audioBufferSourceNodeRenderer !== null) {\n                this._audioBufferSourceNodeRenderer.stop = when;\n            }\n        }\n    };\n};\n//# sourceMappingURL=audio-buffer-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-buffer-source-node-renderer-factory.js\n\nconst createAudioBufferSourceNodeRendererFactory = (connectAudioParam, createNativeAudioBufferSourceNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeAudioBufferSourceNodes = new WeakMap();\n        let start = null;\n        let stop = null;\n        const createAudioBufferSourceNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeAudioBufferSourceNode = getNativeAudioNode(proxy);\n            /*\n             * If the initially used nativeAudioBufferSourceNode was not constructed on the same OfflineAudioContext it needs to be created\n             * again.\n             */\n            const nativeAudioBufferSourceNodeIsOwnedByContext = isOwnedByContext(nativeAudioBufferSourceNode, nativeOfflineAudioContext);\n            if (!nativeAudioBufferSourceNodeIsOwnedByContext) {\n                const options = {\n                    buffer: nativeAudioBufferSourceNode.buffer,\n                    channelCount: nativeAudioBufferSourceNode.channelCount,\n                    channelCountMode: nativeAudioBufferSourceNode.channelCountMode,\n                    channelInterpretation: nativeAudioBufferSourceNode.channelInterpretation,\n                    // Bug #149: Safari does not yet support the detune AudioParam.\n                    loop: nativeAudioBufferSourceNode.loop,\n                    loopEnd: nativeAudioBufferSourceNode.loopEnd,\n                    loopStart: nativeAudioBufferSourceNode.loopStart,\n                    playbackRate: nativeAudioBufferSourceNode.playbackRate.value\n                };\n                nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode(nativeOfflineAudioContext, options);\n                if (start !== null) {\n                    nativeAudioBufferSourceNode.start(...start);\n                }\n                if (stop !== null) {\n                    nativeAudioBufferSourceNode.stop(stop);\n                }\n            }\n            renderedNativeAudioBufferSourceNodes.set(nativeOfflineAudioContext, nativeAudioBufferSourceNode);\n            if (!nativeAudioBufferSourceNodeIsOwnedByContext) {\n                // Bug #149: Safari does not yet support the detune AudioParam.\n                await renderAutomation(nativeOfflineAudioContext, proxy.playbackRate, nativeAudioBufferSourceNode.playbackRate, trace);\n            }\n            else {\n                // Bug #149: Safari does not yet support the detune AudioParam.\n                await connectAudioParam(nativeOfflineAudioContext, proxy.playbackRate, nativeAudioBufferSourceNode.playbackRate, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAudioBufferSourceNode, trace);\n            return nativeAudioBufferSourceNode;\n        };\n        return {\n            set start(value) {\n                start = value;\n            },\n            set stop(value) {\n                stop = value;\n            },\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeAudioBufferSourceNode = renderedNativeAudioBufferSourceNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeAudioBufferSourceNode !== undefined) {\n                    return Promise.resolve(renderedNativeAudioBufferSourceNode);\n                }\n                return createAudioBufferSourceNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=audio-buffer-source-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/audio-buffer-source-node.js\nconst isAudioBufferSourceNode = (audioNode) => {\n    return \'playbackRate\' in audioNode;\n};\n//# sourceMappingURL=audio-buffer-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/biquad-filter-node.js\nconst isBiquadFilterNode = (audioNode) => {\n    return \'frequency\' in audioNode && \'gain\' in audioNode;\n};\n//# sourceMappingURL=biquad-filter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/constant-source-node.js\nconst isConstantSourceNode = (audioNode) => {\n    return \'offset\' in audioNode;\n};\n//# sourceMappingURL=constant-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/gain-node.js\nconst isGainNode = (audioNode) => {\n    return !(\'frequency\' in audioNode) && \'gain\' in audioNode;\n};\n//# sourceMappingURL=gain-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/oscillator-node.js\nconst isOscillatorNode = (audioNode) => {\n    return \'detune\' in audioNode && \'frequency\' in audioNode;\n};\n//# sourceMappingURL=oscillator-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/stereo-panner-node.js\nconst isStereoPannerNode = (audioNode) => {\n    return \'pan\' in audioNode;\n};\n//# sourceMappingURL=stereo-panner-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-audio-node-connections.js\n\n\nconst get_audio_node_connections_getAudioNodeConnections = (audioNode) => {\n    return get_value_for_key_getValueForKey(AUDIO_NODE_CONNECTIONS_STORE, audioNode);\n};\n//# sourceMappingURL=get-audio-node-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-audio-param-connections.js\n\n\nconst getAudioParamConnections = (audioParam) => {\n    return get_value_for_key_getValueForKey(AUDIO_PARAM_CONNECTIONS_STORE, audioParam);\n};\n//# sourceMappingURL=get-audio-param-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/deactivate-active-audio-node-input-connections.js\n\n\n\n\n\n\n\n\n\n\n\nconst deactivateActiveAudioNodeInputConnections = (audioNode, trace) => {\n    const { activeInputs } = get_audio_node_connections_getAudioNodeConnections(audioNode);\n    activeInputs.forEach((connections) => connections.forEach(([source]) => {\n        if (!trace.includes(audioNode)) {\n            deactivateActiveAudioNodeInputConnections(source, [...trace, audioNode]);\n        }\n    }));\n    const audioParams = isAudioBufferSourceNode(audioNode)\n        ? [\n            // Bug #149: Safari does not yet support the detune AudioParam.\n            audioNode.playbackRate\n        ]\n        : isAudioWorkletNode(audioNode)\n            ? Array.from(audioNode.parameters.values())\n            : isBiquadFilterNode(audioNode)\n                ? [audioNode.Q, audioNode.detune, audioNode.frequency, audioNode.gain]\n                : isConstantSourceNode(audioNode)\n                    ? [audioNode.offset]\n                    : isGainNode(audioNode)\n                        ? [audioNode.gain]\n                        : isOscillatorNode(audioNode)\n                            ? [audioNode.detune, audioNode.frequency]\n                            : isStereoPannerNode(audioNode)\n                                ? [audioNode.pan]\n                                : [];\n    for (const audioParam of audioParams) {\n        const audioParamConnections = getAudioParamConnections(audioParam);\n        if (audioParamConnections !== undefined) {\n            audioParamConnections.activeInputs.forEach(([source]) => deactivateActiveAudioNodeInputConnections(source, trace));\n        }\n    }\n    if (is_active_audio_node_isActiveAudioNode(audioNode)) {\n        setInternalStateToPassive(audioNode);\n    }\n};\n//# sourceMappingURL=deactivate-active-audio-node-input-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/deactivate-audio-graph.js\n\nconst deactivateAudioGraph = (context) => {\n    deactivateActiveAudioNodeInputConnections(context.destination, []);\n};\n//# sourceMappingURL=deactivate-audio-graph.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-valid-latency-hint.js\nconst isValidLatencyHint = (latencyHint) => {\n    return (latencyHint === undefined ||\n        typeof latencyHint === \'number\' ||\n        (typeof latencyHint === \'string\' && (latencyHint === \'balanced\' || latencyHint === \'interactive\' || latencyHint === \'playback\')));\n};\n//# sourceMappingURL=is-valid-latency-hint.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-context-constructor.js\n\n\nconst createAudioContextConstructor = (baseAudioContextConstructor, createInvalidStateError, createNotSupportedError, createUnknownError, mediaElementAudioSourceNodeConstructor, mediaStreamAudioDestinationNodeConstructor, mediaStreamAudioSourceNodeConstructor, mediaStreamTrackAudioSourceNodeConstructor, nativeAudioContextConstructor) => {\n    return class AudioContext extends baseAudioContextConstructor {\n        constructor(options = {}) {\n            if (nativeAudioContextConstructor === null) {\n                throw new Error(\'Missing the native AudioContext constructor.\');\n            }\n            const nativeAudioContext = new nativeAudioContextConstructor(options);\n            // Bug #131 Safari returns null when there are four other AudioContexts running already.\n            if (nativeAudioContext === null) {\n                throw createUnknownError();\n            }\n            // Bug #51 Only Chrome, Edge and Opera throw an error if the given latencyHint is invalid.\n            if (!isValidLatencyHint(options.latencyHint)) {\n                throw new TypeError(`The provided value \'${options.latencyHint}\' is not a valid enum value of type AudioContextLatencyCategory.`);\n            }\n            // Bug #150 Safari does not support setting the sampleRate.\n            if (options.sampleRate !== undefined && nativeAudioContext.sampleRate !== options.sampleRate) {\n                throw createNotSupportedError();\n            }\n            super(nativeAudioContext, 2);\n            const { latencyHint } = options;\n            const { sampleRate } = nativeAudioContext;\n            // @todo The values for \'balanced\', \'interactive\' and \'playback\' are just copied from Chrome\'s implementation.\n            this._baseLatency =\n                typeof nativeAudioContext.baseLatency === \'number\'\n                    ? nativeAudioContext.baseLatency\n                    : latencyHint === \'balanced\'\n                        ? 512 / sampleRate\n                        : latencyHint === \'interactive\' || latencyHint === undefined\n                            ? 256 / sampleRate\n                            : latencyHint === \'playback\'\n                                ? 1024 / sampleRate\n                                : /*\n                                   * @todo The min (256) and max (16384) values are taken from the allowed bufferSize values of a\n                                   * ScriptProcessorNode.\n                                   */\n                                    (Math.max(2, Math.min(128, Math.round((latencyHint * sampleRate) / 128))) * 128) / sampleRate;\n            this._nativeAudioContext = nativeAudioContext;\n            // Bug #188: Safari will set the context\'s state to \'interrupted\' in case the user switches tabs.\n            if (nativeAudioContextConstructor.name === \'webkitAudioContext\') {\n                this._nativeGainNode = nativeAudioContext.createGain();\n                this._nativeOscillatorNode = nativeAudioContext.createOscillator();\n                this._nativeGainNode.gain.value = 1e-37;\n                this._nativeOscillatorNode.connect(this._nativeGainNode).connect(nativeAudioContext.destination);\n                this._nativeOscillatorNode.start();\n            }\n            else {\n                this._nativeGainNode = null;\n                this._nativeOscillatorNode = null;\n            }\n            this._state = null;\n            /*\n             * Bug #34: Chrome, Edge and Opera pretend to be running right away, but fire an onstatechange event when the state actually\n             * changes to \'running\'.\n             */\n            if (nativeAudioContext.state === \'running\') {\n                this._state = \'suspended\';\n                const revokeState = () => {\n                    if (this._state === \'suspended\') {\n                        this._state = null;\n                    }\n                    nativeAudioContext.removeEventListener(\'statechange\', revokeState);\n                };\n                nativeAudioContext.addEventListener(\'statechange\', revokeState);\n            }\n        }\n        get baseLatency() {\n            return this._baseLatency;\n        }\n        get state() {\n            return this._state !== null ? this._state : this._nativeAudioContext.state;\n        }\n        close() {\n            // Bug #35: Firefox does not throw an error if the AudioContext was closed before.\n            if (this.state === \'closed\') {\n                return this._nativeAudioContext.close().then(() => {\n                    throw createInvalidStateError();\n                });\n            }\n            // Bug #34: If the state was set to suspended before it should be revoked now.\n            if (this._state === \'suspended\') {\n                this._state = null;\n            }\n            return this._nativeAudioContext.close().then(() => {\n                if (this._nativeGainNode !== null && this._nativeOscillatorNode !== null) {\n                    this._nativeOscillatorNode.stop();\n                    this._nativeGainNode.disconnect();\n                    this._nativeOscillatorNode.disconnect();\n                }\n                deactivateAudioGraph(this);\n            });\n        }\n        createMediaElementSource(mediaElement) {\n            return new mediaElementAudioSourceNodeConstructor(this, { mediaElement });\n        }\n        createMediaStreamDestination() {\n            return new mediaStreamAudioDestinationNodeConstructor(this);\n        }\n        createMediaStreamSource(mediaStream) {\n            return new mediaStreamAudioSourceNodeConstructor(this, { mediaStream });\n        }\n        createMediaStreamTrackSource(mediaStreamTrack) {\n            return new mediaStreamTrackAudioSourceNodeConstructor(this, { mediaStreamTrack });\n        }\n        resume() {\n            if (this._state === \'suspended\') {\n                return new Promise((resolve, reject) => {\n                    const resolvePromise = () => {\n                        this._nativeAudioContext.removeEventListener(\'statechange\', resolvePromise);\n                        if (this._nativeAudioContext.state === \'running\') {\n                            resolve();\n                        }\n                        else {\n                            this.resume().then(resolve, reject);\n                        }\n                    };\n                    this._nativeAudioContext.addEventListener(\'statechange\', resolvePromise);\n                });\n            }\n            return this._nativeAudioContext.resume().catch((err) => {\n                // Bug #55: Chrome, Edge and Opera do throw an InvalidAccessError instead of an InvalidStateError.\n                // Bug #56: Safari invokes the catch handler but without an error.\n                if (err === undefined || err.code === 15) {\n                    throw createInvalidStateError();\n                }\n                throw err;\n            });\n        }\n        suspend() {\n            return this._nativeAudioContext.suspend().catch((err) => {\n                // Bug #56: Safari invokes the catch handler but without an error.\n                if (err === undefined) {\n                    throw createInvalidStateError();\n                }\n                throw err;\n            });\n        }\n    };\n};\n//# sourceMappingURL=audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-destination-node-constructor.js\nconst createAudioDestinationNodeConstructor = (audioNodeConstructor, createAudioDestinationNodeRenderer, createIndexSizeError, createInvalidStateError, createNativeAudioDestinationNode, getNativeContext, isNativeOfflineAudioContext, renderInputsOfAudioNode) => {\n    return class AudioDestinationNode extends audioNodeConstructor {\n        constructor(context, channelCount) {\n            const nativeContext = getNativeContext(context);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const nativeAudioDestinationNode = createNativeAudioDestinationNode(nativeContext, channelCount, isOffline);\n            const audioDestinationNodeRenderer = ((isOffline ? createAudioDestinationNodeRenderer(renderInputsOfAudioNode) : null));\n            super(context, false, nativeAudioDestinationNode, audioDestinationNodeRenderer);\n            this._isNodeOfNativeOfflineAudioContext = isOffline;\n            this._nativeAudioDestinationNode = nativeAudioDestinationNode;\n        }\n        get channelCount() {\n            return this._nativeAudioDestinationNode.channelCount;\n        }\n        set channelCount(value) {\n            // Bug #52: Chrome, Edge, Opera & Safari do not throw an exception at all.\n            // Bug #54: Firefox does throw an IndexSizeError.\n            if (this._isNodeOfNativeOfflineAudioContext) {\n                throw createInvalidStateError();\n            }\n            // Bug #47: The AudioDestinationNode in Safari does not initialize the maxChannelCount property correctly.\n            if (value > this._nativeAudioDestinationNode.maxChannelCount) {\n                throw createIndexSizeError();\n            }\n            this._nativeAudioDestinationNode.channelCount = value;\n        }\n        get channelCountMode() {\n            return this._nativeAudioDestinationNode.channelCountMode;\n        }\n        set channelCountMode(value) {\n            // Bug #53: No browser does throw an exception yet.\n            if (this._isNodeOfNativeOfflineAudioContext) {\n                throw createInvalidStateError();\n            }\n            this._nativeAudioDestinationNode.channelCountMode = value;\n        }\n        get maxChannelCount() {\n            return this._nativeAudioDestinationNode.maxChannelCount;\n        }\n    };\n};\n//# sourceMappingURL=audio-destination-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-destination-node-renderer-factory.js\nconst createAudioDestinationNodeRenderer = (renderInputsOfAudioNode) => {\n    let nativeAudioDestinationNodePromise = null;\n    const createAudioDestinationNode = async (proxy, nativeOfflineAudioContext, trace) => {\n        const nativeAudioDestinationNode = nativeOfflineAudioContext.destination;\n        await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAudioDestinationNode, trace);\n        return nativeAudioDestinationNode;\n    };\n    return {\n        render(proxy, nativeOfflineAudioContext, trace) {\n            if (nativeAudioDestinationNodePromise === null) {\n                nativeAudioDestinationNodePromise = createAudioDestinationNode(proxy, nativeOfflineAudioContext, trace);\n            }\n            return nativeAudioDestinationNodePromise;\n        }\n    };\n};\n//# sourceMappingURL=audio-destination-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-listener-factory.js\n\nconst createAudioListenerFactory = (createAudioParam, createNativeChannelMergerNode, createNativeConstantSourceNode, createNativeScriptProcessorNode, isNativeOfflineAudioContext) => {\n    return (context, nativeContext) => {\n        const nativeListener = nativeContext.listener;\n        // Bug #117: Only Chrome, Edge & Opera support the new interface already.\n        const createFakeAudioParams = () => {\n            const channelMergerNode = createNativeChannelMergerNode(nativeContext, {\n                channelCount: 1,\n                channelCountMode: \'explicit\',\n                channelInterpretation: \'speakers\',\n                numberOfInputs: 9\n            });\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const scriptProcessorNode = createNativeScriptProcessorNode(nativeContext, 256, 9, 0);\n            const createFakeAudioParam = (input, value) => {\n                const constantSourceNode = createNativeConstantSourceNode(nativeContext, {\n                    channelCount: 1,\n                    channelCountMode: \'explicit\',\n                    channelInterpretation: \'discrete\',\n                    offset: value\n                });\n                constantSourceNode.connect(channelMergerNode, 0, input);\n                // @todo This should be stopped when the context is closed.\n                constantSourceNode.start();\n                Object.defineProperty(constantSourceNode.offset, \'defaultValue\', {\n                    get() {\n                        return value;\n                    }\n                });\n                /*\n                 * Bug #62 & #74: Safari does not support ConstantSourceNodes and does not export the correct values for maxValue and\n                 * minValue for GainNodes.\n                 */\n                return createAudioParam({ context }, isOffline, constantSourceNode.offset, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            };\n            let lastOrientation = [0, 0, -1, 0, 1, 0];\n            let lastPosition = [0, 0, 0];\n            // tslint:disable-next-line:deprecation\n            scriptProcessorNode.onaudioprocess = ({ inputBuffer }) => {\n                const orientation = [\n                    inputBuffer.getChannelData(0)[0],\n                    inputBuffer.getChannelData(1)[0],\n                    inputBuffer.getChannelData(2)[0],\n                    inputBuffer.getChannelData(3)[0],\n                    inputBuffer.getChannelData(4)[0],\n                    inputBuffer.getChannelData(5)[0]\n                ];\n                if (orientation.some((value, index) => value !== lastOrientation[index])) {\n                    nativeListener.setOrientation(...orientation); // tslint:disable-line:deprecation\n                    lastOrientation = orientation;\n                }\n                const positon = [\n                    inputBuffer.getChannelData(6)[0],\n                    inputBuffer.getChannelData(7)[0],\n                    inputBuffer.getChannelData(8)[0]\n                ];\n                if (positon.some((value, index) => value !== lastPosition[index])) {\n                    nativeListener.setPosition(...positon); // tslint:disable-line:deprecation\n                    lastPosition = positon;\n                }\n            };\n            channelMergerNode.connect(scriptProcessorNode);\n            return {\n                forwardX: createFakeAudioParam(0, 0),\n                forwardY: createFakeAudioParam(1, 0),\n                forwardZ: createFakeAudioParam(2, -1),\n                positionX: createFakeAudioParam(6, 0),\n                positionY: createFakeAudioParam(7, 0),\n                positionZ: createFakeAudioParam(8, 0),\n                upX: createFakeAudioParam(3, 0),\n                upY: createFakeAudioParam(4, 1),\n                upZ: createFakeAudioParam(5, 0)\n            };\n        };\n        const { forwardX, forwardY, forwardZ, positionX, positionY, positionZ, upX, upY, upZ } = nativeListener.forwardX === undefined ? createFakeAudioParams() : nativeListener;\n        return {\n            get forwardX() {\n                return forwardX;\n            },\n            get forwardY() {\n                return forwardY;\n            },\n            get forwardZ() {\n                return forwardZ;\n            },\n            get positionX() {\n                return positionX;\n            },\n            get positionY() {\n                return positionY;\n            },\n            get positionZ() {\n                return positionZ;\n            },\n            get upX() {\n                return upX;\n            },\n            get upY() {\n                return upY;\n            },\n            get upZ() {\n                return upZ;\n            }\n        };\n    };\n};\n//# sourceMappingURL=audio-listener-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/audio-node.js\nconst isAudioNode = (audioNodeOrAudioParam) => {\n    return \'context\' in audioNodeOrAudioParam;\n};\n//# sourceMappingURL=audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/audio-node-output-connection.js\n\nconst isAudioNodeOutputConnection = (outputConnection) => {\n    return isAudioNode(outputConnection[0]);\n};\n//# sourceMappingURL=audio-node-output-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/insert-element-in-set.js\nconst insert_element_in_set_insertElementInSet = (set, element, predicate, ignoreDuplicates) => {\n    for (const lmnt of set) {\n        if (predicate(lmnt)) {\n            if (ignoreDuplicates) {\n                return false;\n            }\n            throw Error(\'The set contains at least one similar element.\');\n        }\n    }\n    set.add(element);\n    return true;\n};\n//# sourceMappingURL=insert-element-in-set.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/add-active-input-connection-to-audio-param.js\n\nconst addActiveInputConnectionToAudioParam = (activeInputs, source, [output, eventListener], ignoreDuplicates) => {\n    insert_element_in_set_insertElementInSet(activeInputs, [source, output, eventListener], (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output, ignoreDuplicates);\n};\n//# sourceMappingURL=add-active-input-connection-to-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/add-passive-input-connection-to-audio-param.js\n\nconst addPassiveInputConnectionToAudioParam = (passiveInputs, [source, output, eventListener], ignoreDuplicates) => {\n    const passiveInputConnections = passiveInputs.get(source);\n    if (passiveInputConnections === undefined) {\n        passiveInputs.set(source, new Set([[output, eventListener]]));\n    }\n    else {\n        insert_element_in_set_insertElementInSet(passiveInputConnections, [output, eventListener], (passiveInputConnection) => passiveInputConnection[0] === output, ignoreDuplicates);\n    }\n};\n//# sourceMappingURL=add-passive-input-connection-to-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/native-audio-node-faker.js\nconst isNativeAudioNodeFaker = (nativeAudioNodeOrNativeAudioNodeFaker) => {\n    return \'inputs\' in nativeAudioNodeOrNativeAudioNodeFaker;\n};\n//# sourceMappingURL=native-audio-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/connect-native-audio-node-to-native-audio-node.js\n\nconst connect_native_audio_node_to_native_audio_node_connectNativeAudioNodeToNativeAudioNode = (nativeSourceAudioNode, nativeDestinationAudioNode, output, input) => {\n    if (isNativeAudioNodeFaker(nativeDestinationAudioNode)) {\n        const fakeNativeDestinationAudioNode = nativeDestinationAudioNode.inputs[input];\n        nativeSourceAudioNode.connect(fakeNativeDestinationAudioNode, output, 0);\n        return [fakeNativeDestinationAudioNode, output, 0];\n    }\n    nativeSourceAudioNode.connect(nativeDestinationAudioNode, output, input);\n    return [nativeDestinationAudioNode, output, input];\n};\n//# sourceMappingURL=connect-native-audio-node-to-native-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/delete-active-input-connection.js\nconst deleteActiveInputConnection = (activeInputConnections, source, output) => {\n    for (const activeInputConnection of activeInputConnections) {\n        if (activeInputConnection[0] === source && activeInputConnection[1] === output) {\n            activeInputConnections.delete(activeInputConnection);\n            return activeInputConnection;\n        }\n    }\n    return null;\n};\n//# sourceMappingURL=delete-active-input-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/delete-active-input-connection-to-audio-param.js\n\nconst deleteActiveInputConnectionToAudioParam = (activeInputs, source, output) => {\n    return pickElementFromSet(activeInputs, (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output);\n};\n//# sourceMappingURL=delete-active-input-connection-to-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/delete-event-listeners-of-audio-node.js\n\nconst deleteEventListenerOfAudioNode = (audioNode, eventListener) => {\n    const eventListeners = get_event_listeners_of_audio_node_getEventListenersOfAudioNode(audioNode);\n    if (!eventListeners.delete(eventListener)) {\n        throw new Error(\'Missing the expected event listener.\');\n    }\n};\n//# sourceMappingURL=delete-event-listeners-of-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/delete-passive-input-connection-to-audio-param.js\n\n\nconst deletePassiveInputConnectionToAudioParam = (passiveInputs, source, output) => {\n    const passiveInputConnections = get_value_for_key_getValueForKey(passiveInputs, source);\n    const matchingConnection = pickElementFromSet(passiveInputConnections, (passiveInputConnection) => passiveInputConnection[0] === output);\n    if (passiveInputConnections.size === 0) {\n        passiveInputs.delete(source);\n    }\n    return matchingConnection;\n};\n//# sourceMappingURL=delete-passive-input-connection-to-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/disconnect-native-audio-node-from-native-audio-node.js\n\nconst disconnect_native_audio_node_from_native_audio_node_disconnectNativeAudioNodeFromNativeAudioNode = (nativeSourceAudioNode, nativeDestinationAudioNode, output, input) => {\n    if (isNativeAudioNodeFaker(nativeDestinationAudioNode)) {\n        nativeSourceAudioNode.disconnect(nativeDestinationAudioNode.inputs[input], output, 0);\n    }\n    else {\n        nativeSourceAudioNode.disconnect(nativeDestinationAudioNode, output, input);\n    }\n};\n//# sourceMappingURL=disconnect-native-audio-node-from-native-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-native-audio-node.js\n\n\nconst get_native_audio_node_getNativeAudioNode = (audioNode) => {\n    return get_value_for_key_getValueForKey(AUDIO_NODE_STORE, audioNode);\n};\n//# sourceMappingURL=get-native-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-native-audio-param.js\n\n\nconst get_native_audio_param_getNativeAudioParam = (audioParam) => {\n    return get_value_for_key_getValueForKey(AUDIO_PARAM_STORE, audioParam);\n};\n//# sourceMappingURL=get-native-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-part-of-a-cycle.js\n\nconst is_part_of_a_cycle_isPartOfACycle = (audioNode) => {\n    return CYCLE_COUNTERS.has(audioNode);\n};\n//# sourceMappingURL=is-part-of-a-cycle.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-passive-audio-node.js\n\nconst is_passive_audio_node_isPassiveAudioNode = (audioNode) => {\n    return !ACTIVE_AUDIO_NODE_STORE.has(audioNode);\n};\n//# sourceMappingURL=is-passive-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-node-disconnect-method-support.js\nconst testAudioNodeDisconnectMethodSupport = (nativeAudioContext) => {\n    return new Promise((resolve) => {\n        const analyzer = nativeAudioContext.createScriptProcessor(256, 1, 1);\n        const dummy = nativeAudioContext.createGain();\n        // Bug #95: Safari does not play one sample buffers.\n        const ones = nativeAudioContext.createBuffer(1, 2, 44100);\n        const channelData = ones.getChannelData(0);\n        channelData[0] = 1;\n        channelData[1] = 1;\n        const source = nativeAudioContext.createBufferSource();\n        source.buffer = ones;\n        source.loop = true;\n        source.connect(analyzer).connect(nativeAudioContext.destination);\n        source.connect(dummy);\n        source.disconnect(dummy);\n        // tslint:disable-next-line:deprecation\n        analyzer.onaudioprocess = (event) => {\n            const chnnlDt = event.inputBuffer.getChannelData(0);\n            if (Array.prototype.some.call(chnnlDt, (sample) => sample === 1)) {\n                resolve(true);\n            }\n            else {\n                resolve(false);\n            }\n            source.stop();\n            analyzer.onaudioprocess = null; // tslint:disable-line:deprecation\n            source.disconnect(analyzer);\n            analyzer.disconnect(nativeAudioContext.destination);\n        };\n        source.start();\n    });\n};\n//# sourceMappingURL=test-audio-node-disconnect-method-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/visit-each-audio-node-once.js\nconst visitEachAudioNodeOnce = (cycles, visitor) => {\n    const counts = new Map();\n    for (const cycle of cycles) {\n        for (const audioNode of cycle) {\n            const count = counts.get(audioNode);\n            counts.set(audioNode, count === undefined ? 1 : count + 1);\n        }\n    }\n    counts.forEach((count, audioNode) => visitor(audioNode, count));\n};\n//# sourceMappingURL=visit-each-audio-node-once.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/native-audio-node.js\nconst native_audio_node_isNativeAudioNode = (nativeAudioNodeOrAudioParam) => {\n    return \'context\' in nativeAudioNodeOrAudioParam;\n};\n//# sourceMappingURL=native-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-node-disconnect-method.js\n\nconst wrapAudioNodeDisconnectMethod = (nativeAudioNode) => {\n    const connections = new Map();\n    nativeAudioNode.connect = ((connect) => {\n        // tslint:disable-next-line:invalid-void\n        return (destination, output = 0, input = 0) => {\n            const returnValue = native_audio_node_isNativeAudioNode(destination) ? connect(destination, output, input) : connect(destination, output);\n            // Save the new connection only if the calls to connect above didn\'t throw an error.\n            const connectionsToDestination = connections.get(destination);\n            if (connectionsToDestination === undefined) {\n                connections.set(destination, [{ input, output }]);\n            }\n            else {\n                if (connectionsToDestination.every((connection) => connection.input !== input || connection.output !== output)) {\n                    connectionsToDestination.push({ input, output });\n                }\n            }\n            return returnValue;\n        };\n    })(nativeAudioNode.connect.bind(nativeAudioNode));\n    nativeAudioNode.disconnect = ((disconnect) => {\n        return (destinationOrOutput, output, input) => {\n            disconnect.apply(nativeAudioNode);\n            if (destinationOrOutput === undefined) {\n                connections.clear();\n            }\n            else if (typeof destinationOrOutput === \'number\') {\n                for (const [destination, connectionsToDestination] of connections) {\n                    const filteredConnections = connectionsToDestination.filter((connection) => connection.output !== destinationOrOutput);\n                    if (filteredConnections.length === 0) {\n                        connections.delete(destination);\n                    }\n                    else {\n                        connections.set(destination, filteredConnections);\n                    }\n                }\n            }\n            else if (connections.has(destinationOrOutput)) {\n                if (output === undefined) {\n                    connections.delete(destinationOrOutput);\n                }\n                else {\n                    const connectionsToDestination = connections.get(destinationOrOutput);\n                    if (connectionsToDestination !== undefined) {\n                        const filteredConnections = connectionsToDestination.filter((connection) => connection.output !== output && (connection.input !== input || input === undefined));\n                        if (filteredConnections.length === 0) {\n                            connections.delete(destinationOrOutput);\n                        }\n                        else {\n                            connections.set(destinationOrOutput, filteredConnections);\n                        }\n                    }\n                }\n            }\n            for (const [destination, connectionsToDestination] of connections) {\n                connectionsToDestination.forEach((connection) => {\n                    if (native_audio_node_isNativeAudioNode(destination)) {\n                        nativeAudioNode.connect(destination, connection.output, connection.input);\n                    }\n                    else {\n                        nativeAudioNode.connect(destination, connection.output);\n                    }\n                });\n            }\n        };\n    })(nativeAudioNode.disconnect);\n};\n//# sourceMappingURL=wrap-audio-node-disconnect-method.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-node-constructor.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst addConnectionToAudioParamOfAudioContext = (source, destination, output, isOffline) => {\n    const { activeInputs, passiveInputs } = getAudioParamConnections(destination);\n    const { outputs } = get_audio_node_connections_getAudioNodeConnections(source);\n    const eventListeners = get_event_listeners_of_audio_node_getEventListenersOfAudioNode(source);\n    const eventListener = (isActive) => {\n        const nativeAudioNode = get_native_audio_node_getNativeAudioNode(source);\n        const nativeAudioParam = get_native_audio_param_getNativeAudioParam(destination);\n        if (isActive) {\n            const partialConnection = deletePassiveInputConnectionToAudioParam(passiveInputs, source, output);\n            addActiveInputConnectionToAudioParam(activeInputs, source, partialConnection, false);\n            if (!isOffline && !is_part_of_a_cycle_isPartOfACycle(source)) {\n                nativeAudioNode.connect(nativeAudioParam, output);\n            }\n        }\n        else {\n            const partialConnection = deleteActiveInputConnectionToAudioParam(activeInputs, source, output);\n            addPassiveInputConnectionToAudioParam(passiveInputs, partialConnection, false);\n            if (!isOffline && !is_part_of_a_cycle_isPartOfACycle(source)) {\n                nativeAudioNode.disconnect(nativeAudioParam, output);\n            }\n        }\n    };\n    if (insert_element_in_set_insertElementInSet(outputs, [destination, output], (outputConnection) => outputConnection[0] === destination && outputConnection[1] === output, true)) {\n        eventListeners.add(eventListener);\n        if (is_active_audio_node_isActiveAudioNode(source)) {\n            addActiveInputConnectionToAudioParam(activeInputs, source, [output, eventListener], true);\n        }\n        else {\n            addPassiveInputConnectionToAudioParam(passiveInputs, [source, output, eventListener], true);\n        }\n        return true;\n    }\n    return false;\n};\nconst deleteInputConnectionOfAudioNode = (source, destination, output, input) => {\n    const { activeInputs, passiveInputs } = get_audio_node_connections_getAudioNodeConnections(destination);\n    const activeInputConnection = deleteActiveInputConnection(activeInputs[input], source, output);\n    if (activeInputConnection === null) {\n        const passiveInputConnection = deletePassiveInputConnectionToAudioNode(passiveInputs, source, output, input);\n        return [passiveInputConnection[2], false];\n    }\n    return [activeInputConnection[2], true];\n};\nconst deleteInputConnectionOfAudioParam = (source, destination, output) => {\n    const { activeInputs, passiveInputs } = getAudioParamConnections(destination);\n    const activeInputConnection = deleteActiveInputConnection(activeInputs, source, output);\n    if (activeInputConnection === null) {\n        const passiveInputConnection = deletePassiveInputConnectionToAudioParam(passiveInputs, source, output);\n        return [passiveInputConnection[1], false];\n    }\n    return [activeInputConnection[2], true];\n};\nconst deleteInputsOfAudioNode = (source, isOffline, destination, output, input) => {\n    const [listener, isActive] = deleteInputConnectionOfAudioNode(source, destination, output, input);\n    if (listener !== null) {\n        deleteEventListenerOfAudioNode(source, listener);\n        if (isActive && !isOffline && !is_part_of_a_cycle_isPartOfACycle(source)) {\n            disconnect_native_audio_node_from_native_audio_node_disconnectNativeAudioNodeFromNativeAudioNode(get_native_audio_node_getNativeAudioNode(source), get_native_audio_node_getNativeAudioNode(destination), output, input);\n        }\n    }\n    if (is_active_audio_node_isActiveAudioNode(destination)) {\n        const { activeInputs } = get_audio_node_connections_getAudioNodeConnections(destination);\n        setInternalStateToPassiveWhenNecessary(destination, activeInputs);\n    }\n};\nconst deleteInputsOfAudioParam = (source, isOffline, destination, output) => {\n    const [listener, isActive] = deleteInputConnectionOfAudioParam(source, destination, output);\n    if (listener !== null) {\n        deleteEventListenerOfAudioNode(source, listener);\n        if (isActive && !isOffline && !is_part_of_a_cycle_isPartOfACycle(source)) {\n            get_native_audio_node_getNativeAudioNode(source).disconnect(get_native_audio_param_getNativeAudioParam(destination), output);\n        }\n    }\n};\nconst deleteAnyConnection = (source, isOffline) => {\n    const audioNodeConnectionsOfSource = get_audio_node_connections_getAudioNodeConnections(source);\n    const destinations = [];\n    for (const outputConnection of audioNodeConnectionsOfSource.outputs) {\n        if (isAudioNodeOutputConnection(outputConnection)) {\n            deleteInputsOfAudioNode(source, isOffline, ...outputConnection);\n        }\n        else {\n            deleteInputsOfAudioParam(source, isOffline, ...outputConnection);\n        }\n        destinations.push(outputConnection[0]);\n    }\n    audioNodeConnectionsOfSource.outputs.clear();\n    return destinations;\n};\nconst deleteConnectionAtOutput = (source, isOffline, output) => {\n    const audioNodeConnectionsOfSource = get_audio_node_connections_getAudioNodeConnections(source);\n    const destinations = [];\n    for (const outputConnection of audioNodeConnectionsOfSource.outputs) {\n        if (outputConnection[1] === output) {\n            if (isAudioNodeOutputConnection(outputConnection)) {\n                deleteInputsOfAudioNode(source, isOffline, ...outputConnection);\n            }\n            else {\n                deleteInputsOfAudioParam(source, isOffline, ...outputConnection);\n            }\n            destinations.push(outputConnection[0]);\n            audioNodeConnectionsOfSource.outputs.delete(outputConnection);\n        }\n    }\n    return destinations;\n};\nconst deleteConnectionToDestination = (source, isOffline, destination, output, input) => {\n    const audioNodeConnectionsOfSource = get_audio_node_connections_getAudioNodeConnections(source);\n    return Array.from(audioNodeConnectionsOfSource.outputs)\n        .filter((outputConnection) => outputConnection[0] === destination &&\n        (output === undefined || outputConnection[1] === output) &&\n        (input === undefined || outputConnection[2] === input))\n        .map((outputConnection) => {\n        if (isAudioNodeOutputConnection(outputConnection)) {\n            deleteInputsOfAudioNode(source, isOffline, ...outputConnection);\n        }\n        else {\n            deleteInputsOfAudioParam(source, isOffline, ...outputConnection);\n        }\n        audioNodeConnectionsOfSource.outputs.delete(outputConnection);\n        return outputConnection[0];\n    });\n};\nconst createAudioNodeConstructor = (addAudioNodeConnections, addConnectionToAudioNode, cacheTestResult, createIncrementCycleCounter, createIndexSizeError, createInvalidAccessError, createNotSupportedError, decrementCycleCounter, detectCycles, eventTargetConstructor, getNativeContext, isNativeAudioContext, isNativeAudioNode, isNativeAudioParam, isNativeOfflineAudioContext) => {\n    return class AudioNode extends eventTargetConstructor {\n        constructor(context, isActive, nativeAudioNode, audioNodeRenderer) {\n            super(nativeAudioNode);\n            this._context = context;\n            this._nativeAudioNode = nativeAudioNode;\n            const nativeContext = getNativeContext(context);\n            // Bug #12: Safari does not support to disconnect a specific destination.\n            if (isNativeAudioContext(nativeContext) &&\n                true !==\n                    cacheTestResult(testAudioNodeDisconnectMethodSupport, () => {\n                        return testAudioNodeDisconnectMethodSupport(nativeContext);\n                    })) {\n                wrapAudioNodeDisconnectMethod(nativeAudioNode);\n            }\n            AUDIO_NODE_STORE.set(this, nativeAudioNode);\n            EVENT_LISTENERS.set(this, new Set());\n            if (context.state !== \'closed\' && isActive) {\n                setInternalStateToActive(this);\n            }\n            addAudioNodeConnections(this, audioNodeRenderer, nativeAudioNode);\n        }\n        get channelCount() {\n            return this._nativeAudioNode.channelCount;\n        }\n        set channelCount(value) {\n            this._nativeAudioNode.channelCount = value;\n        }\n        get channelCountMode() {\n            return this._nativeAudioNode.channelCountMode;\n        }\n        set channelCountMode(value) {\n            this._nativeAudioNode.channelCountMode = value;\n        }\n        get channelInterpretation() {\n            return this._nativeAudioNode.channelInterpretation;\n        }\n        set channelInterpretation(value) {\n            this._nativeAudioNode.channelInterpretation = value;\n        }\n        get context() {\n            return this._context;\n        }\n        get numberOfInputs() {\n            return this._nativeAudioNode.numberOfInputs;\n        }\n        get numberOfOutputs() {\n            return this._nativeAudioNode.numberOfOutputs;\n        }\n        // tslint:disable-next-line:invalid-void\n        connect(destination, output = 0, input = 0) {\n            // Bug #174: Safari does expose a wrong numberOfOutputs for MediaStreamAudioDestinationNodes.\n            if (output < 0 || output >= this._nativeAudioNode.numberOfOutputs) {\n                throw createIndexSizeError();\n            }\n            const nativeContext = getNativeContext(this._context);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            if (isNativeAudioNode(destination) || isNativeAudioParam(destination)) {\n                throw createInvalidAccessError();\n            }\n            if (isAudioNode(destination)) {\n                const nativeDestinationAudioNode = get_native_audio_node_getNativeAudioNode(destination);\n                try {\n                    const connection = connect_native_audio_node_to_native_audio_node_connectNativeAudioNodeToNativeAudioNode(this._nativeAudioNode, nativeDestinationAudioNode, output, input);\n                    const isPassive = is_passive_audio_node_isPassiveAudioNode(this);\n                    if (isOffline || isPassive) {\n                        this._nativeAudioNode.disconnect(...connection);\n                    }\n                    if (this.context.state !== \'closed\' && !isPassive && is_passive_audio_node_isPassiveAudioNode(destination)) {\n                        setInternalStateToActive(destination);\n                    }\n                }\n                catch (err) {\n                    // Bug #41: Safari does not throw the correct exception so far.\n                    if (err.code === 12) {\n                        throw createInvalidAccessError();\n                    }\n                    throw err;\n                }\n                const isNewConnectionToAudioNode = addConnectionToAudioNode(this, destination, output, input, isOffline);\n                // Bug #164: Only Firefox detects cycles so far.\n                if (isNewConnectionToAudioNode) {\n                    const cycles = detectCycles([this], destination);\n                    visitEachAudioNodeOnce(cycles, createIncrementCycleCounter(isOffline));\n                }\n                return destination;\n            }\n            const nativeAudioParam = get_native_audio_param_getNativeAudioParam(destination);\n            /*\n             * Bug #147 & #153: Safari does not support to connect an input signal to the playbackRate AudioParam of an\n             * AudioBufferSourceNode. This can\'t be easily detected and that\'s why the outdated name property is used here to identify\n             * Safari.\n             */\n            if (nativeAudioParam.name === \'playbackRate\') {\n                throw createNotSupportedError();\n            }\n            try {\n                this._nativeAudioNode.connect(nativeAudioParam, output);\n                if (isOffline || is_passive_audio_node_isPassiveAudioNode(this)) {\n                    this._nativeAudioNode.disconnect(nativeAudioParam, output);\n                }\n            }\n            catch (err) {\n                // Bug #58: Only Firefox does throw an InvalidStateError yet.\n                if (err.code === 12) {\n                    throw createInvalidAccessError();\n                }\n                throw err;\n            }\n            const isNewConnectionToAudioParam = addConnectionToAudioParamOfAudioContext(this, destination, output, isOffline);\n            // Bug #164: Only Firefox detects cycles so far.\n            if (isNewConnectionToAudioParam) {\n                const cycles = detectCycles([this], destination);\n                visitEachAudioNodeOnce(cycles, createIncrementCycleCounter(isOffline));\n            }\n        }\n        disconnect(destinationOrOutput, output, input) {\n            let destinations;\n            const nativeContext = getNativeContext(this._context);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            if (destinationOrOutput === undefined) {\n                destinations = deleteAnyConnection(this, isOffline);\n            }\n            else if (typeof destinationOrOutput === \'number\') {\n                if (destinationOrOutput < 0 || destinationOrOutput >= this.numberOfOutputs) {\n                    throw createIndexSizeError();\n                }\n                destinations = deleteConnectionAtOutput(this, isOffline, destinationOrOutput);\n            }\n            else {\n                if (output !== undefined && (output < 0 || output >= this.numberOfOutputs)) {\n                    throw createIndexSizeError();\n                }\n                if (isAudioNode(destinationOrOutput) && input !== undefined && (input < 0 || input >= destinationOrOutput.numberOfInputs)) {\n                    throw createIndexSizeError();\n                }\n                destinations = deleteConnectionToDestination(this, isOffline, destinationOrOutput, output, input);\n                if (destinations.length === 0) {\n                    throw createInvalidAccessError();\n                }\n            }\n            // Bug #164: Only Firefox detects cycles so far.\n            for (const destination of destinations) {\n                const cycles = detectCycles([this], destination);\n                visitEachAudioNodeOnce(cycles, decrementCycleCounter);\n            }\n        }\n    };\n};\n//# sourceMappingURL=audio-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-param-factory.js\n\nconst createAudioParamFactory = (addAudioParamConnections, audioParamAudioNodeStore, audioParamStore, createAudioParamRenderer, createCancelAndHoldAutomationEvent, createCancelScheduledValuesAutomationEvent, createExponentialRampToValueAutomationEvent, createLinearRampToValueAutomationEvent, createSetTargetAutomationEvent, createSetValueAutomationEvent, createSetValueCurveAutomationEvent, nativeAudioContextConstructor, setValueAtTimeUntilPossible) => {\n    return (audioNode, isAudioParamOfOfflineAudioContext, nativeAudioParam, maxValue = null, minValue = null) => {\n        const automationEventList = new bundle["AutomationEventList"](nativeAudioParam.defaultValue);\n        const audioParamRenderer = isAudioParamOfOfflineAudioContext ? createAudioParamRenderer(automationEventList) : null;\n        const audioParam = {\n            get defaultValue() {\n                return nativeAudioParam.defaultValue;\n            },\n            get maxValue() {\n                return maxValue === null ? nativeAudioParam.maxValue : maxValue;\n            },\n            get minValue() {\n                return minValue === null ? nativeAudioParam.minValue : minValue;\n            },\n            get value() {\n                return nativeAudioParam.value;\n            },\n            set value(value) {\n                nativeAudioParam.value = value;\n                // Bug #98: Firefox & Safari do not yet treat the value setter like a call to setValueAtTime().\n                audioParam.setValueAtTime(value, audioNode.context.currentTime);\n            },\n            cancelAndHoldAtTime(cancelTime) {\n                // Bug #28: Firefox & Safari do not yet implement cancelAndHoldAtTime().\n                if (typeof nativeAudioParam.cancelAndHoldAtTime === \'function\') {\n                    if (audioParamRenderer === null) {\n                        automationEventList.flush(audioNode.context.currentTime);\n                    }\n                    automationEventList.add(createCancelAndHoldAutomationEvent(cancelTime));\n                    nativeAudioParam.cancelAndHoldAtTime(cancelTime);\n                }\n                else {\n                    const previousLastEvent = Array.from(automationEventList).pop();\n                    if (audioParamRenderer === null) {\n                        automationEventList.flush(audioNode.context.currentTime);\n                    }\n                    automationEventList.add(createCancelAndHoldAutomationEvent(cancelTime));\n                    const currentLastEvent = Array.from(automationEventList).pop();\n                    nativeAudioParam.cancelScheduledValues(cancelTime);\n                    if (previousLastEvent !== currentLastEvent && currentLastEvent !== undefined) {\n                        if (currentLastEvent.type === \'exponentialRampToValue\') {\n                            nativeAudioParam.exponentialRampToValueAtTime(currentLastEvent.value, currentLastEvent.endTime);\n                        }\n                        else if (currentLastEvent.type === \'linearRampToValue\') {\n                            nativeAudioParam.linearRampToValueAtTime(currentLastEvent.value, currentLastEvent.endTime);\n                        }\n                        else if (currentLastEvent.type === \'setValue\') {\n                            nativeAudioParam.setValueAtTime(currentLastEvent.value, currentLastEvent.startTime);\n                        }\n                        else if (currentLastEvent.type === \'setValueCurve\') {\n                            nativeAudioParam.setValueCurveAtTime(currentLastEvent.values, currentLastEvent.startTime, currentLastEvent.duration);\n                        }\n                    }\n                }\n                return audioParam;\n            },\n            cancelScheduledValues(cancelTime) {\n                if (audioParamRenderer === null) {\n                    automationEventList.flush(audioNode.context.currentTime);\n                }\n                automationEventList.add(createCancelScheduledValuesAutomationEvent(cancelTime));\n                nativeAudioParam.cancelScheduledValues(cancelTime);\n                return audioParam;\n            },\n            exponentialRampToValueAtTime(value, endTime) {\n                // Bug #45: Safari does not throw an error yet.\n                if (value === 0) {\n                    throw new RangeError();\n                }\n                // Bug #187: Safari does not throw an error yet.\n                if (!Number.isFinite(endTime) || endTime < 0) {\n                    throw new RangeError();\n                }\n                if (audioParamRenderer === null) {\n                    automationEventList.flush(audioNode.context.currentTime);\n                }\n                automationEventList.add(createExponentialRampToValueAutomationEvent(value, endTime));\n                nativeAudioParam.exponentialRampToValueAtTime(value, endTime);\n                return audioParam;\n            },\n            linearRampToValueAtTime(value, endTime) {\n                if (audioParamRenderer === null) {\n                    automationEventList.flush(audioNode.context.currentTime);\n                }\n                automationEventList.add(createLinearRampToValueAutomationEvent(value, endTime));\n                nativeAudioParam.linearRampToValueAtTime(value, endTime);\n                return audioParam;\n            },\n            setTargetAtTime(target, startTime, timeConstant) {\n                if (audioParamRenderer === null) {\n                    automationEventList.flush(audioNode.context.currentTime);\n                }\n                automationEventList.add(createSetTargetAutomationEvent(target, startTime, timeConstant));\n                nativeAudioParam.setTargetAtTime(target, startTime, timeConstant);\n                return audioParam;\n            },\n            setValueAtTime(value, startTime) {\n                if (audioParamRenderer === null) {\n                    automationEventList.flush(audioNode.context.currentTime);\n                }\n                automationEventList.add(createSetValueAutomationEvent(value, startTime));\n                nativeAudioParam.setValueAtTime(value, startTime);\n                return audioParam;\n            },\n            setValueCurveAtTime(values, startTime, duration) {\n                // Bug 183: Safari only accepts a Float32Array.\n                const convertedValues = values instanceof Float32Array ? values : new Float32Array(values);\n                /*\n                 * Bug #152: Safari does not correctly interpolate the values of the curve.\n                 * @todo Unfortunately there is no way to test for this behavior in a synchronous fashion which is why testing for the\n                 * existence of the webkitAudioContext is used as a workaround here.\n                 */\n                if (nativeAudioContextConstructor !== null && nativeAudioContextConstructor.name === \'webkitAudioContext\') {\n                    const endTime = startTime + duration;\n                    const sampleRate = audioNode.context.sampleRate;\n                    const firstSample = Math.ceil(startTime * sampleRate);\n                    const lastSample = Math.floor(endTime * sampleRate);\n                    const numberOfInterpolatedValues = lastSample - firstSample;\n                    const interpolatedValues = new Float32Array(numberOfInterpolatedValues);\n                    for (let i = 0; i < numberOfInterpolatedValues; i += 1) {\n                        const theoreticIndex = ((convertedValues.length - 1) / duration) * ((firstSample + i) / sampleRate - startTime);\n                        const lowerIndex = Math.floor(theoreticIndex);\n                        const upperIndex = Math.ceil(theoreticIndex);\n                        interpolatedValues[i] =\n                            lowerIndex === upperIndex\n                                ? convertedValues[lowerIndex]\n                                : (1 - (theoreticIndex - lowerIndex)) * convertedValues[lowerIndex] +\n                                    (1 - (upperIndex - theoreticIndex)) * convertedValues[upperIndex];\n                    }\n                    if (audioParamRenderer === null) {\n                        automationEventList.flush(audioNode.context.currentTime);\n                    }\n                    automationEventList.add(createSetValueCurveAutomationEvent(interpolatedValues, startTime, duration));\n                    nativeAudioParam.setValueCurveAtTime(interpolatedValues, startTime, duration);\n                    const timeOfLastSample = lastSample / sampleRate;\n                    if (timeOfLastSample < endTime) {\n                        setValueAtTimeUntilPossible(audioParam, interpolatedValues[interpolatedValues.length - 1], timeOfLastSample);\n                    }\n                    setValueAtTimeUntilPossible(audioParam, convertedValues[convertedValues.length - 1], endTime);\n                }\n                else {\n                    if (audioParamRenderer === null) {\n                        automationEventList.flush(audioNode.context.currentTime);\n                    }\n                    automationEventList.add(createSetValueCurveAutomationEvent(convertedValues, startTime, duration));\n                    nativeAudioParam.setValueCurveAtTime(convertedValues, startTime, duration);\n                }\n                return audioParam;\n            }\n        };\n        audioParamStore.set(audioParam, nativeAudioParam);\n        audioParamAudioNodeStore.set(audioParam, audioNode);\n        addAudioParamConnections(audioParam, audioParamRenderer);\n        return audioParam;\n    };\n};\n//# sourceMappingURL=audio-param-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-param-renderer.js\nconst audio_param_renderer_createAudioParamRenderer = (automationEventList) => {\n    return {\n        replay(audioParam) {\n            for (const automationEvent of automationEventList) {\n                if (automationEvent.type === \'exponentialRampToValue\') {\n                    const { endTime, value } = automationEvent;\n                    audioParam.exponentialRampToValueAtTime(value, endTime);\n                }\n                else if (automationEvent.type === \'linearRampToValue\') {\n                    const { endTime, value } = automationEvent;\n                    audioParam.linearRampToValueAtTime(value, endTime);\n                }\n                else if (automationEvent.type === \'setTarget\') {\n                    const { startTime, target, timeConstant } = automationEvent;\n                    audioParam.setTargetAtTime(target, startTime, timeConstant);\n                }\n                else if (automationEvent.type === \'setValue\') {\n                    const { startTime, value } = automationEvent;\n                    audioParam.setValueAtTime(value, startTime);\n                }\n                else if (automationEvent.type === \'setValueCurve\') {\n                    const { duration, startTime, values } = automationEvent;\n                    audioParam.setValueCurveAtTime(values, startTime, duration);\n                }\n                else {\n                    throw new Error("Can\'t apply an unknown automation.");\n                }\n            }\n        }\n    };\n};\n//# sourceMappingURL=audio-param-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/read-only-map.js\nclass ReadOnlyMap {\n    constructor(parameters) {\n        this._map = new Map(parameters);\n    }\n    get size() {\n        return this._map.size;\n    }\n    entries() {\n        return this._map.entries();\n    }\n    forEach(callback, thisArg = null) {\n        return this._map.forEach((value, key) => callback.call(thisArg, value, key, this));\n    }\n    get(name) {\n        return this._map.get(name);\n    }\n    has(name) {\n        return this._map.has(name);\n    }\n    keys() {\n        return this._map.keys();\n    }\n    values() {\n        return this._map.values();\n    }\n}\n//# sourceMappingURL=read-only-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-worklet-node-constructor.js\n\n\nconst audio_worklet_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    // Bug #61: The channelCountMode should be \'max\' according to the spec but is set to \'explicit\' to achieve consistent behavior.\n    channelCountMode: \'explicit\',\n    channelInterpretation: \'speakers\',\n    numberOfInputs: 1,\n    numberOfOutputs: 1,\n    parameterData: {},\n    processorOptions: {}\n};\nconst createAudioWorkletNodeConstructor = (addUnrenderedAudioWorkletNode, audioNodeConstructor, createAudioParam, createAudioWorkletNodeRenderer, createNativeAudioWorkletNode, getAudioNodeConnections, getBackupOfflineAudioContext, getNativeContext, isNativeOfflineAudioContext, nativeAudioWorkletNodeConstructor, sanitizeAudioWorkletNodeOptions, setActiveAudioWorkletNodeInputs, wrapEventListener) => {\n    return class AudioWorkletNode extends audioNodeConstructor {\n        constructor(context, name, options) {\n            var _a;\n            const nativeContext = getNativeContext(context);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const mergedOptions = sanitizeAudioWorkletNodeOptions({ ...audio_worklet_node_constructor_DEFAULT_OPTIONS, ...options });\n            const nodeNameToProcessorConstructorMap = NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS.get(nativeContext);\n            const processorConstructor = nodeNameToProcessorConstructorMap === null || nodeNameToProcessorConstructorMap === void 0 ? void 0 : nodeNameToProcessorConstructorMap.get(name);\n            // Bug #186: Chrome, Edge and Opera do not allow to create an AudioWorkletNode on a closed AudioContext.\n            const nativeContextOrBackupOfflineAudioContext = isOffline || nativeContext.state !== \'closed\'\n                ? nativeContext\n                : (_a = getBackupOfflineAudioContext(nativeContext)) !== null && _a !== void 0 ? _a : nativeContext;\n            const nativeAudioWorkletNode = createNativeAudioWorkletNode(nativeContextOrBackupOfflineAudioContext, isOffline ? null : context.baseLatency, nativeAudioWorkletNodeConstructor, name, processorConstructor, mergedOptions);\n            const audioWorkletNodeRenderer = ((isOffline ? createAudioWorkletNodeRenderer(name, mergedOptions, processorConstructor) : null));\n            /*\n             * @todo Add a mechanism to switch an AudioWorkletNode to passive once the process() function of the AudioWorkletProcessor\n             * returns false.\n             */\n            super(context, true, nativeAudioWorkletNode, audioWorkletNodeRenderer);\n            const parameters = [];\n            nativeAudioWorkletNode.parameters.forEach((nativeAudioParam, nm) => {\n                const audioParam = createAudioParam(this, isOffline, nativeAudioParam);\n                parameters.push([nm, audioParam]);\n            });\n            this._nativeAudioWorkletNode = nativeAudioWorkletNode;\n            this._onprocessorerror = null;\n            this._parameters = new ReadOnlyMap(parameters);\n            /*\n             * Bug #86 & #87: Invoking the renderer of an AudioWorkletNode might be necessary if it has no direct or indirect connection to\n             * the destination.\n             */\n            if (isOffline) {\n                addUnrenderedAudioWorkletNode(nativeContext, this);\n            }\n            const { activeInputs } = getAudioNodeConnections(this);\n            setActiveAudioWorkletNodeInputs(nativeAudioWorkletNode, activeInputs);\n        }\n        get onprocessorerror() {\n            return this._onprocessorerror;\n        }\n        set onprocessorerror(value) {\n            const wrappedListener = typeof value === \'function\' ? wrapEventListener(this, value) : null;\n            this._nativeAudioWorkletNode.onprocessorerror = wrappedListener;\n            const nativeOnProcessorError = this._nativeAudioWorkletNode.onprocessorerror;\n            this._onprocessorerror =\n                nativeOnProcessorError !== null && nativeOnProcessorError === wrappedListener\n                    ? value\n                    : nativeOnProcessorError;\n        }\n        get parameters() {\n            if (this._parameters === null) {\n                // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.\n                return this._nativeAudioWorkletNode.parameters;\n            }\n            return this._parameters;\n        }\n        get port() {\n            return this._nativeAudioWorkletNode.port;\n        }\n    };\n};\n//# sourceMappingURL=audio-worklet-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/copy-from-channel.js\nfunction copyFromChannel(audioBuffer, \n// @todo There is currently no way to define something like { [ key: number | string ]: Float32Array }\nparent, key, channelNumber, bufferOffset) {\n    if (typeof audioBuffer.copyFromChannel === \'function\') {\n        // The byteLength will be 0 when the ArrayBuffer was transferred.\n        if (parent[key].byteLength === 0) {\n            parent[key] = new Float32Array(128);\n        }\n        audioBuffer.copyFromChannel(parent[key], channelNumber, bufferOffset);\n        // Bug #5: Safari does not support copyFromChannel().\n    }\n    else {\n        const channelData = audioBuffer.getChannelData(channelNumber);\n        // The byteLength will be 0 when the ArrayBuffer was transferred.\n        if (parent[key].byteLength === 0) {\n            parent[key] = channelData.slice(bufferOffset, bufferOffset + 128);\n        }\n        else {\n            const slicedInput = new Float32Array(channelData.buffer, bufferOffset * Float32Array.BYTES_PER_ELEMENT, 128);\n            parent[key].set(slicedInput);\n        }\n    }\n}\n//# sourceMappingURL=copy-from-channel.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/copy-to-channel.js\nconst copyToChannel = (audioBuffer, parent, key, channelNumber, bufferOffset) => {\n    if (typeof audioBuffer.copyToChannel === \'function\') {\n        // The byteLength will be 0 when the ArrayBuffer was transferred.\n        if (parent[key].byteLength !== 0) {\n            audioBuffer.copyToChannel(parent[key], channelNumber, bufferOffset);\n        }\n        // Bug #5: Safari does not support copyToChannel().\n    }\n    else {\n        // The byteLength will be 0 when the ArrayBuffer was transferred.\n        if (parent[key].byteLength !== 0) {\n            audioBuffer.getChannelData(channelNumber).set(parent[key], bufferOffset);\n        }\n    }\n};\n//# sourceMappingURL=copy-to-channel.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/create-nested-arrays.js\nconst createNestedArrays = (x, y) => {\n    const arrays = [];\n    for (let i = 0; i < x; i += 1) {\n        const array = [];\n        const length = typeof y === \'number\' ? y : y[i];\n        for (let j = 0; j < length; j += 1) {\n            array.push(new Float32Array(128));\n        }\n        arrays.push(array);\n    }\n    return arrays;\n};\n//# sourceMappingURL=create-nested-arrays.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/get-audio-worklet-processor.js\n\n\n\nconst getAudioWorkletProcessor = (nativeOfflineAudioContext, proxy) => {\n    const nodeToProcessorMap = get_value_for_key_getValueForKey(NODE_TO_PROCESSOR_MAPS, nativeOfflineAudioContext);\n    const nativeAudioWorkletNode = get_native_audio_node_getNativeAudioNode(proxy);\n    return get_value_for_key_getValueForKey(nodeToProcessorMap, nativeAudioWorkletNode);\n};\n//# sourceMappingURL=get-audio-worklet-processor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/audio-worklet-node-renderer-factory.js\n\n\n\n\n\n\nconst processBuffer = async (proxy, renderedBuffer, nativeOfflineAudioContext, options, outputChannelCount, processorConstructor, exposeCurrentFrameAndCurrentTime) => {\n    // Ceil the length to the next full render quantum.\n    // Bug #17: Safari does not yet expose the length.\n    const length = renderedBuffer === null ? Math.ceil(proxy.context.length / 128) * 128 : renderedBuffer.length;\n    const numberOfInputChannels = options.channelCount * options.numberOfInputs;\n    const numberOfOutputChannels = outputChannelCount.reduce((sum, value) => sum + value, 0);\n    const processedBuffer = numberOfOutputChannels === 0\n        ? null\n        : nativeOfflineAudioContext.createBuffer(numberOfOutputChannels, length, nativeOfflineAudioContext.sampleRate);\n    if (processorConstructor === undefined) {\n        throw new Error(\'Missing the processor constructor.\');\n    }\n    const audioNodeConnections = get_audio_node_connections_getAudioNodeConnections(proxy);\n    const audioWorkletProcessor = await getAudioWorkletProcessor(nativeOfflineAudioContext, proxy);\n    const inputs = createNestedArrays(options.numberOfInputs, options.channelCount);\n    const outputs = createNestedArrays(options.numberOfOutputs, outputChannelCount);\n    const parameters = Array.from(proxy.parameters.keys()).reduce((prmtrs, name) => ({ ...prmtrs, [name]: new Float32Array(128) }), {});\n    for (let i = 0; i < length; i += 128) {\n        if (options.numberOfInputs > 0 && renderedBuffer !== null) {\n            for (let j = 0; j < options.numberOfInputs; j += 1) {\n                for (let k = 0; k < options.channelCount; k += 1) {\n                    copyFromChannel(renderedBuffer, inputs[j], k, k, i);\n                }\n            }\n        }\n        if (processorConstructor.parameterDescriptors !== undefined && renderedBuffer !== null) {\n            processorConstructor.parameterDescriptors.forEach(({ name }, index) => {\n                copyFromChannel(renderedBuffer, parameters, name, numberOfInputChannels + index, i);\n            });\n        }\n        for (let j = 0; j < options.numberOfInputs; j += 1) {\n            for (let k = 0; k < outputChannelCount[j]; k += 1) {\n                // The byteLength will be 0 when the ArrayBuffer was transferred.\n                if (outputs[j][k].byteLength === 0) {\n                    outputs[j][k] = new Float32Array(128);\n                }\n            }\n        }\n        try {\n            const potentiallyEmptyInputs = inputs.map((input, index) => {\n                if (audioNodeConnections.activeInputs[index].size === 0) {\n                    return [];\n                }\n                return input;\n            });\n            const activeSourceFlag = exposeCurrentFrameAndCurrentTime(i / nativeOfflineAudioContext.sampleRate, nativeOfflineAudioContext.sampleRate, () => audioWorkletProcessor.process(potentiallyEmptyInputs, outputs, parameters));\n            if (processedBuffer !== null) {\n                for (let j = 0, outputChannelSplitterNodeOutput = 0; j < options.numberOfOutputs; j += 1) {\n                    for (let k = 0; k < outputChannelCount[j]; k += 1) {\n                        copyToChannel(processedBuffer, outputs[j], k, outputChannelSplitterNodeOutput + k, i);\n                    }\n                    outputChannelSplitterNodeOutput += outputChannelCount[j];\n                }\n            }\n            if (!activeSourceFlag) {\n                break;\n            }\n        }\n        catch (error) {\n            proxy.dispatchEvent(new ErrorEvent(\'processorerror\', {\n                colno: error.colno,\n                filename: error.filename,\n                lineno: error.lineno,\n                message: error.message\n            }));\n            break;\n        }\n    }\n    return processedBuffer;\n};\nconst createAudioWorkletNodeRendererFactory = (connectAudioParam, connectMultipleOutputs, createNativeAudioBufferSourceNode, createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeConstantSourceNode, createNativeGainNode, deleteUnrenderedAudioWorkletNode, disconnectMultipleOutputs, exposeCurrentFrameAndCurrentTime, getNativeAudioNode, nativeAudioWorkletNodeConstructor, nativeOfflineAudioContextConstructor, renderAutomation, renderInputsOfAudioNode, renderNativeOfflineAudioContext) => {\n    return (name, options, processorConstructor) => {\n        const renderedNativeAudioNodes = new WeakMap();\n        let processedBufferPromise = null;\n        const createAudioNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeAudioWorkletNode = getNativeAudioNode(proxy);\n            let nativeOutputNodes = null;\n            const nativeAudioWorkletNodeIsOwnedByContext = isOwnedByContext(nativeAudioWorkletNode, nativeOfflineAudioContext);\n            const outputChannelCount = Array.isArray(options.outputChannelCount)\n                ? options.outputChannelCount\n                : Array.from(options.outputChannelCount);\n            // Bug #61: Only Chrome, Edge, Firefox & Opera have an implementation of the AudioWorkletNode yet.\n            if (nativeAudioWorkletNodeConstructor === null) {\n                const numberOfOutputChannels = outputChannelCount.reduce((sum, value) => sum + value, 0);\n                const outputChannelSplitterNode = createNativeChannelSplitterNode(nativeOfflineAudioContext, {\n                    channelCount: Math.max(1, numberOfOutputChannels),\n                    channelCountMode: \'explicit\',\n                    channelInterpretation: \'discrete\',\n                    numberOfOutputs: Math.max(1, numberOfOutputChannels)\n                });\n                const outputChannelMergerNodes = [];\n                for (let i = 0; i < proxy.numberOfOutputs; i += 1) {\n                    outputChannelMergerNodes.push(createNativeChannelMergerNode(nativeOfflineAudioContext, {\n                        channelCount: 1,\n                        channelCountMode: \'explicit\',\n                        channelInterpretation: \'speakers\',\n                        numberOfInputs: outputChannelCount[i]\n                    }));\n                }\n                const outputGainNode = createNativeGainNode(nativeOfflineAudioContext, {\n                    channelCount: options.channelCount,\n                    channelCountMode: options.channelCountMode,\n                    channelInterpretation: options.channelInterpretation,\n                    gain: 1\n                });\n                outputGainNode.connect = connectMultipleOutputs.bind(null, outputChannelMergerNodes);\n                outputGainNode.disconnect = disconnectMultipleOutputs.bind(null, outputChannelMergerNodes);\n                nativeOutputNodes = [outputChannelSplitterNode, outputChannelMergerNodes, outputGainNode];\n            }\n            else if (!nativeAudioWorkletNodeIsOwnedByContext) {\n                nativeAudioWorkletNode = new nativeAudioWorkletNodeConstructor(nativeOfflineAudioContext, name);\n            }\n            renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeOutputNodes === null ? nativeAudioWorkletNode : nativeOutputNodes[2]);\n            if (nativeOutputNodes !== null) {\n                if (processedBufferPromise === null) {\n                    if (processorConstructor === undefined) {\n                        throw new Error(\'Missing the processor constructor.\');\n                    }\n                    if (nativeOfflineAudioContextConstructor === null) {\n                        throw new Error(\'Missing the native OfflineAudioContext constructor.\');\n                    }\n                    // Bug #47: The AudioDestinationNode in Safari gets not initialized correctly.\n                    const numberOfInputChannels = proxy.channelCount * proxy.numberOfInputs;\n                    const numberOfParameters = processorConstructor.parameterDescriptors === undefined ? 0 : processorConstructor.parameterDescriptors.length;\n                    const numberOfChannels = numberOfInputChannels + numberOfParameters;\n                    const renderBuffer = async () => {\n                        const partialOfflineAudioContext = new nativeOfflineAudioContextConstructor(numberOfChannels, \n                        // Ceil the length to the next full render quantum.\n                        // Bug #17: Safari does not yet expose the length.\n                        Math.ceil(proxy.context.length / 128) * 128, nativeOfflineAudioContext.sampleRate);\n                        const gainNodes = [];\n                        const inputChannelSplitterNodes = [];\n                        for (let i = 0; i < options.numberOfInputs; i += 1) {\n                            gainNodes.push(createNativeGainNode(partialOfflineAudioContext, {\n                                channelCount: options.channelCount,\n                                channelCountMode: options.channelCountMode,\n                                channelInterpretation: options.channelInterpretation,\n                                gain: 1\n                            }));\n                            inputChannelSplitterNodes.push(createNativeChannelSplitterNode(partialOfflineAudioContext, {\n                                channelCount: options.channelCount,\n                                channelCountMode: \'explicit\',\n                                channelInterpretation: \'discrete\',\n                                numberOfOutputs: options.channelCount\n                            }));\n                        }\n                        const constantSourceNodes = await Promise.all(Array.from(proxy.parameters.values()).map(async (audioParam) => {\n                            const constantSourceNode = createNativeConstantSourceNode(partialOfflineAudioContext, {\n                                channelCount: 1,\n                                channelCountMode: \'explicit\',\n                                channelInterpretation: \'discrete\',\n                                offset: audioParam.value\n                            });\n                            await renderAutomation(partialOfflineAudioContext, audioParam, constantSourceNode.offset, trace);\n                            return constantSourceNode;\n                        }));\n                        const inputChannelMergerNode = createNativeChannelMergerNode(partialOfflineAudioContext, {\n                            channelCount: 1,\n                            channelCountMode: \'explicit\',\n                            channelInterpretation: \'speakers\',\n                            numberOfInputs: Math.max(1, numberOfInputChannels + numberOfParameters)\n                        });\n                        for (let i = 0; i < options.numberOfInputs; i += 1) {\n                            gainNodes[i].connect(inputChannelSplitterNodes[i]);\n                            for (let j = 0; j < options.channelCount; j += 1) {\n                                inputChannelSplitterNodes[i].connect(inputChannelMergerNode, j, i * options.channelCount + j);\n                            }\n                        }\n                        for (const [index, constantSourceNode] of constantSourceNodes.entries()) {\n                            constantSourceNode.connect(inputChannelMergerNode, 0, numberOfInputChannels + index);\n                            constantSourceNode.start(0);\n                        }\n                        inputChannelMergerNode.connect(partialOfflineAudioContext.destination);\n                        await Promise.all(gainNodes.map((gainNode) => renderInputsOfAudioNode(proxy, partialOfflineAudioContext, gainNode, trace)));\n                        return renderNativeOfflineAudioContext(partialOfflineAudioContext);\n                    };\n                    processedBufferPromise = processBuffer(proxy, numberOfChannels === 0 ? null : await renderBuffer(), nativeOfflineAudioContext, options, outputChannelCount, processorConstructor, exposeCurrentFrameAndCurrentTime);\n                }\n                const processedBuffer = await processedBufferPromise;\n                const audioBufferSourceNode = createNativeAudioBufferSourceNode(nativeOfflineAudioContext, {\n                    buffer: null,\n                    channelCount: 2,\n                    channelCountMode: \'max\',\n                    channelInterpretation: \'speakers\',\n                    loop: false,\n                    loopEnd: 0,\n                    loopStart: 0,\n                    playbackRate: 1\n                });\n                const [outputChannelSplitterNode, outputChannelMergerNodes, outputGainNode] = nativeOutputNodes;\n                if (processedBuffer !== null) {\n                    audioBufferSourceNode.buffer = processedBuffer;\n                    audioBufferSourceNode.start(0);\n                }\n                audioBufferSourceNode.connect(outputChannelSplitterNode);\n                for (let i = 0, outputChannelSplitterNodeOutput = 0; i < proxy.numberOfOutputs; i += 1) {\n                    const outputChannelMergerNode = outputChannelMergerNodes[i];\n                    for (let j = 0; j < outputChannelCount[i]; j += 1) {\n                        outputChannelSplitterNode.connect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);\n                    }\n                    outputChannelSplitterNodeOutput += outputChannelCount[i];\n                }\n                return outputGainNode;\n            }\n            if (!nativeAudioWorkletNodeIsOwnedByContext) {\n                for (const [nm, audioParam] of proxy.parameters.entries()) {\n                    await renderAutomation(nativeOfflineAudioContext, audioParam, \n                    // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.\n                    nativeAudioWorkletNode.parameters.get(nm), trace);\n                }\n            }\n            else {\n                for (const [nm, audioParam] of proxy.parameters.entries()) {\n                    await connectAudioParam(nativeOfflineAudioContext, audioParam, \n                    // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.\n                    nativeAudioWorkletNode.parameters.get(nm), trace);\n                }\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAudioWorkletNode, trace);\n            return nativeAudioWorkletNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                deleteUnrenderedAudioWorkletNode(nativeOfflineAudioContext, proxy);\n                const renderedNativeAudioWorkletNodeOrGainNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeAudioWorkletNodeOrGainNode !== undefined) {\n                    return Promise.resolve(renderedNativeAudioWorkletNodeOrGainNode);\n                }\n                return createAudioNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=audio-worklet-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/base-audio-context-constructor.js\nconst createBaseAudioContextConstructor = (addAudioWorkletModule, analyserNodeConstructor, audioBufferConstructor, audioBufferSourceNodeConstructor, biquadFilterNodeConstructor, channelMergerNodeConstructor, channelSplitterNodeConstructor, constantSourceNodeConstructor, convolverNodeConstructor, decodeAudioData, delayNodeConstructor, dynamicsCompressorNodeConstructor, gainNodeConstructor, iIRFilterNodeConstructor, minimalBaseAudioContextConstructor, oscillatorNodeConstructor, pannerNodeConstructor, periodicWaveConstructor, stereoPannerNodeConstructor, waveShaperNodeConstructor) => {\n    return class BaseAudioContext extends minimalBaseAudioContextConstructor {\n        constructor(_nativeContext, numberOfChannels) {\n            super(_nativeContext, numberOfChannels);\n            this._nativeContext = _nativeContext;\n            this._audioWorklet =\n                addAudioWorkletModule === undefined\n                    ? undefined\n                    : {\n                        addModule: (moduleURL, options) => {\n                            return addAudioWorkletModule(this, moduleURL, options);\n                        }\n                    };\n        }\n        get audioWorklet() {\n            return this._audioWorklet;\n        }\n        createAnalyser() {\n            return new analyserNodeConstructor(this);\n        }\n        createBiquadFilter() {\n            return new biquadFilterNodeConstructor(this);\n        }\n        createBuffer(numberOfChannels, length, sampleRate) {\n            return new audioBufferConstructor({ length, numberOfChannels, sampleRate });\n        }\n        createBufferSource() {\n            return new audioBufferSourceNodeConstructor(this);\n        }\n        createChannelMerger(numberOfInputs = 6) {\n            return new channelMergerNodeConstructor(this, { numberOfInputs });\n        }\n        createChannelSplitter(numberOfOutputs = 6) {\n            return new channelSplitterNodeConstructor(this, { numberOfOutputs });\n        }\n        createConstantSource() {\n            return new constantSourceNodeConstructor(this);\n        }\n        createConvolver() {\n            return new convolverNodeConstructor(this);\n        }\n        createDelay(maxDelayTime = 1) {\n            return new delayNodeConstructor(this, { maxDelayTime });\n        }\n        createDynamicsCompressor() {\n            return new dynamicsCompressorNodeConstructor(this);\n        }\n        createGain() {\n            return new gainNodeConstructor(this);\n        }\n        createIIRFilter(feedforward, feedback) {\n            return new iIRFilterNodeConstructor(this, { feedback, feedforward });\n        }\n        createOscillator() {\n            return new oscillatorNodeConstructor(this);\n        }\n        createPanner() {\n            return new pannerNodeConstructor(this);\n        }\n        createPeriodicWave(real, imag, constraints = { disableNormalization: false }) {\n            return new periodicWaveConstructor(this, { ...constraints, imag, real });\n        }\n        createStereoPanner() {\n            return new stereoPannerNodeConstructor(this);\n        }\n        createWaveShaper() {\n            return new waveShaperNodeConstructor(this);\n        }\n        decodeAudioData(audioData, successCallback, errorCallback) {\n            return decodeAudioData(this._nativeContext, audioData)\n                .then((audioBuffer) => {\n                if (typeof successCallback === \'function\') {\n                    successCallback(audioBuffer);\n                }\n                return audioBuffer;\n            })\n                .catch((err) => {\n                if (typeof errorCallback === \'function\') {\n                    errorCallback(err);\n                }\n                throw err;\n            });\n        }\n    };\n};\n//# sourceMappingURL=base-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/biquad-filter-node-constructor.js\n\nconst biquad_filter_node_constructor_DEFAULT_OPTIONS = {\n    Q: 1,\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    detune: 0,\n    frequency: 350,\n    gain: 0,\n    type: \'lowpass\'\n};\nconst createBiquadFilterNodeConstructor = (audioNodeConstructor, createAudioParam, createBiquadFilterNodeRenderer, createInvalidAccessError, createNativeBiquadFilterNode, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class BiquadFilterNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...biquad_filter_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeBiquadFilterNode = createNativeBiquadFilterNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const biquadFilterNodeRenderer = (isOffline ? createBiquadFilterNodeRenderer() : null);\n            super(context, false, nativeBiquadFilterNode, biquadFilterNodeRenderer);\n            // Bug #80: Safari does not export the correct values for maxValue and minValue.\n            this._Q = createAudioParam(this, isOffline, nativeBiquadFilterNode.Q, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            // Bug #78: Firefox & Safari do not export the correct values for maxValue and minValue.\n            this._detune = createAudioParam(this, isOffline, nativeBiquadFilterNode.detune, 1200 * Math.log2(MOST_POSITIVE_SINGLE_FLOAT), -1200 * Math.log2(MOST_POSITIVE_SINGLE_FLOAT));\n            // Bug #77: Firefox & Safari do not export the correct value for minValue.\n            this._frequency = createAudioParam(this, isOffline, nativeBiquadFilterNode.frequency, context.sampleRate / 2, 0);\n            // Bug #79: Firefox & Safari do not export the correct values for maxValue and minValue.\n            this._gain = createAudioParam(this, isOffline, nativeBiquadFilterNode.gain, 40 * Math.log10(MOST_POSITIVE_SINGLE_FLOAT), MOST_NEGATIVE_SINGLE_FLOAT);\n            this._nativeBiquadFilterNode = nativeBiquadFilterNode;\n            // @todo Determine a meaningful tail-time instead of just using one second.\n            setAudioNodeTailTime(this, 1);\n        }\n        get detune() {\n            return this._detune;\n        }\n        get frequency() {\n            return this._frequency;\n        }\n        get gain() {\n            return this._gain;\n        }\n        get Q() {\n            return this._Q;\n        }\n        get type() {\n            return this._nativeBiquadFilterNode.type;\n        }\n        set type(value) {\n            this._nativeBiquadFilterNode.type = value;\n        }\n        getFrequencyResponse(frequencyHz, magResponse, phaseResponse) {\n            // Bug #189: Safari does throw an InvalidStateError.\n            try {\n                this._nativeBiquadFilterNode.getFrequencyResponse(frequencyHz, magResponse, phaseResponse);\n            }\n            catch (err) {\n                if (err.code === 11) {\n                    throw createInvalidAccessError();\n                }\n                throw err;\n            }\n            // Bug #68: Safari does not throw an error if the parameters differ in their length.\n            if (frequencyHz.length !== magResponse.length || magResponse.length !== phaseResponse.length) {\n                throw createInvalidAccessError();\n            }\n        }\n    };\n};\n//# sourceMappingURL=biquad-filter-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/biquad-filter-node-renderer-factory.js\n\nconst createBiquadFilterNodeRendererFactory = (connectAudioParam, createNativeBiquadFilterNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeBiquadFilterNodes = new WeakMap();\n        const createBiquadFilterNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeBiquadFilterNode = getNativeAudioNode(proxy);\n            /*\n             * If the initially used nativeBiquadFilterNode was not constructed on the same OfflineAudioContext it needs to be created\n             * again.\n             */\n            const nativeBiquadFilterNodeIsOwnedByContext = isOwnedByContext(nativeBiquadFilterNode, nativeOfflineAudioContext);\n            if (!nativeBiquadFilterNodeIsOwnedByContext) {\n                const options = {\n                    Q: nativeBiquadFilterNode.Q.value,\n                    channelCount: nativeBiquadFilterNode.channelCount,\n                    channelCountMode: nativeBiquadFilterNode.channelCountMode,\n                    channelInterpretation: nativeBiquadFilterNode.channelInterpretation,\n                    detune: nativeBiquadFilterNode.detune.value,\n                    frequency: nativeBiquadFilterNode.frequency.value,\n                    gain: nativeBiquadFilterNode.gain.value,\n                    type: nativeBiquadFilterNode.type\n                };\n                nativeBiquadFilterNode = createNativeBiquadFilterNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeBiquadFilterNodes.set(nativeOfflineAudioContext, nativeBiquadFilterNode);\n            if (!nativeBiquadFilterNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.Q, nativeBiquadFilterNode.Q, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.detune, nativeBiquadFilterNode.detune, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.frequency, nativeBiquadFilterNode.frequency, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.gain, nativeBiquadFilterNode.gain, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.Q, nativeBiquadFilterNode.Q, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.detune, nativeBiquadFilterNode.detune, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.frequency, nativeBiquadFilterNode.frequency, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.gain, nativeBiquadFilterNode.gain, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeBiquadFilterNode, trace);\n            return nativeBiquadFilterNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeBiquadFilterNode = renderedNativeBiquadFilterNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeBiquadFilterNode !== undefined) {\n                    return Promise.resolve(renderedNativeBiquadFilterNode);\n                }\n                return createBiquadFilterNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=biquad-filter-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/cache-test-result.js\nconst createCacheTestResult = (ongoingTests, testResults) => {\n    return (tester, test) => {\n        const cachedTestResult = testResults.get(tester);\n        if (cachedTestResult !== undefined) {\n            return cachedTestResult;\n        }\n        const ongoingTest = ongoingTests.get(tester);\n        if (ongoingTest !== undefined) {\n            return ongoingTest;\n        }\n        try {\n            const synchronousTestResult = test();\n            if (synchronousTestResult instanceof Promise) {\n                ongoingTests.set(tester, synchronousTestResult);\n                return synchronousTestResult\n                    .catch(() => false)\n                    .then((finalTestResult) => {\n                    ongoingTests.delete(tester);\n                    testResults.set(tester, finalTestResult);\n                    return finalTestResult;\n                });\n            }\n            testResults.set(tester, synchronousTestResult);\n            return synchronousTestResult;\n        }\n        catch {\n            testResults.set(tester, false);\n            return false;\n        }\n    };\n};\n//# sourceMappingURL=cache-test-result.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/channel-merger-node-constructor.js\nconst channel_merger_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 1,\n    channelCountMode: \'explicit\',\n    channelInterpretation: \'speakers\',\n    numberOfInputs: 6\n};\nconst createChannelMergerNodeConstructor = (audioNodeConstructor, createChannelMergerNodeRenderer, createNativeChannelMergerNode, getNativeContext, isNativeOfflineAudioContext) => {\n    return class ChannelMergerNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...channel_merger_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeChannelMergerNode = createNativeChannelMergerNode(nativeContext, mergedOptions);\n            const channelMergerNodeRenderer = ((isNativeOfflineAudioContext(nativeContext) ? createChannelMergerNodeRenderer() : null));\n            super(context, false, nativeChannelMergerNode, channelMergerNodeRenderer);\n        }\n    };\n};\n//# sourceMappingURL=channel-merger-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/channel-merger-node-renderer-factory.js\n\nconst createChannelMergerNodeRendererFactory = (createNativeChannelMergerNode, getNativeAudioNode, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeAudioNodes = new WeakMap();\n        const createAudioNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeAudioNode = getNativeAudioNode(proxy);\n            // If the initially used nativeAudioNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeAudioNodeIsOwnedByContext = isOwnedByContext(nativeAudioNode, nativeOfflineAudioContext);\n            if (!nativeAudioNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeAudioNode.channelCount,\n                    channelCountMode: nativeAudioNode.channelCountMode,\n                    channelInterpretation: nativeAudioNode.channelInterpretation,\n                    numberOfInputs: nativeAudioNode.numberOfInputs\n                };\n                nativeAudioNode = createNativeChannelMergerNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeAudioNode);\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAudioNode, trace);\n            return nativeAudioNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeAudioNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeAudioNode !== undefined) {\n                    return Promise.resolve(renderedNativeAudioNode);\n                }\n                return createAudioNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=channel-merger-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/channel-splitter-node-constructor.js\nconst channel_splitter_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 6,\n    channelCountMode: \'explicit\',\n    channelInterpretation: \'discrete\',\n    numberOfOutputs: 6\n};\nconst createChannelSplitterNodeConstructor = (audioNodeConstructor, createChannelSplitterNodeRenderer, createNativeChannelSplitterNode, getNativeContext, isNativeOfflineAudioContext, sanitizeChannelSplitterOptions) => {\n    return class ChannelSplitterNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = sanitizeChannelSplitterOptions({ ...channel_splitter_node_constructor_DEFAULT_OPTIONS, ...options });\n            const nativeChannelSplitterNode = createNativeChannelSplitterNode(nativeContext, mergedOptions);\n            const channelSplitterNodeRenderer = ((isNativeOfflineAudioContext(nativeContext) ? createChannelSplitterNodeRenderer() : null));\n            super(context, false, nativeChannelSplitterNode, channelSplitterNodeRenderer);\n        }\n    };\n};\n//# sourceMappingURL=channel-splitter-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/channel-splitter-node-renderer-factory.js\n\nconst createChannelSplitterNodeRendererFactory = (createNativeChannelSplitterNode, getNativeAudioNode, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeAudioNodes = new WeakMap();\n        const createAudioNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeAudioNode = getNativeAudioNode(proxy);\n            // If the initially used nativeAudioNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeAudioNodeIsOwnedByContext = isOwnedByContext(nativeAudioNode, nativeOfflineAudioContext);\n            if (!nativeAudioNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeAudioNode.channelCount,\n                    channelCountMode: nativeAudioNode.channelCountMode,\n                    channelInterpretation: nativeAudioNode.channelInterpretation,\n                    numberOfOutputs: nativeAudioNode.numberOfOutputs\n                };\n                nativeAudioNode = createNativeChannelSplitterNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeAudioNode);\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAudioNode, trace);\n            return nativeAudioNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeAudioNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeAudioNode !== undefined) {\n                    return Promise.resolve(renderedNativeAudioNode);\n                }\n                return createAudioNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=channel-splitter-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/connect-audio-param.js\nconst createConnectAudioParam = (renderInputsOfAudioParam) => {\n    return (nativeOfflineAudioContext, audioParam, nativeAudioParam, trace) => {\n        return renderInputsOfAudioParam(audioParam, nativeOfflineAudioContext, nativeAudioParam, trace);\n    };\n};\n//# sourceMappingURL=connect-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/connect-multiple-outputs.js\n\nconst createConnectMultipleOutputs = (createIndexSizeError) => {\n    return (outputAudioNodes, destination, output = 0, input = 0) => {\n        const outputAudioNode = outputAudioNodes[output];\n        if (outputAudioNode === undefined) {\n            throw createIndexSizeError();\n        }\n        if (native_audio_node_isNativeAudioNode(destination)) {\n            return outputAudioNode.connect(destination, 0, input);\n        }\n        return outputAudioNode.connect(destination, 0);\n    };\n};\n//# sourceMappingURL=connect-multiple-outputs.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/connected-native-audio-buffer-source-node-factory.js\nconst createConnectedNativeAudioBufferSourceNodeFactory = (createNativeAudioBufferSourceNode) => {\n    return (nativeContext, nativeAudioNode) => {\n        const nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode(nativeContext, {\n            buffer: null,\n            channelCount: 2,\n            channelCountMode: \'max\',\n            channelInterpretation: \'speakers\',\n            loop: false,\n            loopEnd: 0,\n            loopStart: 0,\n            playbackRate: 1\n        });\n        const nativeAudioBuffer = nativeContext.createBuffer(1, 2, 44100);\n        nativeAudioBufferSourceNode.buffer = nativeAudioBuffer;\n        nativeAudioBufferSourceNode.loop = true;\n        nativeAudioBufferSourceNode.connect(nativeAudioNode);\n        nativeAudioBufferSourceNode.start();\n        return () => {\n            nativeAudioBufferSourceNode.stop();\n            nativeAudioBufferSourceNode.disconnect(nativeAudioNode);\n        };\n    };\n};\n//# sourceMappingURL=connected-native-audio-buffer-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/constant-source-node-constructor.js\n\n\n\n\nconst constant_source_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    offset: 1\n};\nconst createConstantSourceNodeConstructor = (audioNodeConstructor, createAudioParam, createConstantSourceNodeRendererFactory, createNativeConstantSourceNode, getNativeContext, isNativeOfflineAudioContext, wrapEventListener) => {\n    return class ConstantSourceNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...constant_source_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeConstantSourceNode = createNativeConstantSourceNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const constantSourceNodeRenderer = ((isOffline ? createConstantSourceNodeRendererFactory() : null));\n            super(context, false, nativeConstantSourceNode, constantSourceNodeRenderer);\n            this._constantSourceNodeRenderer = constantSourceNodeRenderer;\n            this._nativeConstantSourceNode = nativeConstantSourceNode;\n            /*\n             * Bug #62 & #74: Safari does not support ConstantSourceNodes and does not export the correct values for maxValue and minValue\n             * for GainNodes.\n             */\n            this._offset = createAudioParam(this, isOffline, nativeConstantSourceNode.offset, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            this._onended = null;\n        }\n        get offset() {\n            return this._offset;\n        }\n        get onended() {\n            return this._onended;\n        }\n        set onended(value) {\n            const wrappedListener = typeof value === \'function\' ? wrapEventListener(this, value) : null;\n            this._nativeConstantSourceNode.onended = wrappedListener;\n            const nativeOnEnded = this._nativeConstantSourceNode.onended;\n            this._onended = nativeOnEnded !== null && nativeOnEnded === wrappedListener ? value : nativeOnEnded;\n        }\n        start(when = 0) {\n            this._nativeConstantSourceNode.start(when);\n            if (this._constantSourceNodeRenderer !== null) {\n                this._constantSourceNodeRenderer.start = when;\n            }\n            if (this.context.state !== \'closed\') {\n                setInternalStateToActive(this);\n                const resetInternalStateToPassive = () => {\n                    this._nativeConstantSourceNode.removeEventListener(\'ended\', resetInternalStateToPassive);\n                    if (is_active_audio_node_isActiveAudioNode(this)) {\n                        setInternalStateToPassive(this);\n                    }\n                };\n                this._nativeConstantSourceNode.addEventListener(\'ended\', resetInternalStateToPassive);\n            }\n        }\n        stop(when = 0) {\n            this._nativeConstantSourceNode.stop(when);\n            if (this._constantSourceNodeRenderer !== null) {\n                this._constantSourceNodeRenderer.stop = when;\n            }\n        }\n    };\n};\n//# sourceMappingURL=constant-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/constant-source-node-renderer-factory.js\n\nconst constant_source_node_renderer_factory_createConstantSourceNodeRendererFactory = (connectAudioParam, createNativeConstantSourceNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeConstantSourceNodes = new WeakMap();\n        let start = null;\n        let stop = null;\n        const createConstantSourceNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeConstantSourceNode = getNativeAudioNode(proxy);\n            /*\n             * If the initially used nativeConstantSourceNode was not constructed on the same OfflineAudioContext it needs to be created\n             * again.\n             */\n            const nativeConstantSourceNodeIsOwnedByContext = isOwnedByContext(nativeConstantSourceNode, nativeOfflineAudioContext);\n            if (!nativeConstantSourceNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeConstantSourceNode.channelCount,\n                    channelCountMode: nativeConstantSourceNode.channelCountMode,\n                    channelInterpretation: nativeConstantSourceNode.channelInterpretation,\n                    offset: nativeConstantSourceNode.offset.value\n                };\n                nativeConstantSourceNode = createNativeConstantSourceNode(nativeOfflineAudioContext, options);\n                if (start !== null) {\n                    nativeConstantSourceNode.start(start);\n                }\n                if (stop !== null) {\n                    nativeConstantSourceNode.stop(stop);\n                }\n            }\n            renderedNativeConstantSourceNodes.set(nativeOfflineAudioContext, nativeConstantSourceNode);\n            if (!nativeConstantSourceNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.offset, nativeConstantSourceNode.offset, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.offset, nativeConstantSourceNode.offset, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeConstantSourceNode, trace);\n            return nativeConstantSourceNode;\n        };\n        return {\n            set start(value) {\n                start = value;\n            },\n            set stop(value) {\n                stop = value;\n            },\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeConstantSourceNode = renderedNativeConstantSourceNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeConstantSourceNode !== undefined) {\n                    return Promise.resolve(renderedNativeConstantSourceNode);\n                }\n                return createConstantSourceNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=constant-source-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/convert-number-to-unsigned-long.js\nconst createConvertNumberToUnsignedLong = (unit32Array) => {\n    return (value) => {\n        unit32Array[0] = value;\n        return unit32Array[0];\n    };\n};\n//# sourceMappingURL=convert-number-to-unsigned-long.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/convolver-node-constructor.js\nconst convolver_node_constructor_DEFAULT_OPTIONS = {\n    buffer: null,\n    channelCount: 2,\n    channelCountMode: \'clamped-max\',\n    channelInterpretation: \'speakers\',\n    disableNormalization: false\n};\nconst createConvolverNodeConstructor = (audioNodeConstructor, createConvolverNodeRenderer, createNativeConvolverNode, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class ConvolverNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...convolver_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeConvolverNode = createNativeConvolverNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const convolverNodeRenderer = (isOffline ? createConvolverNodeRenderer() : null);\n            super(context, false, nativeConvolverNode, convolverNodeRenderer);\n            this._isBufferNullified = false;\n            this._nativeConvolverNode = nativeConvolverNode;\n            if (mergedOptions.buffer !== null) {\n                setAudioNodeTailTime(this, mergedOptions.buffer.duration);\n            }\n        }\n        get buffer() {\n            if (this._isBufferNullified) {\n                return null;\n            }\n            return this._nativeConvolverNode.buffer;\n        }\n        set buffer(value) {\n            this._nativeConvolverNode.buffer = value;\n            // Bug #115: Safari does not allow to set the buffer to null.\n            if (value === null && this._nativeConvolverNode.buffer !== null) {\n                const nativeContext = this._nativeConvolverNode.context;\n                this._nativeConvolverNode.buffer = nativeContext.createBuffer(1, 1, 44100);\n                this._isBufferNullified = true;\n                setAudioNodeTailTime(this, 0);\n            }\n            else {\n                this._isBufferNullified = false;\n                setAudioNodeTailTime(this, this._nativeConvolverNode.buffer === null ? 0 : this._nativeConvolverNode.buffer.duration);\n            }\n        }\n        get normalize() {\n            return this._nativeConvolverNode.normalize;\n        }\n        set normalize(value) {\n            this._nativeConvolverNode.normalize = value;\n        }\n    };\n};\n//# sourceMappingURL=convolver-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/convolver-node-renderer-factory.js\n\n\nconst createConvolverNodeRendererFactory = (createNativeConvolverNode, getNativeAudioNode, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeConvolverNodes = new WeakMap();\n        const createConvolverNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeConvolverNode = getNativeAudioNode(proxy);\n            // If the initially used nativeConvolverNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeConvolverNodeIsOwnedByContext = isOwnedByContext(nativeConvolverNode, nativeOfflineAudioContext);\n            if (!nativeConvolverNodeIsOwnedByContext) {\n                const options = {\n                    buffer: nativeConvolverNode.buffer,\n                    channelCount: nativeConvolverNode.channelCount,\n                    channelCountMode: nativeConvolverNode.channelCountMode,\n                    channelInterpretation: nativeConvolverNode.channelInterpretation,\n                    disableNormalization: !nativeConvolverNode.normalize\n                };\n                nativeConvolverNode = createNativeConvolverNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeConvolverNodes.set(nativeOfflineAudioContext, nativeConvolverNode);\n            if (isNativeAudioNodeFaker(nativeConvolverNode)) {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeConvolverNode.inputs[0], trace);\n            }\n            else {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeConvolverNode, trace);\n            }\n            return nativeConvolverNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeConvolverNode = renderedNativeConvolverNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeConvolverNode !== undefined) {\n                    return Promise.resolve(renderedNativeConvolverNode);\n                }\n                return createConvolverNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=convolver-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/create-native-offline-audio-context.js\nconst createCreateNativeOfflineAudioContext = (createNotSupportedError, nativeOfflineAudioContextConstructor) => {\n    return (numberOfChannels, length, sampleRate) => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            throw new Error(\'Missing the native OfflineAudioContext constructor.\');\n        }\n        try {\n            return new nativeOfflineAudioContextConstructor(numberOfChannels, length, sampleRate);\n        }\n        catch (err) {\n            // Bug #143, #144 & #146: Safari throws a SyntaxError when numberOfChannels, length or sampleRate are invalid.\n            if (err.name === \'SyntaxError\') {\n                throw createNotSupportedError();\n            }\n            throw err;\n        }\n    };\n};\n//# sourceMappingURL=create-native-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/data-clone-error.js\nconst data_clone_error_createDataCloneError = () => new DOMException(\'\', \'DataCloneError\');\n//# sourceMappingURL=data-clone-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/detach-array-buffer.js\nconst detachArrayBuffer = (arrayBuffer) => {\n    const { port1 } = new MessageChannel();\n    port1.postMessage(arrayBuffer, [arrayBuffer]);\n};\n//# sourceMappingURL=detach-array-buffer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/decode-audio-data.js\n\n\nconst createDecodeAudioData = (audioBufferStore, cacheTestResult, createDataCloneError, createEncodingError, detachedArrayBuffers, getNativeContext, isNativeContext, testAudioBufferCopyChannelMethodsOutOfBoundsSupport, testPromiseSupport, wrapAudioBufferCopyChannelMethods, wrapAudioBufferCopyChannelMethodsOutOfBounds) => {\n    return (anyContext, audioData) => {\n        const nativeContext = isNativeContext(anyContext) ? anyContext : getNativeContext(anyContext);\n        // Bug #43: Only Chrome, Edge and Opera do throw a DataCloneError.\n        if (detachedArrayBuffers.has(audioData)) {\n            const err = createDataCloneError();\n            return Promise.reject(err);\n        }\n        // The audioData parameter maybe of a type which can\'t be added to a WeakSet.\n        try {\n            detachedArrayBuffers.add(audioData);\n        }\n        catch {\n            // Ignore errors.\n        }\n        // Bug #21: Safari does not support promises yet.\n        if (cacheTestResult(testPromiseSupport, () => testPromiseSupport(nativeContext))) {\n            return nativeContext.decodeAudioData(audioData).then((audioBuffer) => {\n                // Bug #157: Firefox does not allow the bufferOffset to be out-of-bounds.\n                if (!cacheTestResult(testAudioBufferCopyChannelMethodsOutOfBoundsSupport, () => testAudioBufferCopyChannelMethodsOutOfBoundsSupport(audioBuffer))) {\n                    wrapAudioBufferCopyChannelMethodsOutOfBounds(audioBuffer);\n                }\n                audioBufferStore.add(audioBuffer);\n                return audioBuffer;\n            });\n        }\n        // Bug #21: Safari does not return a Promise yet.\n        return new Promise((resolve, reject) => {\n            const complete = () => {\n                // Bug #133: Safari does neuter the ArrayBuffer.\n                try {\n                    detachArrayBuffer(audioData);\n                }\n                catch {\n                    // Ignore errors.\n                }\n            };\n            const fail = (err) => {\n                reject(err);\n                complete();\n            };\n            // Bug #26: Safari throws a synchronous error.\n            try {\n                // Bug #1: Safari requires a successCallback.\n                nativeContext.decodeAudioData(audioData, (audioBuffer) => {\n                    // Bug #5: Safari does not support copyFromChannel() and copyToChannel().\n                    // Bug #100: Safari does throw a wrong error when calling getChannelData() with an out-of-bounds value.\n                    if (typeof audioBuffer.copyFromChannel !== \'function\') {\n                        wrapAudioBufferCopyChannelMethods(audioBuffer);\n                        wrapAudioBufferGetChannelDataMethod(audioBuffer);\n                    }\n                    audioBufferStore.add(audioBuffer);\n                    complete();\n                    resolve(audioBuffer);\n                }, (err) => {\n                    // Bug #4: Safari returns null instead of an error.\n                    if (err === null) {\n                        fail(createEncodingError());\n                    }\n                    else {\n                        fail(err);\n                    }\n                });\n            }\n            catch (err) {\n                fail(err);\n            }\n        });\n    };\n};\n//# sourceMappingURL=decode-audio-data.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/decrement-cycle-counter.js\n\nconst createDecrementCycleCounter = (connectNativeAudioNodeToNativeAudioNode, cycleCounters, getAudioNodeConnections, getNativeAudioNode, getNativeAudioParam, getNativeContext, isActiveAudioNode, isNativeOfflineAudioContext) => {\n    return (audioNode, count) => {\n        const cycleCounter = cycleCounters.get(audioNode);\n        if (cycleCounter === undefined) {\n            throw new Error(\'Missing the expected cycle count.\');\n        }\n        const nativeContext = getNativeContext(audioNode.context);\n        const isOffline = isNativeOfflineAudioContext(nativeContext);\n        if (cycleCounter === count) {\n            cycleCounters.delete(audioNode);\n            if (!isOffline && isActiveAudioNode(audioNode)) {\n                const nativeSourceAudioNode = getNativeAudioNode(audioNode);\n                const { outputs } = getAudioNodeConnections(audioNode);\n                for (const output of outputs) {\n                    if (isAudioNodeOutputConnection(output)) {\n                        const nativeDestinationAudioNode = getNativeAudioNode(output[0]);\n                        connectNativeAudioNodeToNativeAudioNode(nativeSourceAudioNode, nativeDestinationAudioNode, output[1], output[2]);\n                    }\n                    else {\n                        const nativeDestinationAudioParam = getNativeAudioParam(output[0]);\n                        nativeSourceAudioNode.connect(nativeDestinationAudioParam, output[1]);\n                    }\n                }\n            }\n        }\n        else {\n            cycleCounters.set(audioNode, cycleCounter - count);\n        }\n    };\n};\n//# sourceMappingURL=decrement-cycle-counter.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/delay-node-constructor.js\nconst delay_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    delayTime: 0,\n    maxDelayTime: 1\n};\nconst createDelayNodeConstructor = (audioNodeConstructor, createAudioParam, createDelayNodeRenderer, createNativeDelayNode, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class DelayNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...delay_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeDelayNode = createNativeDelayNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const delayNodeRenderer = (isOffline ? createDelayNodeRenderer(mergedOptions.maxDelayTime) : null);\n            super(context, false, nativeDelayNode, delayNodeRenderer);\n            this._delayTime = createAudioParam(this, isOffline, nativeDelayNode.delayTime);\n            setAudioNodeTailTime(this, mergedOptions.maxDelayTime);\n        }\n        get delayTime() {\n            return this._delayTime;\n        }\n    };\n};\n//# sourceMappingURL=delay-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/delay-node-renderer-factory.js\n\nconst createDelayNodeRendererFactory = (connectAudioParam, createNativeDelayNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return (maxDelayTime) => {\n        const renderedNativeDelayNodes = new WeakMap();\n        const createDelayNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeDelayNode = getNativeAudioNode(proxy);\n            // If the initially used nativeDelayNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeDelayNodeIsOwnedByContext = isOwnedByContext(nativeDelayNode, nativeOfflineAudioContext);\n            if (!nativeDelayNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeDelayNode.channelCount,\n                    channelCountMode: nativeDelayNode.channelCountMode,\n                    channelInterpretation: nativeDelayNode.channelInterpretation,\n                    delayTime: nativeDelayNode.delayTime.value,\n                    maxDelayTime\n                };\n                nativeDelayNode = createNativeDelayNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeDelayNodes.set(nativeOfflineAudioContext, nativeDelayNode);\n            if (!nativeDelayNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.delayTime, nativeDelayNode.delayTime, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.delayTime, nativeDelayNode.delayTime, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeDelayNode, trace);\n            return nativeDelayNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeDelayNode = renderedNativeDelayNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeDelayNode !== undefined) {\n                    return Promise.resolve(renderedNativeDelayNode);\n                }\n                return createDelayNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=delay-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/delete-active-input-connection-to-audio-node.js\nconst createDeleteActiveInputConnectionToAudioNode = (pickElementFromSet) => {\n    return (activeInputs, source, output, input) => {\n        return pickElementFromSet(activeInputs[input], (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output);\n    };\n};\n//# sourceMappingURL=delete-active-input-connection-to-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/delete-unrendered-audio-worklet-node.js\nconst createDeleteUnrenderedAudioWorkletNode = (getUnrenderedAudioWorkletNodes) => {\n    return (nativeContext, audioWorkletNode) => {\n        getUnrenderedAudioWorkletNodes(nativeContext).delete(audioWorkletNode);\n    };\n};\n//# sourceMappingURL=delete-unrendered-audio-worklet-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/guards/delay-node.js\nconst isDelayNode = (audioNode) => {\n    return \'delayTime\' in audioNode;\n};\n//# sourceMappingURL=delay-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/detect-cycles.js\n\n\nconst createDetectCycles = (audioParamAudioNodeStore, getAudioNodeConnections, getValueForKey) => {\n    return function detectCycles(chain, nextLink) {\n        const audioNode = isAudioNode(nextLink) ? nextLink : getValueForKey(audioParamAudioNodeStore, nextLink);\n        if (isDelayNode(audioNode)) {\n            return [];\n        }\n        if (chain[0] === audioNode) {\n            return [chain];\n        }\n        if (chain.includes(audioNode)) {\n            return [];\n        }\n        const { outputs } = getAudioNodeConnections(audioNode);\n        return Array.from(outputs)\n            .map((outputConnection) => detectCycles([...chain, audioNode], outputConnection[0]))\n            .reduce((mergedCycles, nestedCycles) => mergedCycles.concat(nestedCycles), []);\n    };\n};\n//# sourceMappingURL=detect-cycles.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/disconnect-multiple-outputs.js\n\nconst getOutputAudioNodeAtIndex = (createIndexSizeError, outputAudioNodes, output) => {\n    const outputAudioNode = outputAudioNodes[output];\n    if (outputAudioNode === undefined) {\n        throw createIndexSizeError();\n    }\n    return outputAudioNode;\n};\nconst createDisconnectMultipleOutputs = (createIndexSizeError) => {\n    return (outputAudioNodes, destinationOrOutput = undefined, output = undefined, input = 0) => {\n        if (destinationOrOutput === undefined) {\n            return outputAudioNodes.forEach((outputAudioNode) => outputAudioNode.disconnect());\n        }\n        if (typeof destinationOrOutput === \'number\') {\n            return getOutputAudioNodeAtIndex(createIndexSizeError, outputAudioNodes, destinationOrOutput).disconnect();\n        }\n        if (native_audio_node_isNativeAudioNode(destinationOrOutput)) {\n            if (output === undefined) {\n                return outputAudioNodes.forEach((outputAudioNode) => outputAudioNode.disconnect(destinationOrOutput));\n            }\n            if (input === undefined) {\n                return getOutputAudioNodeAtIndex(createIndexSizeError, outputAudioNodes, output).disconnect(destinationOrOutput, 0);\n            }\n            return getOutputAudioNodeAtIndex(createIndexSizeError, outputAudioNodes, output).disconnect(destinationOrOutput, 0, input);\n        }\n        if (output === undefined) {\n            return outputAudioNodes.forEach((outputAudioNode) => outputAudioNode.disconnect(destinationOrOutput));\n        }\n        return getOutputAudioNodeAtIndex(createIndexSizeError, outputAudioNodes, output).disconnect(destinationOrOutput, 0);\n    };\n};\n//# sourceMappingURL=disconnect-multiple-outputs.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/dynamics-compressor-node-constructor.js\nconst dynamics_compressor_node_constructor_DEFAULT_OPTIONS = {\n    attack: 0.003,\n    channelCount: 2,\n    channelCountMode: \'clamped-max\',\n    channelInterpretation: \'speakers\',\n    knee: 30,\n    ratio: 12,\n    release: 0.25,\n    threshold: -24\n};\nconst createDynamicsCompressorNodeConstructor = (audioNodeConstructor, createAudioParam, createDynamicsCompressorNodeRenderer, createNativeDynamicsCompressorNode, createNotSupportedError, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class DynamicsCompressorNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...dynamics_compressor_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeDynamicsCompressorNode = createNativeDynamicsCompressorNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const dynamicsCompressorNodeRenderer = (isOffline ? createDynamicsCompressorNodeRenderer() : null);\n            super(context, false, nativeDynamicsCompressorNode, dynamicsCompressorNodeRenderer);\n            this._attack = createAudioParam(this, isOffline, nativeDynamicsCompressorNode.attack);\n            this._knee = createAudioParam(this, isOffline, nativeDynamicsCompressorNode.knee);\n            this._nativeDynamicsCompressorNode = nativeDynamicsCompressorNode;\n            this._ratio = createAudioParam(this, isOffline, nativeDynamicsCompressorNode.ratio);\n            this._release = createAudioParam(this, isOffline, nativeDynamicsCompressorNode.release);\n            this._threshold = createAudioParam(this, isOffline, nativeDynamicsCompressorNode.threshold);\n            setAudioNodeTailTime(this, 0.006);\n        }\n        get attack() {\n            return this._attack;\n        }\n        // Bug #108: Safari allows a channelCount of three and above which is why the getter and setter needs to be overwritten here.\n        get channelCount() {\n            return this._nativeDynamicsCompressorNode.channelCount;\n        }\n        set channelCount(value) {\n            const previousChannelCount = this._nativeDynamicsCompressorNode.channelCount;\n            this._nativeDynamicsCompressorNode.channelCount = value;\n            if (value > 2) {\n                this._nativeDynamicsCompressorNode.channelCount = previousChannelCount;\n                throw createNotSupportedError();\n            }\n        }\n        /*\n         * Bug #109: Only Chrome, Firefox and Opera disallow a channelCountMode of \'max\' yet which is why the getter and setter needs to be\n         * overwritten here.\n         */\n        get channelCountMode() {\n            return this._nativeDynamicsCompressorNode.channelCountMode;\n        }\n        set channelCountMode(value) {\n            const previousChannelCount = this._nativeDynamicsCompressorNode.channelCountMode;\n            this._nativeDynamicsCompressorNode.channelCountMode = value;\n            if (value === \'max\') {\n                this._nativeDynamicsCompressorNode.channelCountMode = previousChannelCount;\n                throw createNotSupportedError();\n            }\n        }\n        get knee() {\n            return this._knee;\n        }\n        get ratio() {\n            return this._ratio;\n        }\n        get reduction() {\n            // Bug #111: Safari returns an AudioParam instead of a number.\n            if (typeof this._nativeDynamicsCompressorNode.reduction.value === \'number\') {\n                return this._nativeDynamicsCompressorNode.reduction.value;\n            }\n            return this._nativeDynamicsCompressorNode.reduction;\n        }\n        get release() {\n            return this._release;\n        }\n        get threshold() {\n            return this._threshold;\n        }\n    };\n};\n//# sourceMappingURL=dynamics-compressor-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/dynamics-compressor-node-renderer-factory.js\n\nconst createDynamicsCompressorNodeRendererFactory = (connectAudioParam, createNativeDynamicsCompressorNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeDynamicsCompressorNodes = new WeakMap();\n        const createDynamicsCompressorNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeDynamicsCompressorNode = getNativeAudioNode(proxy);\n            /*\n             * If the initially used nativeDynamicsCompressorNode was not constructed on the same OfflineAudioContext it needs to be\n             * created again.\n             */\n            const nativeDynamicsCompressorNodeIsOwnedByContext = isOwnedByContext(nativeDynamicsCompressorNode, nativeOfflineAudioContext);\n            if (!nativeDynamicsCompressorNodeIsOwnedByContext) {\n                const options = {\n                    attack: nativeDynamicsCompressorNode.attack.value,\n                    channelCount: nativeDynamicsCompressorNode.channelCount,\n                    channelCountMode: nativeDynamicsCompressorNode.channelCountMode,\n                    channelInterpretation: nativeDynamicsCompressorNode.channelInterpretation,\n                    knee: nativeDynamicsCompressorNode.knee.value,\n                    ratio: nativeDynamicsCompressorNode.ratio.value,\n                    release: nativeDynamicsCompressorNode.release.value,\n                    threshold: nativeDynamicsCompressorNode.threshold.value\n                };\n                nativeDynamicsCompressorNode = createNativeDynamicsCompressorNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeDynamicsCompressorNodes.set(nativeOfflineAudioContext, nativeDynamicsCompressorNode);\n            if (!nativeDynamicsCompressorNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.attack, nativeDynamicsCompressorNode.attack, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.knee, nativeDynamicsCompressorNode.knee, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.ratio, nativeDynamicsCompressorNode.ratio, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.release, nativeDynamicsCompressorNode.release, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.threshold, nativeDynamicsCompressorNode.threshold, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.attack, nativeDynamicsCompressorNode.attack, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.knee, nativeDynamicsCompressorNode.knee, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.ratio, nativeDynamicsCompressorNode.ratio, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.release, nativeDynamicsCompressorNode.release, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.threshold, nativeDynamicsCompressorNode.threshold, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeDynamicsCompressorNode, trace);\n            return nativeDynamicsCompressorNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeDynamicsCompressorNode = renderedNativeDynamicsCompressorNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeDynamicsCompressorNode !== undefined) {\n                    return Promise.resolve(renderedNativeDynamicsCompressorNode);\n                }\n                return createDynamicsCompressorNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=dynamics-compressor-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/encoding-error.js\nconst encoding_error_createEncodingError = () => new DOMException(\'\', \'EncodingError\');\n//# sourceMappingURL=encoding-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/evaluate-source.js\nconst createEvaluateSource = (window) => {\n    return (source) => new Promise((resolve, reject) => {\n        if (window === null) {\n            // Bug #182 Chrome, Edge and Opera do throw an instance of a SyntaxError instead of a DOMException.\n            reject(new SyntaxError());\n            return;\n        }\n        const head = window.document.head;\n        if (head === null) {\n            // Bug #182 Chrome, Edge and Opera do throw an instance of a SyntaxError instead of a DOMException.\n            reject(new SyntaxError());\n        }\n        else {\n            const script = window.document.createElement(\'script\');\n            // @todo Safari doesn\'t like URLs with a type of \'application/javascript; charset=utf-8\'.\n            const blob = new Blob([source], { type: \'application/javascript\' });\n            const url = URL.createObjectURL(blob);\n            const originalOnErrorHandler = window.onerror;\n            const removeErrorEventListenerAndRevokeUrl = () => {\n                window.onerror = originalOnErrorHandler;\n                URL.revokeObjectURL(url);\n            };\n            window.onerror = (message, src, lineno, colno, error) => {\n                // @todo Edge thinks the source is the one of the html document.\n                if (src === url || (src === window.location.href && lineno === 1 && colno === 1)) {\n                    removeErrorEventListenerAndRevokeUrl();\n                    reject(error);\n                    return false;\n                }\n                if (originalOnErrorHandler !== null) {\n                    return originalOnErrorHandler(message, src, lineno, colno, error);\n                }\n            };\n            script.onerror = () => {\n                removeErrorEventListenerAndRevokeUrl();\n                // Bug #182 Chrome, Edge and Opera do throw an instance of a SyntaxError instead of a DOMException.\n                reject(new SyntaxError());\n            };\n            script.onload = () => {\n                removeErrorEventListenerAndRevokeUrl();\n                resolve();\n            };\n            script.src = url;\n            script.type = \'module\';\n            head.appendChild(script);\n        }\n    });\n};\n//# sourceMappingURL=evaluate-source.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/event-target-constructor.js\nconst createEventTargetConstructor = (wrapEventListener) => {\n    return class EventTarget {\n        constructor(_nativeEventTarget) {\n            this._nativeEventTarget = _nativeEventTarget;\n            this._listeners = new WeakMap();\n        }\n        addEventListener(type, listener, options) {\n            if (listener !== null) {\n                let wrappedEventListener = this._listeners.get(listener);\n                if (wrappedEventListener === undefined) {\n                    wrappedEventListener = wrapEventListener(this, listener);\n                    if (typeof listener === \'function\') {\n                        this._listeners.set(listener, wrappedEventListener);\n                    }\n                }\n                this._nativeEventTarget.addEventListener(type, wrappedEventListener, options);\n            }\n        }\n        dispatchEvent(event) {\n            return this._nativeEventTarget.dispatchEvent(event);\n        }\n        removeEventListener(type, listener, options) {\n            const wrappedEventListener = listener === null ? undefined : this._listeners.get(listener);\n            this._nativeEventTarget.removeEventListener(type, wrappedEventListener === undefined ? null : wrappedEventListener, options);\n        }\n    };\n};\n//# sourceMappingURL=event-target-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/expose-current-frame-and-current-time.js\nconst createExposeCurrentFrameAndCurrentTime = (window) => {\n    return (currentTime, sampleRate, fn) => {\n        Object.defineProperties(window, {\n            currentFrame: {\n                configurable: true,\n                get() {\n                    return Math.round(currentTime * sampleRate);\n                }\n            },\n            currentTime: {\n                configurable: true,\n                get() {\n                    return currentTime;\n                }\n            }\n        });\n        try {\n            return fn();\n        }\n        finally {\n            if (window !== null) {\n                delete window.currentFrame;\n                delete window.currentTime;\n            }\n        }\n    };\n};\n//# sourceMappingURL=expose-current-frame-and-current-time.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/fetch-source.js\nconst createFetchSource = (createAbortError) => {\n    return async (url) => {\n        try {\n            const response = await fetch(url);\n            if (response.ok) {\n                return [await response.text(), response.url];\n            }\n        }\n        catch {\n            // Ignore errors.\n        } // tslint:disable-line:no-empty\n        throw createAbortError();\n    };\n};\n//# sourceMappingURL=fetch-source.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/gain-node-constructor.js\n\nconst gain_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    gain: 1\n};\nconst createGainNodeConstructor = (audioNodeConstructor, createAudioParam, createGainNodeRenderer, createNativeGainNode, getNativeContext, isNativeOfflineAudioContext) => {\n    return class GainNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...gain_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeGainNode = createNativeGainNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const gainNodeRenderer = (isOffline ? createGainNodeRenderer() : null);\n            super(context, false, nativeGainNode, gainNodeRenderer);\n            // Bug #74: Safari does not export the correct values for maxValue and minValue.\n            this._gain = createAudioParam(this, isOffline, nativeGainNode.gain, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n        }\n        get gain() {\n            return this._gain;\n        }\n    };\n};\n//# sourceMappingURL=gain-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/gain-node-renderer-factory.js\n\nconst createGainNodeRendererFactory = (connectAudioParam, createNativeGainNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeGainNodes = new WeakMap();\n        const createGainNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeGainNode = getNativeAudioNode(proxy);\n            // If the initially used nativeGainNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeGainNodeIsOwnedByContext = isOwnedByContext(nativeGainNode, nativeOfflineAudioContext);\n            if (!nativeGainNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeGainNode.channelCount,\n                    channelCountMode: nativeGainNode.channelCountMode,\n                    channelInterpretation: nativeGainNode.channelInterpretation,\n                    gain: nativeGainNode.gain.value\n                };\n                nativeGainNode = createNativeGainNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeGainNodes.set(nativeOfflineAudioContext, nativeGainNode);\n            if (!nativeGainNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.gain, nativeGainNode.gain, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.gain, nativeGainNode.gain, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeGainNode, trace);\n            return nativeGainNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeGainNode = renderedNativeGainNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeGainNode !== undefined) {\n                    return Promise.resolve(renderedNativeGainNode);\n                }\n                return createGainNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=gain-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-active-audio-worklet-node-inputs.js\nconst createGetActiveAudioWorkletNodeInputs = (activeAudioWorkletNodeInputsStore, getValueForKey) => {\n    return (nativeAudioWorkletNode) => getValueForKey(activeAudioWorkletNodeInputsStore, nativeAudioWorkletNode);\n};\n//# sourceMappingURL=get-active-audio-worklet-node-inputs.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-audio-node-renderer.js\nconst createGetAudioNodeRenderer = (getAudioNodeConnections) => {\n    return (audioNode) => {\n        const audioNodeConnections = getAudioNodeConnections(audioNode);\n        if (audioNodeConnections.renderer === null) {\n            throw new Error(\'Missing the renderer of the given AudioNode in the audio graph.\');\n        }\n        return audioNodeConnections.renderer;\n    };\n};\n//# sourceMappingURL=get-audio-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-audio-node-tail-time.js\nconst createGetAudioNodeTailTime = (audioNodeTailTimeStore) => {\n    return (audioNode) => { var _a; return (_a = audioNodeTailTimeStore.get(audioNode)) !== null && _a !== void 0 ? _a : 0; };\n};\n//# sourceMappingURL=get-audio-node-tail-time.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-audio-param-renderer.js\nconst createGetAudioParamRenderer = (getAudioParamConnections) => {\n    return (audioParam) => {\n        const audioParamConnections = getAudioParamConnections(audioParam);\n        if (audioParamConnections.renderer === null) {\n            throw new Error(\'Missing the renderer of the given AudioParam in the audio graph.\');\n        }\n        return audioParamConnections.renderer;\n    };\n};\n//# sourceMappingURL=get-audio-param-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-backup-offline-audio-context.js\nconst createGetBackupOfflineAudioContext = (backupOfflineAudioContextStore) => {\n    return (nativeContext) => {\n        return backupOfflineAudioContextStore.get(nativeContext);\n    };\n};\n//# sourceMappingURL=get-backup-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/invalid-state-error.js\nconst invalid_state_error_createInvalidStateError = () => new DOMException(\'\', \'InvalidStateError\');\n//# sourceMappingURL=invalid-state-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-native-context.js\n\nconst createGetNativeContext = (contextStore) => {\n    return (context) => {\n        const nativeContext = contextStore.get(context);\n        if (nativeContext === undefined) {\n            throw invalid_state_error_createInvalidStateError();\n        }\n        return (nativeContext);\n    };\n};\n//# sourceMappingURL=get-native-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-or-create-backup-offline-audio-context.js\nconst createGetOrCreateBackupOfflineAudioContext = (backupOfflineAudioContextStore, nativeOfflineAudioContextConstructor) => {\n    return (nativeContext) => {\n        let backupOfflineAudioContext = backupOfflineAudioContextStore.get(nativeContext);\n        if (backupOfflineAudioContext !== undefined) {\n            return backupOfflineAudioContext;\n        }\n        if (nativeOfflineAudioContextConstructor === null) {\n            throw new Error(\'Missing the native OfflineAudioContext constructor.\');\n        }\n        backupOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 8000);\n        backupOfflineAudioContextStore.set(nativeContext, backupOfflineAudioContext);\n        return backupOfflineAudioContext;\n    };\n};\n//# sourceMappingURL=get-or-create-backup-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/get-unrendered-audio-worklet-nodes.js\nconst createGetUnrenderedAudioWorkletNodes = (unrenderedAudioWorkletNodeStore) => {\n    return (nativeContext) => {\n        const unrenderedAudioWorkletNodes = unrenderedAudioWorkletNodeStore.get(nativeContext);\n        if (unrenderedAudioWorkletNodes === undefined) {\n            throw new Error(\'The context has no set of AudioWorkletNodes.\');\n        }\n        return unrenderedAudioWorkletNodes;\n    };\n};\n//# sourceMappingURL=get-unrendered-audio-worklet-nodes.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/invalid-access-error.js\nconst invalid_access_error_createInvalidAccessError = () => new DOMException(\'\', \'InvalidAccessError\');\n//# sourceMappingURL=invalid-access-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-iir-filter-node-get-frequency-response-method.js\n\nconst wrapIIRFilterNodeGetFrequencyResponseMethod = (nativeIIRFilterNode) => {\n    nativeIIRFilterNode.getFrequencyResponse = ((getFrequencyResponse) => {\n        return (frequencyHz, magResponse, phaseResponse) => {\n            if (frequencyHz.length !== magResponse.length || magResponse.length !== phaseResponse.length) {\n                throw invalid_access_error_createInvalidAccessError();\n            }\n            return getFrequencyResponse.call(nativeIIRFilterNode, frequencyHz, magResponse, phaseResponse);\n        };\n    })(nativeIIRFilterNode.getFrequencyResponse);\n};\n//# sourceMappingURL=wrap-iir-filter-node-get-frequency-response-method.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/iir-filter-node-constructor.js\n\nconst iir_filter_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\'\n};\nconst createIIRFilterNodeConstructor = (audioNodeConstructor, createNativeIIRFilterNode, createIIRFilterNodeRenderer, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class IIRFilterNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const mergedOptions = { ...iir_filter_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeIIRFilterNode = createNativeIIRFilterNode(nativeContext, isOffline ? null : context.baseLatency, mergedOptions);\n            const iirFilterNodeRenderer = ((isOffline ? createIIRFilterNodeRenderer(mergedOptions.feedback, mergedOptions.feedforward) : null));\n            super(context, false, nativeIIRFilterNode, iirFilterNodeRenderer);\n            // Bug #23 & #24: FirefoxDeveloper does not throw an InvalidAccessError.\n            // @todo Write a test which allows other browsers to remain unpatched.\n            wrapIIRFilterNodeGetFrequencyResponseMethod(nativeIIRFilterNode);\n            this._nativeIIRFilterNode = nativeIIRFilterNode;\n            // @todo Determine a meaningful tail-time instead of just using one second.\n            setAudioNodeTailTime(this, 1);\n        }\n        getFrequencyResponse(frequencyHz, magResponse, phaseResponse) {\n            return this._nativeIIRFilterNode.getFrequencyResponse(frequencyHz, magResponse, phaseResponse);\n        }\n    };\n};\n//# sourceMappingURL=iir-filter-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/filter-buffer.js\n// This implementation as shamelessly inspired by source code of\n// tslint:disable-next-line:max-line-length\n// {@link https://chromium.googlesource.com/chromium/src.git/+/master/third_party/WebKit/Source/platform/audio/IIRFilter.cpp|Chromium\'s IIRFilter}.\nconst filterBuffer = (feedback, feedbackLength, feedforward, feedforwardLength, minLength, xBuffer, yBuffer, bufferIndex, bufferLength, input, output) => {\n    const inputLength = input.length;\n    let i = bufferIndex;\n    for (let j = 0; j < inputLength; j += 1) {\n        let y = feedforward[0] * input[j];\n        for (let k = 1; k < minLength; k += 1) {\n            const x = (i - k) & (bufferLength - 1); // tslint:disable-line:no-bitwise\n            y += feedforward[k] * xBuffer[x];\n            y -= feedback[k] * yBuffer[x];\n        }\n        for (let k = minLength; k < feedforwardLength; k += 1) {\n            y += feedforward[k] * xBuffer[(i - k) & (bufferLength - 1)]; // tslint:disable-line:no-bitwise\n        }\n        for (let k = minLength; k < feedbackLength; k += 1) {\n            y -= feedback[k] * yBuffer[(i - k) & (bufferLength - 1)]; // tslint:disable-line:no-bitwise\n        }\n        xBuffer[i] = input[j];\n        yBuffer[i] = y;\n        i = (i + 1) & (bufferLength - 1); // tslint:disable-line:no-bitwise\n        output[j] = y;\n    }\n    return i;\n};\n//# sourceMappingURL=filter-buffer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/iir-filter-node-renderer-factory.js\n\n\nconst filterFullBuffer = (renderedBuffer, nativeOfflineAudioContext, feedback, feedforward) => {\n    const convertedFeedback = feedback instanceof Float64Array ? feedback : new Float64Array(feedback);\n    const convertedFeedforward = feedforward instanceof Float64Array ? feedforward : new Float64Array(feedforward);\n    const feedbackLength = convertedFeedback.length;\n    const feedforwardLength = convertedFeedforward.length;\n    const minLength = Math.min(feedbackLength, feedforwardLength);\n    if (convertedFeedback[0] !== 1) {\n        for (let i = 0; i < feedbackLength; i += 1) {\n            convertedFeedforward[i] /= convertedFeedback[0];\n        }\n        for (let i = 1; i < feedforwardLength; i += 1) {\n            convertedFeedback[i] /= convertedFeedback[0];\n        }\n    }\n    const bufferLength = 32;\n    const xBuffer = new Float32Array(bufferLength);\n    const yBuffer = new Float32Array(bufferLength);\n    const filteredBuffer = nativeOfflineAudioContext.createBuffer(renderedBuffer.numberOfChannels, renderedBuffer.length, renderedBuffer.sampleRate);\n    const numberOfChannels = renderedBuffer.numberOfChannels;\n    for (let i = 0; i < numberOfChannels; i += 1) {\n        const input = renderedBuffer.getChannelData(i);\n        const output = filteredBuffer.getChannelData(i);\n        xBuffer.fill(0);\n        yBuffer.fill(0);\n        filterBuffer(convertedFeedback, feedbackLength, convertedFeedforward, feedforwardLength, minLength, xBuffer, yBuffer, 0, bufferLength, input, output);\n    }\n    return filteredBuffer;\n};\nconst createIIRFilterNodeRendererFactory = (createNativeAudioBufferSourceNode, getNativeAudioNode, nativeOfflineAudioContextConstructor, renderInputsOfAudioNode, renderNativeOfflineAudioContext) => {\n    return (feedback, feedforward) => {\n        const renderedNativeAudioNodes = new WeakMap();\n        let filteredBufferPromise = null;\n        const createAudioNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeAudioBufferSourceNode = null;\n            let nativeIIRFilterNode = getNativeAudioNode(proxy);\n            // If the initially used nativeIIRFilterNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeIIRFilterNodeIsOwnedByContext = isOwnedByContext(nativeIIRFilterNode, nativeOfflineAudioContext);\n            // Bug #9: Safari does not support IIRFilterNodes.\n            if (nativeOfflineAudioContext.createIIRFilter === undefined) {\n                nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode(nativeOfflineAudioContext, {\n                    buffer: null,\n                    channelCount: 2,\n                    channelCountMode: \'max\',\n                    channelInterpretation: \'speakers\',\n                    loop: false,\n                    loopEnd: 0,\n                    loopStart: 0,\n                    playbackRate: 1\n                });\n            }\n            else if (!nativeIIRFilterNodeIsOwnedByContext) {\n                // @todo TypeScript defines the parameters of createIIRFilter() as arrays of numbers.\n                nativeIIRFilterNode = nativeOfflineAudioContext.createIIRFilter(feedforward, feedback);\n            }\n            renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeAudioBufferSourceNode === null ? nativeIIRFilterNode : nativeAudioBufferSourceNode);\n            if (nativeAudioBufferSourceNode !== null) {\n                if (filteredBufferPromise === null) {\n                    if (nativeOfflineAudioContextConstructor === null) {\n                        throw new Error(\'Missing the native OfflineAudioContext constructor.\');\n                    }\n                    const partialOfflineAudioContext = new nativeOfflineAudioContextConstructor(\n                    // Bug #47: The AudioDestinationNode in Safari gets not initialized correctly.\n                    proxy.context.destination.channelCount, \n                    // Bug #17: Safari does not yet expose the length.\n                    proxy.context.length, nativeOfflineAudioContext.sampleRate);\n                    filteredBufferPromise = (async () => {\n                        await renderInputsOfAudioNode(proxy, partialOfflineAudioContext, partialOfflineAudioContext.destination, trace);\n                        const renderedBuffer = await renderNativeOfflineAudioContext(partialOfflineAudioContext);\n                        return filterFullBuffer(renderedBuffer, nativeOfflineAudioContext, feedback, feedforward);\n                    })();\n                }\n                const filteredBuffer = await filteredBufferPromise;\n                nativeAudioBufferSourceNode.buffer = filteredBuffer;\n                nativeAudioBufferSourceNode.start(0);\n                return nativeAudioBufferSourceNode;\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeIIRFilterNode, trace);\n            return nativeIIRFilterNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeAudioNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeAudioNode !== undefined) {\n                    return Promise.resolve(renderedNativeAudioNode);\n                }\n                return createAudioNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=iir-filter-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/increment-cycle-counter-factory.js\n\nconst createIncrementCycleCounterFactory = (cycleCounters, disconnectNativeAudioNodeFromNativeAudioNode, getAudioNodeConnections, getNativeAudioNode, getNativeAudioParam, isActiveAudioNode) => {\n    return (isOffline) => {\n        return (audioNode, count) => {\n            const cycleCounter = cycleCounters.get(audioNode);\n            if (cycleCounter === undefined) {\n                if (!isOffline && isActiveAudioNode(audioNode)) {\n                    const nativeSourceAudioNode = getNativeAudioNode(audioNode);\n                    const { outputs } = getAudioNodeConnections(audioNode);\n                    for (const output of outputs) {\n                        if (isAudioNodeOutputConnection(output)) {\n                            const nativeDestinationAudioNode = getNativeAudioNode(output[0]);\n                            disconnectNativeAudioNodeFromNativeAudioNode(nativeSourceAudioNode, nativeDestinationAudioNode, output[1], output[2]);\n                        }\n                        else {\n                            const nativeDestinationAudioParam = getNativeAudioParam(output[0]);\n                            nativeSourceAudioNode.disconnect(nativeDestinationAudioParam, output[1]);\n                        }\n                    }\n                }\n                cycleCounters.set(audioNode, count);\n            }\n            else {\n                cycleCounters.set(audioNode, cycleCounter + count);\n            }\n        };\n    };\n};\n//# sourceMappingURL=increment-cycle-counter-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-any-audio-context.js\nconst createIsAnyAudioContext = (contextStore, isNativeAudioContext) => {\n    return (anything) => {\n        const nativeContext = contextStore.get(anything);\n        return isNativeAudioContext(nativeContext) || isNativeAudioContext(anything);\n    };\n};\n//# sourceMappingURL=is-any-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-any-audio-node.js\nconst createIsAnyAudioNode = (audioNodeStore, isNativeAudioNode) => {\n    return (anything) => audioNodeStore.has(anything) || isNativeAudioNode(anything);\n};\n//# sourceMappingURL=is-any-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-any-audio-param.js\nconst createIsAnyAudioParam = (audioParamStore, isNativeAudioParam) => {\n    return (anything) => audioParamStore.has(anything) || isNativeAudioParam(anything);\n};\n//# sourceMappingURL=is-any-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-any-offline-audio-context.js\nconst createIsAnyOfflineAudioContext = (contextStore, isNativeOfflineAudioContext) => {\n    return (anything) => {\n        const nativeContext = contextStore.get(anything);\n        return isNativeOfflineAudioContext(nativeContext) || isNativeOfflineAudioContext(anything);\n    };\n};\n//# sourceMappingURL=is-any-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-native-audio-context.js\nconst createIsNativeAudioContext = (nativeAudioContextConstructor) => {\n    return (anything) => {\n        return nativeAudioContextConstructor !== null && anything instanceof nativeAudioContextConstructor;\n    };\n};\n//# sourceMappingURL=is-native-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-native-audio-node.js\nconst createIsNativeAudioNode = (window) => {\n    return (anything) => {\n        return window !== null && typeof window.AudioNode === \'function\' && anything instanceof window.AudioNode;\n    };\n};\n//# sourceMappingURL=is-native-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-native-audio-param.js\nconst createIsNativeAudioParam = (window) => {\n    return (anything) => {\n        return window !== null && typeof window.AudioParam === \'function\' && anything instanceof window.AudioParam;\n    };\n};\n//# sourceMappingURL=is-native-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-native-context.js\nconst createIsNativeContext = (isNativeAudioContext, isNativeOfflineAudioContext) => {\n    return (anything) => {\n        return isNativeAudioContext(anything) || isNativeOfflineAudioContext(anything);\n    };\n};\n//# sourceMappingURL=is-native-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-native-offline-audio-context.js\nconst createIsNativeOfflineAudioContext = (nativeOfflineAudioContextConstructor) => {\n    return (anything) => {\n        return nativeOfflineAudioContextConstructor !== null && anything instanceof nativeOfflineAudioContextConstructor;\n    };\n};\n//# sourceMappingURL=is-native-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-secure-context.js\nconst createIsSecureContext = (window) => window !== null && window.isSecureContext;\n//# sourceMappingURL=is-secure-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/is-supported-promise.js\nconst createIsSupportedPromise = async (cacheTestResult, testAudioBufferCopyChannelMethodsSubarraySupport, testAudioContextCloseMethodSupport, testAudioContextDecodeAudioDataMethodTypeErrorSupport, testAudioContextOptionsSupport, testAudioNodeConnectMethodSupport, testAudioWorkletProcessorNoOutputsSupport, testChannelMergerNodeChannelCountSupport, testConstantSourceNodeAccurateSchedulingSupport, testConvolverNodeBufferReassignabilitySupport, testConvolverNodeChannelCountSupport, testDomExceptionContrucorSupport, testIsSecureContextSupport, testMediaStreamAudioSourceNodeMediaStreamWithoutAudioTrackSupport, testStereoPannerNodeDefaultValueSupport, testTransferablesSupport) => {\n    if (cacheTestResult(testAudioBufferCopyChannelMethodsSubarraySupport, testAudioBufferCopyChannelMethodsSubarraySupport) &&\n        cacheTestResult(testAudioContextCloseMethodSupport, testAudioContextCloseMethodSupport) &&\n        cacheTestResult(testAudioContextOptionsSupport, testAudioContextOptionsSupport) &&\n        cacheTestResult(testAudioNodeConnectMethodSupport, testAudioNodeConnectMethodSupport) &&\n        cacheTestResult(testChannelMergerNodeChannelCountSupport, testChannelMergerNodeChannelCountSupport) &&\n        cacheTestResult(testConstantSourceNodeAccurateSchedulingSupport, testConstantSourceNodeAccurateSchedulingSupport) &&\n        cacheTestResult(testConvolverNodeBufferReassignabilitySupport, testConvolverNodeBufferReassignabilitySupport) &&\n        cacheTestResult(testConvolverNodeChannelCountSupport, testConvolverNodeChannelCountSupport) &&\n        cacheTestResult(testDomExceptionContrucorSupport, testDomExceptionContrucorSupport) &&\n        cacheTestResult(testIsSecureContextSupport, testIsSecureContextSupport) &&\n        cacheTestResult(testMediaStreamAudioSourceNodeMediaStreamWithoutAudioTrackSupport, testMediaStreamAudioSourceNodeMediaStreamWithoutAudioTrackSupport)) {\n        const results = await Promise.all([\n            cacheTestResult(testAudioContextDecodeAudioDataMethodTypeErrorSupport, testAudioContextDecodeAudioDataMethodTypeErrorSupport),\n            cacheTestResult(testAudioWorkletProcessorNoOutputsSupport, testAudioWorkletProcessorNoOutputsSupport),\n            cacheTestResult(testStereoPannerNodeDefaultValueSupport, testStereoPannerNodeDefaultValueSupport),\n            cacheTestResult(testTransferablesSupport, testTransferablesSupport)\n        ]);\n        return results.every((result) => result);\n    }\n    return false;\n};\n//# sourceMappingURL=is-supported-promise.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/media-element-audio-source-node-constructor.js\nconst createMediaElementAudioSourceNodeConstructor = (audioNodeConstructor, createNativeMediaElementAudioSourceNode, getNativeContext, isNativeOfflineAudioContext) => {\n    return class MediaElementAudioSourceNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const nativeMediaElementAudioSourceNode = createNativeMediaElementAudioSourceNode(nativeContext, options);\n            // Bug #171: Safari allows to create a MediaElementAudioSourceNode with an OfflineAudioContext.\n            if (isNativeOfflineAudioContext(nativeContext)) {\n                throw TypeError();\n            }\n            super(context, true, nativeMediaElementAudioSourceNode, null);\n            this._nativeMediaElementAudioSourceNode = nativeMediaElementAudioSourceNode;\n        }\n        get mediaElement() {\n            return this._nativeMediaElementAudioSourceNode.mediaElement;\n        }\n    };\n};\n//# sourceMappingURL=media-element-audio-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/media-stream-audio-destination-node-constructor.js\nconst media_stream_audio_destination_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'explicit\',\n    channelInterpretation: \'speakers\'\n};\nconst createMediaStreamAudioDestinationNodeConstructor = (audioNodeConstructor, createNativeMediaStreamAudioDestinationNode, getNativeContext, isNativeOfflineAudioContext) => {\n    return class MediaStreamAudioDestinationNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            // Bug #173: Safari allows to create a MediaStreamAudioDestinationNode with an OfflineAudioContext.\n            if (isNativeOfflineAudioContext(nativeContext)) {\n                throw new TypeError();\n            }\n            const mergedOptions = { ...media_stream_audio_destination_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeMediaStreamAudioDestinationNode = createNativeMediaStreamAudioDestinationNode(nativeContext, mergedOptions);\n            super(context, false, nativeMediaStreamAudioDestinationNode, null);\n            this._nativeMediaStreamAudioDestinationNode = nativeMediaStreamAudioDestinationNode;\n        }\n        get stream() {\n            return this._nativeMediaStreamAudioDestinationNode.stream;\n        }\n    };\n};\n//# sourceMappingURL=media-stream-audio-destination-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/media-stream-audio-source-node-constructor.js\nconst createMediaStreamAudioSourceNodeConstructor = (audioNodeConstructor, createNativeMediaStreamAudioSourceNode, getNativeContext, isNativeOfflineAudioContext) => {\n    return class MediaStreamAudioSourceNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const nativeMediaStreamAudioSourceNode = createNativeMediaStreamAudioSourceNode(nativeContext, options);\n            // Bug #172: Safari allows to create a MediaStreamAudioSourceNode with an OfflineAudioContext.\n            if (isNativeOfflineAudioContext(nativeContext)) {\n                throw new TypeError();\n            }\n            super(context, true, nativeMediaStreamAudioSourceNode, null);\n            this._nativeMediaStreamAudioSourceNode = nativeMediaStreamAudioSourceNode;\n        }\n        get mediaStream() {\n            return this._nativeMediaStreamAudioSourceNode.mediaStream;\n        }\n    };\n};\n//# sourceMappingURL=media-stream-audio-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/media-stream-track-audio-source-node-constructor.js\nconst createMediaStreamTrackAudioSourceNodeConstructor = (audioNodeConstructor, createNativeMediaStreamTrackAudioSourceNode, getNativeContext) => {\n    return class MediaStreamTrackAudioSourceNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const nativeMediaStreamTrackAudioSourceNode = createNativeMediaStreamTrackAudioSourceNode(nativeContext, options);\n            super(context, true, nativeMediaStreamTrackAudioSourceNode, null);\n        }\n    };\n};\n//# sourceMappingURL=media-stream-track-audio-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/minimal-audio-context-constructor.js\n\n\nconst createMinimalAudioContextConstructor = (createInvalidStateError, createNotSupportedError, createUnknownError, minimalBaseAudioContextConstructor, nativeAudioContextConstructor) => {\n    return class MinimalAudioContext extends minimalBaseAudioContextConstructor {\n        constructor(options = {}) {\n            if (nativeAudioContextConstructor === null) {\n                throw new Error(\'Missing the native AudioContext constructor.\');\n            }\n            const nativeAudioContext = new nativeAudioContextConstructor(options);\n            // Bug #131 Safari returns null when there are four other AudioContexts running already.\n            if (nativeAudioContext === null) {\n                throw createUnknownError();\n            }\n            // Bug #51 Only Chrome Edge, and Opera throw an error if the given latencyHint is invalid.\n            if (!isValidLatencyHint(options.latencyHint)) {\n                throw new TypeError(`The provided value \'${options.latencyHint}\' is not a valid enum value of type AudioContextLatencyCategory.`);\n            }\n            // Bug #150 Safari does not support setting the sampleRate.\n            if (options.sampleRate !== undefined && nativeAudioContext.sampleRate !== options.sampleRate) {\n                throw createNotSupportedError();\n            }\n            super(nativeAudioContext, 2);\n            const { latencyHint } = options;\n            const { sampleRate } = nativeAudioContext;\n            // @todo The values for \'balanced\', \'interactive\' and \'playback\' are just copied from Chrome\'s implementation.\n            this._baseLatency =\n                typeof nativeAudioContext.baseLatency === \'number\'\n                    ? nativeAudioContext.baseLatency\n                    : latencyHint === \'balanced\'\n                        ? 512 / sampleRate\n                        : latencyHint === \'interactive\' || latencyHint === undefined\n                            ? 256 / sampleRate\n                            : latencyHint === \'playback\'\n                                ? 1024 / sampleRate\n                                : /*\n                                   * @todo The min (256) and max (16384) values are taken from the allowed bufferSize values of a\n                                   * ScriptProcessorNode.\n                                   */\n                                    (Math.max(2, Math.min(128, Math.round((latencyHint * sampleRate) / 128))) * 128) / sampleRate;\n            this._nativeAudioContext = nativeAudioContext;\n            // Bug #188: Safari will set the context\'s state to \'interrupted\' in case the user switches tabs.\n            if (nativeAudioContextConstructor.name === \'webkitAudioContext\') {\n                this._nativeGainNode = nativeAudioContext.createGain();\n                this._nativeOscillatorNode = nativeAudioContext.createOscillator();\n                this._nativeGainNode.gain.value = 1e-37;\n                this._nativeOscillatorNode.connect(this._nativeGainNode).connect(nativeAudioContext.destination);\n                this._nativeOscillatorNode.start();\n            }\n            else {\n                this._nativeGainNode = null;\n                this._nativeOscillatorNode = null;\n            }\n            this._state = null;\n            /*\n             * Bug #34: Chrome, Edge and Opera pretend to be running right away, but fire an onstatechange event when the state actually\n             * changes to \'running\'.\n             */\n            if (nativeAudioContext.state === \'running\') {\n                this._state = \'suspended\';\n                const revokeState = () => {\n                    if (this._state === \'suspended\') {\n                        this._state = null;\n                    }\n                    nativeAudioContext.removeEventListener(\'statechange\', revokeState);\n                };\n                nativeAudioContext.addEventListener(\'statechange\', revokeState);\n            }\n        }\n        get baseLatency() {\n            return this._baseLatency;\n        }\n        get state() {\n            return this._state !== null ? this._state : this._nativeAudioContext.state;\n        }\n        close() {\n            // Bug #35: Firefox does not throw an error if the AudioContext was closed before.\n            if (this.state === \'closed\') {\n                return this._nativeAudioContext.close().then(() => {\n                    throw createInvalidStateError();\n                });\n            }\n            // Bug #34: If the state was set to suspended before it should be revoked now.\n            if (this._state === \'suspended\') {\n                this._state = null;\n            }\n            return this._nativeAudioContext.close().then(() => {\n                if (this._nativeGainNode !== null && this._nativeOscillatorNode !== null) {\n                    this._nativeOscillatorNode.stop();\n                    this._nativeGainNode.disconnect();\n                    this._nativeOscillatorNode.disconnect();\n                }\n                deactivateAudioGraph(this);\n            });\n        }\n        resume() {\n            if (this._state === \'suspended\') {\n                return new Promise((resolve, reject) => {\n                    const resolvePromise = () => {\n                        this._nativeAudioContext.removeEventListener(\'statechange\', resolvePromise);\n                        if (this._nativeAudioContext.state === \'running\') {\n                            resolve();\n                        }\n                        else {\n                            this.resume().then(resolve, reject);\n                        }\n                    };\n                    this._nativeAudioContext.addEventListener(\'statechange\', resolvePromise);\n                });\n            }\n            return this._nativeAudioContext.resume().catch((err) => {\n                // Bug #55: Chrome, Edge and Opera do throw an InvalidAccessError instead of an InvalidStateError.\n                // Bug #56: Safari invokes the catch handler but without an error.\n                if (err === undefined || err.code === 15) {\n                    throw createInvalidStateError();\n                }\n                throw err;\n            });\n        }\n        suspend() {\n            return this._nativeAudioContext.suspend().catch((err) => {\n                // Bug #56: Safari invokes the catch handler but without an error.\n                if (err === undefined) {\n                    throw createInvalidStateError();\n                }\n                throw err;\n            });\n        }\n    };\n};\n//# sourceMappingURL=minimal-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/minimal-base-audio-context-constructor.js\n\nconst createMinimalBaseAudioContextConstructor = (audioDestinationNodeConstructor, createAudioListener, eventTargetConstructor, isNativeOfflineAudioContext, unrenderedAudioWorkletNodeStore, wrapEventListener) => {\n    return class MinimalBaseAudioContext extends eventTargetConstructor {\n        constructor(_nativeContext, numberOfChannels) {\n            super(_nativeContext);\n            this._nativeContext = _nativeContext;\n            CONTEXT_STORE.set(this, _nativeContext);\n            if (isNativeOfflineAudioContext(_nativeContext)) {\n                unrenderedAudioWorkletNodeStore.set(_nativeContext, new Set());\n            }\n            this._destination = new audioDestinationNodeConstructor(this, numberOfChannels);\n            this._listener = createAudioListener(this, _nativeContext);\n            this._onstatechange = null;\n        }\n        get currentTime() {\n            return this._nativeContext.currentTime;\n        }\n        get destination() {\n            return this._destination;\n        }\n        get listener() {\n            return this._listener;\n        }\n        get onstatechange() {\n            return this._onstatechange;\n        }\n        set onstatechange(value) {\n            const wrappedListener = typeof value === \'function\' ? wrapEventListener(this, value) : null;\n            this._nativeContext.onstatechange = wrappedListener;\n            const nativeOnStateChange = this._nativeContext.onstatechange;\n            this._onstatechange =\n                nativeOnStateChange !== null && nativeOnStateChange === wrappedListener\n                    ? value\n                    : nativeOnStateChange;\n        }\n        get sampleRate() {\n            return this._nativeContext.sampleRate;\n        }\n        get state() {\n            return this._nativeContext.state;\n        }\n    };\n};\n//# sourceMappingURL=minimal-base-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-promise-support.js\nconst test_promise_support_testPromiseSupport = (nativeContext) => {\n    // This 12 numbers represent the 48 bytes of an empty WAVE file with a single sample.\n    const uint32Array = new Uint32Array([1179011410, 40, 1163280727, 544501094, 16, 131073, 44100, 176400, 1048580, 1635017060, 4, 0]);\n    try {\n        // Bug #1: Safari requires a successCallback.\n        const promise = nativeContext.decodeAudioData(uint32Array.buffer, () => {\n            // Ignore the success callback.\n        });\n        if (promise === undefined) {\n            return false;\n        }\n        promise.catch(() => {\n            // Ignore rejected errors.\n        });\n        return true;\n    }\n    catch {\n        // Ignore errors.\n    }\n    return false;\n};\n//# sourceMappingURL=test-promise-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/minimal-offline-audio-context-constructor.js\n\n\nconst minimal_offline_audio_context_constructor_DEFAULT_OPTIONS = {\n    numberOfChannels: 1\n};\nconst createMinimalOfflineAudioContextConstructor = (cacheTestResult, createInvalidStateError, createNativeOfflineAudioContext, minimalBaseAudioContextConstructor, startRendering) => {\n    return class MinimalOfflineAudioContext extends minimalBaseAudioContextConstructor {\n        constructor(options) {\n            const { length, numberOfChannels, sampleRate } = { ...minimal_offline_audio_context_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeOfflineAudioContext = createNativeOfflineAudioContext(numberOfChannels, length, sampleRate);\n            // #21 Safari does not support promises and therefore would fire the statechange event before the promise can be resolved.\n            if (!cacheTestResult(test_promise_support_testPromiseSupport, () => test_promise_support_testPromiseSupport(nativeOfflineAudioContext))) {\n                nativeOfflineAudioContext.addEventListener(\'statechange\', (() => {\n                    let i = 0;\n                    const delayStateChangeEvent = (event) => {\n                        if (this._state === \'running\') {\n                            if (i > 0) {\n                                nativeOfflineAudioContext.removeEventListener(\'statechange\', delayStateChangeEvent);\n                                event.stopImmediatePropagation();\n                                this._waitForThePromiseToSettle(event);\n                            }\n                            else {\n                                i += 1;\n                            }\n                        }\n                    };\n                    return delayStateChangeEvent;\n                })());\n            }\n            super(nativeOfflineAudioContext, numberOfChannels);\n            this._length = length;\n            this._nativeOfflineAudioContext = nativeOfflineAudioContext;\n            this._state = null;\n        }\n        get length() {\n            // Bug #17: Safari does not yet expose the length.\n            if (this._nativeOfflineAudioContext.length === undefined) {\n                return this._length;\n            }\n            return this._nativeOfflineAudioContext.length;\n        }\n        get state() {\n            return this._state === null ? this._nativeOfflineAudioContext.state : this._state;\n        }\n        startRendering() {\n            /*\n             * Bug #9 & #59: It is theoretically possible that startRendering() will first render a partialOfflineAudioContext. Therefore\n             * the state of the nativeOfflineAudioContext might no transition to running immediately.\n             */\n            if (this._state === \'running\') {\n                return Promise.reject(createInvalidStateError());\n            }\n            this._state = \'running\';\n            return startRendering(this.destination, this._nativeOfflineAudioContext).finally(() => {\n                this._state = null;\n                deactivateAudioGraph(this);\n            });\n        }\n        _waitForThePromiseToSettle(event) {\n            if (this._state === null) {\n                this._nativeOfflineAudioContext.dispatchEvent(event);\n            }\n            else {\n                setTimeout(() => this._waitForThePromiseToSettle(event));\n            }\n        }\n    };\n};\n//# sourceMappingURL=minimal-offline-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/monitor-connections.js\nconst createMonitorConnections = (insertElementInSet, isNativeAudioNode) => {\n    return (nativeAudioNode, whenConnected, whenDisconnected) => {\n        const connections = new Set();\n        nativeAudioNode.connect = ((connect) => {\n            // tslint:disable-next-line:invalid-void\n            return (destination, output = 0, input = 0) => {\n                const wasDisconnected = connections.size === 0;\n                if (isNativeAudioNode(destination)) {\n                    // @todo TypeScript cannot infer the overloaded signature with 3 arguments yet.\n                    connect.call(nativeAudioNode, destination, output, input);\n                    insertElementInSet(connections, [destination, output, input], (connection) => connection[0] === destination && connection[1] === output && connection[2] === input, true);\n                    if (wasDisconnected) {\n                        whenConnected();\n                    }\n                    return destination;\n                }\n                connect.call(nativeAudioNode, destination, output);\n                insertElementInSet(connections, [destination, output], (connection) => connection[0] === destination && connection[1] === output, true);\n                if (wasDisconnected) {\n                    whenConnected();\n                }\n                return;\n            };\n        })(nativeAudioNode.connect);\n        nativeAudioNode.disconnect = ((disconnect) => {\n            return (destinationOrOutput, output, input) => {\n                const wasConnected = connections.size > 0;\n                if (destinationOrOutput === undefined) {\n                    disconnect.apply(nativeAudioNode);\n                    connections.clear();\n                }\n                else if (typeof destinationOrOutput === \'number\') {\n                    // @todo TypeScript cannot infer the overloaded signature with 1 argument yet.\n                    disconnect.call(nativeAudioNode, destinationOrOutput);\n                    for (const connection of connections) {\n                        if (connection[1] === destinationOrOutput) {\n                            connections.delete(connection);\n                        }\n                    }\n                }\n                else {\n                    if (isNativeAudioNode(destinationOrOutput)) {\n                        // @todo TypeScript cannot infer the overloaded signature with 3 arguments yet.\n                        disconnect.call(nativeAudioNode, destinationOrOutput, output, input);\n                    }\n                    else {\n                        // @todo TypeScript cannot infer the overloaded signature with 2 arguments yet.\n                        disconnect.call(nativeAudioNode, destinationOrOutput, output);\n                    }\n                    for (const connection of connections) {\n                        if (connection[0] === destinationOrOutput &&\n                            (output === undefined || connection[1] === output) &&\n                            (input === undefined || connection[2] === input)) {\n                            connections.delete(connection);\n                        }\n                    }\n                }\n                const isDisconnected = connections.size === 0;\n                if (wasConnected && isDisconnected) {\n                    whenDisconnected();\n                }\n            };\n        })(nativeAudioNode.disconnect);\n        return nativeAudioNode;\n    };\n};\n//# sourceMappingURL=monitor-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/assign-native-audio-node-option.js\nconst assignNativeAudioNodeOption = (nativeAudioNode, options, option) => {\n    const value = options[option];\n    if (value !== undefined && value !== nativeAudioNode[option]) {\n        nativeAudioNode[option] = value;\n    }\n};\n//# sourceMappingURL=assign-native-audio-node-option.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/assign-native-audio-node-options.js\n\nconst assignNativeAudioNodeOptions = (nativeAudioNode, options) => {\n    assignNativeAudioNodeOption(nativeAudioNode, options, \'channelCount\');\n    assignNativeAudioNodeOption(nativeAudioNode, options, \'channelCountMode\');\n    assignNativeAudioNodeOption(nativeAudioNode, options, \'channelInterpretation\');\n};\n//# sourceMappingURL=assign-native-audio-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-analyser-node-get-float-time-domain-data-method-support.js\nconst testAnalyserNodeGetFloatTimeDomainDataMethodSupport = (nativeAnalyserNode) => {\n    return typeof nativeAnalyserNode.getFloatTimeDomainData === \'function\';\n};\n//# sourceMappingURL=test-analyser-node-get-float-time-domain-data-method-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-analyser-node-get-float-time-domain-data-method.js\nconst wrapAnalyserNodeGetFloatTimeDomainDataMethod = (nativeAnalyserNode) => {\n    nativeAnalyserNode.getFloatTimeDomainData = (array) => {\n        const byteTimeDomainData = new Uint8Array(array.length);\n        nativeAnalyserNode.getByteTimeDomainData(byteTimeDomainData);\n        const length = Math.max(byteTimeDomainData.length, nativeAnalyserNode.fftSize);\n        for (let i = 0; i < length; i += 1) {\n            array[i] = (byteTimeDomainData[i] - 128) * 0.0078125;\n        }\n        return array;\n    };\n};\n//# sourceMappingURL=wrap-analyser-node-get-float-time-domain-data-method.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-analyser-node-factory.js\n\n\n\n\nconst createNativeAnalyserNodeFactory = (cacheTestResult, createIndexSizeError) => {\n    return (nativeContext, options) => {\n        const nativeAnalyserNode = nativeContext.createAnalyser();\n        // Bug #37: Firefox does not create an AnalyserNode with the default properties.\n        assignNativeAudioNodeOptions(nativeAnalyserNode, options);\n        // Bug #118: Safari does not throw an error if maxDecibels is not more than minDecibels.\n        if (!(options.maxDecibels > options.minDecibels)) {\n            throw createIndexSizeError();\n        }\n        assignNativeAudioNodeOption(nativeAnalyserNode, options, \'fftSize\');\n        assignNativeAudioNodeOption(nativeAnalyserNode, options, \'maxDecibels\');\n        assignNativeAudioNodeOption(nativeAnalyserNode, options, \'minDecibels\');\n        assignNativeAudioNodeOption(nativeAnalyserNode, options, \'smoothingTimeConstant\');\n        // Bug #36: Safari does not support getFloatTimeDomainData() yet.\n        if (!cacheTestResult(testAnalyserNodeGetFloatTimeDomainDataMethodSupport, () => testAnalyserNodeGetFloatTimeDomainDataMethodSupport(nativeAnalyserNode))) {\n            wrapAnalyserNodeGetFloatTimeDomainDataMethod(nativeAnalyserNode);\n        }\n        return nativeAnalyserNode;\n    };\n};\n//# sourceMappingURL=native-analyser-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-buffer-constructor.js\nconst createNativeAudioBufferConstructor = (window) => {\n    if (window === null) {\n        return null;\n    }\n    if (window.hasOwnProperty(\'AudioBuffer\')) {\n        return window.AudioBuffer;\n    }\n    return null;\n};\n//# sourceMappingURL=native-audio-buffer-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/assign-native-audio-node-audio-param-value.js\nconst assignNativeAudioNodeAudioParamValue = (nativeAudioNode, options, audioParam) => {\n    const value = options[audioParam];\n    if (value !== undefined && value !== nativeAudioNode[audioParam].value) {\n        nativeAudioNode[audioParam].value = value;\n    }\n};\n//# sourceMappingURL=assign-native-audio-node-audio-param-value.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-buffer-source-node-start-method-consecutive-calls.js\n\nconst wrapAudioBufferSourceNodeStartMethodConsecutiveCalls = (nativeAudioBufferSourceNode) => {\n    nativeAudioBufferSourceNode.start = ((start) => {\n        let isScheduled = false;\n        return (when = 0, offset = 0, duration) => {\n            if (isScheduled) {\n                throw invalid_state_error_createInvalidStateError();\n            }\n            start.call(nativeAudioBufferSourceNode, when, offset, duration);\n            isScheduled = true;\n        };\n    })(nativeAudioBufferSourceNode.start);\n};\n//# sourceMappingURL=wrap-audio-buffer-source-node-start-method-consecutive-calls.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-scheduled-source-node-start-method-negative-parameters.js\nconst wrapAudioScheduledSourceNodeStartMethodNegativeParameters = (nativeAudioScheduledSourceNode) => {\n    nativeAudioScheduledSourceNode.start = ((start) => {\n        return (when = 0, offset = 0, duration) => {\n            if ((typeof duration === \'number\' && duration < 0) || offset < 0 || when < 0) {\n                throw new RangeError("The parameters can\'t be negative.");\n            }\n            // @todo TypeScript cannot infer the overloaded signature with 3 arguments yet.\n            start.call(nativeAudioScheduledSourceNode, when, offset, duration);\n        };\n    })(nativeAudioScheduledSourceNode.start);\n};\n//# sourceMappingURL=wrap-audio-scheduled-source-node-start-method-negative-parameters.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-scheduled-source-node-stop-method-negative-parameters.js\nconst wrapAudioScheduledSourceNodeStopMethodNegativeParameters = (nativeAudioScheduledSourceNode) => {\n    nativeAudioScheduledSourceNode.stop = ((stop) => {\n        return (when = 0) => {\n            if (when < 0) {\n                throw new RangeError("The parameter can\'t be negative.");\n            }\n            stop.call(nativeAudioScheduledSourceNode, when);\n        };\n    })(nativeAudioScheduledSourceNode.stop);\n};\n//# sourceMappingURL=wrap-audio-scheduled-source-node-stop-method-negative-parameters.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-buffer-source-node-factory.js\n\n\n\n\n\n\nconst createNativeAudioBufferSourceNodeFactory = (addSilentConnection, cacheTestResult, testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport, testAudioBufferSourceNodeStartMethodOffsetClampingSupport, testAudioBufferSourceNodeStopMethodNullifiedBufferSupport, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, wrapAudioBufferSourceNodeStartMethodOffsetClampling, wrapAudioBufferSourceNodeStopMethodNullifiedBuffer, wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls) => {\n    return (nativeContext, options) => {\n        const nativeAudioBufferSourceNode = nativeContext.createBufferSource();\n        assignNativeAudioNodeOptions(nativeAudioBufferSourceNode, options);\n        assignNativeAudioNodeAudioParamValue(nativeAudioBufferSourceNode, options, \'playbackRate\');\n        assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, \'buffer\');\n        // Bug #149: Safari does not yet support the detune AudioParam.\n        assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, \'loop\');\n        assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, \'loopEnd\');\n        assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, \'loopStart\');\n        // Bug #69: Safari does allow calls to start() of an already scheduled AudioBufferSourceNode.\n        if (!cacheTestResult(testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport, () => testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport(nativeContext))) {\n            wrapAudioBufferSourceNodeStartMethodConsecutiveCalls(nativeAudioBufferSourceNode);\n        }\n        // Bug #154 & #155: Safari does not handle offsets which are equal to or greater than the duration of the buffer.\n        if (!cacheTestResult(testAudioBufferSourceNodeStartMethodOffsetClampingSupport, () => testAudioBufferSourceNodeStartMethodOffsetClampingSupport(nativeContext))) {\n            wrapAudioBufferSourceNodeStartMethodOffsetClampling(nativeAudioBufferSourceNode);\n        }\n        // Bug #162: Safari does throw an error when stop() is called on an AudioBufferSourceNode which has no buffer assigned to it.\n        if (!cacheTestResult(testAudioBufferSourceNodeStopMethodNullifiedBufferSupport, () => testAudioBufferSourceNodeStopMethodNullifiedBufferSupport(nativeContext))) {\n            wrapAudioBufferSourceNodeStopMethodNullifiedBuffer(nativeAudioBufferSourceNode, nativeContext);\n        }\n        // Bug #44: Safari does not throw a RangeError yet.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, () => testAudioScheduledSourceNodeStartMethodNegativeParametersSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStartMethodNegativeParameters(nativeAudioBufferSourceNode);\n        }\n        // Bug #19: Safari does not ignore calls to stop() of an already stopped AudioBufferSourceNode.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, () => testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls(nativeAudioBufferSourceNode, nativeContext);\n        }\n        // Bug #44: Only Firefox does not throw a RangeError yet.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, () => testAudioScheduledSourceNodeStopMethodNegativeParametersSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStopMethodNegativeParameters(nativeAudioBufferSourceNode);\n        }\n        // Bug #175: Safari will not fire an ended event if the AudioBufferSourceNode is unconnected.\n        addSilentConnection(nativeContext, nativeAudioBufferSourceNode);\n        return nativeAudioBufferSourceNode;\n    };\n};\n//# sourceMappingURL=native-audio-buffer-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-context-constructor.js\nconst createNativeAudioContextConstructor = (window) => {\n    if (window === null) {\n        return null;\n    }\n    if (window.hasOwnProperty(\'AudioContext\')) {\n        return window.AudioContext;\n    }\n    return window.hasOwnProperty(\'webkitAudioContext\') ? window.webkitAudioContext : null;\n};\n//# sourceMappingURL=native-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-destination-node.js\nconst createNativeAudioDestinationNodeFactory = (createNativeGainNode, overwriteAccessors) => {\n    return (nativeContext, channelCount, isNodeOfNativeOfflineAudioContext) => {\n        const nativeAudioDestinationNode = nativeContext.destination;\n        // Bug #132: Safari does not have the correct channelCount.\n        if (nativeAudioDestinationNode.channelCount !== channelCount) {\n            try {\n                nativeAudioDestinationNode.channelCount = channelCount;\n            }\n            catch {\n                // Bug #169: Safari throws an error on each attempt to change the channelCount.\n            }\n        }\n        // Bug #83: Safari does not have the correct channelCountMode.\n        if (isNodeOfNativeOfflineAudioContext && nativeAudioDestinationNode.channelCountMode !== \'explicit\') {\n            nativeAudioDestinationNode.channelCountMode = \'explicit\';\n        }\n        // Bug #47: The AudioDestinationNode in Safari does not initialize the maxChannelCount property correctly.\n        if (nativeAudioDestinationNode.maxChannelCount === 0) {\n            Object.defineProperty(nativeAudioDestinationNode, \'maxChannelCount\', {\n                value: channelCount\n            });\n        }\n        // Bug #168: No browser does yet have an AudioDestinationNode with an output.\n        const gainNode = createNativeGainNode(nativeContext, {\n            channelCount,\n            channelCountMode: nativeAudioDestinationNode.channelCountMode,\n            channelInterpretation: nativeAudioDestinationNode.channelInterpretation,\n            gain: 1\n        });\n        overwriteAccessors(gainNode, \'channelCount\', (get) => () => get.call(gainNode), (set) => (value) => {\n            set.call(gainNode, value);\n            try {\n                nativeAudioDestinationNode.channelCount = value;\n            }\n            catch (err) {\n                // Bug #169: Safari throws an error on each attempt to change the channelCount.\n                if (value > nativeAudioDestinationNode.maxChannelCount) {\n                    throw err;\n                }\n            }\n        });\n        overwriteAccessors(gainNode, \'channelCountMode\', (get) => () => get.call(gainNode), (set) => (value) => {\n            set.call(gainNode, value);\n            nativeAudioDestinationNode.channelCountMode = value;\n        });\n        overwriteAccessors(gainNode, \'channelInterpretation\', (get) => () => get.call(gainNode), (set) => (value) => {\n            set.call(gainNode, value);\n            nativeAudioDestinationNode.channelInterpretation = value;\n        });\n        Object.defineProperty(gainNode, \'maxChannelCount\', {\n            get: () => nativeAudioDestinationNode.maxChannelCount\n        });\n        // @todo This should be disconnected when the context is closed.\n        gainNode.connect(nativeAudioDestinationNode);\n        return gainNode;\n    };\n};\n//# sourceMappingURL=native-audio-destination-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-worklet-node-constructor.js\nconst createNativeAudioWorkletNodeConstructor = (window) => {\n    if (window === null) {\n        return null;\n    }\n    return window.hasOwnProperty(\'AudioWorkletNode\') ? window.AudioWorkletNode : null;\n};\n//# sourceMappingURL=native-audio-worklet-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-clonability-of-audio-worklet-node-options.js\nconst testClonabilityOfAudioWorkletNodeOptions = (audioWorkletNodeOptions) => {\n    const { port1 } = new MessageChannel();\n    try {\n        // This will throw an error if the audioWorkletNodeOptions are not clonable.\n        port1.postMessage(audioWorkletNodeOptions);\n    }\n    finally {\n        port1.close();\n    }\n};\n//# sourceMappingURL=test-clonability-of-audio-worklet-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-worklet-node-factory.js\n\nconst createNativeAudioWorkletNodeFactory = (createInvalidStateError, createNativeAudioWorkletNodeFaker, createNativeGainNode, createNotSupportedError, monitorConnections) => {\n    return (nativeContext, baseLatency, nativeAudioWorkletNodeConstructor, name, processorConstructor, options) => {\n        if (nativeAudioWorkletNodeConstructor !== null) {\n            try {\n                const nativeAudioWorkletNode = new nativeAudioWorkletNodeConstructor(nativeContext, name, options);\n                const patchedEventListeners = new Map();\n                let onprocessorerror = null;\n                Object.defineProperties(nativeAudioWorkletNode, {\n                    /*\n                     * Bug #61: Overwriting the property accessors for channelCount and channelCountMode is necessary as long as some\n                     * browsers have no native implementation to achieve a consistent behavior.\n                     */\n                    channelCount: {\n                        get: () => options.channelCount,\n                        set: () => {\n                            throw createInvalidStateError();\n                        }\n                    },\n                    channelCountMode: {\n                        get: () => \'explicit\',\n                        set: () => {\n                            throw createInvalidStateError();\n                        }\n                    },\n                    // Bug #156: Chrome and Edge do not yet fire an ErrorEvent.\n                    onprocessorerror: {\n                        get: () => onprocessorerror,\n                        set: (value) => {\n                            if (typeof onprocessorerror === \'function\') {\n                                nativeAudioWorkletNode.removeEventListener(\'processorerror\', onprocessorerror);\n                            }\n                            onprocessorerror = typeof value === \'function\' ? value : null;\n                            if (typeof onprocessorerror === \'function\') {\n                                nativeAudioWorkletNode.addEventListener(\'processorerror\', onprocessorerror);\n                            }\n                        }\n                    }\n                });\n                nativeAudioWorkletNode.addEventListener = ((addEventListener) => {\n                    return (...args) => {\n                        if (args[0] === \'processorerror\') {\n                            const unpatchedEventListener = typeof args[1] === \'function\'\n                                ? args[1]\n                                : typeof args[1] === \'object\' && args[1] !== null && typeof args[1].handleEvent === \'function\'\n                                    ? args[1].handleEvent\n                                    : null;\n                            if (unpatchedEventListener !== null) {\n                                const patchedEventListener = patchedEventListeners.get(args[1]);\n                                if (patchedEventListener !== undefined) {\n                                    args[1] = patchedEventListener;\n                                }\n                                else {\n                                    args[1] = (event) => {\n                                        // Bug #178: Chrome, Edge and Opera do fire an event of type error.\n                                        if (event.type === \'error\') {\n                                            Object.defineProperties(event, {\n                                                type: { value: \'processorerror\' }\n                                            });\n                                            unpatchedEventListener(event);\n                                        }\n                                        else {\n                                            unpatchedEventListener(new ErrorEvent(args[0], { ...event }));\n                                        }\n                                    };\n                                    patchedEventListeners.set(unpatchedEventListener, args[1]);\n                                }\n                            }\n                        }\n                        // Bug #178: Chrome, Edge and Opera do fire an event of type error.\n                        addEventListener.call(nativeAudioWorkletNode, \'error\', args[1], args[2]);\n                        return addEventListener.call(nativeAudioWorkletNode, ...args);\n                    };\n                })(nativeAudioWorkletNode.addEventListener);\n                nativeAudioWorkletNode.removeEventListener = ((removeEventListener) => {\n                    return (...args) => {\n                        if (args[0] === \'processorerror\') {\n                            const patchedEventListener = patchedEventListeners.get(args[1]);\n                            if (patchedEventListener !== undefined) {\n                                patchedEventListeners.delete(args[1]);\n                                args[1] = patchedEventListener;\n                            }\n                        }\n                        // Bug #178: Chrome, Edge and Opera do fire an event of type error.\n                        removeEventListener.call(nativeAudioWorkletNode, \'error\', args[1], args[2]);\n                        return removeEventListener.call(nativeAudioWorkletNode, args[0], args[1], args[2]);\n                    };\n                })(nativeAudioWorkletNode.removeEventListener);\n                /*\n                 * Bug #86: Chrome and Edge do not invoke the process() function if the corresponding AudioWorkletNode is unconnected but\n                 * has an output.\n                 */\n                if (options.numberOfOutputs !== 0) {\n                    const nativeGainNode = createNativeGainNode(nativeContext, {\n                        channelCount: 1,\n                        channelCountMode: \'explicit\',\n                        channelInterpretation: \'discrete\',\n                        gain: 0\n                    });\n                    nativeAudioWorkletNode.connect(nativeGainNode).connect(nativeContext.destination);\n                    const whenConnected = () => nativeGainNode.disconnect();\n                    const whenDisconnected = () => nativeGainNode.connect(nativeContext.destination);\n                    // @todo Disconnect the connection when the process() function of the AudioWorkletNode returns false.\n                    return monitorConnections(nativeAudioWorkletNode, whenConnected, whenDisconnected);\n                }\n                return nativeAudioWorkletNode;\n            }\n            catch (err) {\n                // Bug #60: Chrome, Edge & Opera throw an InvalidStateError instead of a NotSupportedError.\n                if (err.code === 11) {\n                    throw createNotSupportedError();\n                }\n                throw err;\n            }\n        }\n        // Bug #61: Only Chrome & Opera have an implementation of the AudioWorkletNode yet.\n        if (processorConstructor === undefined) {\n            throw createNotSupportedError();\n        }\n        testClonabilityOfAudioWorkletNodeOptions(options);\n        return createNativeAudioWorkletNodeFaker(nativeContext, baseLatency, processorConstructor, options);\n    };\n};\n//# sourceMappingURL=native-audio-worklet-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/compute-buffer-size.js\nconst computeBufferSize = (baseLatency, sampleRate) => {\n    if (baseLatency === null) {\n        return 512;\n    }\n    return Math.max(512, Math.min(16384, Math.pow(2, Math.round(Math.log2(baseLatency * sampleRate)))));\n};\n//# sourceMappingURL=compute-buffer-size.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/clone-audio-worklet-node-options.js\nconst cloneAudioWorkletNodeOptions = (audioWorkletNodeOptions) => {\n    return new Promise((resolve, reject) => {\n        const { port1, port2 } = new MessageChannel();\n        port1.onmessage = ({ data }) => {\n            port1.close();\n            port2.close();\n            resolve(data);\n        };\n        port1.onmessageerror = ({ data }) => {\n            port1.close();\n            port2.close();\n            reject(data);\n        };\n        // This will throw an error if the audioWorkletNodeOptions are not clonable.\n        port2.postMessage(audioWorkletNodeOptions);\n    });\n};\n//# sourceMappingURL=clone-audio-worklet-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/create-audio-worklet-processor-promise.js\n\nconst createAudioWorkletProcessorPromise = async (processorConstructor, audioWorkletNodeOptions) => {\n    const clonedAudioWorkletNodeOptions = await cloneAudioWorkletNodeOptions(audioWorkletNodeOptions);\n    return new processorConstructor(clonedAudioWorkletNodeOptions);\n};\n//# sourceMappingURL=create-audio-worklet-processor-promise.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/create-audio-worklet-processor.js\n\n\nconst createAudioWorkletProcessor = (nativeContext, nativeAudioWorkletNode, processorConstructor, audioWorkletNodeOptions) => {\n    let nodeToProcessorMap = NODE_TO_PROCESSOR_MAPS.get(nativeContext);\n    if (nodeToProcessorMap === undefined) {\n        nodeToProcessorMap = new WeakMap();\n        NODE_TO_PROCESSOR_MAPS.set(nativeContext, nodeToProcessorMap);\n    }\n    const audioWorkletProcessorPromise = createAudioWorkletProcessorPromise(processorConstructor, audioWorkletNodeOptions);\n    nodeToProcessorMap.set(nativeAudioWorkletNode, audioWorkletProcessorPromise);\n    return audioWorkletProcessorPromise;\n};\n//# sourceMappingURL=create-audio-worklet-processor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-audio-worklet-node-faker-factory.js\n\n\n\n\n\n\n\nconst createNativeAudioWorkletNodeFakerFactory = (connectMultipleOutputs, createIndexSizeError, createInvalidStateError, createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeConstantSourceNode, createNativeGainNode, createNativeScriptProcessorNode, createNotSupportedError, disconnectMultipleOutputs, exposeCurrentFrameAndCurrentTime, getActiveAudioWorkletNodeInputs, monitorConnections) => {\n    return (nativeContext, baseLatency, processorConstructor, options) => {\n        if (options.numberOfInputs === 0 && options.numberOfOutputs === 0) {\n            throw createNotSupportedError();\n        }\n        const outputChannelCount = Array.isArray(options.outputChannelCount)\n            ? options.outputChannelCount\n            : Array.from(options.outputChannelCount);\n        // @todo Check if any of the channelCount values is greater than the implementation\'s maximum number of channels.\n        if (outputChannelCount.some((channelCount) => channelCount < 1)) {\n            throw createNotSupportedError();\n        }\n        if (outputChannelCount.length !== options.numberOfOutputs) {\n            throw createIndexSizeError();\n        }\n        // Bug #61: This is not part of the standard but required for the faker to work.\n        if (options.channelCountMode !== \'explicit\') {\n            throw createNotSupportedError();\n        }\n        const numberOfInputChannels = options.channelCount * options.numberOfInputs;\n        const numberOfOutputChannels = outputChannelCount.reduce((sum, value) => sum + value, 0);\n        const numberOfParameters = processorConstructor.parameterDescriptors === undefined ? 0 : processorConstructor.parameterDescriptors.length;\n        // Bug #61: This is not part of the standard but required for the faker to work.\n        if (numberOfInputChannels + numberOfParameters > 6 || numberOfOutputChannels > 6) {\n            throw createNotSupportedError();\n        }\n        const messageChannel = new MessageChannel();\n        const gainNodes = [];\n        const inputChannelSplitterNodes = [];\n        for (let i = 0; i < options.numberOfInputs; i += 1) {\n            gainNodes.push(createNativeGainNode(nativeContext, {\n                channelCount: options.channelCount,\n                channelCountMode: options.channelCountMode,\n                channelInterpretation: options.channelInterpretation,\n                gain: 1\n            }));\n            inputChannelSplitterNodes.push(createNativeChannelSplitterNode(nativeContext, {\n                channelCount: options.channelCount,\n                channelCountMode: \'explicit\',\n                channelInterpretation: \'discrete\',\n                numberOfOutputs: options.channelCount\n            }));\n        }\n        const constantSourceNodes = [];\n        if (processorConstructor.parameterDescriptors !== undefined) {\n            for (const { defaultValue, maxValue, minValue, name } of processorConstructor.parameterDescriptors) {\n                const constantSourceNode = createNativeConstantSourceNode(nativeContext, {\n                    channelCount: 1,\n                    channelCountMode: \'explicit\',\n                    channelInterpretation: \'discrete\',\n                    offset: options.parameterData[name] !== undefined\n                        ? options.parameterData[name]\n                        : defaultValue === undefined\n                            ? 0\n                            : defaultValue\n                });\n                Object.defineProperties(constantSourceNode.offset, {\n                    defaultValue: {\n                        get: () => (defaultValue === undefined ? 0 : defaultValue)\n                    },\n                    maxValue: {\n                        get: () => (maxValue === undefined ? MOST_POSITIVE_SINGLE_FLOAT : maxValue)\n                    },\n                    minValue: {\n                        get: () => (minValue === undefined ? MOST_NEGATIVE_SINGLE_FLOAT : minValue)\n                    }\n                });\n                constantSourceNodes.push(constantSourceNode);\n            }\n        }\n        const inputChannelMergerNode = createNativeChannelMergerNode(nativeContext, {\n            channelCount: 1,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'speakers\',\n            numberOfInputs: Math.max(1, numberOfInputChannels + numberOfParameters)\n        });\n        const bufferSize = computeBufferSize(baseLatency, nativeContext.sampleRate);\n        const scriptProcessorNode = createNativeScriptProcessorNode(nativeContext, bufferSize, numberOfInputChannels + numberOfParameters, \n        // Bug #87: Only Firefox will fire an AudioProcessingEvent if there is no connected output.\n        Math.max(1, numberOfOutputChannels));\n        const outputChannelSplitterNode = createNativeChannelSplitterNode(nativeContext, {\n            channelCount: Math.max(1, numberOfOutputChannels),\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\',\n            numberOfOutputs: Math.max(1, numberOfOutputChannels)\n        });\n        const outputChannelMergerNodes = [];\n        for (let i = 0; i < options.numberOfOutputs; i += 1) {\n            outputChannelMergerNodes.push(createNativeChannelMergerNode(nativeContext, {\n                channelCount: 1,\n                channelCountMode: \'explicit\',\n                channelInterpretation: \'speakers\',\n                numberOfInputs: outputChannelCount[i]\n            }));\n        }\n        for (let i = 0; i < options.numberOfInputs; i += 1) {\n            gainNodes[i].connect(inputChannelSplitterNodes[i]);\n            for (let j = 0; j < options.channelCount; j += 1) {\n                inputChannelSplitterNodes[i].connect(inputChannelMergerNode, j, i * options.channelCount + j);\n            }\n        }\n        const parameterMap = new ReadOnlyMap(processorConstructor.parameterDescriptors === undefined\n            ? []\n            : processorConstructor.parameterDescriptors.map(({ name }, index) => {\n                const constantSourceNode = constantSourceNodes[index];\n                constantSourceNode.connect(inputChannelMergerNode, 0, numberOfInputChannels + index);\n                constantSourceNode.start(0);\n                return [name, constantSourceNode.offset];\n            }));\n        inputChannelMergerNode.connect(scriptProcessorNode);\n        let channelInterpretation = options.channelInterpretation;\n        let onprocessorerror = null;\n        // Bug #87: Expose at least one output to make this node connectable.\n        const outputAudioNodes = options.numberOfOutputs === 0 ? [scriptProcessorNode] : outputChannelMergerNodes;\n        const nativeAudioWorkletNodeFaker = {\n            get bufferSize() {\n                return bufferSize;\n            },\n            get channelCount() {\n                return options.channelCount;\n            },\n            set channelCount(_) {\n                // Bug #61: This is not part of the standard but required for the faker to work.\n                throw createInvalidStateError();\n            },\n            get channelCountMode() {\n                return options.channelCountMode;\n            },\n            set channelCountMode(_) {\n                // Bug #61: This is not part of the standard but required for the faker to work.\n                throw createInvalidStateError();\n            },\n            get channelInterpretation() {\n                return channelInterpretation;\n            },\n            set channelInterpretation(value) {\n                for (const gainNode of gainNodes) {\n                    gainNode.channelInterpretation = value;\n                }\n                channelInterpretation = value;\n            },\n            get context() {\n                return scriptProcessorNode.context;\n            },\n            get inputs() {\n                return gainNodes;\n            },\n            get numberOfInputs() {\n                return options.numberOfInputs;\n            },\n            get numberOfOutputs() {\n                return options.numberOfOutputs;\n            },\n            get onprocessorerror() {\n                return onprocessorerror;\n            },\n            set onprocessorerror(value) {\n                if (typeof onprocessorerror === \'function\') {\n                    nativeAudioWorkletNodeFaker.removeEventListener(\'processorerror\', onprocessorerror);\n                }\n                onprocessorerror = typeof value === \'function\' ? value : null;\n                if (typeof onprocessorerror === \'function\') {\n                    nativeAudioWorkletNodeFaker.addEventListener(\'processorerror\', onprocessorerror);\n                }\n            },\n            get parameters() {\n                return parameterMap;\n            },\n            get port() {\n                return messageChannel.port2;\n            },\n            addEventListener(...args) {\n                return scriptProcessorNode.addEventListener(args[0], args[1], args[2]);\n            },\n            connect: connectMultipleOutputs.bind(null, outputAudioNodes),\n            disconnect: disconnectMultipleOutputs.bind(null, outputAudioNodes),\n            dispatchEvent(...args) {\n                return scriptProcessorNode.dispatchEvent(args[0]);\n            },\n            removeEventListener(...args) {\n                return scriptProcessorNode.removeEventListener(args[0], args[1], args[2]);\n            }\n        };\n        const patchedEventListeners = new Map();\n        messageChannel.port1.addEventListener = ((addEventListener) => {\n            return (...args) => {\n                if (args[0] === \'message\') {\n                    const unpatchedEventListener = typeof args[1] === \'function\'\n                        ? args[1]\n                        : typeof args[1] === \'object\' && args[1] !== null && typeof args[1].handleEvent === \'function\'\n                            ? args[1].handleEvent\n                            : null;\n                    if (unpatchedEventListener !== null) {\n                        const patchedEventListener = patchedEventListeners.get(args[1]);\n                        if (patchedEventListener !== undefined) {\n                            args[1] = patchedEventListener;\n                        }\n                        else {\n                            args[1] = (event) => {\n                                exposeCurrentFrameAndCurrentTime(nativeContext.currentTime, nativeContext.sampleRate, () => unpatchedEventListener(event));\n                            };\n                            patchedEventListeners.set(unpatchedEventListener, args[1]);\n                        }\n                    }\n                }\n                return addEventListener.call(messageChannel.port1, args[0], args[1], args[2]);\n            };\n        })(messageChannel.port1.addEventListener);\n        messageChannel.port1.removeEventListener = ((removeEventListener) => {\n            return (...args) => {\n                if (args[0] === \'message\') {\n                    const patchedEventListener = patchedEventListeners.get(args[1]);\n                    if (patchedEventListener !== undefined) {\n                        patchedEventListeners.delete(args[1]);\n                        args[1] = patchedEventListener;\n                    }\n                }\n                return removeEventListener.call(messageChannel.port1, args[0], args[1], args[2]);\n            };\n        })(messageChannel.port1.removeEventListener);\n        let onmessage = null;\n        Object.defineProperty(messageChannel.port1, \'onmessage\', {\n            get: () => onmessage,\n            set: (value) => {\n                if (typeof onmessage === \'function\') {\n                    messageChannel.port1.removeEventListener(\'message\', onmessage);\n                }\n                onmessage = typeof value === \'function\' ? value : null;\n                if (typeof onmessage === \'function\') {\n                    messageChannel.port1.addEventListener(\'message\', onmessage);\n                    messageChannel.port1.start();\n                }\n            }\n        });\n        processorConstructor.prototype.port = messageChannel.port1;\n        let audioWorkletProcessor = null;\n        const audioWorkletProcessorPromise = createAudioWorkletProcessor(nativeContext, nativeAudioWorkletNodeFaker, processorConstructor, options);\n        audioWorkletProcessorPromise.then((dWrkltPrcssr) => (audioWorkletProcessor = dWrkltPrcssr));\n        const inputs = createNestedArrays(options.numberOfInputs, options.channelCount);\n        const outputs = createNestedArrays(options.numberOfOutputs, outputChannelCount);\n        const parameters = processorConstructor.parameterDescriptors === undefined\n            ? []\n            : processorConstructor.parameterDescriptors.reduce((prmtrs, { name }) => ({ ...prmtrs, [name]: new Float32Array(128) }), {});\n        let isActive = true;\n        const disconnectOutputsGraph = () => {\n            if (options.numberOfOutputs > 0) {\n                scriptProcessorNode.disconnect(outputChannelSplitterNode);\n            }\n            for (let i = 0, outputChannelSplitterNodeOutput = 0; i < options.numberOfOutputs; i += 1) {\n                const outputChannelMergerNode = outputChannelMergerNodes[i];\n                for (let j = 0; j < outputChannelCount[i]; j += 1) {\n                    outputChannelSplitterNode.disconnect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);\n                }\n                outputChannelSplitterNodeOutput += outputChannelCount[i];\n            }\n        };\n        const activeInputIndexes = new Map();\n        // tslint:disable-next-line:deprecation\n        scriptProcessorNode.onaudioprocess = ({ inputBuffer, outputBuffer }) => {\n            if (audioWorkletProcessor !== null) {\n                const activeInputs = getActiveAudioWorkletNodeInputs(nativeAudioWorkletNodeFaker);\n                for (let i = 0; i < bufferSize; i += 128) {\n                    for (let j = 0; j < options.numberOfInputs; j += 1) {\n                        for (let k = 0; k < options.channelCount; k += 1) {\n                            copyFromChannel(inputBuffer, inputs[j], k, k, i);\n                        }\n                    }\n                    if (processorConstructor.parameterDescriptors !== undefined) {\n                        processorConstructor.parameterDescriptors.forEach(({ name }, index) => {\n                            copyFromChannel(inputBuffer, parameters, name, numberOfInputChannels + index, i);\n                        });\n                    }\n                    for (let j = 0; j < options.numberOfInputs; j += 1) {\n                        for (let k = 0; k < outputChannelCount[j]; k += 1) {\n                            // The byteLength will be 0 when the ArrayBuffer was transferred.\n                            if (outputs[j][k].byteLength === 0) {\n                                outputs[j][k] = new Float32Array(128);\n                            }\n                        }\n                    }\n                    try {\n                        const potentiallyEmptyInputs = inputs.map((input, index) => {\n                            const activeInput = activeInputs[index];\n                            if (activeInput.size > 0) {\n                                activeInputIndexes.set(index, bufferSize / 128);\n                                return input;\n                            }\n                            const count = activeInputIndexes.get(index);\n                            if (count === undefined) {\n                                return [];\n                            }\n                            if (input.every((channelData) => channelData.every((sample) => sample === 0))) {\n                                if (count === 1) {\n                                    activeInputIndexes.delete(index);\n                                }\n                                else {\n                                    activeInputIndexes.set(index, count - 1);\n                                }\n                            }\n                            return input;\n                        });\n                        const activeSourceFlag = exposeCurrentFrameAndCurrentTime(nativeContext.currentTime + i / nativeContext.sampleRate, nativeContext.sampleRate, () => audioWorkletProcessor.process(potentiallyEmptyInputs, outputs, parameters));\n                        isActive = activeSourceFlag;\n                        for (let j = 0, outputChannelSplitterNodeOutput = 0; j < options.numberOfOutputs; j += 1) {\n                            for (let k = 0; k < outputChannelCount[j]; k += 1) {\n                                copyToChannel(outputBuffer, outputs[j], k, outputChannelSplitterNodeOutput + k, i);\n                            }\n                            outputChannelSplitterNodeOutput += outputChannelCount[j];\n                        }\n                    }\n                    catch (error) {\n                        isActive = false;\n                        nativeAudioWorkletNodeFaker.dispatchEvent(new ErrorEvent(\'processorerror\', {\n                            colno: error.colno,\n                            filename: error.filename,\n                            lineno: error.lineno,\n                            message: error.message\n                        }));\n                    }\n                    if (!isActive) {\n                        for (let j = 0; j < options.numberOfInputs; j += 1) {\n                            gainNodes[j].disconnect(inputChannelSplitterNodes[j]);\n                            for (let k = 0; k < options.channelCount; k += 1) {\n                                inputChannelSplitterNodes[i].disconnect(inputChannelMergerNode, k, j * options.channelCount + k);\n                            }\n                        }\n                        if (processorConstructor.parameterDescriptors !== undefined) {\n                            const length = processorConstructor.parameterDescriptors.length;\n                            for (let j = 0; j < length; j += 1) {\n                                const constantSourceNode = constantSourceNodes[j];\n                                constantSourceNode.disconnect(inputChannelMergerNode, 0, numberOfInputChannels + j);\n                                constantSourceNode.stop();\n                            }\n                        }\n                        inputChannelMergerNode.disconnect(scriptProcessorNode);\n                        scriptProcessorNode.onaudioprocess = null; // tslint:disable-line:deprecation\n                        if (isConnected) {\n                            disconnectOutputsGraph();\n                        }\n                        else {\n                            disconnectFakeGraph();\n                        }\n                        break;\n                    }\n                }\n            }\n        };\n        let isConnected = false;\n        // Bug #87: Only Firefox will fire an AudioProcessingEvent if there is no connected output.\n        const nativeGainNode = createNativeGainNode(nativeContext, {\n            channelCount: 1,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\',\n            gain: 0\n        });\n        const connectFakeGraph = () => scriptProcessorNode.connect(nativeGainNode).connect(nativeContext.destination);\n        const disconnectFakeGraph = () => {\n            scriptProcessorNode.disconnect(nativeGainNode);\n            nativeGainNode.disconnect();\n        };\n        const whenConnected = () => {\n            if (isActive) {\n                disconnectFakeGraph();\n                if (options.numberOfOutputs > 0) {\n                    scriptProcessorNode.connect(outputChannelSplitterNode);\n                }\n                for (let i = 0, outputChannelSplitterNodeOutput = 0; i < options.numberOfOutputs; i += 1) {\n                    const outputChannelMergerNode = outputChannelMergerNodes[i];\n                    for (let j = 0; j < outputChannelCount[i]; j += 1) {\n                        outputChannelSplitterNode.connect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);\n                    }\n                    outputChannelSplitterNodeOutput += outputChannelCount[i];\n                }\n            }\n            isConnected = true;\n        };\n        const whenDisconnected = () => {\n            if (isActive) {\n                connectFakeGraph();\n                disconnectOutputsGraph();\n            }\n            isConnected = false;\n        };\n        connectFakeGraph();\n        return monitorConnections(nativeAudioWorkletNodeFaker, whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=native-audio-worklet-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-biquad-filter-node.js\n\n\n\nconst native_biquad_filter_node_createNativeBiquadFilterNode = (nativeContext, options) => {\n    const nativeBiquadFilterNode = nativeContext.createBiquadFilter();\n    assignNativeAudioNodeOptions(nativeBiquadFilterNode, options);\n    assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, \'Q\');\n    assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, \'detune\');\n    assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, \'frequency\');\n    assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, \'gain\');\n    assignNativeAudioNodeOption(nativeBiquadFilterNode, options, \'type\');\n    return nativeBiquadFilterNode;\n};\n//# sourceMappingURL=native-biquad-filter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-channel-merger-node-factory.js\n\nconst createNativeChannelMergerNodeFactory = (nativeAudioContextConstructor, wrapChannelMergerNode) => {\n    return (nativeContext, options) => {\n        const nativeChannelMergerNode = nativeContext.createChannelMerger(options.numberOfInputs);\n        /*\n         * Bug #20: Safari requires a connection of any kind to treat the input signal correctly.\n         * @todo Unfortunately there is no way to test for this behavior in a synchronous fashion which is why testing for the existence of\n         * the webkitAudioContext is used as a workaround here.\n         */\n        if (nativeAudioContextConstructor !== null && nativeAudioContextConstructor.name === \'webkitAudioContext\') {\n            wrapChannelMergerNode(nativeContext, nativeChannelMergerNode);\n        }\n        assignNativeAudioNodeOptions(nativeChannelMergerNode, options);\n        return nativeChannelMergerNode;\n    };\n};\n//# sourceMappingURL=native-channel-merger-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-channel-splitter-node.js\n\nconst wrapChannelSplitterNode = (channelSplitterNode) => {\n    const channelCount = channelSplitterNode.numberOfOutputs;\n    // Bug #97: Safari does not throw an error when attempting to change the channelCount to something other than its initial value.\n    Object.defineProperty(channelSplitterNode, \'channelCount\', {\n        get: () => channelCount,\n        set: (value) => {\n            if (value !== channelCount) {\n                throw invalid_state_error_createInvalidStateError();\n            }\n        }\n    });\n    // Bug #30: Safari does not throw an error when attempting to change the channelCountMode to something other than explicit.\n    Object.defineProperty(channelSplitterNode, \'channelCountMode\', {\n        get: () => \'explicit\',\n        set: (value) => {\n            if (value !== \'explicit\') {\n                throw invalid_state_error_createInvalidStateError();\n            }\n        }\n    });\n    // Bug #32: Safari does not throw an error when attempting to change the channelInterpretation to something other than discrete.\n    Object.defineProperty(channelSplitterNode, \'channelInterpretation\', {\n        get: () => \'discrete\',\n        set: (value) => {\n            if (value !== \'discrete\') {\n                throw invalid_state_error_createInvalidStateError();\n            }\n        }\n    });\n};\n//# sourceMappingURL=wrap-channel-splitter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-channel-splitter-node.js\n\n\nconst native_channel_splitter_node_createNativeChannelSplitterNode = (nativeContext, options) => {\n    const nativeChannelSplitterNode = nativeContext.createChannelSplitter(options.numberOfOutputs);\n    // Bug #96: Safari does not have the correct channelCount.\n    // Bug #29: Safari does not have the correct channelCountMode.\n    // Bug #31: Safari does not have the correct channelInterpretation.\n    assignNativeAudioNodeOptions(nativeChannelSplitterNode, options);\n    // Bug #29, #30, #31, #32, #96 & #97: Only Chrome, Edge, Firefox & Opera partially support the spec yet.\n    wrapChannelSplitterNode(nativeChannelSplitterNode);\n    return nativeChannelSplitterNode;\n};\n//# sourceMappingURL=native-channel-splitter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-constant-source-node-factory.js\n\n\n\n\nconst createNativeConstantSourceNodeFactory = (addSilentConnection, cacheTestResult, createNativeConstantSourceNodeFaker, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport) => {\n    return (nativeContext, options) => {\n        // Bug #62: Safari does not support ConstantSourceNodes.\n        if (nativeContext.createConstantSource === undefined) {\n            return createNativeConstantSourceNodeFaker(nativeContext, options);\n        }\n        const nativeConstantSourceNode = nativeContext.createConstantSource();\n        assignNativeAudioNodeOptions(nativeConstantSourceNode, options);\n        assignNativeAudioNodeAudioParamValue(nativeConstantSourceNode, options, \'offset\');\n        // Bug #44: Safari does not throw a RangeError yet.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, () => testAudioScheduledSourceNodeStartMethodNegativeParametersSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStartMethodNegativeParameters(nativeConstantSourceNode);\n        }\n        // Bug #44: Only Firefox does not throw a RangeError yet.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, () => testAudioScheduledSourceNodeStopMethodNegativeParametersSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStopMethodNegativeParameters(nativeConstantSourceNode);\n        }\n        // Bug #175: Safari will not fire an ended event if the ConstantSourceNode is unconnected.\n        addSilentConnection(nativeContext, nativeConstantSourceNode);\n        return nativeConstantSourceNode;\n    };\n};\n//# sourceMappingURL=native-constant-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/intercept-connections.js\nconst interceptConnections = (original, interceptor) => {\n    original.connect = interceptor.connect.bind(interceptor);\n    original.disconnect = interceptor.disconnect.bind(interceptor);\n    return original;\n};\n//# sourceMappingURL=intercept-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-constant-source-node-faker-factory.js\n\nconst createNativeConstantSourceNodeFakerFactory = (addSilentConnection, createNativeAudioBufferSourceNode, createNativeGainNode, monitorConnections) => {\n    return (nativeContext, { offset, ...audioNodeOptions }) => {\n        const audioBuffer = nativeContext.createBuffer(1, 2, 44100);\n        const audioBufferSourceNode = createNativeAudioBufferSourceNode(nativeContext, {\n            buffer: null,\n            channelCount: 2,\n            channelCountMode: \'max\',\n            channelInterpretation: \'speakers\',\n            loop: false,\n            loopEnd: 0,\n            loopStart: 0,\n            playbackRate: 1\n        });\n        const gainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, gain: offset });\n        // Bug #5: Safari does not support copyFromChannel() and copyToChannel().\n        const channelData = audioBuffer.getChannelData(0);\n        // Bug #95: Safari does not play or loop one sample buffers.\n        channelData[0] = 1;\n        channelData[1] = 1;\n        audioBufferSourceNode.buffer = audioBuffer;\n        audioBufferSourceNode.loop = true;\n        const nativeConstantSourceNodeFaker = {\n            get bufferSize() {\n                return undefined;\n            },\n            get channelCount() {\n                return gainNode.channelCount;\n            },\n            set channelCount(value) {\n                gainNode.channelCount = value;\n            },\n            get channelCountMode() {\n                return gainNode.channelCountMode;\n            },\n            set channelCountMode(value) {\n                gainNode.channelCountMode = value;\n            },\n            get channelInterpretation() {\n                return gainNode.channelInterpretation;\n            },\n            set channelInterpretation(value) {\n                gainNode.channelInterpretation = value;\n            },\n            get context() {\n                return gainNode.context;\n            },\n            get inputs() {\n                return [];\n            },\n            get numberOfInputs() {\n                return audioBufferSourceNode.numberOfInputs;\n            },\n            get numberOfOutputs() {\n                return gainNode.numberOfOutputs;\n            },\n            get offset() {\n                return gainNode.gain;\n            },\n            get onended() {\n                return audioBufferSourceNode.onended;\n            },\n            set onended(value) {\n                audioBufferSourceNode.onended = value;\n            },\n            addEventListener(...args) {\n                return audioBufferSourceNode.addEventListener(args[0], args[1], args[2]);\n            },\n            dispatchEvent(...args) {\n                return audioBufferSourceNode.dispatchEvent(args[0]);\n            },\n            removeEventListener(...args) {\n                return audioBufferSourceNode.removeEventListener(args[0], args[1], args[2]);\n            },\n            start(when = 0) {\n                audioBufferSourceNode.start.call(audioBufferSourceNode, when);\n            },\n            stop(when = 0) {\n                audioBufferSourceNode.stop.call(audioBufferSourceNode, when);\n            }\n        };\n        const whenConnected = () => audioBufferSourceNode.connect(gainNode);\n        const whenDisconnected = () => audioBufferSourceNode.disconnect(gainNode);\n        // Bug #175: Safari will not fire an ended event if the AudioBufferSourceNode is unconnected.\n        addSilentConnection(nativeContext, audioBufferSourceNode);\n        return monitorConnections(interceptConnections(nativeConstantSourceNodeFaker, gainNode), whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=native-constant-source-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-convolver-node-factory.js\n\n\nconst createNativeConvolverNodeFactory = (createNotSupportedError, overwriteAccessors) => {\n    return (nativeContext, options) => {\n        const nativeConvolverNode = nativeContext.createConvolver();\n        assignNativeAudioNodeOptions(nativeConvolverNode, options);\n        // The normalize property needs to be set before setting the buffer.\n        if (options.disableNormalization === nativeConvolverNode.normalize) {\n            nativeConvolverNode.normalize = !options.disableNormalization;\n        }\n        assignNativeAudioNodeOption(nativeConvolverNode, options, \'buffer\');\n        // Bug #113: Safari does allow to set the channelCount to a value larger than 2.\n        if (options.channelCount > 2) {\n            throw createNotSupportedError();\n        }\n        overwriteAccessors(nativeConvolverNode, \'channelCount\', (get) => () => get.call(nativeConvolverNode), (set) => (value) => {\n            if (value > 2) {\n                throw createNotSupportedError();\n            }\n            return set.call(nativeConvolverNode, value);\n        });\n        // Bug #114: Safari allows to set the channelCountMode to \'max\'.\n        if (options.channelCountMode === \'max\') {\n            throw createNotSupportedError();\n        }\n        overwriteAccessors(nativeConvolverNode, \'channelCountMode\', (get) => () => get.call(nativeConvolverNode), (set) => (value) => {\n            if (value === \'max\') {\n                throw createNotSupportedError();\n            }\n            return set.call(nativeConvolverNode, value);\n        });\n        return nativeConvolverNode;\n    };\n};\n//# sourceMappingURL=native-convolver-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-delay-node.js\n\n\nconst native_delay_node_createNativeDelayNode = (nativeContext, options) => {\n    const nativeDelayNode = nativeContext.createDelay(options.maxDelayTime);\n    assignNativeAudioNodeOptions(nativeDelayNode, options);\n    assignNativeAudioNodeAudioParamValue(nativeDelayNode, options, \'delayTime\');\n    return nativeDelayNode;\n};\n//# sourceMappingURL=native-delay-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-dynamics-compressor-node-factory.js\n\n\nconst createNativeDynamicsCompressorNodeFactory = (createNotSupportedError) => {\n    return (nativeContext, options) => {\n        const nativeDynamicsCompressorNode = nativeContext.createDynamicsCompressor();\n        assignNativeAudioNodeOptions(nativeDynamicsCompressorNode, options);\n        // Bug #108: Safari allows a channelCount of three and above.\n        if (options.channelCount > 2) {\n            throw createNotSupportedError();\n        }\n        // Bug #109: Only Chrome, Firefox and Opera disallow a channelCountMode of \'max\'.\n        if (options.channelCountMode === \'max\') {\n            throw createNotSupportedError();\n        }\n        assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, \'attack\');\n        assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, \'knee\');\n        assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, \'ratio\');\n        assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, \'release\');\n        assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, \'threshold\');\n        return nativeDynamicsCompressorNode;\n    };\n};\n//# sourceMappingURL=native-dynamics-compressor-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-gain-node.js\n\n\nconst native_gain_node_createNativeGainNode = (nativeContext, options) => {\n    const nativeGainNode = nativeContext.createGain();\n    assignNativeAudioNodeOptions(nativeGainNode, options);\n    assignNativeAudioNodeAudioParamValue(nativeGainNode, options, \'gain\');\n    return nativeGainNode;\n};\n//# sourceMappingURL=native-gain-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-iir-filter-node-factory.js\n\nconst createNativeIIRFilterNodeFactory = (createNativeIIRFilterNodeFaker) => {\n    return (nativeContext, baseLatency, options) => {\n        // Bug #9: Safari does not support IIRFilterNodes.\n        if (nativeContext.createIIRFilter === undefined) {\n            return createNativeIIRFilterNodeFaker(nativeContext, baseLatency, options);\n        }\n        // @todo TypeScript defines the parameters of createIIRFilter() as arrays of numbers.\n        const nativeIIRFilterNode = nativeContext.createIIRFilter(options.feedforward, options.feedback);\n        assignNativeAudioNodeOptions(nativeIIRFilterNode, options);\n        return nativeIIRFilterNode;\n    };\n};\n//# sourceMappingURL=native-iir-filter-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-iir-filter-node-faker-factory.js\n\n\n\nfunction divide(a, b) {\n    const denominator = b[0] * b[0] + b[1] * b[1];\n    return [(a[0] * b[0] + a[1] * b[1]) / denominator, (a[1] * b[0] - a[0] * b[1]) / denominator];\n}\nfunction multiply(a, b) {\n    return [a[0] * b[0] - a[1] * b[1], a[0] * b[1] + a[1] * b[0]];\n}\nfunction evaluatePolynomial(coefficient, z) {\n    let result = [0, 0];\n    for (let i = coefficient.length - 1; i >= 0; i -= 1) {\n        result = multiply(result, z);\n        result[0] += coefficient[i];\n    }\n    return result;\n}\nconst createNativeIIRFilterNodeFakerFactory = (createInvalidAccessError, createInvalidStateError, createNativeScriptProcessorNode, createNotSupportedError) => {\n    return (nativeContext, baseLatency, { channelCount, channelCountMode, channelInterpretation, feedback, feedforward }) => {\n        const bufferSize = computeBufferSize(baseLatency, nativeContext.sampleRate);\n        const convertedFeedback = feedback instanceof Float64Array ? feedback : new Float64Array(feedback);\n        const convertedFeedforward = feedforward instanceof Float64Array ? feedforward : new Float64Array(feedforward);\n        const feedbackLength = convertedFeedback.length;\n        const feedforwardLength = convertedFeedforward.length;\n        const minLength = Math.min(feedbackLength, feedforwardLength);\n        if (feedbackLength === 0 || feedbackLength > 20) {\n            throw createNotSupportedError();\n        }\n        if (convertedFeedback[0] === 0) {\n            throw createInvalidStateError();\n        }\n        if (feedforwardLength === 0 || feedforwardLength > 20) {\n            throw createNotSupportedError();\n        }\n        if (convertedFeedforward[0] === 0) {\n            throw createInvalidStateError();\n        }\n        if (convertedFeedback[0] !== 1) {\n            for (let i = 0; i < feedforwardLength; i += 1) {\n                convertedFeedforward[i] /= convertedFeedback[0];\n            }\n            for (let i = 1; i < feedbackLength; i += 1) {\n                convertedFeedback[i] /= convertedFeedback[0];\n            }\n        }\n        const scriptProcessorNode = createNativeScriptProcessorNode(nativeContext, bufferSize, channelCount, channelCount);\n        scriptProcessorNode.channelCount = channelCount;\n        scriptProcessorNode.channelCountMode = channelCountMode;\n        scriptProcessorNode.channelInterpretation = channelInterpretation;\n        const bufferLength = 32;\n        const bufferIndexes = [];\n        const xBuffers = [];\n        const yBuffers = [];\n        for (let i = 0; i < channelCount; i += 1) {\n            bufferIndexes.push(0);\n            const xBuffer = new Float32Array(bufferLength);\n            const yBuffer = new Float32Array(bufferLength);\n            xBuffer.fill(0);\n            yBuffer.fill(0);\n            xBuffers.push(xBuffer);\n            yBuffers.push(yBuffer);\n        }\n        // tslint:disable-next-line:deprecation\n        scriptProcessorNode.onaudioprocess = (event) => {\n            const inputBuffer = event.inputBuffer;\n            const outputBuffer = event.outputBuffer;\n            const numberOfChannels = inputBuffer.numberOfChannels;\n            for (let i = 0; i < numberOfChannels; i += 1) {\n                const input = inputBuffer.getChannelData(i);\n                const output = outputBuffer.getChannelData(i);\n                bufferIndexes[i] = filterBuffer(convertedFeedback, feedbackLength, convertedFeedforward, feedforwardLength, minLength, xBuffers[i], yBuffers[i], bufferIndexes[i], bufferLength, input, output);\n            }\n        };\n        const nyquist = nativeContext.sampleRate / 2;\n        const nativeIIRFilterNodeFaker = {\n            get bufferSize() {\n                return bufferSize;\n            },\n            get channelCount() {\n                return scriptProcessorNode.channelCount;\n            },\n            set channelCount(value) {\n                scriptProcessorNode.channelCount = value;\n            },\n            get channelCountMode() {\n                return scriptProcessorNode.channelCountMode;\n            },\n            set channelCountMode(value) {\n                scriptProcessorNode.channelCountMode = value;\n            },\n            get channelInterpretation() {\n                return scriptProcessorNode.channelInterpretation;\n            },\n            set channelInterpretation(value) {\n                scriptProcessorNode.channelInterpretation = value;\n            },\n            get context() {\n                return scriptProcessorNode.context;\n            },\n            get inputs() {\n                return [scriptProcessorNode];\n            },\n            get numberOfInputs() {\n                return scriptProcessorNode.numberOfInputs;\n            },\n            get numberOfOutputs() {\n                return scriptProcessorNode.numberOfOutputs;\n            },\n            addEventListener(...args) {\n                // @todo Dissallow adding an audioprocess listener.\n                return scriptProcessorNode.addEventListener(args[0], args[1], args[2]);\n            },\n            dispatchEvent(...args) {\n                return scriptProcessorNode.dispatchEvent(args[0]);\n            },\n            getFrequencyResponse(frequencyHz, magResponse, phaseResponse) {\n                if (frequencyHz.length !== magResponse.length || magResponse.length !== phaseResponse.length) {\n                    throw createInvalidAccessError();\n                }\n                const length = frequencyHz.length;\n                for (let i = 0; i < length; i += 1) {\n                    const omega = -Math.PI * (frequencyHz[i] / nyquist);\n                    const z = [Math.cos(omega), Math.sin(omega)];\n                    const numerator = evaluatePolynomial(convertedFeedforward, z);\n                    const denominator = evaluatePolynomial(convertedFeedback, z);\n                    const response = divide(numerator, denominator);\n                    magResponse[i] = Math.sqrt(response[0] * response[0] + response[1] * response[1]);\n                    phaseResponse[i] = Math.atan2(response[1], response[0]);\n                }\n            },\n            removeEventListener(...args) {\n                return scriptProcessorNode.removeEventListener(args[0], args[1], args[2]);\n            }\n        };\n        return interceptConnections(nativeIIRFilterNodeFaker, scriptProcessorNode);\n    };\n};\n//# sourceMappingURL=native-iir-filter-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-media-element-audio-source-node.js\nconst createNativeMediaElementAudioSourceNode = (nativeAudioContext, options) => {\n    return nativeAudioContext.createMediaElementSource(options.mediaElement);\n};\n//# sourceMappingURL=native-media-element-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-media-stream-audio-destination-node.js\n\nconst createNativeMediaStreamAudioDestinationNode = (nativeAudioContext, options) => {\n    const nativeMediaStreamAudioDestinationNode = nativeAudioContext.createMediaStreamDestination();\n    assignNativeAudioNodeOptions(nativeMediaStreamAudioDestinationNode, options);\n    // Bug #174: Safari does expose a wrong numberOfOutputs.\n    if (nativeMediaStreamAudioDestinationNode.numberOfOutputs === 1) {\n        Object.defineProperty(nativeMediaStreamAudioDestinationNode, \'numberOfOutputs\', { get: () => 0 });\n    }\n    return nativeMediaStreamAudioDestinationNode;\n};\n//# sourceMappingURL=native-media-stream-audio-destination-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-media-stream-audio-source-node.js\nconst createNativeMediaStreamAudioSourceNode = (nativeAudioContext, { mediaStream }) => {\n    const audioStreamTracks = mediaStream.getAudioTracks();\n    /*\n     * Bug #151: Safari does not use the audio track as input anymore if it gets removed from the mediaStream after construction.\n     * Bug #159: Safari picks the first audio track if the MediaStream has more than one audio track.\n     */\n    audioStreamTracks.sort((a, b) => (a.id < b.id ? -1 : a.id > b.id ? 1 : 0));\n    const filteredAudioStreamTracks = audioStreamTracks.slice(0, 1);\n    const nativeMediaStreamAudioSourceNode = nativeAudioContext.createMediaStreamSource(new MediaStream(filteredAudioStreamTracks));\n    /*\n     * Bug #151 & #159: The given mediaStream gets reconstructed before it gets passed to the native node which is why the accessor needs\n     * to be overwritten as it would otherwise expose the reconstructed version.\n     */\n    Object.defineProperty(nativeMediaStreamAudioSourceNode, \'mediaStream\', { value: mediaStream });\n    return nativeMediaStreamAudioSourceNode;\n};\n//# sourceMappingURL=native-media-stream-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-media-stream-track-audio-source-node-factory.js\nconst createNativeMediaStreamTrackAudioSourceNodeFactory = (createInvalidStateError, isNativeOfflineAudioContext) => {\n    return (nativeAudioContext, { mediaStreamTrack }) => {\n        // Bug #121: Only Firefox does yet support the MediaStreamTrackAudioSourceNode.\n        if (typeof nativeAudioContext.createMediaStreamTrackSource === \'function\') {\n            return nativeAudioContext.createMediaStreamTrackSource(mediaStreamTrack);\n        }\n        const mediaStream = new MediaStream([mediaStreamTrack]);\n        const nativeMediaStreamAudioSourceNode = nativeAudioContext.createMediaStreamSource(mediaStream);\n        // Bug #120: Firefox does not throw an error if the mediaStream has no audio track.\n        if (mediaStreamTrack.kind !== \'audio\') {\n            throw createInvalidStateError();\n        }\n        // Bug #172: Safari allows to create a MediaStreamAudioSourceNode with an OfflineAudioContext.\n        if (isNativeOfflineAudioContext(nativeAudioContext)) {\n            throw new TypeError();\n        }\n        return nativeMediaStreamAudioSourceNode;\n    };\n};\n//# sourceMappingURL=native-media-stream-track-audio-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-offline-audio-context-constructor.js\nconst createNativeOfflineAudioContextConstructor = (window) => {\n    if (window === null) {\n        return null;\n    }\n    if (window.hasOwnProperty(\'OfflineAudioContext\')) {\n        return window.OfflineAudioContext;\n    }\n    return window.hasOwnProperty(\'webkitOfflineAudioContext\') ? window.webkitOfflineAudioContext : null;\n};\n//# sourceMappingURL=native-offline-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-oscillator-node-factory.js\n\n\n\n\n\nconst createNativeOscillatorNodeFactory = (addSilentConnection, cacheTestResult, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls) => {\n    return (nativeContext, options) => {\n        const nativeOscillatorNode = nativeContext.createOscillator();\n        assignNativeAudioNodeOptions(nativeOscillatorNode, options);\n        assignNativeAudioNodeAudioParamValue(nativeOscillatorNode, options, \'detune\');\n        assignNativeAudioNodeAudioParamValue(nativeOscillatorNode, options, \'frequency\');\n        if (options.periodicWave !== undefined) {\n            nativeOscillatorNode.setPeriodicWave(options.periodicWave);\n        }\n        else {\n            assignNativeAudioNodeOption(nativeOscillatorNode, options, \'type\');\n        }\n        // Bug #44: Only Chrome, Edge & Opera throw a RangeError yet.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, () => testAudioScheduledSourceNodeStartMethodNegativeParametersSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStartMethodNegativeParameters(nativeOscillatorNode);\n        }\n        // Bug #19: Safari does not ignore calls to stop() of an already stopped AudioBufferSourceNode.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, () => testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls(nativeOscillatorNode, nativeContext);\n        }\n        // Bug #44: Only Firefox does not throw a RangeError yet.\n        if (!cacheTestResult(testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, () => testAudioScheduledSourceNodeStopMethodNegativeParametersSupport(nativeContext))) {\n            wrapAudioScheduledSourceNodeStopMethodNegativeParameters(nativeOscillatorNode);\n        }\n        // Bug #175: Safari will not fire an ended event if the OscillatorNode is unconnected.\n        addSilentConnection(nativeContext, nativeOscillatorNode);\n        return nativeOscillatorNode;\n    };\n};\n//# sourceMappingURL=native-oscillator-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-panner-node-factory.js\n\n\n\nconst createNativePannerNodeFactory = (createNativePannerNodeFaker) => {\n    return (nativeContext, options) => {\n        const nativePannerNode = nativeContext.createPanner();\n        // Bug #124: Safari does not support modifying the orientation and the position with AudioParams.\n        if (nativePannerNode.orientationX === undefined) {\n            return createNativePannerNodeFaker(nativeContext, options);\n        }\n        assignNativeAudioNodeOptions(nativePannerNode, options);\n        assignNativeAudioNodeAudioParamValue(nativePannerNode, options, \'orientationX\');\n        assignNativeAudioNodeAudioParamValue(nativePannerNode, options, \'orientationY\');\n        assignNativeAudioNodeAudioParamValue(nativePannerNode, options, \'orientationZ\');\n        assignNativeAudioNodeAudioParamValue(nativePannerNode, options, \'positionX\');\n        assignNativeAudioNodeAudioParamValue(nativePannerNode, options, \'positionY\');\n        assignNativeAudioNodeAudioParamValue(nativePannerNode, options, \'positionZ\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'coneInnerAngle\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'coneOuterAngle\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'coneOuterGain\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'distanceModel\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'maxDistance\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'panningModel\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'refDistance\');\n        assignNativeAudioNodeOption(nativePannerNode, options, \'rolloffFactor\');\n        return nativePannerNode;\n    };\n};\n//# sourceMappingURL=native-panner-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-panner-node-faker-factory.js\n\n\nconst createNativePannerNodeFakerFactory = (connectNativeAudioNodeToNativeAudioNode, createInvalidStateError, createNativeChannelMergerNode, createNativeGainNode, createNativeScriptProcessorNode, createNativeWaveShaperNode, createNotSupportedError, disconnectNativeAudioNodeFromNativeAudioNode, monitorConnections) => {\n    return (nativeContext, { coneInnerAngle, coneOuterAngle, coneOuterGain, distanceModel, maxDistance, orientationX, orientationY, orientationZ, panningModel, positionX, positionY, positionZ, refDistance, rolloffFactor, ...audioNodeOptions }) => {\n        const pannerNode = nativeContext.createPanner();\n        // Bug #125: Safari does not throw an error yet.\n        if (audioNodeOptions.channelCount > 2) {\n            throw createNotSupportedError();\n        }\n        // Bug #126: Safari does not throw an error yet.\n        if (audioNodeOptions.channelCountMode === \'max\') {\n            throw createNotSupportedError();\n        }\n        assignNativeAudioNodeOptions(pannerNode, audioNodeOptions);\n        const SINGLE_CHANNEL_OPTIONS = {\n            channelCount: 1,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\'\n        };\n        const channelMergerNode = createNativeChannelMergerNode(nativeContext, {\n            ...SINGLE_CHANNEL_OPTIONS,\n            channelInterpretation: \'speakers\',\n            numberOfInputs: 6\n        });\n        const inputGainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, gain: 1 });\n        const orientationXGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 1 });\n        const orientationYGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        const orientationZGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        const positionXGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        const positionYGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        const positionZGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        const scriptProcessorNode = createNativeScriptProcessorNode(nativeContext, 256, 6, 1);\n        const waveShaperNode = createNativeWaveShaperNode(nativeContext, {\n            ...SINGLE_CHANNEL_OPTIONS,\n            curve: new Float32Array([1, 1]),\n            oversample: \'none\'\n        });\n        let lastOrientation = [orientationX, orientationY, orientationZ];\n        let lastPosition = [positionX, positionY, positionZ];\n        // tslint:disable-next-line:deprecation\n        scriptProcessorNode.onaudioprocess = ({ inputBuffer }) => {\n            const orientation = [\n                inputBuffer.getChannelData(0)[0],\n                inputBuffer.getChannelData(1)[0],\n                inputBuffer.getChannelData(2)[0]\n            ];\n            if (orientation.some((value, index) => value !== lastOrientation[index])) {\n                pannerNode.setOrientation(...orientation); // tslint:disable-line:deprecation\n                lastOrientation = orientation;\n            }\n            const positon = [\n                inputBuffer.getChannelData(3)[0],\n                inputBuffer.getChannelData(4)[0],\n                inputBuffer.getChannelData(5)[0]\n            ];\n            if (positon.some((value, index) => value !== lastPosition[index])) {\n                pannerNode.setPosition(...positon); // tslint:disable-line:deprecation\n                lastPosition = positon;\n            }\n        };\n        Object.defineProperty(orientationYGainNode.gain, \'defaultValue\', { get: () => 0 });\n        Object.defineProperty(orientationZGainNode.gain, \'defaultValue\', { get: () => 0 });\n        Object.defineProperty(positionXGainNode.gain, \'defaultValue\', { get: () => 0 });\n        Object.defineProperty(positionYGainNode.gain, \'defaultValue\', { get: () => 0 });\n        Object.defineProperty(positionZGainNode.gain, \'defaultValue\', { get: () => 0 });\n        const nativePannerNodeFaker = {\n            get bufferSize() {\n                return undefined;\n            },\n            get channelCount() {\n                return pannerNode.channelCount;\n            },\n            set channelCount(value) {\n                // Bug #125: Safari does not throw an error yet.\n                if (value > 2) {\n                    throw createNotSupportedError();\n                }\n                inputGainNode.channelCount = value;\n                pannerNode.channelCount = value;\n            },\n            get channelCountMode() {\n                return pannerNode.channelCountMode;\n            },\n            set channelCountMode(value) {\n                // Bug #126: Safari does not throw an error yet.\n                if (value === \'max\') {\n                    throw createNotSupportedError();\n                }\n                inputGainNode.channelCountMode = value;\n                pannerNode.channelCountMode = value;\n            },\n            get channelInterpretation() {\n                return pannerNode.channelInterpretation;\n            },\n            set channelInterpretation(value) {\n                inputGainNode.channelInterpretation = value;\n                pannerNode.channelInterpretation = value;\n            },\n            get coneInnerAngle() {\n                return pannerNode.coneInnerAngle;\n            },\n            set coneInnerAngle(value) {\n                pannerNode.coneInnerAngle = value;\n            },\n            get coneOuterAngle() {\n                return pannerNode.coneOuterAngle;\n            },\n            set coneOuterAngle(value) {\n                pannerNode.coneOuterAngle = value;\n            },\n            get coneOuterGain() {\n                return pannerNode.coneOuterGain;\n            },\n            set coneOuterGain(value) {\n                // Bug #127: Safari does not throw an InvalidStateError yet.\n                if (value < 0 || value > 1) {\n                    throw createInvalidStateError();\n                }\n                pannerNode.coneOuterGain = value;\n            },\n            get context() {\n                return pannerNode.context;\n            },\n            get distanceModel() {\n                return pannerNode.distanceModel;\n            },\n            set distanceModel(value) {\n                pannerNode.distanceModel = value;\n            },\n            get inputs() {\n                return [inputGainNode];\n            },\n            get maxDistance() {\n                return pannerNode.maxDistance;\n            },\n            set maxDistance(value) {\n                // Bug #128: Safari does not throw an error yet.\n                if (value < 0) {\n                    throw new RangeError();\n                }\n                pannerNode.maxDistance = value;\n            },\n            get numberOfInputs() {\n                return pannerNode.numberOfInputs;\n            },\n            get numberOfOutputs() {\n                return pannerNode.numberOfOutputs;\n            },\n            get orientationX() {\n                return orientationXGainNode.gain;\n            },\n            get orientationY() {\n                return orientationYGainNode.gain;\n            },\n            get orientationZ() {\n                return orientationZGainNode.gain;\n            },\n            get panningModel() {\n                return pannerNode.panningModel;\n            },\n            set panningModel(value) {\n                pannerNode.panningModel = value;\n            },\n            get positionX() {\n                return positionXGainNode.gain;\n            },\n            get positionY() {\n                return positionYGainNode.gain;\n            },\n            get positionZ() {\n                return positionZGainNode.gain;\n            },\n            get refDistance() {\n                return pannerNode.refDistance;\n            },\n            set refDistance(value) {\n                // Bug #129: Safari does not throw an error yet.\n                if (value < 0) {\n                    throw new RangeError();\n                }\n                pannerNode.refDistance = value;\n            },\n            get rolloffFactor() {\n                return pannerNode.rolloffFactor;\n            },\n            set rolloffFactor(value) {\n                // Bug #130: Safari does not throw an error yet.\n                if (value < 0) {\n                    throw new RangeError();\n                }\n                pannerNode.rolloffFactor = value;\n            },\n            addEventListener(...args) {\n                return inputGainNode.addEventListener(args[0], args[1], args[2]);\n            },\n            dispatchEvent(...args) {\n                return inputGainNode.dispatchEvent(args[0]);\n            },\n            removeEventListener(...args) {\n                return inputGainNode.removeEventListener(args[0], args[1], args[2]);\n            }\n        };\n        if (coneInnerAngle !== nativePannerNodeFaker.coneInnerAngle) {\n            nativePannerNodeFaker.coneInnerAngle = coneInnerAngle;\n        }\n        if (coneOuterAngle !== nativePannerNodeFaker.coneOuterAngle) {\n            nativePannerNodeFaker.coneOuterAngle = coneOuterAngle;\n        }\n        if (coneOuterGain !== nativePannerNodeFaker.coneOuterGain) {\n            nativePannerNodeFaker.coneOuterGain = coneOuterGain;\n        }\n        if (distanceModel !== nativePannerNodeFaker.distanceModel) {\n            nativePannerNodeFaker.distanceModel = distanceModel;\n        }\n        if (maxDistance !== nativePannerNodeFaker.maxDistance) {\n            nativePannerNodeFaker.maxDistance = maxDistance;\n        }\n        if (orientationX !== nativePannerNodeFaker.orientationX.value) {\n            nativePannerNodeFaker.orientationX.value = orientationX;\n        }\n        if (orientationY !== nativePannerNodeFaker.orientationY.value) {\n            nativePannerNodeFaker.orientationY.value = orientationY;\n        }\n        if (orientationZ !== nativePannerNodeFaker.orientationZ.value) {\n            nativePannerNodeFaker.orientationZ.value = orientationZ;\n        }\n        if (panningModel !== nativePannerNodeFaker.panningModel) {\n            nativePannerNodeFaker.panningModel = panningModel;\n        }\n        if (positionX !== nativePannerNodeFaker.positionX.value) {\n            nativePannerNodeFaker.positionX.value = positionX;\n        }\n        if (positionY !== nativePannerNodeFaker.positionY.value) {\n            nativePannerNodeFaker.positionY.value = positionY;\n        }\n        if (positionZ !== nativePannerNodeFaker.positionZ.value) {\n            nativePannerNodeFaker.positionZ.value = positionZ;\n        }\n        if (refDistance !== nativePannerNodeFaker.refDistance) {\n            nativePannerNodeFaker.refDistance = refDistance;\n        }\n        if (rolloffFactor !== nativePannerNodeFaker.rolloffFactor) {\n            nativePannerNodeFaker.rolloffFactor = rolloffFactor;\n        }\n        if (lastOrientation[0] !== 1 || lastOrientation[1] !== 0 || lastOrientation[2] !== 0) {\n            pannerNode.setOrientation(...lastOrientation); // tslint:disable-line:deprecation\n        }\n        if (lastPosition[0] !== 0 || lastPosition[1] !== 0 || lastPosition[2] !== 0) {\n            pannerNode.setPosition(...lastPosition); // tslint:disable-line:deprecation\n        }\n        const whenConnected = () => {\n            inputGainNode.connect(pannerNode);\n            // Bug #119: Safari does not fully support the WaveShaperNode.\n            connectNativeAudioNodeToNativeAudioNode(inputGainNode, waveShaperNode, 0, 0);\n            waveShaperNode.connect(orientationXGainNode).connect(channelMergerNode, 0, 0);\n            waveShaperNode.connect(orientationYGainNode).connect(channelMergerNode, 0, 1);\n            waveShaperNode.connect(orientationZGainNode).connect(channelMergerNode, 0, 2);\n            waveShaperNode.connect(positionXGainNode).connect(channelMergerNode, 0, 3);\n            waveShaperNode.connect(positionYGainNode).connect(channelMergerNode, 0, 4);\n            waveShaperNode.connect(positionZGainNode).connect(channelMergerNode, 0, 5);\n            channelMergerNode.connect(scriptProcessorNode).connect(nativeContext.destination);\n        };\n        const whenDisconnected = () => {\n            inputGainNode.disconnect(pannerNode);\n            // Bug #119: Safari does not fully support the WaveShaperNode.\n            disconnectNativeAudioNodeFromNativeAudioNode(inputGainNode, waveShaperNode, 0, 0);\n            waveShaperNode.disconnect(orientationXGainNode);\n            orientationXGainNode.disconnect(channelMergerNode);\n            waveShaperNode.disconnect(orientationYGainNode);\n            orientationYGainNode.disconnect(channelMergerNode);\n            waveShaperNode.disconnect(orientationZGainNode);\n            orientationZGainNode.disconnect(channelMergerNode);\n            waveShaperNode.disconnect(positionXGainNode);\n            positionXGainNode.disconnect(channelMergerNode);\n            waveShaperNode.disconnect(positionYGainNode);\n            positionYGainNode.disconnect(channelMergerNode);\n            waveShaperNode.disconnect(positionZGainNode);\n            positionZGainNode.disconnect(channelMergerNode);\n            channelMergerNode.disconnect(scriptProcessorNode);\n            scriptProcessorNode.disconnect(nativeContext.destination);\n        };\n        return monitorConnections(interceptConnections(nativePannerNodeFaker, pannerNode), whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=native-panner-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-periodic-wave-factory.js\nconst createNativePeriodicWaveFactory = (createIndexSizeError) => {\n    return (nativeContext, { disableNormalization, imag, real }) => {\n        // Bug #180: Safari does not allow to use ordinary arrays.\n        const convertedImag = imag instanceof Float32Array ? imag : new Float32Array(imag);\n        const convertedReal = real instanceof Float32Array ? real : new Float32Array(real);\n        const nativePeriodicWave = nativeContext.createPeriodicWave(convertedReal, convertedImag, { disableNormalization });\n        // Bug #181: Safari does not throw an IndexSizeError so far if the given arrays have less than two values.\n        if (Array.from(imag).length < 2) {\n            throw createIndexSizeError();\n        }\n        return nativePeriodicWave;\n    };\n};\n//# sourceMappingURL=native-periodic-wave-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-script-processor-node.js\nconst native_script_processor_node_createNativeScriptProcessorNode = (nativeContext, bufferSize, numberOfInputChannels, numberOfOutputChannels) => {\n    return nativeContext.createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels);\n};\n//# sourceMappingURL=native-script-processor-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-stereo-panner-node-factory.js\n\n\nconst createNativeStereoPannerNodeFactory = (createNativeStereoPannerNodeFaker, createNotSupportedError) => {\n    return (nativeContext, options) => {\n        const channelCountMode = options.channelCountMode;\n        /*\n         * Bug #105: The channelCountMode of \'clamped-max\' should be supported. However it is not possible to write a polyfill for Safari\n         * which supports it and therefore it can\'t be supported at all.\n         */\n        if (channelCountMode === \'clamped-max\') {\n            throw createNotSupportedError();\n        }\n        // Bug #105: Safari does not support the StereoPannerNode.\n        if (nativeContext.createStereoPanner === undefined) {\n            return createNativeStereoPannerNodeFaker(nativeContext, options);\n        }\n        const nativeStereoPannerNode = nativeContext.createStereoPanner();\n        assignNativeAudioNodeOptions(nativeStereoPannerNode, options);\n        assignNativeAudioNodeAudioParamValue(nativeStereoPannerNode, options, \'pan\');\n        /*\n         * Bug #105: The channelCountMode of \'clamped-max\' should be supported. However it is not possible to write a polyfill for Safari\n         * which supports it and therefore it can\'t be supported at all.\n         */\n        Object.defineProperty(nativeStereoPannerNode, \'channelCountMode\', {\n            get: () => channelCountMode,\n            set: (value) => {\n                if (value !== channelCountMode) {\n                    throw createNotSupportedError();\n                }\n            }\n        });\n        return nativeStereoPannerNode;\n    };\n};\n//# sourceMappingURL=native-stereo-panner-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-stereo-panner-node-faker-factory.js\n\nconst createNativeStereoPannerNodeFakerFactory = (createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeGainNode, createNativeWaveShaperNode, createNotSupportedError, monitorConnections) => {\n    // The curve has a size of 14bit plus 1 value to have an exact representation for zero. This value has been determined experimentally.\n    const CURVE_SIZE = 16385;\n    const DC_CURVE = new Float32Array([1, 1]);\n    const HALF_PI = Math.PI / 2;\n    const SINGLE_CHANNEL_OPTIONS = { channelCount: 1, channelCountMode: \'explicit\', channelInterpretation: \'discrete\' };\n    const SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS = { ...SINGLE_CHANNEL_OPTIONS, oversample: \'none\' };\n    const buildInternalGraphForMono = (nativeContext, inputGainNode, panGainNode, channelMergerNode) => {\n        const leftWaveShaperCurve = new Float32Array(CURVE_SIZE);\n        const rightWaveShaperCurve = new Float32Array(CURVE_SIZE);\n        for (let i = 0; i < CURVE_SIZE; i += 1) {\n            const x = (i / (CURVE_SIZE - 1)) * HALF_PI;\n            leftWaveShaperCurve[i] = Math.cos(x);\n            rightWaveShaperCurve[i] = Math.sin(x);\n        }\n        const leftGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const leftWaveShaperNode = (createNativeWaveShaperNode(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: leftWaveShaperCurve }));\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const panWaveShaperNode = (createNativeWaveShaperNode(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: DC_CURVE }));\n        const rightGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const rightWaveShaperNode = (createNativeWaveShaperNode(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: rightWaveShaperCurve }));\n        return {\n            connectGraph() {\n                inputGainNode.connect(leftGainNode);\n                inputGainNode.connect(panWaveShaperNode.inputs === undefined ? panWaveShaperNode : panWaveShaperNode.inputs[0]);\n                inputGainNode.connect(rightGainNode);\n                panWaveShaperNode.connect(panGainNode);\n                panGainNode.connect(leftWaveShaperNode.inputs === undefined ? leftWaveShaperNode : leftWaveShaperNode.inputs[0]);\n                panGainNode.connect(rightWaveShaperNode.inputs === undefined ? rightWaveShaperNode : rightWaveShaperNode.inputs[0]);\n                leftWaveShaperNode.connect(leftGainNode.gain);\n                rightWaveShaperNode.connect(rightGainNode.gain);\n                leftGainNode.connect(channelMergerNode, 0, 0);\n                rightGainNode.connect(channelMergerNode, 0, 1);\n            },\n            disconnectGraph() {\n                inputGainNode.disconnect(leftGainNode);\n                inputGainNode.disconnect(panWaveShaperNode.inputs === undefined ? panWaveShaperNode : panWaveShaperNode.inputs[0]);\n                inputGainNode.disconnect(rightGainNode);\n                panWaveShaperNode.disconnect(panGainNode);\n                panGainNode.disconnect(leftWaveShaperNode.inputs === undefined ? leftWaveShaperNode : leftWaveShaperNode.inputs[0]);\n                panGainNode.disconnect(rightWaveShaperNode.inputs === undefined ? rightWaveShaperNode : rightWaveShaperNode.inputs[0]);\n                leftWaveShaperNode.disconnect(leftGainNode.gain);\n                rightWaveShaperNode.disconnect(rightGainNode.gain);\n                leftGainNode.disconnect(channelMergerNode, 0, 0);\n                rightGainNode.disconnect(channelMergerNode, 0, 1);\n            }\n        };\n    };\n    const buildInternalGraphForStereo = (nativeContext, inputGainNode, panGainNode, channelMergerNode) => {\n        const leftInputForLeftOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);\n        const leftInputForRightOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);\n        const rightInputForLeftOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);\n        const rightInputForRightOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);\n        const centerIndex = Math.floor(CURVE_SIZE / 2);\n        for (let i = 0; i < CURVE_SIZE; i += 1) {\n            if (i > centerIndex) {\n                const x = ((i - centerIndex) / (CURVE_SIZE - 1 - centerIndex)) * HALF_PI;\n                leftInputForLeftOutputWaveShaperCurve[i] = Math.cos(x);\n                leftInputForRightOutputWaveShaperCurve[i] = Math.sin(x);\n                rightInputForLeftOutputWaveShaperCurve[i] = 0;\n                rightInputForRightOutputWaveShaperCurve[i] = 1;\n            }\n            else {\n                const x = (i / (CURVE_SIZE - 1 - centerIndex)) * HALF_PI;\n                leftInputForLeftOutputWaveShaperCurve[i] = 1;\n                leftInputForRightOutputWaveShaperCurve[i] = 0;\n                rightInputForLeftOutputWaveShaperCurve[i] = Math.cos(x);\n                rightInputForRightOutputWaveShaperCurve[i] = Math.sin(x);\n            }\n        }\n        const channelSplitterNode = createNativeChannelSplitterNode(nativeContext, {\n            channelCount: 2,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\',\n            numberOfOutputs: 2\n        });\n        const leftInputForLeftOutputGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const leftInputForLeftOutputWaveShaperNode = createNativeWaveShaperNode(nativeContext, {\n            ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,\n            curve: leftInputForLeftOutputWaveShaperCurve\n        });\n        const leftInputForRightOutputGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const leftInputForRightOutputWaveShaperNode = createNativeWaveShaperNode(nativeContext, {\n            ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,\n            curve: leftInputForRightOutputWaveShaperCurve\n        });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const panWaveShaperNode = (createNativeWaveShaperNode(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: DC_CURVE }));\n        const rightInputForLeftOutputGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const rightInputForLeftOutputWaveShaperNode = createNativeWaveShaperNode(nativeContext, {\n            ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,\n            curve: rightInputForLeftOutputWaveShaperCurve\n        });\n        const rightInputForRightOutputGainNode = createNativeGainNode(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });\n        // Bug #119: Safari does not fully support the WaveShaperNode.\n        const rightInputForRightOutputWaveShaperNode = createNativeWaveShaperNode(nativeContext, {\n            ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,\n            curve: rightInputForRightOutputWaveShaperCurve\n        });\n        return {\n            connectGraph() {\n                inputGainNode.connect(channelSplitterNode);\n                inputGainNode.connect(panWaveShaperNode.inputs === undefined ? panWaveShaperNode : panWaveShaperNode.inputs[0]);\n                channelSplitterNode.connect(leftInputForLeftOutputGainNode, 0);\n                channelSplitterNode.connect(leftInputForRightOutputGainNode, 0);\n                channelSplitterNode.connect(rightInputForLeftOutputGainNode, 1);\n                channelSplitterNode.connect(rightInputForRightOutputGainNode, 1);\n                panWaveShaperNode.connect(panGainNode);\n                panGainNode.connect(leftInputForLeftOutputWaveShaperNode.inputs === undefined\n                    ? leftInputForLeftOutputWaveShaperNode\n                    : leftInputForLeftOutputWaveShaperNode.inputs[0]);\n                panGainNode.connect(leftInputForRightOutputWaveShaperNode.inputs === undefined\n                    ? leftInputForRightOutputWaveShaperNode\n                    : leftInputForRightOutputWaveShaperNode.inputs[0]);\n                panGainNode.connect(rightInputForLeftOutputWaveShaperNode.inputs === undefined\n                    ? rightInputForLeftOutputWaveShaperNode\n                    : rightInputForLeftOutputWaveShaperNode.inputs[0]);\n                panGainNode.connect(rightInputForRightOutputWaveShaperNode.inputs === undefined\n                    ? rightInputForRightOutputWaveShaperNode\n                    : rightInputForRightOutputWaveShaperNode.inputs[0]);\n                leftInputForLeftOutputWaveShaperNode.connect(leftInputForLeftOutputGainNode.gain);\n                leftInputForRightOutputWaveShaperNode.connect(leftInputForRightOutputGainNode.gain);\n                rightInputForLeftOutputWaveShaperNode.connect(rightInputForLeftOutputGainNode.gain);\n                rightInputForRightOutputWaveShaperNode.connect(rightInputForRightOutputGainNode.gain);\n                leftInputForLeftOutputGainNode.connect(channelMergerNode, 0, 0);\n                rightInputForLeftOutputGainNode.connect(channelMergerNode, 0, 0);\n                leftInputForRightOutputGainNode.connect(channelMergerNode, 0, 1);\n                rightInputForRightOutputGainNode.connect(channelMergerNode, 0, 1);\n            },\n            disconnectGraph() {\n                inputGainNode.disconnect(channelSplitterNode);\n                inputGainNode.disconnect(panWaveShaperNode.inputs === undefined ? panWaveShaperNode : panWaveShaperNode.inputs[0]);\n                channelSplitterNode.disconnect(leftInputForLeftOutputGainNode, 0);\n                channelSplitterNode.disconnect(leftInputForRightOutputGainNode, 0);\n                channelSplitterNode.disconnect(rightInputForLeftOutputGainNode, 1);\n                channelSplitterNode.disconnect(rightInputForRightOutputGainNode, 1);\n                panWaveShaperNode.disconnect(panGainNode);\n                panGainNode.disconnect(leftInputForLeftOutputWaveShaperNode.inputs === undefined\n                    ? leftInputForLeftOutputWaveShaperNode\n                    : leftInputForLeftOutputWaveShaperNode.inputs[0]);\n                panGainNode.disconnect(leftInputForRightOutputWaveShaperNode.inputs === undefined\n                    ? leftInputForRightOutputWaveShaperNode\n                    : leftInputForRightOutputWaveShaperNode.inputs[0]);\n                panGainNode.disconnect(rightInputForLeftOutputWaveShaperNode.inputs === undefined\n                    ? rightInputForLeftOutputWaveShaperNode\n                    : rightInputForLeftOutputWaveShaperNode.inputs[0]);\n                panGainNode.disconnect(rightInputForRightOutputWaveShaperNode.inputs === undefined\n                    ? rightInputForRightOutputWaveShaperNode\n                    : rightInputForRightOutputWaveShaperNode.inputs[0]);\n                leftInputForLeftOutputWaveShaperNode.disconnect(leftInputForLeftOutputGainNode.gain);\n                leftInputForRightOutputWaveShaperNode.disconnect(leftInputForRightOutputGainNode.gain);\n                rightInputForLeftOutputWaveShaperNode.disconnect(rightInputForLeftOutputGainNode.gain);\n                rightInputForRightOutputWaveShaperNode.disconnect(rightInputForRightOutputGainNode.gain);\n                leftInputForLeftOutputGainNode.disconnect(channelMergerNode, 0, 0);\n                rightInputForLeftOutputGainNode.disconnect(channelMergerNode, 0, 0);\n                leftInputForRightOutputGainNode.disconnect(channelMergerNode, 0, 1);\n                rightInputForRightOutputGainNode.disconnect(channelMergerNode, 0, 1);\n            }\n        };\n    };\n    const buildInternalGraph = (nativeContext, channelCount, inputGainNode, panGainNode, channelMergerNode) => {\n        if (channelCount === 1) {\n            return buildInternalGraphForMono(nativeContext, inputGainNode, panGainNode, channelMergerNode);\n        }\n        if (channelCount === 2) {\n            return buildInternalGraphForStereo(nativeContext, inputGainNode, panGainNode, channelMergerNode);\n        }\n        throw createNotSupportedError();\n    };\n    return (nativeContext, { channelCount, channelCountMode, pan, ...audioNodeOptions }) => {\n        if (channelCountMode === \'max\') {\n            throw createNotSupportedError();\n        }\n        const channelMergerNode = createNativeChannelMergerNode(nativeContext, {\n            ...audioNodeOptions,\n            channelCount: 1,\n            channelCountMode,\n            numberOfInputs: 2\n        });\n        const inputGainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, channelCount, channelCountMode, gain: 1 });\n        const panGainNode = createNativeGainNode(nativeContext, {\n            channelCount: 1,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\',\n            gain: pan\n        });\n        let { connectGraph, disconnectGraph } = buildInternalGraph(nativeContext, channelCount, inputGainNode, panGainNode, channelMergerNode);\n        Object.defineProperty(panGainNode.gain, \'defaultValue\', { get: () => 0 });\n        Object.defineProperty(panGainNode.gain, \'maxValue\', { get: () => 1 });\n        Object.defineProperty(panGainNode.gain, \'minValue\', { get: () => -1 });\n        const nativeStereoPannerNodeFakerFactory = {\n            get bufferSize() {\n                return undefined;\n            },\n            get channelCount() {\n                return inputGainNode.channelCount;\n            },\n            set channelCount(value) {\n                if (inputGainNode.channelCount !== value) {\n                    if (isConnected) {\n                        disconnectGraph();\n                    }\n                    ({ connectGraph, disconnectGraph } = buildInternalGraph(nativeContext, value, inputGainNode, panGainNode, channelMergerNode));\n                    if (isConnected) {\n                        connectGraph();\n                    }\n                }\n                inputGainNode.channelCount = value;\n            },\n            get channelCountMode() {\n                return inputGainNode.channelCountMode;\n            },\n            set channelCountMode(value) {\n                if (value === \'clamped-max\' || value === \'max\') {\n                    throw createNotSupportedError();\n                }\n                inputGainNode.channelCountMode = value;\n            },\n            get channelInterpretation() {\n                return inputGainNode.channelInterpretation;\n            },\n            set channelInterpretation(value) {\n                inputGainNode.channelInterpretation = value;\n            },\n            get context() {\n                return inputGainNode.context;\n            },\n            get inputs() {\n                return [inputGainNode];\n            },\n            get numberOfInputs() {\n                return inputGainNode.numberOfInputs;\n            },\n            get numberOfOutputs() {\n                return inputGainNode.numberOfOutputs;\n            },\n            get pan() {\n                return panGainNode.gain;\n            },\n            addEventListener(...args) {\n                return inputGainNode.addEventListener(args[0], args[1], args[2]);\n            },\n            dispatchEvent(...args) {\n                return inputGainNode.dispatchEvent(args[0]);\n            },\n            removeEventListener(...args) {\n                return inputGainNode.removeEventListener(args[0], args[1], args[2]);\n            }\n        };\n        let isConnected = false;\n        const whenConnected = () => {\n            connectGraph();\n            isConnected = true;\n        };\n        const whenDisconnected = () => {\n            disconnectGraph();\n            isConnected = false;\n        };\n        return monitorConnections(interceptConnections(nativeStereoPannerNodeFakerFactory, channelMergerNode), whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=native-stereo-panner-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-wave-shaper-node-factory.js\n\n\nconst createNativeWaveShaperNodeFactory = (createConnectedNativeAudioBufferSourceNode, createInvalidStateError, createNativeWaveShaperNodeFaker, isDCCurve, monitorConnections, nativeAudioContextConstructor, overwriteAccessors) => {\n    return (nativeContext, options) => {\n        const nativeWaveShaperNode = nativeContext.createWaveShaper();\n        /*\n         * Bug #119: Safari does not correctly map the values.\n         * @todo Unfortunately there is no way to test for this behavior in a synchronous fashion which is why testing for the existence of\n         * the webkitAudioContext is used as a workaround here. Testing for the automationRate property is necessary because this workaround\n         * isn\'t necessary anymore since v14.0.2 of Safari.\n         */\n        if (nativeAudioContextConstructor !== null &&\n            nativeAudioContextConstructor.name === \'webkitAudioContext\' &&\n            nativeContext.createGain().gain.automationRate === undefined) {\n            return createNativeWaveShaperNodeFaker(nativeContext, options);\n        }\n        assignNativeAudioNodeOptions(nativeWaveShaperNode, options);\n        const curve = options.curve === null || options.curve instanceof Float32Array ? options.curve : new Float32Array(options.curve);\n        // Bug #104: Chrome, Edge and Opera will throw an InvalidAccessError when the curve has less than two samples.\n        if (curve !== null && curve.length < 2) {\n            throw createInvalidStateError();\n        }\n        // Only values of type Float32Array can be assigned to the curve property.\n        assignNativeAudioNodeOption(nativeWaveShaperNode, { curve }, \'curve\');\n        assignNativeAudioNodeOption(nativeWaveShaperNode, options, \'oversample\');\n        let disconnectNativeAudioBufferSourceNode = null;\n        let isConnected = false;\n        overwriteAccessors(nativeWaveShaperNode, \'curve\', (get) => () => get.call(nativeWaveShaperNode), (set) => (value) => {\n            set.call(nativeWaveShaperNode, value);\n            if (isConnected) {\n                if (isDCCurve(value) && disconnectNativeAudioBufferSourceNode === null) {\n                    disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode(nativeContext, nativeWaveShaperNode);\n                }\n                else if (!isDCCurve(value) && disconnectNativeAudioBufferSourceNode !== null) {\n                    disconnectNativeAudioBufferSourceNode();\n                    disconnectNativeAudioBufferSourceNode = null;\n                }\n            }\n            return value;\n        });\n        const whenConnected = () => {\n            isConnected = true;\n            if (isDCCurve(nativeWaveShaperNode.curve)) {\n                disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode(nativeContext, nativeWaveShaperNode);\n            }\n        };\n        const whenDisconnected = () => {\n            isConnected = false;\n            if (disconnectNativeAudioBufferSourceNode !== null) {\n                disconnectNativeAudioBufferSourceNode();\n                disconnectNativeAudioBufferSourceNode = null;\n            }\n        };\n        return monitorConnections(nativeWaveShaperNode, whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=native-wave-shaper-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/native-wave-shaper-node-faker-factory.js\n\n\nconst createNativeWaveShaperNodeFakerFactory = (createConnectedNativeAudioBufferSourceNode, createInvalidStateError, createNativeGainNode, isDCCurve, monitorConnections) => {\n    return (nativeContext, { curve, oversample, ...audioNodeOptions }) => {\n        const negativeWaveShaperNode = nativeContext.createWaveShaper();\n        const positiveWaveShaperNode = nativeContext.createWaveShaper();\n        assignNativeAudioNodeOptions(negativeWaveShaperNode, audioNodeOptions);\n        assignNativeAudioNodeOptions(positiveWaveShaperNode, audioNodeOptions);\n        const inputGainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, gain: 1 });\n        const invertGainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, gain: -1 });\n        const outputGainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, gain: 1 });\n        const revertGainNode = createNativeGainNode(nativeContext, { ...audioNodeOptions, gain: -1 });\n        let disconnectNativeAudioBufferSourceNode = null;\n        let isConnected = false;\n        let unmodifiedCurve = null;\n        const nativeWaveShaperNodeFaker = {\n            get bufferSize() {\n                return undefined;\n            },\n            get channelCount() {\n                return negativeWaveShaperNode.channelCount;\n            },\n            set channelCount(value) {\n                inputGainNode.channelCount = value;\n                invertGainNode.channelCount = value;\n                negativeWaveShaperNode.channelCount = value;\n                outputGainNode.channelCount = value;\n                positiveWaveShaperNode.channelCount = value;\n                revertGainNode.channelCount = value;\n            },\n            get channelCountMode() {\n                return negativeWaveShaperNode.channelCountMode;\n            },\n            set channelCountMode(value) {\n                inputGainNode.channelCountMode = value;\n                invertGainNode.channelCountMode = value;\n                negativeWaveShaperNode.channelCountMode = value;\n                outputGainNode.channelCountMode = value;\n                positiveWaveShaperNode.channelCountMode = value;\n                revertGainNode.channelCountMode = value;\n            },\n            get channelInterpretation() {\n                return negativeWaveShaperNode.channelInterpretation;\n            },\n            set channelInterpretation(value) {\n                inputGainNode.channelInterpretation = value;\n                invertGainNode.channelInterpretation = value;\n                negativeWaveShaperNode.channelInterpretation = value;\n                outputGainNode.channelInterpretation = value;\n                positiveWaveShaperNode.channelInterpretation = value;\n                revertGainNode.channelInterpretation = value;\n            },\n            get context() {\n                return negativeWaveShaperNode.context;\n            },\n            get curve() {\n                return unmodifiedCurve;\n            },\n            set curve(value) {\n                // Bug #102: Safari does not throw an InvalidStateError when the curve has less than two samples.\n                if (value !== null && value.length < 2) {\n                    throw createInvalidStateError();\n                }\n                if (value === null) {\n                    negativeWaveShaperNode.curve = value;\n                    positiveWaveShaperNode.curve = value;\n                }\n                else {\n                    const curveLength = value.length;\n                    const negativeCurve = new Float32Array(curveLength + 2 - (curveLength % 2));\n                    const positiveCurve = new Float32Array(curveLength + 2 - (curveLength % 2));\n                    negativeCurve[0] = value[0];\n                    positiveCurve[0] = -value[curveLength - 1];\n                    const length = Math.ceil((curveLength + 1) / 2);\n                    const centerIndex = (curveLength + 1) / 2 - 1;\n                    for (let i = 1; i < length; i += 1) {\n                        const theoreticIndex = (i / length) * centerIndex;\n                        const lowerIndex = Math.floor(theoreticIndex);\n                        const upperIndex = Math.ceil(theoreticIndex);\n                        negativeCurve[i] =\n                            lowerIndex === upperIndex\n                                ? value[lowerIndex]\n                                : (1 - (theoreticIndex - lowerIndex)) * value[lowerIndex] +\n                                    (1 - (upperIndex - theoreticIndex)) * value[upperIndex];\n                        positiveCurve[i] =\n                            lowerIndex === upperIndex\n                                ? -value[curveLength - 1 - lowerIndex]\n                                : -((1 - (theoreticIndex - lowerIndex)) * value[curveLength - 1 - lowerIndex]) -\n                                    (1 - (upperIndex - theoreticIndex)) * value[curveLength - 1 - upperIndex];\n                    }\n                    negativeCurve[length] = curveLength % 2 === 1 ? value[length - 1] : (value[length - 2] + value[length - 1]) / 2;\n                    negativeWaveShaperNode.curve = negativeCurve;\n                    positiveWaveShaperNode.curve = positiveCurve;\n                }\n                unmodifiedCurve = value;\n                if (isConnected) {\n                    if (isDCCurve(unmodifiedCurve) && disconnectNativeAudioBufferSourceNode === null) {\n                        disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode(nativeContext, inputGainNode);\n                    }\n                    else if (disconnectNativeAudioBufferSourceNode !== null) {\n                        disconnectNativeAudioBufferSourceNode();\n                        disconnectNativeAudioBufferSourceNode = null;\n                    }\n                }\n            },\n            get inputs() {\n                return [inputGainNode];\n            },\n            get numberOfInputs() {\n                return negativeWaveShaperNode.numberOfInputs;\n            },\n            get numberOfOutputs() {\n                return negativeWaveShaperNode.numberOfOutputs;\n            },\n            get oversample() {\n                return negativeWaveShaperNode.oversample;\n            },\n            set oversample(value) {\n                negativeWaveShaperNode.oversample = value;\n                positiveWaveShaperNode.oversample = value;\n            },\n            addEventListener(...args) {\n                return inputGainNode.addEventListener(args[0], args[1], args[2]);\n            },\n            dispatchEvent(...args) {\n                return inputGainNode.dispatchEvent(args[0]);\n            },\n            removeEventListener(...args) {\n                return inputGainNode.removeEventListener(args[0], args[1], args[2]);\n            }\n        };\n        if (curve !== null) {\n            // Only values of type Float32Array can be assigned to the curve property.\n            nativeWaveShaperNodeFaker.curve = curve instanceof Float32Array ? curve : new Float32Array(curve);\n        }\n        if (oversample !== nativeWaveShaperNodeFaker.oversample) {\n            nativeWaveShaperNodeFaker.oversample = oversample;\n        }\n        const whenConnected = () => {\n            inputGainNode.connect(negativeWaveShaperNode).connect(outputGainNode);\n            inputGainNode.connect(invertGainNode).connect(positiveWaveShaperNode).connect(revertGainNode).connect(outputGainNode);\n            isConnected = true;\n            if (isDCCurve(unmodifiedCurve)) {\n                disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode(nativeContext, inputGainNode);\n            }\n        };\n        const whenDisconnected = () => {\n            inputGainNode.disconnect(negativeWaveShaperNode);\n            negativeWaveShaperNode.disconnect(outputGainNode);\n            inputGainNode.disconnect(invertGainNode);\n            invertGainNode.disconnect(positiveWaveShaperNode);\n            positiveWaveShaperNode.disconnect(revertGainNode);\n            revertGainNode.disconnect(outputGainNode);\n            isConnected = false;\n            if (disconnectNativeAudioBufferSourceNode !== null) {\n                disconnectNativeAudioBufferSourceNode();\n                disconnectNativeAudioBufferSourceNode = null;\n            }\n        };\n        return monitorConnections(interceptConnections(nativeWaveShaperNodeFaker, outputGainNode), whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=native-wave-shaper-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/not-supported-error.js\nconst not_supported_error_createNotSupportedError = () => new DOMException(\'\', \'NotSupportedError\');\n//# sourceMappingURL=not-supported-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/offline-audio-context-constructor.js\n\n\nconst offline_audio_context_constructor_DEFAULT_OPTIONS = {\n    numberOfChannels: 1\n};\nconst createOfflineAudioContextConstructor = (baseAudioContextConstructor, cacheTestResult, createInvalidStateError, createNativeOfflineAudioContext, startRendering) => {\n    return class OfflineAudioContext extends baseAudioContextConstructor {\n        constructor(a, b, c) {\n            let options;\n            if (typeof a === \'number\' && b !== undefined && c !== undefined) {\n                options = { length: b, numberOfChannels: a, sampleRate: c };\n            }\n            else if (typeof a === \'object\') {\n                options = a;\n            }\n            else {\n                throw new Error(\'The given parameters are not valid.\');\n            }\n            const { length, numberOfChannels, sampleRate } = { ...offline_audio_context_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeOfflineAudioContext = createNativeOfflineAudioContext(numberOfChannels, length, sampleRate);\n            // #21 Safari does not support promises and therefore would fire the statechange event before the promise can be resolved.\n            if (!cacheTestResult(test_promise_support_testPromiseSupport, () => test_promise_support_testPromiseSupport(nativeOfflineAudioContext))) {\n                nativeOfflineAudioContext.addEventListener(\'statechange\', (() => {\n                    let i = 0;\n                    const delayStateChangeEvent = (event) => {\n                        if (this._state === \'running\') {\n                            if (i > 0) {\n                                nativeOfflineAudioContext.removeEventListener(\'statechange\', delayStateChangeEvent);\n                                event.stopImmediatePropagation();\n                                this._waitForThePromiseToSettle(event);\n                            }\n                            else {\n                                i += 1;\n                            }\n                        }\n                    };\n                    return delayStateChangeEvent;\n                })());\n            }\n            super(nativeOfflineAudioContext, numberOfChannels);\n            this._length = length;\n            this._nativeOfflineAudioContext = nativeOfflineAudioContext;\n            this._state = null;\n        }\n        get length() {\n            // Bug #17: Safari does not yet expose the length.\n            if (this._nativeOfflineAudioContext.length === undefined) {\n                return this._length;\n            }\n            return this._nativeOfflineAudioContext.length;\n        }\n        get state() {\n            return this._state === null ? this._nativeOfflineAudioContext.state : this._state;\n        }\n        startRendering() {\n            /*\n             * Bug #9 & #59: It is theoretically possible that startRendering() will first render a partialOfflineAudioContext. Therefore\n             * the state of the nativeOfflineAudioContext might no transition to running immediately.\n             */\n            if (this._state === \'running\') {\n                return Promise.reject(createInvalidStateError());\n            }\n            this._state = \'running\';\n            return startRendering(this.destination, this._nativeOfflineAudioContext).finally(() => {\n                this._state = null;\n                deactivateAudioGraph(this);\n            });\n        }\n        _waitForThePromiseToSettle(event) {\n            if (this._state === null) {\n                this._nativeOfflineAudioContext.dispatchEvent(event);\n            }\n            else {\n                setTimeout(() => this._waitForThePromiseToSettle(event));\n            }\n        }\n    };\n};\n//# sourceMappingURL=offline-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/oscillator-node-constructor.js\n\n\n\nconst oscillator_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    detune: 0,\n    frequency: 440,\n    periodicWave: undefined,\n    type: \'sine\'\n};\nconst createOscillatorNodeConstructor = (audioNodeConstructor, createAudioParam, createNativeOscillatorNode, createOscillatorNodeRenderer, getNativeContext, isNativeOfflineAudioContext, wrapEventListener) => {\n    return class OscillatorNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...oscillator_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeOscillatorNode = createNativeOscillatorNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const oscillatorNodeRenderer = (isOffline ? createOscillatorNodeRenderer() : null);\n            const nyquist = context.sampleRate / 2;\n            super(context, false, nativeOscillatorNode, oscillatorNodeRenderer);\n            // Bug #81: Firefox & Safari do not export the correct values for maxValue and minValue.\n            this._detune = createAudioParam(this, isOffline, nativeOscillatorNode.detune, 153600, -153600);\n            // Bug #76: Safari does not export the correct values for maxValue and minValue.\n            this._frequency = createAudioParam(this, isOffline, nativeOscillatorNode.frequency, nyquist, -nyquist);\n            this._nativeOscillatorNode = nativeOscillatorNode;\n            this._onended = null;\n            this._oscillatorNodeRenderer = oscillatorNodeRenderer;\n            if (this._oscillatorNodeRenderer !== null && mergedOptions.periodicWave !== undefined) {\n                this._oscillatorNodeRenderer.periodicWave =\n                    mergedOptions.periodicWave;\n            }\n        }\n        get detune() {\n            return this._detune;\n        }\n        get frequency() {\n            return this._frequency;\n        }\n        get onended() {\n            return this._onended;\n        }\n        set onended(value) {\n            const wrappedListener = typeof value === \'function\' ? wrapEventListener(this, value) : null;\n            this._nativeOscillatorNode.onended = wrappedListener;\n            const nativeOnEnded = this._nativeOscillatorNode.onended;\n            this._onended = nativeOnEnded !== null && nativeOnEnded === wrappedListener ? value : nativeOnEnded;\n        }\n        get type() {\n            return this._nativeOscillatorNode.type;\n        }\n        set type(value) {\n            this._nativeOscillatorNode.type = value;\n            if (this._oscillatorNodeRenderer !== null) {\n                this._oscillatorNodeRenderer.periodicWave = null;\n            }\n        }\n        setPeriodicWave(periodicWave) {\n            this._nativeOscillatorNode.setPeriodicWave(periodicWave);\n            if (this._oscillatorNodeRenderer !== null) {\n                this._oscillatorNodeRenderer.periodicWave = periodicWave;\n            }\n        }\n        start(when = 0) {\n            this._nativeOscillatorNode.start(when);\n            if (this._oscillatorNodeRenderer !== null) {\n                this._oscillatorNodeRenderer.start = when;\n            }\n            if (this.context.state !== \'closed\') {\n                setInternalStateToActive(this);\n                const resetInternalStateToPassive = () => {\n                    this._nativeOscillatorNode.removeEventListener(\'ended\', resetInternalStateToPassive);\n                    if (is_active_audio_node_isActiveAudioNode(this)) {\n                        setInternalStateToPassive(this);\n                    }\n                };\n                this._nativeOscillatorNode.addEventListener(\'ended\', resetInternalStateToPassive);\n            }\n        }\n        stop(when = 0) {\n            this._nativeOscillatorNode.stop(when);\n            if (this._oscillatorNodeRenderer !== null) {\n                this._oscillatorNodeRenderer.stop = when;\n            }\n        }\n    };\n};\n//# sourceMappingURL=oscillator-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/oscillator-node-renderer-factory.js\n\nconst createOscillatorNodeRendererFactory = (connectAudioParam, createNativeOscillatorNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeOscillatorNodes = new WeakMap();\n        let periodicWave = null;\n        let start = null;\n        let stop = null;\n        const createOscillatorNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeOscillatorNode = getNativeAudioNode(proxy);\n            // If the initially used nativeOscillatorNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeOscillatorNodeIsOwnedByContext = isOwnedByContext(nativeOscillatorNode, nativeOfflineAudioContext);\n            if (!nativeOscillatorNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeOscillatorNode.channelCount,\n                    channelCountMode: nativeOscillatorNode.channelCountMode,\n                    channelInterpretation: nativeOscillatorNode.channelInterpretation,\n                    detune: nativeOscillatorNode.detune.value,\n                    frequency: nativeOscillatorNode.frequency.value,\n                    periodicWave: periodicWave === null ? undefined : periodicWave,\n                    type: nativeOscillatorNode.type\n                };\n                nativeOscillatorNode = createNativeOscillatorNode(nativeOfflineAudioContext, options);\n                if (start !== null) {\n                    nativeOscillatorNode.start(start);\n                }\n                if (stop !== null) {\n                    nativeOscillatorNode.stop(stop);\n                }\n            }\n            renderedNativeOscillatorNodes.set(nativeOfflineAudioContext, nativeOscillatorNode);\n            if (!nativeOscillatorNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.detune, nativeOscillatorNode.detune, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.frequency, nativeOscillatorNode.frequency, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.detune, nativeOscillatorNode.detune, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.frequency, nativeOscillatorNode.frequency, trace);\n            }\n            await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeOscillatorNode, trace);\n            return nativeOscillatorNode;\n        };\n        return {\n            set periodicWave(value) {\n                periodicWave = value;\n            },\n            set start(value) {\n                start = value;\n            },\n            set stop(value) {\n                stop = value;\n            },\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeOscillatorNode = renderedNativeOscillatorNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeOscillatorNode !== undefined) {\n                    return Promise.resolve(renderedNativeOscillatorNode);\n                }\n                return createOscillatorNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=oscillator-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/panner-node-constructor.js\n\nconst panner_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'clamped-max\',\n    channelInterpretation: \'speakers\',\n    coneInnerAngle: 360,\n    coneOuterAngle: 360,\n    coneOuterGain: 0,\n    distanceModel: \'inverse\',\n    maxDistance: 10000,\n    orientationX: 1,\n    orientationY: 0,\n    orientationZ: 0,\n    panningModel: \'equalpower\',\n    positionX: 0,\n    positionY: 0,\n    positionZ: 0,\n    refDistance: 1,\n    rolloffFactor: 1\n};\nconst createPannerNodeConstructor = (audioNodeConstructor, createAudioParam, createNativePannerNode, createPannerNodeRenderer, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class PannerNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...panner_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativePannerNode = createNativePannerNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const pannerNodeRenderer = (isOffline ? createPannerNodeRenderer() : null);\n            super(context, false, nativePannerNode, pannerNodeRenderer);\n            this._nativePannerNode = nativePannerNode;\n            // Bug #74: Safari does not export the correct values for maxValue and minValue.\n            this._orientationX = createAudioParam(this, isOffline, nativePannerNode.orientationX, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            this._orientationY = createAudioParam(this, isOffline, nativePannerNode.orientationY, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            this._orientationZ = createAudioParam(this, isOffline, nativePannerNode.orientationZ, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            this._positionX = createAudioParam(this, isOffline, nativePannerNode.positionX, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            this._positionY = createAudioParam(this, isOffline, nativePannerNode.positionY, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            this._positionZ = createAudioParam(this, isOffline, nativePannerNode.positionZ, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);\n            // @todo Determine a meaningful tail-time instead of just using one second.\n            setAudioNodeTailTime(this, 1);\n        }\n        get coneInnerAngle() {\n            return this._nativePannerNode.coneInnerAngle;\n        }\n        set coneInnerAngle(value) {\n            this._nativePannerNode.coneInnerAngle = value;\n        }\n        get coneOuterAngle() {\n            return this._nativePannerNode.coneOuterAngle;\n        }\n        set coneOuterAngle(value) {\n            this._nativePannerNode.coneOuterAngle = value;\n        }\n        get coneOuterGain() {\n            return this._nativePannerNode.coneOuterGain;\n        }\n        set coneOuterGain(value) {\n            this._nativePannerNode.coneOuterGain = value;\n        }\n        get distanceModel() {\n            return this._nativePannerNode.distanceModel;\n        }\n        set distanceModel(value) {\n            this._nativePannerNode.distanceModel = value;\n        }\n        get maxDistance() {\n            return this._nativePannerNode.maxDistance;\n        }\n        set maxDistance(value) {\n            this._nativePannerNode.maxDistance = value;\n        }\n        get orientationX() {\n            return this._orientationX;\n        }\n        get orientationY() {\n            return this._orientationY;\n        }\n        get orientationZ() {\n            return this._orientationZ;\n        }\n        get panningModel() {\n            return this._nativePannerNode.panningModel;\n        }\n        set panningModel(value) {\n            this._nativePannerNode.panningModel = value;\n        }\n        get positionX() {\n            return this._positionX;\n        }\n        get positionY() {\n            return this._positionY;\n        }\n        get positionZ() {\n            return this._positionZ;\n        }\n        get refDistance() {\n            return this._nativePannerNode.refDistance;\n        }\n        set refDistance(value) {\n            this._nativePannerNode.refDistance = value;\n        }\n        get rolloffFactor() {\n            return this._nativePannerNode.rolloffFactor;\n        }\n        set rolloffFactor(value) {\n            this._nativePannerNode.rolloffFactor = value;\n        }\n    };\n};\n//# sourceMappingURL=panner-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/panner-node-renderer-factory.js\n\n\nconst createPannerNodeRendererFactory = (connectAudioParam, createNativeChannelMergerNode, createNativeConstantSourceNode, createNativeGainNode, createNativePannerNode, getNativeAudioNode, nativeOfflineAudioContextConstructor, renderAutomation, renderInputsOfAudioNode, renderNativeOfflineAudioContext) => {\n    return () => {\n        const renderedNativeAudioNodes = new WeakMap();\n        let renderedBufferPromise = null;\n        const createAudioNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeGainNode = null;\n            let nativePannerNode = getNativeAudioNode(proxy);\n            const commonAudioNodeOptions = {\n                channelCount: nativePannerNode.channelCount,\n                channelCountMode: nativePannerNode.channelCountMode,\n                channelInterpretation: nativePannerNode.channelInterpretation\n            };\n            const commonNativePannerNodeOptions = {\n                ...commonAudioNodeOptions,\n                coneInnerAngle: nativePannerNode.coneInnerAngle,\n                coneOuterAngle: nativePannerNode.coneOuterAngle,\n                coneOuterGain: nativePannerNode.coneOuterGain,\n                distanceModel: nativePannerNode.distanceModel,\n                maxDistance: nativePannerNode.maxDistance,\n                panningModel: nativePannerNode.panningModel,\n                refDistance: nativePannerNode.refDistance,\n                rolloffFactor: nativePannerNode.rolloffFactor\n            };\n            // If the initially used nativePannerNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativePannerNodeIsOwnedByContext = isOwnedByContext(nativePannerNode, nativeOfflineAudioContext);\n            // Bug #124: Safari does not support modifying the orientation and the position with AudioParams.\n            if (\'bufferSize\' in nativePannerNode) {\n                nativeGainNode = createNativeGainNode(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 1 });\n            }\n            else if (!nativePannerNodeIsOwnedByContext) {\n                const options = {\n                    ...commonNativePannerNodeOptions,\n                    orientationX: nativePannerNode.orientationX.value,\n                    orientationY: nativePannerNode.orientationY.value,\n                    orientationZ: nativePannerNode.orientationZ.value,\n                    positionX: nativePannerNode.positionX.value,\n                    positionY: nativePannerNode.positionY.value,\n                    positionZ: nativePannerNode.positionZ.value\n                };\n                nativePannerNode = createNativePannerNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeGainNode === null ? nativePannerNode : nativeGainNode);\n            if (nativeGainNode !== null) {\n                if (renderedBufferPromise === null) {\n                    if (nativeOfflineAudioContextConstructor === null) {\n                        throw new Error(\'Missing the native OfflineAudioContext constructor.\');\n                    }\n                    const partialOfflineAudioContext = new nativeOfflineAudioContextConstructor(6, \n                    // Bug #17: Safari does not yet expose the length.\n                    proxy.context.length, nativeOfflineAudioContext.sampleRate);\n                    const nativeChannelMergerNode = createNativeChannelMergerNode(partialOfflineAudioContext, {\n                        channelCount: 1,\n                        channelCountMode: \'explicit\',\n                        channelInterpretation: \'speakers\',\n                        numberOfInputs: 6\n                    });\n                    nativeChannelMergerNode.connect(partialOfflineAudioContext.destination);\n                    renderedBufferPromise = (async () => {\n                        const nativeConstantSourceNodes = await Promise.all([\n                            proxy.orientationX,\n                            proxy.orientationY,\n                            proxy.orientationZ,\n                            proxy.positionX,\n                            proxy.positionY,\n                            proxy.positionZ\n                        ].map(async (audioParam, index) => {\n                            const nativeConstantSourceNode = createNativeConstantSourceNode(partialOfflineAudioContext, {\n                                channelCount: 1,\n                                channelCountMode: \'explicit\',\n                                channelInterpretation: \'discrete\',\n                                offset: index === 0 ? 1 : 0\n                            });\n                            await renderAutomation(partialOfflineAudioContext, audioParam, nativeConstantSourceNode.offset, trace);\n                            return nativeConstantSourceNode;\n                        }));\n                        for (let i = 0; i < 6; i += 1) {\n                            nativeConstantSourceNodes[i].connect(nativeChannelMergerNode, 0, i);\n                            nativeConstantSourceNodes[i].start(0);\n                        }\n                        return renderNativeOfflineAudioContext(partialOfflineAudioContext);\n                    })();\n                }\n                const renderedBuffer = await renderedBufferPromise;\n                const inputGainNode = createNativeGainNode(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 1 });\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, inputGainNode, trace);\n                const channelDatas = [];\n                for (let i = 0; i < renderedBuffer.numberOfChannels; i += 1) {\n                    channelDatas.push(renderedBuffer.getChannelData(i));\n                }\n                let lastOrientation = [channelDatas[0][0], channelDatas[1][0], channelDatas[2][0]];\n                let lastPosition = [channelDatas[3][0], channelDatas[4][0], channelDatas[5][0]];\n                let gateGainNode = createNativeGainNode(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 1 });\n                let partialPannerNode = createNativePannerNode(nativeOfflineAudioContext, {\n                    ...commonNativePannerNodeOptions,\n                    orientationX: lastOrientation[0],\n                    orientationY: lastOrientation[1],\n                    orientationZ: lastOrientation[2],\n                    positionX: lastPosition[0],\n                    positionY: lastPosition[1],\n                    positionZ: lastPosition[2]\n                });\n                inputGainNode.connect(gateGainNode).connect(partialPannerNode.inputs[0]);\n                partialPannerNode.connect(nativeGainNode);\n                for (let i = 128; i < renderedBuffer.length; i += 128) {\n                    const orientation = [channelDatas[0][i], channelDatas[1][i], channelDatas[2][i]];\n                    const positon = [channelDatas[3][i], channelDatas[4][i], channelDatas[5][i]];\n                    if (orientation.some((value, index) => value !== lastOrientation[index]) ||\n                        positon.some((value, index) => value !== lastPosition[index])) {\n                        lastOrientation = orientation;\n                        lastPosition = positon;\n                        const currentTime = i / nativeOfflineAudioContext.sampleRate;\n                        gateGainNode.gain.setValueAtTime(0, currentTime);\n                        gateGainNode = createNativeGainNode(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 0 });\n                        partialPannerNode = createNativePannerNode(nativeOfflineAudioContext, {\n                            ...commonNativePannerNodeOptions,\n                            orientationX: lastOrientation[0],\n                            orientationY: lastOrientation[1],\n                            orientationZ: lastOrientation[2],\n                            positionX: lastPosition[0],\n                            positionY: lastPosition[1],\n                            positionZ: lastPosition[2]\n                        });\n                        gateGainNode.gain.setValueAtTime(1, currentTime);\n                        inputGainNode.connect(gateGainNode).connect(partialPannerNode.inputs[0]);\n                        partialPannerNode.connect(nativeGainNode);\n                    }\n                }\n                return nativeGainNode;\n            }\n            if (!nativePannerNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.orientationX, nativePannerNode.orientationX, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.orientationY, nativePannerNode.orientationY, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.orientationZ, nativePannerNode.orientationZ, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.positionX, nativePannerNode.positionX, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.positionY, nativePannerNode.positionY, trace);\n                await renderAutomation(nativeOfflineAudioContext, proxy.positionZ, nativePannerNode.positionZ, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.orientationX, nativePannerNode.orientationX, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.orientationY, nativePannerNode.orientationY, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.orientationZ, nativePannerNode.orientationZ, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.positionX, nativePannerNode.positionX, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.positionY, nativePannerNode.positionY, trace);\n                await connectAudioParam(nativeOfflineAudioContext, proxy.positionZ, nativePannerNode.positionZ, trace);\n            }\n            if (isNativeAudioNodeFaker(nativePannerNode)) {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativePannerNode.inputs[0], trace);\n            }\n            else {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativePannerNode, trace);\n            }\n            return nativePannerNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeGainNodeOrNativePannerNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeGainNodeOrNativePannerNode !== undefined) {\n                    return Promise.resolve(renderedNativeGainNodeOrNativePannerNode);\n                }\n                return createAudioNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=panner-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/periodic-wave-constructor.js\nconst periodic_wave_constructor_DEFAULT_OPTIONS = {\n    disableNormalization: false\n};\nconst createPeriodicWaveConstructor = (createNativePeriodicWave, getNativeContext, periodicWaveStore, sanitizePeriodicWaveOptions) => {\n    return class PeriodicWave {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = sanitizePeriodicWaveOptions({ ...periodic_wave_constructor_DEFAULT_OPTIONS, ...options });\n            const periodicWave = createNativePeriodicWave(nativeContext, mergedOptions);\n            periodicWaveStore.add(periodicWave);\n            // This does violate all good pratices but it is used here to simplify the handling of periodic waves.\n            return periodicWave;\n        }\n        static [Symbol.hasInstance](instance) {\n            return ((instance !== null && typeof instance === \'object\' && Object.getPrototypeOf(instance) === PeriodicWave.prototype) ||\n                periodicWaveStore.has(instance));\n        }\n    };\n};\n//# sourceMappingURL=periodic-wave-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/render-automation.js\nconst createRenderAutomation = (getAudioParamRenderer, renderInputsOfAudioParam) => {\n    return (nativeOfflineAudioContext, audioParam, nativeAudioParam, trace) => {\n        const audioParamRenderer = getAudioParamRenderer(audioParam);\n        audioParamRenderer.replay(nativeAudioParam);\n        return renderInputsOfAudioParam(audioParam, nativeOfflineAudioContext, nativeAudioParam, trace);\n    };\n};\n//# sourceMappingURL=render-automation.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/render-inputs-of-audio-node.js\nconst createRenderInputsOfAudioNode = (getAudioNodeConnections, getAudioNodeRenderer, isPartOfACycle) => {\n    return async (audioNode, nativeOfflineAudioContext, nativeAudioNode, trace) => {\n        const audioNodeConnections = getAudioNodeConnections(audioNode);\n        const nextTrace = [...trace, audioNode];\n        await Promise.all(audioNodeConnections.activeInputs\n            .map((connections, input) => Array.from(connections)\n            .filter(([source]) => !nextTrace.includes(source))\n            .map(async ([source, output]) => {\n            const audioNodeRenderer = getAudioNodeRenderer(source);\n            const renderedNativeAudioNode = await audioNodeRenderer.render(source, nativeOfflineAudioContext, nextTrace);\n            const destination = audioNode.context.destination;\n            if (!isPartOfACycle(source) && (audioNode !== destination || !isPartOfACycle(audioNode))) {\n                renderedNativeAudioNode.connect(nativeAudioNode, output, input);\n            }\n        }))\n            .reduce((allRenderingPromises, renderingPromises) => [...allRenderingPromises, ...renderingPromises], []));\n    };\n};\n//# sourceMappingURL=render-inputs-of-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/render-inputs-of-audio-param.js\nconst createRenderInputsOfAudioParam = (getAudioNodeRenderer, getAudioParamConnections, isPartOfACycle) => {\n    return async (audioParam, nativeOfflineAudioContext, nativeAudioParam, trace) => {\n        const audioParamConnections = getAudioParamConnections(audioParam);\n        await Promise.all(Array.from(audioParamConnections.activeInputs).map(async ([source, output]) => {\n            const audioNodeRenderer = getAudioNodeRenderer(source);\n            const renderedNativeAudioNode = await audioNodeRenderer.render(source, nativeOfflineAudioContext, trace);\n            if (!isPartOfACycle(source)) {\n                renderedNativeAudioNode.connect(nativeAudioParam, output);\n            }\n        }));\n    };\n};\n//# sourceMappingURL=render-inputs-of-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/render-native-offline-audio-context.js\n\nconst createRenderNativeOfflineAudioContext = (cacheTestResult, createNativeGainNode, createNativeScriptProcessorNode, testOfflineAudioContextCurrentTimeSupport) => {\n    return (nativeOfflineAudioContext) => {\n        // Bug #21: Safari does not support promises yet.\n        if (cacheTestResult(test_promise_support_testPromiseSupport, () => test_promise_support_testPromiseSupport(nativeOfflineAudioContext))) {\n            // Bug #158: Chrome and Edge do not advance currentTime if it is not accessed while rendering the audio.\n            return Promise.resolve(cacheTestResult(testOfflineAudioContextCurrentTimeSupport, testOfflineAudioContextCurrentTimeSupport)).then((isOfflineAudioContextCurrentTimeSupported) => {\n                if (!isOfflineAudioContextCurrentTimeSupported) {\n                    const scriptProcessorNode = createNativeScriptProcessorNode(nativeOfflineAudioContext, 512, 0, 1);\n                    nativeOfflineAudioContext.oncomplete = () => {\n                        scriptProcessorNode.onaudioprocess = null; // tslint:disable-line:deprecation\n                        scriptProcessorNode.disconnect();\n                    };\n                    scriptProcessorNode.onaudioprocess = () => nativeOfflineAudioContext.currentTime; // tslint:disable-line:deprecation\n                    scriptProcessorNode.connect(nativeOfflineAudioContext.destination);\n                }\n                return nativeOfflineAudioContext.startRendering();\n            });\n        }\n        return new Promise((resolve) => {\n            // Bug #48: Safari does not render an OfflineAudioContext without any connected node.\n            const gainNode = createNativeGainNode(nativeOfflineAudioContext, {\n                channelCount: 1,\n                channelCountMode: \'explicit\',\n                channelInterpretation: \'discrete\',\n                gain: 0\n            });\n            nativeOfflineAudioContext.oncomplete = (event) => {\n                gainNode.disconnect();\n                resolve(event.renderedBuffer);\n            };\n            gainNode.connect(nativeOfflineAudioContext.destination);\n            nativeOfflineAudioContext.startRendering();\n        });\n    };\n};\n//# sourceMappingURL=render-native-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/set-active-audio-worklet-node-inputs.js\nconst createSetActiveAudioWorkletNodeInputs = (activeAudioWorkletNodeInputsStore) => {\n    return (nativeAudioWorkletNode, activeInputs) => {\n        activeAudioWorkletNodeInputsStore.set(nativeAudioWorkletNode, activeInputs);\n    };\n};\n//# sourceMappingURL=set-active-audio-worklet-node-inputs.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/set-audio-node-tail-time.js\nconst createSetAudioNodeTailTime = (audioNodeTailTimeStore) => {\n    return (audioNode, tailTime) => audioNodeTailTimeStore.set(audioNode, tailTime);\n};\n//# sourceMappingURL=set-audio-node-tail-time.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/start-rendering.js\n\nconst createStartRendering = (audioBufferStore, cacheTestResult, getAudioNodeRenderer, getUnrenderedAudioWorkletNodes, renderNativeOfflineAudioContext, testAudioBufferCopyChannelMethodsOutOfBoundsSupport, wrapAudioBufferCopyChannelMethods, wrapAudioBufferCopyChannelMethodsOutOfBounds) => {\n    const trace = [];\n    return (destination, nativeOfflineAudioContext) => getAudioNodeRenderer(destination)\n        .render(destination, nativeOfflineAudioContext, trace)\n        /*\n         * Bug #86 & #87: Invoking the renderer of an AudioWorkletNode might be necessary if it has no direct or indirect connection to the\n         * destination.\n         */\n        .then(() => Promise.all(Array.from(getUnrenderedAudioWorkletNodes(nativeOfflineAudioContext)).map((audioWorkletNode) => getAudioNodeRenderer(audioWorkletNode).render(audioWorkletNode, nativeOfflineAudioContext, trace))))\n        .then(() => renderNativeOfflineAudioContext(nativeOfflineAudioContext))\n        .then((audioBuffer) => {\n        // Bug #5: Safari does not support copyFromChannel() and copyToChannel().\n        // Bug #100: Safari does throw a wrong error when calling getChannelData() with an out-of-bounds value.\n        if (typeof audioBuffer.copyFromChannel !== \'function\') {\n            wrapAudioBufferCopyChannelMethods(audioBuffer);\n            wrapAudioBufferGetChannelDataMethod(audioBuffer);\n            // Bug #157: Firefox does not allow the bufferOffset to be out-of-bounds.\n        }\n        else if (!cacheTestResult(testAudioBufferCopyChannelMethodsOutOfBoundsSupport, () => testAudioBufferCopyChannelMethodsOutOfBoundsSupport(audioBuffer))) {\n            wrapAudioBufferCopyChannelMethodsOutOfBounds(audioBuffer);\n        }\n        audioBufferStore.add(audioBuffer);\n        return audioBuffer;\n    });\n};\n//# sourceMappingURL=start-rendering.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/stereo-panner-node-constructor.js\nconst stereo_panner_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    /*\n     * Bug #105: The channelCountMode should be \'clamped-max\' according to the spec but is set to \'explicit\' to achieve consistent\n     * behavior.\n     */\n    channelCountMode: \'explicit\',\n    channelInterpretation: \'speakers\',\n    pan: 0\n};\nconst createStereoPannerNodeConstructor = (audioNodeConstructor, createAudioParam, createNativeStereoPannerNode, createStereoPannerNodeRenderer, getNativeContext, isNativeOfflineAudioContext) => {\n    return class StereoPannerNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...stereo_panner_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeStereoPannerNode = createNativeStereoPannerNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const stereoPannerNodeRenderer = (isOffline ? createStereoPannerNodeRenderer() : null);\n            super(context, false, nativeStereoPannerNode, stereoPannerNodeRenderer);\n            this._pan = createAudioParam(this, isOffline, nativeStereoPannerNode.pan);\n        }\n        get pan() {\n            return this._pan;\n        }\n    };\n};\n//# sourceMappingURL=stereo-panner-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/stereo-panner-node-renderer-factory.js\n\n\nconst createStereoPannerNodeRendererFactory = (connectAudioParam, createNativeStereoPannerNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeStereoPannerNodes = new WeakMap();\n        const createStereoPannerNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeStereoPannerNode = getNativeAudioNode(proxy);\n            /*\n             * If the initially used nativeStereoPannerNode was not constructed on the same OfflineAudioContext it needs to be created\n             * again.\n             */\n            const nativeStereoPannerNodeIsOwnedByContext = isOwnedByContext(nativeStereoPannerNode, nativeOfflineAudioContext);\n            if (!nativeStereoPannerNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeStereoPannerNode.channelCount,\n                    channelCountMode: nativeStereoPannerNode.channelCountMode,\n                    channelInterpretation: nativeStereoPannerNode.channelInterpretation,\n                    pan: nativeStereoPannerNode.pan.value\n                };\n                nativeStereoPannerNode = createNativeStereoPannerNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeStereoPannerNodes.set(nativeOfflineAudioContext, nativeStereoPannerNode);\n            if (!nativeStereoPannerNodeIsOwnedByContext) {\n                await renderAutomation(nativeOfflineAudioContext, proxy.pan, nativeStereoPannerNode.pan, trace);\n            }\n            else {\n                await connectAudioParam(nativeOfflineAudioContext, proxy.pan, nativeStereoPannerNode.pan, trace);\n            }\n            if (isNativeAudioNodeFaker(nativeStereoPannerNode)) {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeStereoPannerNode.inputs[0], trace);\n            }\n            else {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeStereoPannerNode, trace);\n            }\n            return nativeStereoPannerNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeStereoPannerNode = renderedNativeStereoPannerNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeStereoPannerNode !== undefined) {\n                    return Promise.resolve(renderedNativeStereoPannerNode);\n                }\n                return createStereoPannerNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=stereo-panner-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-buffer-constructor-support.js\n// Bug #33: Safari exposes an AudioBuffer but it can\'t be used as a constructor.\nconst createTestAudioBufferConstructorSupport = (nativeAudioBufferConstructor) => {\n    return () => {\n        if (nativeAudioBufferConstructor === null) {\n            return false;\n        }\n        try {\n            new nativeAudioBufferConstructor({ length: 1, sampleRate: 44100 }); // tslint:disable-line:no-unused-expression\n        }\n        catch {\n            return false;\n        }\n        return true;\n    };\n};\n//# sourceMappingURL=test-audio-buffer-constructor-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-buffer-copy-channel-methods-subarray-support.js\n/*\n * Firefox up to version 67 didn\'t fully support the copyFromChannel() and copyToChannel() methods. Therefore testing one of those methods\n * is enough to know if the other one is supported as well.\n */\nconst createTestAudioBufferCopyChannelMethodsSubarraySupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        const nativeAudioBuffer = nativeOfflineAudioContext.createBuffer(1, 1, 44100);\n        // Bug #5: Safari does not support copyFromChannel() and copyToChannel().\n        if (nativeAudioBuffer.copyToChannel === undefined) {\n            return true;\n        }\n        const source = new Float32Array(2);\n        try {\n            nativeAudioBuffer.copyFromChannel(source, 0, 0);\n        }\n        catch {\n            return false;\n        }\n        return true;\n    };\n};\n//# sourceMappingURL=test-audio-buffer-copy-channel-methods-subarray-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-context-close-method-support.js\nconst createTestAudioContextCloseMethodSupport = (nativeAudioContextConstructor) => {\n    return () => {\n        if (nativeAudioContextConstructor === null) {\n            return false;\n        }\n        // Try to check the prototype before constructing the AudioContext.\n        if (nativeAudioContextConstructor.prototype !== undefined && nativeAudioContextConstructor.prototype.close !== undefined) {\n            return true;\n        }\n        const audioContext = new nativeAudioContextConstructor();\n        const isAudioContextClosable = audioContext.close !== undefined;\n        try {\n            audioContext.close();\n        }\n        catch {\n            // Ignore errors.\n        }\n        return isAudioContextClosable;\n    };\n};\n//# sourceMappingURL=test-audio-context-close-method-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-context-decode-audio-data-method-type-error-support.js\n/**\n * Edge up to version 14, Firefox up to version 52, Safari up to version 9 and maybe other browsers\n * did not refuse to decode invalid parameters with a TypeError.\n */\nconst createTestAudioContextDecodeAudioDataMethodTypeErrorSupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return Promise.resolve(false);\n        }\n        const offlineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        // Bug #21: Safari does not support promises yet.\n        return new Promise((resolve) => {\n            let isPending = true;\n            const resolvePromise = (err) => {\n                if (isPending) {\n                    isPending = false;\n                    offlineAudioContext.startRendering();\n                    resolve(err instanceof TypeError);\n                }\n            };\n            let promise;\n            // Bug #26: Safari throws a synchronous error.\n            try {\n                promise = offlineAudioContext\n                    // Bug #1: Safari requires a successCallback.\n                    .decodeAudioData(null, () => {\n                    // Ignore the success callback.\n                }, resolvePromise);\n            }\n            catch (err) {\n                resolvePromise(err);\n            }\n            // Bug #21: Safari does not support promises yet.\n            if (promise !== undefined) {\n                // Bug #6: Chrome, Edge, Firefox and Opera do not call the errorCallback.\n                promise.catch(resolvePromise);\n            }\n        });\n    };\n};\n//# sourceMappingURL=test-audio-context-decode-audio-data-method-type-error-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-context-options-support.js\nconst createTestAudioContextOptionsSupport = (nativeAudioContextConstructor) => {\n    return () => {\n        if (nativeAudioContextConstructor === null) {\n            return false;\n        }\n        let audioContext;\n        try {\n            audioContext = new nativeAudioContextConstructor({ latencyHint: \'balanced\' });\n        }\n        catch {\n            return false;\n        }\n        audioContext.close();\n        return true;\n    };\n};\n//# sourceMappingURL=test-audio-context-options-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-node-connect-method-support.js\n// Safari up to version 12.0 (but not v12.1) didn\'t return the destination in case it was an AudioNode.\nconst createTestAudioNodeConnectMethodSupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        const nativeGainNode = nativeOfflineAudioContext.createGain();\n        const isSupported = nativeGainNode.connect(nativeGainNode) === nativeGainNode;\n        nativeGainNode.disconnect(nativeGainNode);\n        return isSupported;\n    };\n};\n//# sourceMappingURL=test-audio-node-connect-method-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-worklet-processor-no-outputs-support.js\n/**\n * Chrome version 66 and 67 did not call the process() function of an AudioWorkletProcessor if it had no outputs. AudioWorklet support was\n * enabled by default in version 66.\n */\nconst createTestAudioWorkletProcessorNoOutputsSupport = (nativeAudioWorkletNodeConstructor, nativeOfflineAudioContextConstructor) => {\n    return async () => {\n        // Bug #61: If there is no native AudioWorkletNode it gets faked and therefore it is no problem if the it doesn\'t exist.\n        if (nativeAudioWorkletNodeConstructor === null) {\n            return true;\n        }\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const blob = new Blob([\'class A extends AudioWorkletProcessor{process(){this.port.postMessage(0)}}registerProcessor("a",A)\'], {\n            type: \'application/javascript; charset=utf-8\'\n        });\n        const offlineAudioContext = new nativeOfflineAudioContextConstructor(1, 128, 8000);\n        const url = URL.createObjectURL(blob);\n        let isCallingProcess = false;\n        try {\n            await offlineAudioContext.audioWorklet.addModule(url);\n            const audioWorkletNode = new nativeAudioWorkletNodeConstructor(offlineAudioContext, \'a\', { numberOfOutputs: 0 });\n            const oscillator = offlineAudioContext.createOscillator();\n            audioWorkletNode.port.onmessage = () => (isCallingProcess = true);\n            oscillator.connect(audioWorkletNode);\n            oscillator.start(0);\n            await offlineAudioContext.startRendering();\n            if (!isCallingProcess) {\n                await new Promise((resolve) => setTimeout(resolve, 5));\n            }\n        }\n        catch {\n            // Ignore errors.\n        }\n        finally {\n            URL.revokeObjectURL(url);\n        }\n        return isCallingProcess;\n    };\n};\n//# sourceMappingURL=test-audio-worklet-processor-no-outputs-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-audio-worklet-processor-post-message-support.js\n// Bug #179: Firefox does not allow to transfer any buffer which has been passed to the process() method as an argument.\nconst createTestAudioWorkletProcessorPostMessageSupport = (nativeAudioWorkletNodeConstructor, nativeOfflineAudioContextConstructor) => {\n    return async () => {\n        // Bug #61: If there is no native AudioWorkletNode it gets faked and therefore it is no problem if the it doesn\'t exist.\n        if (nativeAudioWorkletNodeConstructor === null) {\n            return true;\n        }\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const blob = new Blob([\'class A extends AudioWorkletProcessor{process(i){this.port.postMessage(i,[i[0][0].buffer])}}registerProcessor("a",A)\'], {\n            type: \'application/javascript; charset=utf-8\'\n        });\n        const offlineAudioContext = new nativeOfflineAudioContextConstructor(1, 128, 8000);\n        const url = URL.createObjectURL(blob);\n        let isEmittingMessageEvents = false;\n        let isEmittingProcessorErrorEvents = false;\n        try {\n            await offlineAudioContext.audioWorklet.addModule(url);\n            const audioWorkletNode = new nativeAudioWorkletNodeConstructor(offlineAudioContext, \'a\', { numberOfOutputs: 0 });\n            const oscillator = offlineAudioContext.createOscillator();\n            audioWorkletNode.port.onmessage = () => (isEmittingMessageEvents = true);\n            audioWorkletNode.onprocessorerror = () => (isEmittingProcessorErrorEvents = true);\n            oscillator.connect(audioWorkletNode);\n            await offlineAudioContext.startRendering();\n        }\n        catch {\n            // Ignore errors.\n        }\n        finally {\n            URL.revokeObjectURL(url);\n        }\n        return isEmittingMessageEvents && !isEmittingProcessorErrorEvents;\n    };\n};\n//# sourceMappingURL=test-audio-worklet-processor-post-message-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-channel-merger-node-channel-count-support.js\n/**\n * Firefox up to version 69 did not throw an error when setting a different channelCount or channelCountMode.\n */\nconst createTestChannelMergerNodeChannelCountSupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const offlineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        const nativeChannelMergerNode = offlineAudioContext.createChannelMerger();\n        /**\n         * Bug #15: Safari does not return the default properties. It still needs to be patched. This test is supposed to test the support\n         * in other browsers.\n         */\n        if (nativeChannelMergerNode.channelCountMode === \'max\') {\n            return true;\n        }\n        try {\n            nativeChannelMergerNode.channelCount = 2;\n        }\n        catch {\n            return true;\n        }\n        return false;\n    };\n};\n//# sourceMappingURL=test-channel-merger-node-channel-count-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-constant-source-node-accurate-scheduling-support.js\nconst createTestConstantSourceNodeAccurateSchedulingSupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        // Bug #62: Safari does not support ConstantSourceNodes.\n        if (nativeOfflineAudioContext.createConstantSource === undefined) {\n            return true;\n        }\n        const nativeConstantSourceNode = nativeOfflineAudioContext.createConstantSource();\n        /*\n         * @todo This is using bug #75 to detect bug #70. That works because both bugs were unique to\n         * the implementation of Firefox right now, but it could probably be done in a better way.\n         */\n        return nativeConstantSourceNode.offset.maxValue !== Number.POSITIVE_INFINITY;\n    };\n};\n//# sourceMappingURL=test-constant-source-node-accurate-scheduling-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-convolver-node-buffer-reassignability-support.js\n// Opera up to version 57 did not allow to reassign the buffer of a ConvolverNode.\nconst createTestConvolverNodeBufferReassignabilitySupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const offlineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        const nativeConvolverNode = offlineAudioContext.createConvolver();\n        nativeConvolverNode.buffer = offlineAudioContext.createBuffer(1, 1, offlineAudioContext.sampleRate);\n        try {\n            nativeConvolverNode.buffer = offlineAudioContext.createBuffer(1, 1, offlineAudioContext.sampleRate);\n        }\n        catch {\n            return false;\n        }\n        return true;\n    };\n};\n//# sourceMappingURL=test-convolver-node-buffer-reassignability-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-convolver-node-channel-count-support.js\n// Chrome up to version v80, Edge up to version v80 and Opera up to version v67 did not allow to set the channelCount property of a ConvolverNode to 1. They also did not allow to set the channelCountMode to \'explicit\'.\nconst createTestConvolverNodeChannelCountSupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return false;\n        }\n        const offlineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        const nativeConvolverNode = offlineAudioContext.createConvolver();\n        try {\n            nativeConvolverNode.channelCount = 1;\n        }\n        catch {\n            return false;\n        }\n        return true;\n    };\n};\n//# sourceMappingURL=test-convolver-node-channel-count-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-is-secure-context-support.js\nconst createTestIsSecureContextSupport = (window) => {\n    return () => window !== null && window.hasOwnProperty(\'isSecureContext\');\n};\n//# sourceMappingURL=test-is-secure-context-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-media-stream-audio-source-node-media-stream-without-audio-track-support.js\n// Firefox up to version 68 did not throw an error when creating a MediaStreamAudioSourceNode with a mediaStream that had no audio track.\nconst createTestMediaStreamAudioSourceNodeMediaStreamWithoutAudioTrackSupport = (nativeAudioContextConstructor) => {\n    return () => {\n        if (nativeAudioContextConstructor === null) {\n            return false;\n        }\n        const audioContext = new nativeAudioContextConstructor();\n        try {\n            audioContext.createMediaStreamSource(new MediaStream());\n            return false;\n        }\n        catch (err) {\n            return true;\n        }\n    };\n};\n//# sourceMappingURL=test-media-stream-audio-source-node-media-stream-without-audio-track-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-offline-audio-context-current-time-support.js\nconst createTestOfflineAudioContextCurrentTimeSupport = (createNativeGainNode, nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return Promise.resolve(false);\n        }\n        const nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        // Bug #48: Safari does not render an OfflineAudioContext without any connected node.\n        const gainNode = createNativeGainNode(nativeOfflineAudioContext, {\n            channelCount: 1,\n            channelCountMode: \'explicit\',\n            channelInterpretation: \'discrete\',\n            gain: 0\n        });\n        // Bug #21: Safari does not support promises yet.\n        return new Promise((resolve) => {\n            nativeOfflineAudioContext.oncomplete = () => {\n                gainNode.disconnect();\n                resolve(nativeOfflineAudioContext.currentTime !== 0);\n            };\n            nativeOfflineAudioContext.startRendering();\n        });\n    };\n};\n//# sourceMappingURL=test-offline-audio-context-current-time-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/test-stereo-panner-node-default-value-support.js\n/**\n * Firefox up to version 62 did not kick off the processing of the StereoPannerNode if the value of pan was zero.\n */\nconst createTestStereoPannerNodeDefaultValueSupport = (nativeOfflineAudioContextConstructor) => {\n    return () => {\n        if (nativeOfflineAudioContextConstructor === null) {\n            return Promise.resolve(false);\n        }\n        const nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor(1, 1, 44100);\n        /*\n         * Bug #105: Safari does not support the StereoPannerNode. Therefore the returned value should normally be false but the faker does\n         * support the tested behaviour.\n         */\n        if (nativeOfflineAudioContext.createStereoPanner === undefined) {\n            return Promise.resolve(true);\n        }\n        // Bug #62: Safari does not support ConstantSourceNodes.\n        if (nativeOfflineAudioContext.createConstantSource === undefined) {\n            return Promise.resolve(true);\n        }\n        const constantSourceNode = nativeOfflineAudioContext.createConstantSource();\n        const stereoPanner = nativeOfflineAudioContext.createStereoPanner();\n        constantSourceNode.channelCount = 1;\n        constantSourceNode.offset.value = 1;\n        stereoPanner.channelCount = 1;\n        constantSourceNode.start();\n        constantSourceNode.connect(stereoPanner).connect(nativeOfflineAudioContext.destination);\n        return nativeOfflineAudioContext.startRendering().then((buffer) => buffer.getChannelData(0)[0] !== 1);\n    };\n};\n//# sourceMappingURL=test-stereo-panner-node-default-value-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/unknown-error.js\nconst unknown_error_createUnknownError = () => new DOMException(\'\', \'UnknownError\');\n//# sourceMappingURL=unknown-error.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/wave-shaper-node-constructor.js\nconst wave_shaper_node_constructor_DEFAULT_OPTIONS = {\n    channelCount: 2,\n    channelCountMode: \'max\',\n    channelInterpretation: \'speakers\',\n    curve: null,\n    oversample: \'none\'\n};\nconst createWaveShaperNodeConstructor = (audioNodeConstructor, createInvalidStateError, createNativeWaveShaperNode, createWaveShaperNodeRenderer, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime) => {\n    return class WaveShaperNode extends audioNodeConstructor {\n        constructor(context, options) {\n            const nativeContext = getNativeContext(context);\n            const mergedOptions = { ...wave_shaper_node_constructor_DEFAULT_OPTIONS, ...options };\n            const nativeWaveShaperNode = createNativeWaveShaperNode(nativeContext, mergedOptions);\n            const isOffline = isNativeOfflineAudioContext(nativeContext);\n            const waveShaperNodeRenderer = (isOffline ? createWaveShaperNodeRenderer() : null);\n            // @todo Add a mechanism to only switch a WaveShaperNode to active while it is connected.\n            super(context, true, nativeWaveShaperNode, waveShaperNodeRenderer);\n            this._isCurveNullified = false;\n            this._nativeWaveShaperNode = nativeWaveShaperNode;\n            // @todo Determine a meaningful tail-time instead of just using one second.\n            setAudioNodeTailTime(this, 1);\n        }\n        get curve() {\n            if (this._isCurveNullified) {\n                return null;\n            }\n            return this._nativeWaveShaperNode.curve;\n        }\n        set curve(value) {\n            // Bug #103: Safari does not allow to set the curve to null.\n            if (value === null) {\n                this._isCurveNullified = true;\n                this._nativeWaveShaperNode.curve = new Float32Array([0, 0]);\n            }\n            else {\n                // Bug #102: Safari does not throw an InvalidStateError when the curve has less than two samples.\n                // Bug #104: Chrome, Edge and Opera will throw an InvalidAccessError when the curve has less than two samples.\n                if (value.length < 2) {\n                    throw createInvalidStateError();\n                }\n                this._isCurveNullified = false;\n                this._nativeWaveShaperNode.curve = value;\n            }\n        }\n        get oversample() {\n            return this._nativeWaveShaperNode.oversample;\n        }\n        set oversample(value) {\n            this._nativeWaveShaperNode.oversample = value;\n        }\n    };\n};\n//# sourceMappingURL=wave-shaper-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/wave-shaper-node-renderer-factory.js\n\n\nconst createWaveShaperNodeRendererFactory = (createNativeWaveShaperNode, getNativeAudioNode, renderInputsOfAudioNode) => {\n    return () => {\n        const renderedNativeWaveShaperNodes = new WeakMap();\n        const createWaveShaperNode = async (proxy, nativeOfflineAudioContext, trace) => {\n            let nativeWaveShaperNode = getNativeAudioNode(proxy);\n            // If the initially used nativeWaveShaperNode was not constructed on the same OfflineAudioContext it needs to be created again.\n            const nativeWaveShaperNodeIsOwnedByContext = isOwnedByContext(nativeWaveShaperNode, nativeOfflineAudioContext);\n            if (!nativeWaveShaperNodeIsOwnedByContext) {\n                const options = {\n                    channelCount: nativeWaveShaperNode.channelCount,\n                    channelCountMode: nativeWaveShaperNode.channelCountMode,\n                    channelInterpretation: nativeWaveShaperNode.channelInterpretation,\n                    curve: nativeWaveShaperNode.curve,\n                    oversample: nativeWaveShaperNode.oversample\n                };\n                nativeWaveShaperNode = createNativeWaveShaperNode(nativeOfflineAudioContext, options);\n            }\n            renderedNativeWaveShaperNodes.set(nativeOfflineAudioContext, nativeWaveShaperNode);\n            if (isNativeAudioNodeFaker(nativeWaveShaperNode)) {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeWaveShaperNode.inputs[0], trace);\n            }\n            else {\n                await renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeWaveShaperNode, trace);\n            }\n            return nativeWaveShaperNode;\n        };\n        return {\n            render(proxy, nativeOfflineAudioContext, trace) {\n                const renderedNativeWaveShaperNode = renderedNativeWaveShaperNodes.get(nativeOfflineAudioContext);\n                if (renderedNativeWaveShaperNode !== undefined) {\n                    return Promise.resolve(renderedNativeWaveShaperNode);\n                }\n                return createWaveShaperNode(proxy, nativeOfflineAudioContext, trace);\n            }\n        };\n    };\n};\n//# sourceMappingURL=wave-shaper-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/window.js\nconst createWindow = () => (typeof window === \'undefined\' ? null : window);\n//# sourceMappingURL=window.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/wrap-audio-buffer-copy-channel-methods.js\nconst createWrapAudioBufferCopyChannelMethods = (convertNumberToUnsignedLong, createIndexSizeError) => {\n    return (audioBuffer) => {\n        audioBuffer.copyFromChannel = (destination, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {\n            const bufferOffset = convertNumberToUnsignedLong(bufferOffsetAsNumber);\n            const channelNumber = convertNumberToUnsignedLong(channelNumberAsNumber);\n            if (channelNumber >= audioBuffer.numberOfChannels) {\n                throw createIndexSizeError();\n            }\n            const audioBufferLength = audioBuffer.length;\n            const channelData = audioBuffer.getChannelData(channelNumber);\n            const destinationLength = destination.length;\n            for (let i = bufferOffset < 0 ? -bufferOffset : 0; i + bufferOffset < audioBufferLength && i < destinationLength; i += 1) {\n                destination[i] = channelData[i + bufferOffset];\n            }\n        };\n        audioBuffer.copyToChannel = (source, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {\n            const bufferOffset = convertNumberToUnsignedLong(bufferOffsetAsNumber);\n            const channelNumber = convertNumberToUnsignedLong(channelNumberAsNumber);\n            if (channelNumber >= audioBuffer.numberOfChannels) {\n                throw createIndexSizeError();\n            }\n            const audioBufferLength = audioBuffer.length;\n            const channelData = audioBuffer.getChannelData(channelNumber);\n            const sourceLength = source.length;\n            for (let i = bufferOffset < 0 ? -bufferOffset : 0; i + bufferOffset < audioBufferLength && i < sourceLength; i += 1) {\n                channelData[i + bufferOffset] = source[i];\n            }\n        };\n    };\n};\n//# sourceMappingURL=wrap-audio-buffer-copy-channel-methods.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/wrap-audio-buffer-copy-channel-methods-out-of-bounds.js\nconst createWrapAudioBufferCopyChannelMethodsOutOfBounds = (convertNumberToUnsignedLong) => {\n    return (audioBuffer) => {\n        audioBuffer.copyFromChannel = ((copyFromChannel) => {\n            return (destination, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {\n                const bufferOffset = convertNumberToUnsignedLong(bufferOffsetAsNumber);\n                const channelNumber = convertNumberToUnsignedLong(channelNumberAsNumber);\n                if (bufferOffset < audioBuffer.length) {\n                    return copyFromChannel.call(audioBuffer, destination, channelNumber, bufferOffset);\n                }\n            };\n        })(audioBuffer.copyFromChannel);\n        audioBuffer.copyToChannel = ((copyToChannel) => {\n            return (source, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {\n                const bufferOffset = convertNumberToUnsignedLong(bufferOffsetAsNumber);\n                const channelNumber = convertNumberToUnsignedLong(channelNumberAsNumber);\n                if (bufferOffset < audioBuffer.length) {\n                    return copyToChannel.call(audioBuffer, source, channelNumber, bufferOffset);\n                }\n            };\n        })(audioBuffer.copyToChannel);\n    };\n};\n//# sourceMappingURL=wrap-audio-buffer-copy-channel-methods-out-of-bounds.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/wrap-audio-buffer-source-node-stop-method-nullified-buffer.js\nconst createWrapAudioBufferSourceNodeStopMethodNullifiedBuffer = (overwriteAccessors) => {\n    return (nativeAudioBufferSourceNode, nativeContext) => {\n        const nullifiedBuffer = nativeContext.createBuffer(1, 1, 44100);\n        if (nativeAudioBufferSourceNode.buffer === null) {\n            nativeAudioBufferSourceNode.buffer = nullifiedBuffer;\n        }\n        overwriteAccessors(nativeAudioBufferSourceNode, \'buffer\', (get) => () => {\n            const value = get.call(nativeAudioBufferSourceNode);\n            return value === nullifiedBuffer ? null : value;\n        }, (set) => (value) => {\n            return set.call(nativeAudioBufferSourceNode, value === null ? nullifiedBuffer : value);\n        });\n    };\n};\n//# sourceMappingURL=wrap-audio-buffer-source-node-stop-method-nullified-buffer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/factories/wrap-channel-merger-node.js\nconst createWrapChannelMergerNode = (createInvalidStateError, monitorConnections) => {\n    return (nativeContext, channelMergerNode) => {\n        // Bug #15: Safari does not return the default properties.\n        channelMergerNode.channelCount = 1;\n        channelMergerNode.channelCountMode = \'explicit\';\n        // Bug #16: Safari does not throw an error when setting a different channelCount or channelCountMode.\n        Object.defineProperty(channelMergerNode, \'channelCount\', {\n            get: () => 1,\n            set: () => {\n                throw createInvalidStateError();\n            }\n        });\n        Object.defineProperty(channelMergerNode, \'channelCountMode\', {\n            get: () => \'explicit\',\n            set: () => {\n                throw createInvalidStateError();\n            }\n        });\n        // Bug #20: Safari requires a connection of any kind to treat the input signal correctly.\n        const audioBufferSourceNode = nativeContext.createBufferSource();\n        const whenConnected = () => {\n            const length = channelMergerNode.numberOfInputs;\n            for (let i = 0; i < length; i += 1) {\n                audioBufferSourceNode.connect(channelMergerNode, 0, i);\n            }\n        };\n        const whenDisconnected = () => audioBufferSourceNode.disconnect(channelMergerNode);\n        monitorConnections(channelMergerNode, whenConnected, whenDisconnected);\n    };\n};\n//# sourceMappingURL=wrap-channel-merger-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/is-dc-curve.js\nconst is_dc_curve_isDCCurve = (curve) => {\n    if (curve === null) {\n        return false;\n    }\n    const length = curve.length;\n    if (length % 2 !== 0) {\n        return curve[Math.floor(length / 2)] !== 0;\n    }\n    return curve[length / 2 - 1] + curve[length / 2] !== 0;\n};\n//# sourceMappingURL=is-dc-curve.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/overwrite-accessors.js\nconst overwrite_accessors_overwriteAccessors = (object, property, createGetter, createSetter) => {\n    let prototype = Object.getPrototypeOf(object);\n    while (!prototype.hasOwnProperty(property)) {\n        prototype = Object.getPrototypeOf(prototype);\n    }\n    const { get, set } = Object.getOwnPropertyDescriptor(prototype, property);\n    Object.defineProperty(object, property, { get: createGetter(get), set: createSetter(set) });\n};\n//# sourceMappingURL=overwrite-accessors.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/sanitize-audio-worklet-node-options.js\nconst sanitize_audio_worklet_node_options_sanitizeAudioWorkletNodeOptions = (options) => {\n    return {\n        ...options,\n        outputChannelCount: options.outputChannelCount !== undefined\n            ? options.outputChannelCount\n            : options.numberOfInputs === 1 && options.numberOfOutputs === 1\n                ? /*\n                   * Bug #61: This should be the computedNumberOfChannels, but unfortunately that is almost impossible to fake. That\'s why\n                   * the channelCountMode is required to be \'explicit\' as long as there is not a native implementation in every browser. That\n                   * makes sure the computedNumberOfChannels is equivilant to the channelCount which makes it much easier to compute.\n                   */\n                    [options.channelCount]\n                : Array.from({ length: options.numberOfOutputs }, () => 1)\n    };\n};\n//# sourceMappingURL=sanitize-audio-worklet-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/sanitize-channel-splitter-options.js\nconst sanitizeChannelSplitterOptions = (options) => {\n    return { ...options, channelCount: options.numberOfOutputs };\n};\n//# sourceMappingURL=sanitize-channel-splitter-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/sanitize-periodic-wave-options.js\nconst sanitizePeriodicWaveOptions = (options) => {\n    const { imag, real } = options;\n    if (imag === undefined) {\n        if (real === undefined) {\n            return { ...options, imag: [0, 0], real: [0, 0] };\n        }\n        return { ...options, imag: Array.from(real, () => 0), real };\n    }\n    if (real === undefined) {\n        return { ...options, imag, real: Array.from(imag, () => 0) };\n    }\n    return { ...options, imag, real };\n};\n//# sourceMappingURL=sanitize-periodic-wave-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/set-value-at-time-until-possible.js\nconst set_value_at_time_until_possible_setValueAtTimeUntilPossible = (audioParam, value, startTime) => {\n    try {\n        audioParam.setValueAtTime(value, startTime);\n    }\n    catch (err) {\n        if (err.code !== 9) {\n            throw err;\n        }\n        set_value_at_time_until_possible_setValueAtTimeUntilPossible(audioParam, value, startTime + 1e-7);\n    }\n};\n//# sourceMappingURL=set-value-at-time-until-possible.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-source-node-start-method-consecutive-calls-support.js\nconst test_audio_buffer_source_node_start_method_consecutive_calls_support_testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport = (nativeContext) => {\n    const nativeAudioBufferSourceNode = nativeContext.createBufferSource();\n    nativeAudioBufferSourceNode.start();\n    try {\n        nativeAudioBufferSourceNode.start();\n    }\n    catch {\n        return true;\n    }\n    return false;\n};\n//# sourceMappingURL=test-audio-buffer-source-node-start-method-consecutive-calls-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-source-node-start-method-offset-clamping-support.js\nconst test_audio_buffer_source_node_start_method_offset_clamping_support_testAudioBufferSourceNodeStartMethodOffsetClampingSupport = (nativeContext) => {\n    const nativeAudioBufferSourceNode = nativeContext.createBufferSource();\n    const nativeAudioBuffer = nativeContext.createBuffer(1, 1, 44100);\n    nativeAudioBufferSourceNode.buffer = nativeAudioBuffer;\n    try {\n        nativeAudioBufferSourceNode.start(0, 1);\n    }\n    catch {\n        return false;\n    }\n    return true;\n};\n//# sourceMappingURL=test-audio-buffer-source-node-start-method-offset-clamping-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-source-node-stop-method-nullified-buffer-support.js\nconst test_audio_buffer_source_node_stop_method_nullified_buffer_support_testAudioBufferSourceNodeStopMethodNullifiedBufferSupport = (nativeContext) => {\n    const nativeAudioBufferSourceNode = nativeContext.createBufferSource();\n    nativeAudioBufferSourceNode.start();\n    try {\n        nativeAudioBufferSourceNode.stop();\n    }\n    catch {\n        return false;\n    }\n    return true;\n};\n//# sourceMappingURL=test-audio-buffer-source-node-stop-method-nullified-buffer-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-scheduled-source-node-start-method-negative-parameters-support.js\nconst test_audio_scheduled_source_node_start_method_negative_parameters_support_testAudioScheduledSourceNodeStartMethodNegativeParametersSupport = (nativeContext) => {\n    const nativeAudioBufferSourceNode = nativeContext.createOscillator();\n    try {\n        nativeAudioBufferSourceNode.start(-1);\n    }\n    catch (err) {\n        return err instanceof RangeError;\n    }\n    return false;\n};\n//# sourceMappingURL=test-audio-scheduled-source-node-start-method-negative-parameters-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-scheduled-source-node-stop-method-consecutive-calls-support.js\nconst test_audio_scheduled_source_node_stop_method_consecutive_calls_support_testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport = (nativeContext) => {\n    const nativeAudioBuffer = nativeContext.createBuffer(1, 1, 44100);\n    const nativeAudioBufferSourceNode = nativeContext.createBufferSource();\n    nativeAudioBufferSourceNode.buffer = nativeAudioBuffer;\n    nativeAudioBufferSourceNode.start();\n    nativeAudioBufferSourceNode.stop();\n    try {\n        nativeAudioBufferSourceNode.stop();\n        return true;\n    }\n    catch {\n        return false;\n    }\n};\n//# sourceMappingURL=test-audio-scheduled-source-node-stop-method-consecutive-calls-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-audio-scheduled-source-node-stop-method-negative-parameters-support.js\nconst test_audio_scheduled_source_node_stop_method_negative_parameters_support_testAudioScheduledSourceNodeStopMethodNegativeParametersSupport = (nativeContext) => {\n    const nativeAudioBufferSourceNode = nativeContext.createOscillator();\n    try {\n        nativeAudioBufferSourceNode.stop(-1);\n    }\n    catch (err) {\n        return err instanceof RangeError;\n    }\n    return false;\n};\n//# sourceMappingURL=test-audio-scheduled-source-node-stop-method-negative-parameters-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-dom-exception-constructor-support.js\n/*\n * Bug #122: Edge up to version v18 did not allow to construct a DOMException\'. It also had a couple more bugs but since this is easy to\n * test it\'s used here as a placeholder.\n *\n * Bug #27: Edge up to version v18 did reject an invalid arrayBuffer passed to decodeAudioData() with a DOMException.\n *\n * Bug #50: Edge up to version v18 did not allow to create AudioNodes on a closed context.\n *\n * Bug #57: Edge up to version v18 did not throw an error when assigning the type of an OscillatorNode to \'custom\'.\n *\n * Bug #63: Edge up to version v18 did not expose the mediaElement property of a MediaElementAudioSourceNode.\n *\n * Bug #64: Edge up to version v18 did not support the MediaStreamAudioDestinationNode.\n *\n * Bug #71: Edge up to version v18 did not allow to set the buffer of an AudioBufferSourceNode to null.\n *\n * Bug #93: Edge up to version v18 did set the sampleRate of an AudioContext to zero when it was closed.\n *\n * Bug #101: Edge up to version v18 refused to execute decodeAudioData() on a closed context.\n *\n * Bug #106: Edge up to version v18 did not expose the maxValue and minValue properties of the pan AudioParam of a StereoPannerNode.\n *\n * Bug #110: Edge up to version v18 did not expose the maxValue and minValue properties of the attack, knee, ratio, release and threshold AudioParams of a DynamicsCompressorNode.\n *\n * Bug #123: Edge up to version v18 did not support HRTF as the panningModel for a PannerNode.\n *\n * Bug #145: Edge up to version v18 did throw an IndexSizeError when an OfflineAudioContext was created with a sampleRate of zero.\n *\n * Bug #161: Edge up to version v18 did not expose the maxValue and minValue properties of the delayTime AudioParam of a DelayNode.\n */\nconst testDomExceptionConstructorSupport = () => {\n    try {\n        new DOMException(); // tslint:disable-line:no-unused-expression\n    }\n    catch {\n        return false;\n    }\n    return true;\n};\n//# sourceMappingURL=test-dom-exception-constructor-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/test-transferables-support.js\n// Safari at version 11 did not support transferables.\nconst testTransferablesSupport = () => new Promise((resolve) => {\n    const arrayBuffer = new ArrayBuffer(0);\n    const { port1, port2 } = new MessageChannel();\n    port1.onmessage = ({ data }) => resolve(data !== null);\n    port2.postMessage(arrayBuffer, [arrayBuffer]);\n});\n//# sourceMappingURL=test-transferables-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-buffer-source-node-start-method-offset-clamping.js\nconst wrapAudioBufferSourceNodeStartMethodOffsetClamping = (nativeAudioBufferSourceNode) => {\n    nativeAudioBufferSourceNode.start = ((start) => {\n        return (when = 0, offset = 0, duration) => {\n            const buffer = nativeAudioBufferSourceNode.buffer;\n            // Bug #154: Safari does not clamp the offset if it is equal to or greater than the duration of the buffer.\n            const clampedOffset = buffer === null ? offset : Math.min(buffer.duration, offset);\n            // Bug #155: Safari does not handle the offset correctly if it would cause the buffer to be not be played at all.\n            if (buffer !== null && clampedOffset > buffer.duration - 0.5 / nativeAudioBufferSourceNode.context.sampleRate) {\n                start.call(nativeAudioBufferSourceNode, when, 0, 0);\n            }\n            else {\n                start.call(nativeAudioBufferSourceNode, when, clampedOffset, duration);\n            }\n        };\n    })(nativeAudioBufferSourceNode.start);\n};\n//# sourceMappingURL=wrap-audio-buffer-source-node-start-method-offset-clamping.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-scheduled-source-node-stop-method-consecutive-calls.js\n\nconst wrap_audio_scheduled_source_node_stop_method_consecutive_calls_wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls = (nativeAudioScheduledSourceNode, nativeContext) => {\n    const nativeGainNode = nativeContext.createGain();\n    nativeAudioScheduledSourceNode.connect(nativeGainNode);\n    const disconnectGainNode = ((disconnect) => {\n        return () => {\n            // @todo TypeScript cannot infer the overloaded signature with 1 argument yet.\n            disconnect.call(nativeAudioScheduledSourceNode, nativeGainNode);\n            nativeAudioScheduledSourceNode.removeEventListener(\'ended\', disconnectGainNode);\n        };\n    })(nativeAudioScheduledSourceNode.disconnect);\n    nativeAudioScheduledSourceNode.addEventListener(\'ended\', disconnectGainNode);\n    interceptConnections(nativeAudioScheduledSourceNode, nativeGainNode);\n    nativeAudioScheduledSourceNode.stop = ((stop) => {\n        let isStopped = false;\n        return (when = 0) => {\n            if (isStopped) {\n                try {\n                    stop.call(nativeAudioScheduledSourceNode, when);\n                }\n                catch {\n                    nativeGainNode.gain.setValueAtTime(0, when);\n                }\n            }\n            else {\n                stop.call(nativeAudioScheduledSourceNode, when);\n                isStopped = true;\n            }\n        };\n    })(nativeAudioScheduledSourceNode.stop);\n};\n//# sourceMappingURL=wrap-audio-scheduled-source-node-stop-method-consecutive-calls.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/helpers/wrap-event-listener.js\nconst wrap_event_listener_wrapEventListener = (target, eventListener) => {\n    return (event) => {\n        const descriptor = { value: target };\n        Object.defineProperties(event, {\n            currentTarget: descriptor,\n            target: descriptor\n        });\n        if (typeof eventListener === \'function\') {\n            return eventListener.call(target, event);\n        }\n        return eventListener.handleEvent.call(target, event);\n    };\n};\n//# sourceMappingURL=wrap-event-listener.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/analyser-node.js\n\n//# sourceMappingURL=analyser-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/analyser-options.js\n\n//# sourceMappingURL=analyser-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-buffer.js\n\n//# sourceMappingURL=audio-buffer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-buffer-options.js\n\n//# sourceMappingURL=audio-buffer-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-buffer-source-node.js\n\n//# sourceMappingURL=audio-buffer-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-buffer-source-node-renderer.js\n\n//# sourceMappingURL=audio-buffer-source-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-buffer-source-options.js\n\n//# sourceMappingURL=audio-buffer-source-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-context.js\n\n//# sourceMappingURL=audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-context-options.js\n\n//# sourceMappingURL=audio-context-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-destination-node.js\n\n//# sourceMappingURL=audio-destination-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-listener.js\n\n//# sourceMappingURL=audio-listener.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-node.js\n\n//# sourceMappingURL=audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-node-options.js\n\n//# sourceMappingURL=audio-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-node-renderer.js\n\n//# sourceMappingURL=audio-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-param.js\n\n//# sourceMappingURL=audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-param-descriptor.js\n\n//# sourceMappingURL=audio-param-descriptor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-param-renderer.js\n\n//# sourceMappingURL=audio-param-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-scheduled-source-node.js\n\n//# sourceMappingURL=audio-scheduled-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-scheduled-source-node-event-map.js\n\n//# sourceMappingURL=audio-scheduled-source-node-event-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-worklet.js\n\n//# sourceMappingURL=audio-worklet.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-worklet-node.js\n\n//# sourceMappingURL=audio-worklet-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-worklet-node-event-map.js\n\n//# sourceMappingURL=audio-worklet-node-event-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-worklet-node-options.js\n\n//# sourceMappingURL=audio-worklet-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-worklet-processor.js\n\n//# sourceMappingURL=audio-worklet-processor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/audio-worklet-processor-constructor.js\n\n//# sourceMappingURL=audio-worklet-processor-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/automation.js\n\n//# sourceMappingURL=automation.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/base-audio-context.js\n\n//# sourceMappingURL=base-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/biquad-filter-node.js\n\n//# sourceMappingURL=biquad-filter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/biquad-filter-options.js\n\n//# sourceMappingURL=biquad-filter-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/channel-merger-options.js\n\n//# sourceMappingURL=channel-merger-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/channel-splitter-options.js\n\n//# sourceMappingURL=channel-splitter-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/common-audio-context.js\n\n//# sourceMappingURL=common-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/common-offline-audio-context.js\n\n//# sourceMappingURL=common-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/constant-source-node.js\n\n//# sourceMappingURL=constant-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/constant-source-node-renderer.js\n\n//# sourceMappingURL=constant-source-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/constant-source-options.js\n\n//# sourceMappingURL=constant-source-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/convolver-node.js\n\n//# sourceMappingURL=convolver-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/convolver-options.js\n\n//# sourceMappingURL=convolver-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/delay-node.js\n\n//# sourceMappingURL=delay-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/delay-options.js\n\n//# sourceMappingURL=delay-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/dynamics-compressor-node.js\n\n//# sourceMappingURL=dynamics-compressor-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/dynamics-compressor-options.js\n\n//# sourceMappingURL=dynamics-compressor-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/gain-node.js\n\n//# sourceMappingURL=gain-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/gain-options.js\n\n//# sourceMappingURL=gain-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/iir-filter-node.js\n\n//# sourceMappingURL=iir-filter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/iir-filter-options.js\n\n//# sourceMappingURL=iir-filter-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-element-audio-source-node.js\n\n//# sourceMappingURL=media-element-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-element-audio-source-options.js\n\n//# sourceMappingURL=media-element-audio-source-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-stream-audio-destination-node.js\n\n//# sourceMappingURL=media-stream-audio-destination-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-stream-audio-source-node.js\n\n//# sourceMappingURL=media-stream-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-stream-audio-source-options.js\n\n//# sourceMappingURL=media-stream-audio-source-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-stream-track-audio-source-node.js\n\n//# sourceMappingURL=media-stream-track-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/media-stream-track-audio-source-options.js\n\n//# sourceMappingURL=media-stream-track-audio-source-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/minimal-audio-context.js\n\n//# sourceMappingURL=minimal-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/minimal-base-audio-context.js\n\n//# sourceMappingURL=minimal-base-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/minimal-base-audio-context-event-map.js\n\n//# sourceMappingURL=minimal-base-audio-context-event-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/minimal-offline-audio-context.js\n\n//# sourceMappingURL=minimal-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-audio-node-faker.js\n\n//# sourceMappingURL=native-audio-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-audio-worklet-node-faker.js\n\n//# sourceMappingURL=native-audio-worklet-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-constant-source-node-faker.js\n\n//# sourceMappingURL=native-constant-source-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-convolver-node-faker.js\n\n//# sourceMappingURL=native-convolver-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-iir-filter-node-faker.js\n\n//# sourceMappingURL=native-iir-filter-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-panner-node-faker.js\n\n//# sourceMappingURL=native-panner-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-stereo-panner-node-faker.js\n\n//# sourceMappingURL=native-stereo-panner-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/native-wave-shaper-node-faker.js\n\n//# sourceMappingURL=native-wave-shaper-node-faker.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/offline-audio-completion-event.js\n\n//# sourceMappingURL=offline-audio-completion-event.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/offline-audio-context.js\n\n//# sourceMappingURL=offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/offline-audio-context-constructor.js\n\n//# sourceMappingURL=offline-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/offline-audio-context-options.js\n\n//# sourceMappingURL=offline-audio-context-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/oscillator-node.js\n\n//# sourceMappingURL=oscillator-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/oscillator-node-renderer.js\n\n//# sourceMappingURL=oscillator-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/oscillator-options.js\n\n//# sourceMappingURL=oscillator-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/panner-node.js\n\n//# sourceMappingURL=panner-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/panner-options.js\n\n//# sourceMappingURL=panner-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/periodic-wave.js\n\n//# sourceMappingURL=periodic-wave.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/periodic-wave-constraints.js\n\n//# sourceMappingURL=periodic-wave-constraints.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/periodic-wave-options.js\n\n//# sourceMappingURL=periodic-wave-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/read-only-map.js\n\n//# sourceMappingURL=read-only-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/stereo-panner-node.js\n\n//# sourceMappingURL=stereo-panner-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/stereo-panner-options.js\n\n//# sourceMappingURL=stereo-panner-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/wave-shaper-node.js\n\n//# sourceMappingURL=wave-shaper-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/wave-shaper-options.js\n\n//# sourceMappingURL=wave-shaper-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/worklet-options.js\n// @todo This is currently named IWorkletOptions and not IAudioWorkletOptions because it defines the options of a generic Worklet.\n\n//# sourceMappingURL=worklet-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/interfaces/index.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/abort-error-factory.js\n\n//# sourceMappingURL=abort-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/active-audio-worklet-node-inputs-store.js\n\n//# sourceMappingURL=active-audio-worklet-node-inputs-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/active-input-connection.js\n\n//# sourceMappingURL=active-input-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-active-input-connection-to-audio-node-factory.js\n\n//# sourceMappingURL=add-active-input-connection-to-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-active-input-connection-to-audio-node-function.js\n\n//# sourceMappingURL=add-active-input-connection-to-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-audio-node-connections-factory.js\n\n//# sourceMappingURL=add-audio-node-connections-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-audio-node-connections-function.js\n\n//# sourceMappingURL=add-audio-node-connections-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-audio-param-connections-factory.js\n\n//# sourceMappingURL=add-audio-param-connections-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-audio-param-connections-function.js\n\n//# sourceMappingURL=add-audio-param-connections-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-audio-worklet-module-factory.js\n\n//# sourceMappingURL=add-audio-worklet-module-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-audio-worklet-module-function.js\n\n//# sourceMappingURL=add-audio-worklet-module-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-connection-to-audio-node-factory.js\n\n//# sourceMappingURL=add-connection-to-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-connection-to-audio-node-function.js\n\n//# sourceMappingURL=add-connection-to-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-passive-input-connection-to-audio-node-factory.js\n\n//# sourceMappingURL=add-passive-input-connection-to-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-passive-input-connection-to-audio-node-function.js\n\n//# sourceMappingURL=add-passive-input-connection-to-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-silent-connection-factory.js\n\n//# sourceMappingURL=add-silent-connection-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-silent-connection-function.js\n\n//# sourceMappingURL=add-silent-connection-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-unrendered-audio-worklet-node-factory.js\n\n//# sourceMappingURL=add-unrendered-audio-worklet-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/add-unrendered-audio-worklet-node-function.js\n\n//# sourceMappingURL=add-unrendered-audio-worklet-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/analyser-node-constructor.js\n\n//# sourceMappingURL=analyser-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/analyser-node-constructor-factory.js\n\n//# sourceMappingURL=analyser-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/analyser-node-renderer-factory.js\n\n//# sourceMappingURL=analyser-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/analyser-node-renderer-factory-factory.js\n\n//# sourceMappingURL=analyser-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/any-audio-buffer.js\n\n//# sourceMappingURL=any-audio-buffer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/any-context.js\n\n//# sourceMappingURL=any-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-constructor.js\n\n//# sourceMappingURL=audio-buffer-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-constructor-factory.js\n\n//# sourceMappingURL=audio-buffer-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-source-node-constructor.js\n\n//# sourceMappingURL=audio-buffer-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-source-node-constructor-factory.js\n\n//# sourceMappingURL=audio-buffer-source-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-source-node-renderer.js\n\n//# sourceMappingURL=audio-buffer-source-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-source-node-renderer-factory.js\n\n//# sourceMappingURL=audio-buffer-source-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-source-node-renderer-factory-factory.js\n\n//# sourceMappingURL=audio-buffer-source-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-buffer-store.js\n\n//# sourceMappingURL=audio-buffer-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-context-constructor.js\n\n//# sourceMappingURL=audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-context-constructor-factory.js\n\n//# sourceMappingURL=audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-context-latency-category.js\n\n//# sourceMappingURL=audio-context-latency-category.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-context-state.js\n\n//# sourceMappingURL=audio-context-state.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-destination-node-constructor.js\n\n//# sourceMappingURL=audio-destination-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-destination-node-constructor-factory.js\n\n//# sourceMappingURL=audio-destination-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-destination-node-renderer-factory.js\n\n//# sourceMappingURL=audio-destination-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-listener-factory.js\n\n//# sourceMappingURL=audio-listener-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-listener-factory-factory.js\n\n//# sourceMappingURL=audio-listener-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-connections.js\n\n//# sourceMappingURL=audio-node-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-connections-store.js\n\n//# sourceMappingURL=audio-node-connections-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-constructor.js\n\n//# sourceMappingURL=audio-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-constructor-factory.js\n\n//# sourceMappingURL=audio-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-output-connection.js\n\n//# sourceMappingURL=audio-node-output-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-renderer.js\n\n//# sourceMappingURL=audio-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-store.js\n\n//# sourceMappingURL=audio-node-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-node-tail-time-store.js\n\n//# sourceMappingURL=audio-node-tail-time-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-audio-node-store.js\n\n//# sourceMappingURL=audio-param-audio-node-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-connections.js\n\n//# sourceMappingURL=audio-param-connections.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-connections-store.js\n\n//# sourceMappingURL=audio-param-connections-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-factory.js\n\n//# sourceMappingURL=audio-param-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-factory-factory.js\n\n//# sourceMappingURL=audio-param-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-map.js\n\n//# sourceMappingURL=audio-param-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-output-connection.js\n\n//# sourceMappingURL=audio-param-output-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-renderer-factory.js\n\n//# sourceMappingURL=audio-param-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-param-store.js\n\n//# sourceMappingURL=audio-param-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-worklet-node-constructor.js\n\n//# sourceMappingURL=audio-worklet-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-worklet-node-constructor-factory.js\n\n//# sourceMappingURL=audio-worklet-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-worklet-node-renderer-factory.js\n\n//# sourceMappingURL=audio-worklet-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/audio-worklet-node-renderer-factory-factory.js\n\n//# sourceMappingURL=audio-worklet-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/backup-offline-audio-context-store.js\n\n//# sourceMappingURL=backup-offline-audio-context-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/base-audio-context-constructor.js\n\n//# sourceMappingURL=base-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/base-audio-context-constructor-factory.js\n\n//# sourceMappingURL=base-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/biquad-filter-node-constructor.js\n\n//# sourceMappingURL=biquad-filter-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/biquad-filter-node-constructor-factory.js\n\n//# sourceMappingURL=biquad-filter-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/biquad-filter-node-renderer-factory.js\n\n//# sourceMappingURL=biquad-filter-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/biquad-filter-node-renderer-factory-factory.js\n\n//# sourceMappingURL=biquad-filter-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/biquad-filter-type.js\n\n//# sourceMappingURL=biquad-filter-type.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-count-mode.js\n\n//# sourceMappingURL=channel-count-mode.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-interpretation.js\n\n//# sourceMappingURL=channel-interpretation.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-merger-node-constructor.js\n\n//# sourceMappingURL=channel-merger-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-merger-node-constructor-factory.js\n\n//# sourceMappingURL=channel-merger-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-merger-node-renderer-factory.js\n\n//# sourceMappingURL=channel-merger-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-merger-node-renderer-factory-factory.js\n\n//# sourceMappingURL=channel-merger-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-splitter-node-constructor.js\n\n//# sourceMappingURL=channel-splitter-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-splitter-node-constructor-factory.js\n\n//# sourceMappingURL=channel-splitter-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-splitter-node-renderer-factory.js\n\n//# sourceMappingURL=channel-splitter-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/channel-splitter-node-renderer-factory-factory.js\n\n//# sourceMappingURL=channel-splitter-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/cache-test-result-factory.js\n\n//# sourceMappingURL=cache-test-result-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/cache-test-result-function.js\n\n//# sourceMappingURL=cache-test-result-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connect-audio-param-factory.js\n\n//# sourceMappingURL=connect-audio-param-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connect-audio-param-function.js\n\n//# sourceMappingURL=connect-audio-param-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connect-multiple-outputs-factory.js\n\n//# sourceMappingURL=connect-multiple-outputs-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connect-multiple-outputs-function.js\n\n//# sourceMappingURL=connect-multiple-outputs-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connect-native-audio-node-to-native-audio-node-function.js\n\n//# sourceMappingURL=connect-native-audio-node-to-native-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connected-native-audio-buffer-source-node-factory.js\n\n//# sourceMappingURL=connected-native-audio-buffer-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/connected-native-audio-buffer-source-node-factory-factory.js\n\n//# sourceMappingURL=connected-native-audio-buffer-source-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/constant-source-node-constructor.js\n\n//# sourceMappingURL=constant-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/constant-source-node-constructor-factory.js\n\n//# sourceMappingURL=constant-source-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/constant-source-node-renderer.js\n\n//# sourceMappingURL=constant-source-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/constant-source-node-renderer-factory.js\n\n//# sourceMappingURL=constant-source-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/constant-source-node-renderer-factory-factory.js\n\n//# sourceMappingURL=constant-source-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/constructor.js\n\n//# sourceMappingURL=constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/context.js\n\n//# sourceMappingURL=context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/context-store.js\n\n//# sourceMappingURL=context-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/convert-number-to-unsigned-long-factory.js\n\n//# sourceMappingURL=convert-number-to-unsigned-long-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/convert-number-to-unsigned-long-function.js\n\n//# sourceMappingURL=convert-number-to-unsigned-long-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/convolver-node-constructor.js\n\n//# sourceMappingURL=convolver-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/convolver-node-constructor-factory.js\n\n//# sourceMappingURL=convolver-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/convolver-node-renderer-factory.js\n\n//# sourceMappingURL=convolver-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/convolver-node-renderer-factory-factory.js\n\n//# sourceMappingURL=convolver-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/create-native-offline-audio-context-factory.js\n\n//# sourceMappingURL=create-native-offline-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/create-native-offline-audio-context-function.js\n\n//# sourceMappingURL=create-native-offline-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/cycle-counters.js\n\n//# sourceMappingURL=cycle-counters.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/data-clone-error-factory.js\n\n//# sourceMappingURL=data-clone-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/decode-audio-data-factory.js\n\n//# sourceMappingURL=decode-audio-data-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/decode-audio-data-function.js\n\n//# sourceMappingURL=decode-audio-data-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/decode-error-callback.js\n\n//# sourceMappingURL=decode-error-callback.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/decode-success-callback.js\n\n//# sourceMappingURL=decode-success-callback.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/decrement-cycle-counter-factory.js\n\n//# sourceMappingURL=decrement-cycle-counter-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/decrement-cycle-counter-function.js\n\n//# sourceMappingURL=decrement-cycle-counter-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delay-node-constructor.js\n\n//# sourceMappingURL=delay-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delay-node-constructor-factory.js\n\n//# sourceMappingURL=delay-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delay-node-renderer-factory.js\n\n//# sourceMappingURL=delay-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delay-node-renderer-factory-factory.js\n\n//# sourceMappingURL=delay-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delete-active-input-connection-to-audio-node-factory.js\n\n//# sourceMappingURL=delete-active-input-connection-to-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delete-active-input-connection-to-audio-node-function.js\n\n//# sourceMappingURL=delete-active-input-connection-to-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delete-unrendered-audio-worklet-node-factory.js\n\n//# sourceMappingURL=delete-unrendered-audio-worklet-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/delete-unrendered-audio-worklet-node-function.js\n\n//# sourceMappingURL=delete-unrendered-audio-worklet-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/detect-cycles-factory.js\n\n//# sourceMappingURL=detect-cycles-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/detect-cycles-function.js\n\n//# sourceMappingURL=detect-cycles-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/disconnect-multiple-outputs-factory.js\n\n//# sourceMappingURL=disconnect-multiple-outputs-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/disconnect-multiple-outputs-function.js\n\n//# sourceMappingURL=disconnect-multiple-outputs-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/disconnect-native-audio-node-from-native-audio-node-function.js\n\n//# sourceMappingURL=disconnect-native-audio-node-from-native-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/distance-model-type.js\n\n//# sourceMappingURL=distance-model-type.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/dynamics-compressor-node-constructor.js\n\n//# sourceMappingURL=dynamics-compressor-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/dynamics-compressor-node-constructor-factory.js\n\n//# sourceMappingURL=dynamics-compressor-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/dynamics-compressor-node-renderer-factory.js\n\n//# sourceMappingURL=dynamics-compressor-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/dynamics-compressor-node-renderer-factory-factory.js\n\n//# sourceMappingURL=dynamics-compressor-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/encoding-error-factory.js\n\n//# sourceMappingURL=encoding-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/error-event-handler.js\n\n//# sourceMappingURL=error-event-handler.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/evaluate-audio-worklet-global-scope-function.js\n\n//# sourceMappingURL=evaluate-audio-worklet-global-scope-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/evaluate-source-factory.js\n\n//# sourceMappingURL=evaluate-source-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/evaluate-source-function.js\n\n//# sourceMappingURL=evaluate-source-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/event-handler.js\n\n//# sourceMappingURL=event-handler.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/event-target-constructor.js\n\n//# sourceMappingURL=event-target-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/event-target-constructor-factory.js\n\n//# sourceMappingURL=event-target-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/expose-current-frame-and-current-time-factory.js\n\n//# sourceMappingURL=expose-current-frame-and-current-time-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/expose-current-frame-and-current-time-function.js\n\n//# sourceMappingURL=expose-current-frame-and-current-time-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/fetch-source-factory.js\n\n//# sourceMappingURL=fetch-source-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/fetch-source-function.js\n\n//# sourceMappingURL=fetch-source-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/gain-node-constructor.js\n\n//# sourceMappingURL=gain-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/gain-node-constructor-factory.js\n\n//# sourceMappingURL=gain-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/gain-node-renderer-factory.js\n\n//# sourceMappingURL=gain-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/gain-node-renderer-factory-factory.js\n\n//# sourceMappingURL=gain-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-active-audio-worklet-node-inputs-factory.js\n\n//# sourceMappingURL=get-active-audio-worklet-node-inputs-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-active-audio-worklet-node-inputs-function.js\n\n//# sourceMappingURL=get-active-audio-worklet-node-inputs-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-node-connections-function.js\n\n//# sourceMappingURL=get-audio-node-connections-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-node-renderer-factory.js\n\n//# sourceMappingURL=get-audio-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-node-renderer-function.js\n\n//# sourceMappingURL=get-audio-node-renderer-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-node-tail-time-factory.js\n\n//# sourceMappingURL=get-audio-node-tail-time-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-node-tail-time-function.js\n\n//# sourceMappingURL=get-audio-node-tail-time-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-param-connections-function.js\n\n//# sourceMappingURL=get-audio-param-connections-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-param-renderer-factory.js\n\n//# sourceMappingURL=get-audio-param-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-audio-param-renderer-function.js\n\n//# sourceMappingURL=get-audio-param-renderer-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-backup-offline-audio-context-factory.js\n\n//# sourceMappingURL=get-backup-offline-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-backup-offline-audio-context-function.js\n\n//# sourceMappingURL=get-backup-offline-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-event-listeners-of-audio-node-function.js\n\n//# sourceMappingURL=get-event-listeners-of-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-native-audio-node-function.js\n\n//# sourceMappingURL=get-native-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-native-audio-param-function.js\n\n//# sourceMappingURL=get-native-audio-param-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-native-context-factory.js\n\n//# sourceMappingURL=get-native-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-native-context-function.js\n\n//# sourceMappingURL=get-native-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-or-create-backup-offline-audio-context-factory.js\n\n//# sourceMappingURL=get-or-create-backup-offline-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-or-create-backup-offline-audio-context-function.js\n\n//# sourceMappingURL=get-or-create-backup-offline-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-unrendered-audio-worklet-nodes-factory.js\n\n//# sourceMappingURL=get-unrendered-audio-worklet-nodes-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-unrendered-audio-worklet-nodes-function.js\n\n//# sourceMappingURL=get-unrendered-audio-worklet-nodes-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/get-value-for-key-function.js\n\n//# sourceMappingURL=get-value-for-key-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/iir-filter-node-constructor.js\n\n//# sourceMappingURL=iir-filter-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/iir-filter-node-constructor-factory.js\n\n//# sourceMappingURL=iir-filter-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/iir-filter-node-renderer-factory.js\n\n//# sourceMappingURL=iir-filter-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/iir-filter-node-renderer-factory-factory.js\n\n//# sourceMappingURL=iir-filter-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/increment-cycle-counter-factory.js\n\n//# sourceMappingURL=increment-cycle-counter-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/increment-cycle-counter-factory-factory.js\n\n//# sourceMappingURL=increment-cycle-counter-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/increment-cycle-counter-function.js\n\n//# sourceMappingURL=increment-cycle-counter-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/index-size-error-factory.js\n\n//# sourceMappingURL=index-size-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/insert-element-in-set-function.js\n\n//# sourceMappingURL=insert-element-in-set-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/internal-state-event-listener.js\n\n//# sourceMappingURL=internal-state-event-listener.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/invalid-access-error-factory.js\n\n//# sourceMappingURL=invalid-access-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/invalid-state-error-factory.js\n\n//# sourceMappingURL=invalid-state-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-active-audio-node-function.js\n\n//# sourceMappingURL=is-active-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-audio-context-factory.js\n\n//# sourceMappingURL=is-any-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-audio-context-function.js\n\n//# sourceMappingURL=is-any-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-audio-node-factory.js\n\n//# sourceMappingURL=is-any-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-audio-node-function.js\n\n//# sourceMappingURL=is-any-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-audio-param-factory.js\n\n//# sourceMappingURL=is-any-audio-param-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-audio-param-function.js\n\n//# sourceMappingURL=is-any-audio-param-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-offline-audio-context-factory.js\n\n//# sourceMappingURL=is-any-offline-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-any-offline-audio-context-function.js\n\n//# sourceMappingURL=is-any-offline-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-dc-curve-function.js\n\n//# sourceMappingURL=is-dc-curve-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-audio-context-factory.js\n\n//# sourceMappingURL=is-native-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-audio-context-function.js\n\n//# sourceMappingURL=is-native-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-audio-node-factory.js\n\n//# sourceMappingURL=is-native-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-audio-node-function.js\n\n//# sourceMappingURL=is-native-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-audio-param-factory.js\n\n//# sourceMappingURL=is-native-audio-param-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-audio-param-function.js\n\n//# sourceMappingURL=is-native-audio-param-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-context-factory.js\n\n//# sourceMappingURL=is-native-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-context-function.js\n\n//# sourceMappingURL=is-native-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-offline-audio-context-factory.js\n\n//# sourceMappingURL=is-native-offline-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-native-offline-audio-context-function.js\n\n//# sourceMappingURL=is-native-offline-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-part-of-a-cycle-function.js\n\n//# sourceMappingURL=is-part-of-a-cycle-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-passive-audio-node-function.js\n\n//# sourceMappingURL=is-passive-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-secure-context-factory.js\n\n//# sourceMappingURL=is-secure-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/is-supported-promise-factory.js\n\n//# sourceMappingURL=is-supported-promise-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-element-audio-source-node-constructor.js\n\n//# sourceMappingURL=media-element-audio-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-element-audio-source-node-constructor-factory.js\n\n//# sourceMappingURL=media-element-audio-source-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-stream-audio-destination-node-constructor.js\n\n//# sourceMappingURL=media-stream-audio-destination-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-stream-audio-destination-node-constructor-factory.js\n\n//# sourceMappingURL=media-stream-audio-destination-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-stream-audio-source-node-constructor.js\n\n//# sourceMappingURL=media-stream-audio-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-stream-audio-source-node-constructor-factory.js\n\n//# sourceMappingURL=media-stream-audio-source-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-stream-track-audio-source-node-constructor.js\n\n//# sourceMappingURL=media-stream-track-audio-source-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/media-stream-track-audio-source-node-constructor-factory.js\n\n//# sourceMappingURL=media-stream-track-audio-source-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/minimal-audio-context-constructor.js\n\n//# sourceMappingURL=minimal-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/minimal-audio-context-constructor-factory.js\n\n//# sourceMappingURL=minimal-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/minimal-base-audio-context-constructor.js\n\n//# sourceMappingURL=minimal-base-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/minimal-base-audio-context-constructor-factory.js\n\n//# sourceMappingURL=minimal-base-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/minimal-offline-audio-context-constructor.js\n\n//# sourceMappingURL=minimal-offline-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/minimal-offline-audio-context-constructor-factory.js\n\n//# sourceMappingURL=minimal-offline-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/monitor-connections-factory.js\n\n//# sourceMappingURL=monitor-connections-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/monitor-connections-function.js\n\n//# sourceMappingURL=monitor-connections-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-analyser-node.js\n\n//# sourceMappingURL=native-analyser-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-analyser-node-factory.js\n\n//# sourceMappingURL=native-analyser-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-analyser-node-factory-factory.js\n\n//# sourceMappingURL=native-analyser-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-buffer.js\n\n//# sourceMappingURL=native-audio-buffer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-buffer-constructor.js\n\n//# sourceMappingURL=native-audio-buffer-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-buffer-constructor-factory.js\n\n//# sourceMappingURL=native-audio-buffer-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-buffer-source-node.js\n\n//# sourceMappingURL=native-audio-buffer-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-buffer-source-node-factory.js\n\n//# sourceMappingURL=native-audio-buffer-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-buffer-source-node-factory-factory.js\n\n//# sourceMappingURL=native-audio-buffer-source-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-context.js\n\n//# sourceMappingURL=native-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-context-constructor.js\n\n//# sourceMappingURL=native-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-context-constructor-factory.js\n\n//# sourceMappingURL=native-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-destination-node.js\n\n//# sourceMappingURL=native-audio-destination-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-destination-node-factory.js\n\n//# sourceMappingURL=native-audio-destination-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-destination-node-factory-factory.js\n\n//# sourceMappingURL=native-audio-destination-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-listener.js\n\n//# sourceMappingURL=native-audio-listener.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-node.js\n\n//# sourceMappingURL=native-audio-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-param.js\n\n//# sourceMappingURL=native-audio-param.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-param-map.js\n\n//# sourceMappingURL=native-audio-param-map.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet.js\n\n//# sourceMappingURL=native-audio-worklet.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node.js\n\n//# sourceMappingURL=native-audio-worklet-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-constructor.js\n\n//# sourceMappingURL=native-audio-worklet-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-constructor-factory.js\n\n//# sourceMappingURL=native-audio-worklet-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-factory.js\n\n//# sourceMappingURL=native-audio-worklet-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-factory-factory.js\n\n//# sourceMappingURL=native-audio-worklet-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-faker-factory.js\n\n//# sourceMappingURL=native-audio-worklet-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-faker-factory-factory.js\n\n//# sourceMappingURL=native-audio-worklet-node-faker-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-audio-worklet-node-options.js\n\n//# sourceMappingURL=native-audio-worklet-node-options.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-biquad-filter-node.js\n\n//# sourceMappingURL=native-biquad-filter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-biquad-filter-node-factory.js\n\n//# sourceMappingURL=native-biquad-filter-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-channel-merger-node.js\n\n//# sourceMappingURL=native-channel-merger-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-channel-merger-node-factory.js\n\n//# sourceMappingURL=native-channel-merger-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-channel-merger-node-factory-factory.js\n\n//# sourceMappingURL=native-channel-merger-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-channel-splitter-node.js\n\n//# sourceMappingURL=native-channel-splitter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-channel-splitter-node-factory.js\n\n//# sourceMappingURL=native-channel-splitter-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-constant-source-node.js\n\n//# sourceMappingURL=native-constant-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-constant-source-node-factory.js\n\n//# sourceMappingURL=native-constant-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-constant-source-node-factory-factory.js\n\n//# sourceMappingURL=native-constant-source-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-constant-source-node-faker-factory.js\n\n//# sourceMappingURL=native-constant-source-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-constant-source-node-faker-factory-factory.js\n\n//# sourceMappingURL=native-constant-source-node-faker-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-context.js\n\n//# sourceMappingURL=native-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-convolver-node.js\n\n//# sourceMappingURL=native-convolver-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-convolver-node-factory.js\n\n//# sourceMappingURL=native-convolver-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-convolver-node-factory-factory.js\n\n//# sourceMappingURL=native-convolver-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-delay-node-factory.js\n\n//# sourceMappingURL=native-delay-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-delay-node.js\n\n//# sourceMappingURL=native-delay-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-dynamics-compressor-node.js\n\n//# sourceMappingURL=native-dynamics-compressor-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-dynamics-compressor-node-factory.js\n\n//# sourceMappingURL=native-dynamics-compressor-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-dynamics-compressor-node-factory-factory.js\n\n//# sourceMappingURL=native-dynamics-compressor-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-event-target.js\n\n//# sourceMappingURL=native-event-target.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-gain-node.js\n\n//# sourceMappingURL=native-gain-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-gain-node-factory.js\n\n//# sourceMappingURL=native-gain-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-iir-filter-node.js\n\n//# sourceMappingURL=native-iir-filter-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-iir-filter-node-factory.js\n\n//# sourceMappingURL=native-iir-filter-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-iir-filter-node-factory-factory.js\n\n//# sourceMappingURL=native-iir-filter-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-iir-filter-node-faker-factory.js\n\n//# sourceMappingURL=native-iir-filter-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-iir-filter-node-faker-factory-factory.js\n\n//# sourceMappingURL=native-iir-filter-node-faker-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-element-audio-source-node.js\n\n//# sourceMappingURL=native-media-element-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-element-audio-source-node-factory.js\n\n//# sourceMappingURL=native-media-element-audio-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-audio-destination-node.js\n\n//# sourceMappingURL=native-media-stream-audio-destination-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-audio-destination-node-factory.js\n\n//# sourceMappingURL=native-media-stream-audio-destination-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-audio-source-node.js\n\n//# sourceMappingURL=native-media-stream-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-audio-source-node-factory.js\n\n//# sourceMappingURL=native-media-stream-audio-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-track-audio-source-node.js\n\n//# sourceMappingURL=native-media-stream-track-audio-source-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-track-audio-source-node-factory.js\n\n//# sourceMappingURL=native-media-stream-track-audio-source-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-media-stream-track-audio-source-node-factory-factory.js\n\n//# sourceMappingURL=native-media-stream-track-audio-source-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-offline-audio-context.js\n\n//# sourceMappingURL=native-offline-audio-context.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-offline-audio-context-constructor.js\n\n//# sourceMappingURL=native-offline-audio-context-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-offline-audio-context-constructor-factory.js\n\n//# sourceMappingURL=native-offline-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-oscillator-node.js\n\n//# sourceMappingURL=native-oscillator-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-oscillator-node-factory.js\n\n//# sourceMappingURL=native-oscillator-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-oscillator-node-factory-factory.js\n\n//# sourceMappingURL=native-oscillator-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-panner-node.js\n\n//# sourceMappingURL=native-panner-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-panner-node-factory.js\n\n//# sourceMappingURL=native-panner-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-panner-node-factory-factory.js\n\n//# sourceMappingURL=native-panner-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-panner-node-faker-factory.js\n\n//# sourceMappingURL=native-panner-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-panner-node-faker-factory-factory.js\n\n//# sourceMappingURL=native-panner-node-faker-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-periodic-wave.js\n\n//# sourceMappingURL=native-periodic-wave.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-periodic-wave-factory.js\n\n//# sourceMappingURL=native-periodic-wave-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-periodic-wave-factory-factory.js\n\n//# sourceMappingURL=native-periodic-wave-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-script-processor-node.js\n\n//# sourceMappingURL=native-script-processor-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-script-processor-node-factory.js\n\n//# sourceMappingURL=native-script-processor-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-stereo-panner-node.js\n\n//# sourceMappingURL=native-stereo-panner-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-stereo-panner-node-factory.js\n\n//# sourceMappingURL=native-stereo-panner-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-stereo-panner-node-factory-factory.js\n\n//# sourceMappingURL=native-stereo-panner-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-stereo-panner-node-faker-factory.js\n\n//# sourceMappingURL=native-stereo-panner-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-stereo-panner-node-faker-factory-factory.js\n\n//# sourceMappingURL=native-stereo-panner-node-faker-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-wave-shaper-node.js\n\n//# sourceMappingURL=native-wave-shaper-node.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-wave-shaper-node-factory.js\n\n//# sourceMappingURL=native-wave-shaper-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-wave-shaper-node-factory-factory.js\n\n//# sourceMappingURL=native-wave-shaper-node-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-wave-shaper-node-faker-factory.js\n\n//# sourceMappingURL=native-wave-shaper-node-faker-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/native-wave-shaper-node-faker-factory-factory.js\n\n//# sourceMappingURL=native-wave-shaper-node-faker-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/not-supported-error-factory.js\n\n//# sourceMappingURL=not-supported-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/offline-audio-context-constructor-factory.js\n\n//# sourceMappingURL=offline-audio-context-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/oscillator-node-constructor.js\n\n//# sourceMappingURL=oscillator-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/oscillator-node-constructor-factory.js\n\n//# sourceMappingURL=oscillator-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/oscillator-node-renderer.js\n\n//# sourceMappingURL=oscillator-node-renderer.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/oscillator-node-renderer-factory.js\n\n//# sourceMappingURL=oscillator-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/oscillator-node-renderer-factory-factory.js\n\n//# sourceMappingURL=oscillator-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/oscillator-type.js\n\n//# sourceMappingURL=oscillator-type.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/output-connection.js\n\n//# sourceMappingURL=output-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/over-sample-type.js\n\n//# sourceMappingURL=over-sample-type.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/overwrite-accessors-function.js\n\n//# sourceMappingURL=overwrite-accessors-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/panner-node-constructor.js\n\n//# sourceMappingURL=panner-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/panner-node-constructor-factory.js\n\n//# sourceMappingURL=panner-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/panner-node-renderer-factory.js\n\n//# sourceMappingURL=panner-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/panner-node-renderer-factory-factory.js\n\n//# sourceMappingURL=panner-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/panning-model-type.js\n\n//# sourceMappingURL=panning-model-type.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/passive-audio-node-input-connection.js\n\n//# sourceMappingURL=passive-audio-node-input-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/passive-audio-param-input-connection.js\n\n//# sourceMappingURL=passive-audio-param-input-connection.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/periodic-wave-constructor.js\n\n//# sourceMappingURL=periodic-wave-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/periodic-wave-constructor-factory.js\n\n//# sourceMappingURL=periodic-wave-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/pick-element-from-set-function.js\n\n//# sourceMappingURL=pick-element-from-set-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-automation-factory.js\n\n//# sourceMappingURL=render-automation-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-automation-function.js\n\n//# sourceMappingURL=render-automation-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-inputs-of-audio-node-factory.js\n\n//# sourceMappingURL=render-inputs-of-audio-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-inputs-of-audio-node-function.js\n\n//# sourceMappingURL=render-inputs-of-audio-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-inputs-of-audio-param-factory.js\n\n//# sourceMappingURL=render-inputs-of-audio-param-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-inputs-of-audio-param-function.js\n\n//# sourceMappingURL=render-inputs-of-audio-param-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-native-offline-audio-context-factory.js\n\n//# sourceMappingURL=render-native-offline-audio-context-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/render-native-offline-audio-context-function.js\n\n//# sourceMappingURL=render-native-offline-audio-context-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/sanitize-audio-worklet-node-options-function.js\n\n//# sourceMappingURL=sanitize-audio-worklet-node-options-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/sanitize-channel-splitter-options-function.js\n\n//# sourceMappingURL=sanitize-channel-splitter-options-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/sanitize-periodic-wave-options-function.js\n\n//# sourceMappingURL=sanitize-periodic-wave-options-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/set-active-audio-worklet-node-inputs-factory.js\n\n//# sourceMappingURL=set-active-audio-worklet-node-inputs-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/set-active-audio-worklet-node-inputs-function.js\n\n//# sourceMappingURL=set-active-audio-worklet-node-inputs-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/set-audio-node-tail-time-factory.js\n\n//# sourceMappingURL=set-audio-node-tail-time-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/set-audio-node-tail-time-function.js\n\n//# sourceMappingURL=set-audio-node-tail-time-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/set-value-at-time-until-possible-function.js\n\n//# sourceMappingURL=set-value-at-time-until-possible-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/start-rendering-factory.js\n\n//# sourceMappingURL=start-rendering-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/start-rendering-function.js\n\n//# sourceMappingURL=start-rendering-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/stereo-panner-node-constructor.js\n\n//# sourceMappingURL=stereo-panner-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/stereo-panner-node-constructor-factory.js\n\n//# sourceMappingURL=stereo-panner-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/stereo-panner-node-renderer-factory-factory.js\n\n//# sourceMappingURL=stereo-panner-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/stereo-panner-node-renderer-factory.js\n\n//# sourceMappingURL=stereo-panner-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-buffer-copy-channel-methods-subarray-support-factory.js\n\n//# sourceMappingURL=test-audio-buffer-copy-channel-methods-subarray-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-buffer-constructor-support-factory.js\n\n//# sourceMappingURL=test-audio-buffer-constructor-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-context-close-method-support-factory.js\n\n//# sourceMappingURL=test-audio-context-close-method-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-context-decode-audio-data-method-type-error-support-factory.js\n\n//# sourceMappingURL=test-audio-context-decode-audio-data-method-type-error-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-context-options-support-factory.js\n\n//# sourceMappingURL=test-audio-context-options-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-node-connect-method-support-factory.js\n\n//# sourceMappingURL=test-audio-node-connect-method-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-worklet-processor-no-outputs-support-factory.js\n\n//# sourceMappingURL=test-audio-worklet-processor-no-outputs-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-audio-worklet-processor-post-message-support-factory.js\n\n//# sourceMappingURL=test-audio-worklet-processor-post-message-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-channel-merger-node-channel-count-support-factory.js\n\n//# sourceMappingURL=test-channel-merger-node-channel-count-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-constant-source-node-accurate-scheduling-support-factory.js\n\n//# sourceMappingURL=test-constant-source-node-accurate-scheduling-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-convolver-node-buffer-reassignability-support-factory.js\n\n//# sourceMappingURL=test-convolver-node-buffer-reassignability-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-convolver-node-channel-count-support-factory.js\n\n//# sourceMappingURL=test-convolver-node-channel-count-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-is-secure-context-support-factory.js\n\n//# sourceMappingURL=test-is-secure-context-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-media-stream-audio-source-node-media-stream-without-audio-track-support.js\n\n//# sourceMappingURL=test-media-stream-audio-source-node-media-stream-without-audio-track-support.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-offline-audio-context-current-time-support-factory.js\n\n//# sourceMappingURL=test-offline-audio-context-current-time-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/test-stereo-panner-node-default-value-support-factory.js\n\n//# sourceMappingURL=test-stereo-panner-node-default-value-support-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/unknown-error-factory.js\n\n//# sourceMappingURL=unknown-error-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/unrendered-audio-worklet-node-store.js\n\n//# sourceMappingURL=unrendered-audio-worklet-node-store.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/unrendered-audio-worklet-nodes.js\n\n//# sourceMappingURL=unrendered-audio-worklet-nodes.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wave-shaper-node-constructor.js\n\n//# sourceMappingURL=wave-shaper-node-constructor.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wave-shaper-node-constructor-factory.js\n\n//# sourceMappingURL=wave-shaper-node-constructor-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wave-shaper-node-renderer-factory-factory.js\n\n//# sourceMappingURL=wave-shaper-node-renderer-factory-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wave-shaper-node-renderer-factory.js\n\n//# sourceMappingURL=wave-shaper-node-renderer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/window.js\n\n//# sourceMappingURL=window.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/window-factory.js\n\n//# sourceMappingURL=window-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-copy-channel-methods-factory.js\n\n//# sourceMappingURL=wrap-audio-buffer-copy-channel-methods-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-copy-channel-methods-function.js\n\n//# sourceMappingURL=wrap-audio-buffer-copy-channel-methods-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-copy-channel-methods-out-of-bounds-factory.js\n\n//# sourceMappingURL=wrap-audio-buffer-copy-channel-methods-out-of-bounds-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-copy-channel-methods-out-of-bounds-function.js\n\n//# sourceMappingURL=wrap-audio-buffer-copy-channel-methods-out-of-bounds-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-source-node-start-method-offset-clamping-function.js\n\n//# sourceMappingURL=wrap-audio-buffer-source-node-start-method-offset-clamping-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-source-node-stop-method-nullified-buffer-factory.js\n\n//# sourceMappingURL=wrap-audio-buffer-source-node-stop-method-nullified-buffer-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-buffer-source-node-stop-method-nullified-buffer-function.js\n\n//# sourceMappingURL=wrap-audio-buffer-source-node-stop-method-nullified-buffer-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-audio-scheduled-source-node-stop-method-consecutive-calls-function.js\n\n//# sourceMappingURL=wrap-audio-scheduled-source-node-stop-method-consecutive-calls-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-channel-merger-node-factory.js\n\n//# sourceMappingURL=wrap-channel-merger-node-factory.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-channel-merger-node-function.js\n\n//# sourceMappingURL=wrap-channel-merger-node-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/wrap-event-listener-function.js\n\n//# sourceMappingURL=wrap-event-listener-function.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/types/index.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/standardized-audio-context/build/es2019/module.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/*\n * @todo Explicitly referencing the barrel file seems to be necessary when enabling the\n * isolatedModules compiler option.\n */\n\n\nconst module_addActiveInputConnectionToAudioNode = createAddActiveInputConnectionToAudioNode(insert_element_in_set_insertElementInSet);\nconst module_addPassiveInputConnectionToAudioNode = createAddPassiveInputConnectionToAudioNode(insert_element_in_set_insertElementInSet);\nconst module_deleteActiveInputConnectionToAudioNode = createDeleteActiveInputConnectionToAudioNode(pickElementFromSet);\nconst audioNodeTailTimeStore = new WeakMap();\nconst module_getAudioNodeTailTime = createGetAudioNodeTailTime(audioNodeTailTimeStore);\nconst module_cacheTestResult = createCacheTestResult(new Map(), new WeakMap());\nconst module_window = createWindow();\nconst module_createNativeAnalyserNode = createNativeAnalyserNodeFactory(module_cacheTestResult, index_size_error_createIndexSizeError);\nconst module_getAudioNodeRenderer = createGetAudioNodeRenderer(get_audio_node_connections_getAudioNodeConnections);\nconst module_renderInputsOfAudioNode = createRenderInputsOfAudioNode(get_audio_node_connections_getAudioNodeConnections, module_getAudioNodeRenderer, is_part_of_a_cycle_isPartOfACycle);\nconst createAnalyserNodeRenderer = createAnalyserNodeRendererFactory(module_createNativeAnalyserNode, get_native_audio_node_getNativeAudioNode, module_renderInputsOfAudioNode);\nconst module_getNativeContext = createGetNativeContext(CONTEXT_STORE);\nconst module_nativeOfflineAudioContextConstructor = createNativeOfflineAudioContextConstructor(module_window);\nconst module_isNativeOfflineAudioContext = createIsNativeOfflineAudioContext(module_nativeOfflineAudioContextConstructor);\nconst module_audioParamAudioNodeStore = new WeakMap();\nconst module_eventTargetConstructor = createEventTargetConstructor(wrap_event_listener_wrapEventListener);\nconst module_nativeAudioContextConstructor = createNativeAudioContextConstructor(module_window);\nconst module_isNativeAudioContext = createIsNativeAudioContext(module_nativeAudioContextConstructor);\nconst module_isNativeAudioNode = createIsNativeAudioNode(module_window);\nconst module_isNativeAudioParam = createIsNativeAudioParam(module_window);\nconst module_audioNodeConstructor = createAudioNodeConstructor(createAddAudioNodeConnections(AUDIO_NODE_CONNECTIONS_STORE), createAddConnectionToAudioNode(module_addActiveInputConnectionToAudioNode, module_addPassiveInputConnectionToAudioNode, connect_native_audio_node_to_native_audio_node_connectNativeAudioNodeToNativeAudioNode, module_deleteActiveInputConnectionToAudioNode, disconnect_native_audio_node_from_native_audio_node_disconnectNativeAudioNodeFromNativeAudioNode, get_audio_node_connections_getAudioNodeConnections, module_getAudioNodeTailTime, get_event_listeners_of_audio_node_getEventListenersOfAudioNode, get_native_audio_node_getNativeAudioNode, insert_element_in_set_insertElementInSet, is_active_audio_node_isActiveAudioNode, is_part_of_a_cycle_isPartOfACycle, is_passive_audio_node_isPassiveAudioNode), module_cacheTestResult, createIncrementCycleCounterFactory(CYCLE_COUNTERS, disconnect_native_audio_node_from_native_audio_node_disconnectNativeAudioNodeFromNativeAudioNode, get_audio_node_connections_getAudioNodeConnections, get_native_audio_node_getNativeAudioNode, get_native_audio_param_getNativeAudioParam, is_active_audio_node_isActiveAudioNode), index_size_error_createIndexSizeError, invalid_access_error_createInvalidAccessError, not_supported_error_createNotSupportedError, createDecrementCycleCounter(connect_native_audio_node_to_native_audio_node_connectNativeAudioNodeToNativeAudioNode, CYCLE_COUNTERS, get_audio_node_connections_getAudioNodeConnections, get_native_audio_node_getNativeAudioNode, get_native_audio_param_getNativeAudioParam, module_getNativeContext, is_active_audio_node_isActiveAudioNode, module_isNativeOfflineAudioContext), createDetectCycles(module_audioParamAudioNodeStore, get_audio_node_connections_getAudioNodeConnections, get_value_for_key_getValueForKey), module_eventTargetConstructor, module_getNativeContext, module_isNativeAudioContext, module_isNativeAudioNode, module_isNativeAudioParam, module_isNativeOfflineAudioContext);\nconst analyserNodeConstructor = createAnalyserNodeConstructor(module_audioNodeConstructor, createAnalyserNodeRenderer, index_size_error_createIndexSizeError, module_createNativeAnalyserNode, module_getNativeContext, module_isNativeOfflineAudioContext);\n\nconst module_audioBufferStore = new WeakSet();\nconst module_nativeAudioBufferConstructor = createNativeAudioBufferConstructor(module_window);\nconst convertNumberToUnsignedLong = createConvertNumberToUnsignedLong(new Uint32Array(1));\nconst module_wrapAudioBufferCopyChannelMethods = createWrapAudioBufferCopyChannelMethods(convertNumberToUnsignedLong, index_size_error_createIndexSizeError);\nconst module_wrapAudioBufferCopyChannelMethodsOutOfBounds = createWrapAudioBufferCopyChannelMethodsOutOfBounds(convertNumberToUnsignedLong);\nconst audioBufferConstructor = createAudioBufferConstructor(module_audioBufferStore, module_cacheTestResult, not_supported_error_createNotSupportedError, module_nativeAudioBufferConstructor, module_nativeOfflineAudioContextConstructor, createTestAudioBufferConstructorSupport(module_nativeAudioBufferConstructor), module_wrapAudioBufferCopyChannelMethods, module_wrapAudioBufferCopyChannelMethodsOutOfBounds);\n\nconst module_addSilentConnection = createAddSilentConnection(native_gain_node_createNativeGainNode);\nconst renderInputsOfAudioParam = createRenderInputsOfAudioParam(module_getAudioNodeRenderer, getAudioParamConnections, is_part_of_a_cycle_isPartOfACycle);\nconst module_connectAudioParam = createConnectAudioParam(renderInputsOfAudioParam);\nconst module_createNativeAudioBufferSourceNode = createNativeAudioBufferSourceNodeFactory(module_addSilentConnection, module_cacheTestResult, test_audio_buffer_source_node_start_method_consecutive_calls_support_testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport, test_audio_buffer_source_node_start_method_offset_clamping_support_testAudioBufferSourceNodeStartMethodOffsetClampingSupport, test_audio_buffer_source_node_stop_method_nullified_buffer_support_testAudioBufferSourceNodeStopMethodNullifiedBufferSupport, test_audio_scheduled_source_node_start_method_negative_parameters_support_testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, test_audio_scheduled_source_node_stop_method_consecutive_calls_support_testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, test_audio_scheduled_source_node_stop_method_negative_parameters_support_testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, wrapAudioBufferSourceNodeStartMethodOffsetClamping, createWrapAudioBufferSourceNodeStopMethodNullifiedBuffer(overwrite_accessors_overwriteAccessors), wrap_audio_scheduled_source_node_stop_method_consecutive_calls_wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls);\nconst module_renderAutomation = createRenderAutomation(createGetAudioParamRenderer(getAudioParamConnections), renderInputsOfAudioParam);\nconst module_createAudioBufferSourceNodeRenderer = createAudioBufferSourceNodeRendererFactory(module_connectAudioParam, module_createNativeAudioBufferSourceNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst module_createAudioParam = createAudioParamFactory(createAddAudioParamConnections(AUDIO_PARAM_CONNECTIONS_STORE), module_audioParamAudioNodeStore, AUDIO_PARAM_STORE, audio_param_renderer_createAudioParamRenderer, bundle["createCancelAndHoldAutomationEvent"], bundle["createCancelScheduledValuesAutomationEvent"], bundle["createExponentialRampToValueAutomationEvent"], bundle["createLinearRampToValueAutomationEvent"], bundle["createSetTargetAutomationEvent"], bundle["createSetValueAutomationEvent"], bundle["createSetValueCurveAutomationEvent"], module_nativeAudioContextConstructor, set_value_at_time_until_possible_setValueAtTimeUntilPossible);\nconst audioBufferSourceNodeConstructor = createAudioBufferSourceNodeConstructor(module_audioNodeConstructor, module_createAudioBufferSourceNodeRenderer, module_createAudioParam, invalid_state_error_createInvalidStateError, module_createNativeAudioBufferSourceNode, module_getNativeContext, module_isNativeOfflineAudioContext, wrap_event_listener_wrapEventListener);\n\nconst module_audioDestinationNodeConstructor = createAudioDestinationNodeConstructor(module_audioNodeConstructor, createAudioDestinationNodeRenderer, index_size_error_createIndexSizeError, invalid_state_error_createInvalidStateError, createNativeAudioDestinationNodeFactory(native_gain_node_createNativeGainNode, overwrite_accessors_overwriteAccessors), module_getNativeContext, module_isNativeOfflineAudioContext, module_renderInputsOfAudioNode);\nconst module_createBiquadFilterNodeRenderer = createBiquadFilterNodeRendererFactory(module_connectAudioParam, native_biquad_filter_node_createNativeBiquadFilterNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst module_setAudioNodeTailTime = createSetAudioNodeTailTime(audioNodeTailTimeStore);\nconst biquadFilterNodeConstructor = createBiquadFilterNodeConstructor(module_audioNodeConstructor, module_createAudioParam, module_createBiquadFilterNodeRenderer, invalid_access_error_createInvalidAccessError, native_biquad_filter_node_createNativeBiquadFilterNode, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst module_monitorConnections = createMonitorConnections(insert_element_in_set_insertElementInSet, module_isNativeAudioNode);\nconst module_wrapChannelMergerNode = createWrapChannelMergerNode(invalid_state_error_createInvalidStateError, module_monitorConnections);\nconst module_createNativeChannelMergerNode = createNativeChannelMergerNodeFactory(module_nativeAudioContextConstructor, module_wrapChannelMergerNode);\nconst createChannelMergerNodeRenderer = createChannelMergerNodeRendererFactory(module_createNativeChannelMergerNode, get_native_audio_node_getNativeAudioNode, module_renderInputsOfAudioNode);\nconst channelMergerNodeConstructor = createChannelMergerNodeConstructor(module_audioNodeConstructor, createChannelMergerNodeRenderer, module_createNativeChannelMergerNode, module_getNativeContext, module_isNativeOfflineAudioContext);\nconst createChannelSplitterNodeRenderer = createChannelSplitterNodeRendererFactory(native_channel_splitter_node_createNativeChannelSplitterNode, get_native_audio_node_getNativeAudioNode, module_renderInputsOfAudioNode);\nconst channelSplitterNodeConstructor = createChannelSplitterNodeConstructor(module_audioNodeConstructor, createChannelSplitterNodeRenderer, native_channel_splitter_node_createNativeChannelSplitterNode, module_getNativeContext, module_isNativeOfflineAudioContext, sanitizeChannelSplitterOptions);\nconst module_createNativeConstantSourceNodeFaker = createNativeConstantSourceNodeFakerFactory(module_addSilentConnection, module_createNativeAudioBufferSourceNode, native_gain_node_createNativeGainNode, module_monitorConnections);\nconst module_createNativeConstantSourceNode = createNativeConstantSourceNodeFactory(module_addSilentConnection, module_cacheTestResult, module_createNativeConstantSourceNodeFaker, test_audio_scheduled_source_node_start_method_negative_parameters_support_testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, test_audio_scheduled_source_node_stop_method_negative_parameters_support_testAudioScheduledSourceNodeStopMethodNegativeParametersSupport);\nconst createConstantSourceNodeRenderer = constant_source_node_renderer_factory_createConstantSourceNodeRendererFactory(module_connectAudioParam, module_createNativeConstantSourceNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst constantSourceNodeConstructor = createConstantSourceNodeConstructor(module_audioNodeConstructor, module_createAudioParam, createConstantSourceNodeRenderer, module_createNativeConstantSourceNode, module_getNativeContext, module_isNativeOfflineAudioContext, wrap_event_listener_wrapEventListener);\nconst module_createNativeConvolverNode = createNativeConvolverNodeFactory(not_supported_error_createNotSupportedError, overwrite_accessors_overwriteAccessors);\nconst createConvolverNodeRenderer = createConvolverNodeRendererFactory(module_createNativeConvolverNode, get_native_audio_node_getNativeAudioNode, module_renderInputsOfAudioNode);\nconst convolverNodeConstructor = createConvolverNodeConstructor(module_audioNodeConstructor, createConvolverNodeRenderer, module_createNativeConvolverNode, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst createDelayNodeRenderer = createDelayNodeRendererFactory(module_connectAudioParam, native_delay_node_createNativeDelayNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst delayNodeConstructor = createDelayNodeConstructor(module_audioNodeConstructor, module_createAudioParam, createDelayNodeRenderer, native_delay_node_createNativeDelayNode, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst module_createNativeDynamicsCompressorNode = createNativeDynamicsCompressorNodeFactory(not_supported_error_createNotSupportedError);\nconst createDynamicsCompressorNodeRenderer = createDynamicsCompressorNodeRendererFactory(module_connectAudioParam, module_createNativeDynamicsCompressorNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst dynamicsCompressorNodeConstructor = createDynamicsCompressorNodeConstructor(module_audioNodeConstructor, module_createAudioParam, createDynamicsCompressorNodeRenderer, module_createNativeDynamicsCompressorNode, not_supported_error_createNotSupportedError, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst module_createGainNodeRenderer = createGainNodeRendererFactory(module_connectAudioParam, native_gain_node_createNativeGainNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst gainNodeConstructor = createGainNodeConstructor(module_audioNodeConstructor, module_createAudioParam, module_createGainNodeRenderer, native_gain_node_createNativeGainNode, module_getNativeContext, module_isNativeOfflineAudioContext);\nconst module_createNativeIIRFilterNodeFaker = createNativeIIRFilterNodeFakerFactory(invalid_access_error_createInvalidAccessError, invalid_state_error_createInvalidStateError, native_script_processor_node_createNativeScriptProcessorNode, not_supported_error_createNotSupportedError);\nconst module_renderNativeOfflineAudioContext = createRenderNativeOfflineAudioContext(module_cacheTestResult, native_gain_node_createNativeGainNode, native_script_processor_node_createNativeScriptProcessorNode, createTestOfflineAudioContextCurrentTimeSupport(native_gain_node_createNativeGainNode, module_nativeOfflineAudioContextConstructor));\nconst module_createIIRFilterNodeRenderer = createIIRFilterNodeRendererFactory(module_createNativeAudioBufferSourceNode, get_native_audio_node_getNativeAudioNode, module_nativeOfflineAudioContextConstructor, module_renderInputsOfAudioNode, module_renderNativeOfflineAudioContext);\nconst module_createNativeIIRFilterNode = createNativeIIRFilterNodeFactory(module_createNativeIIRFilterNodeFaker);\nconst iIRFilterNodeConstructor = createIIRFilterNodeConstructor(module_audioNodeConstructor, module_createNativeIIRFilterNode, module_createIIRFilterNodeRenderer, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst module_createAudioListener = createAudioListenerFactory(module_createAudioParam, module_createNativeChannelMergerNode, module_createNativeConstantSourceNode, native_script_processor_node_createNativeScriptProcessorNode, module_isNativeOfflineAudioContext);\nconst module_unrenderedAudioWorkletNodeStore = new WeakMap();\nconst module_minimalBaseAudioContextConstructor = createMinimalBaseAudioContextConstructor(module_audioDestinationNodeConstructor, module_createAudioListener, module_eventTargetConstructor, module_isNativeOfflineAudioContext, module_unrenderedAudioWorkletNodeStore, wrap_event_listener_wrapEventListener);\nconst module_createNativeOscillatorNode = createNativeOscillatorNodeFactory(module_addSilentConnection, module_cacheTestResult, test_audio_scheduled_source_node_start_method_negative_parameters_support_testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, test_audio_scheduled_source_node_stop_method_consecutive_calls_support_testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, test_audio_scheduled_source_node_stop_method_negative_parameters_support_testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, wrap_audio_scheduled_source_node_stop_method_consecutive_calls_wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls);\nconst module_createOscillatorNodeRenderer = createOscillatorNodeRendererFactory(module_connectAudioParam, module_createNativeOscillatorNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst oscillatorNodeConstructor = createOscillatorNodeConstructor(module_audioNodeConstructor, module_createAudioParam, module_createNativeOscillatorNode, module_createOscillatorNodeRenderer, module_getNativeContext, module_isNativeOfflineAudioContext, wrap_event_listener_wrapEventListener);\nconst module_createConnectedNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNodeFactory(module_createNativeAudioBufferSourceNode);\nconst module_createNativeWaveShaperNodeFaker = createNativeWaveShaperNodeFakerFactory(module_createConnectedNativeAudioBufferSourceNode, invalid_state_error_createInvalidStateError, native_gain_node_createNativeGainNode, is_dc_curve_isDCCurve, module_monitorConnections);\nconst module_createNativeWaveShaperNode = createNativeWaveShaperNodeFactory(module_createConnectedNativeAudioBufferSourceNode, invalid_state_error_createInvalidStateError, module_createNativeWaveShaperNodeFaker, is_dc_curve_isDCCurve, module_monitorConnections, module_nativeAudioContextConstructor, overwrite_accessors_overwriteAccessors);\nconst module_createNativePannerNodeFaker = createNativePannerNodeFakerFactory(connect_native_audio_node_to_native_audio_node_connectNativeAudioNodeToNativeAudioNode, invalid_state_error_createInvalidStateError, module_createNativeChannelMergerNode, native_gain_node_createNativeGainNode, native_script_processor_node_createNativeScriptProcessorNode, module_createNativeWaveShaperNode, not_supported_error_createNotSupportedError, disconnect_native_audio_node_from_native_audio_node_disconnectNativeAudioNodeFromNativeAudioNode, module_monitorConnections);\nconst module_createNativePannerNode = createNativePannerNodeFactory(module_createNativePannerNodeFaker);\nconst module_createPannerNodeRenderer = createPannerNodeRendererFactory(module_connectAudioParam, module_createNativeChannelMergerNode, module_createNativeConstantSourceNode, native_gain_node_createNativeGainNode, module_createNativePannerNode, get_native_audio_node_getNativeAudioNode, module_nativeOfflineAudioContextConstructor, module_renderAutomation, module_renderInputsOfAudioNode, module_renderNativeOfflineAudioContext);\nconst pannerNodeConstructor = createPannerNodeConstructor(module_audioNodeConstructor, module_createAudioParam, module_createNativePannerNode, module_createPannerNodeRenderer, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst createNativePeriodicWave = createNativePeriodicWaveFactory(index_size_error_createIndexSizeError);\nconst periodicWaveConstructor = createPeriodicWaveConstructor(createNativePeriodicWave, module_getNativeContext, new WeakSet(), sanitizePeriodicWaveOptions);\nconst module_nativeStereoPannerNodeFakerFactory = createNativeStereoPannerNodeFakerFactory(module_createNativeChannelMergerNode, native_channel_splitter_node_createNativeChannelSplitterNode, native_gain_node_createNativeGainNode, module_createNativeWaveShaperNode, not_supported_error_createNotSupportedError, module_monitorConnections);\nconst module_createNativeStereoPannerNode = createNativeStereoPannerNodeFactory(module_nativeStereoPannerNodeFakerFactory, not_supported_error_createNotSupportedError);\nconst createStereoPannerNodeRenderer = createStereoPannerNodeRendererFactory(module_connectAudioParam, module_createNativeStereoPannerNode, get_native_audio_node_getNativeAudioNode, module_renderAutomation, module_renderInputsOfAudioNode);\nconst stereoPannerNodeConstructor = createStereoPannerNodeConstructor(module_audioNodeConstructor, module_createAudioParam, module_createNativeStereoPannerNode, createStereoPannerNodeRenderer, module_getNativeContext, module_isNativeOfflineAudioContext);\nconst createWaveShaperNodeRenderer = createWaveShaperNodeRendererFactory(module_createNativeWaveShaperNode, get_native_audio_node_getNativeAudioNode, module_renderInputsOfAudioNode);\nconst waveShaperNodeConstructor = createWaveShaperNodeConstructor(module_audioNodeConstructor, invalid_state_error_createInvalidStateError, module_createNativeWaveShaperNode, createWaveShaperNodeRenderer, module_getNativeContext, module_isNativeOfflineAudioContext, module_setAudioNodeTailTime);\nconst isSecureContext = createIsSecureContext(module_window);\nconst module_exposeCurrentFrameAndCurrentTime = createExposeCurrentFrameAndCurrentTime(module_window);\nconst backupOfflineAudioContextStore = new WeakMap();\nconst module_getOrCreateBackupOfflineAudioContext = createGetOrCreateBackupOfflineAudioContext(backupOfflineAudioContextStore, module_nativeOfflineAudioContextConstructor);\nconst module_nativeAudioWorkletNodeConstructor = createNativeAudioWorkletNodeConstructor(module_window);\n// The addAudioWorkletModule() function is only available in a SecureContext.\nconst addAudioWorkletModule = isSecureContext\n    ? createAddAudioWorkletModule(module_cacheTestResult, not_supported_error_createNotSupportedError, createEvaluateSource(module_window), module_exposeCurrentFrameAndCurrentTime, createFetchSource(createAbortError), module_getNativeContext, module_getOrCreateBackupOfflineAudioContext, module_isNativeOfflineAudioContext, new WeakMap(), new WeakMap(), createTestAudioWorkletProcessorPostMessageSupport(module_nativeAudioWorkletNodeConstructor, module_nativeOfflineAudioContextConstructor), \n    // @todo window is guaranteed to be defined because isSecureContext checks that as well.\n    module_window)\n    : undefined;\nconst module_isNativeContext = createIsNativeContext(module_isNativeAudioContext, module_isNativeOfflineAudioContext);\nconst decodeAudioData = createDecodeAudioData(module_audioBufferStore, module_cacheTestResult, data_clone_error_createDataCloneError, encoding_error_createEncodingError, new WeakSet(), module_getNativeContext, module_isNativeContext, test_audio_buffer_copy_channel_methods_out_of_bounds_support_testAudioBufferCopyChannelMethodsOutOfBoundsSupport, test_promise_support_testPromiseSupport, module_wrapAudioBufferCopyChannelMethods, module_wrapAudioBufferCopyChannelMethodsOutOfBounds);\nconst module_baseAudioContextConstructor = createBaseAudioContextConstructor(addAudioWorkletModule, analyserNodeConstructor, audioBufferConstructor, audioBufferSourceNodeConstructor, biquadFilterNodeConstructor, channelMergerNodeConstructor, channelSplitterNodeConstructor, constantSourceNodeConstructor, convolverNodeConstructor, decodeAudioData, delayNodeConstructor, dynamicsCompressorNodeConstructor, gainNodeConstructor, iIRFilterNodeConstructor, module_minimalBaseAudioContextConstructor, oscillatorNodeConstructor, pannerNodeConstructor, periodicWaveConstructor, stereoPannerNodeConstructor, waveShaperNodeConstructor);\nconst module_mediaElementAudioSourceNodeConstructor = createMediaElementAudioSourceNodeConstructor(module_audioNodeConstructor, createNativeMediaElementAudioSourceNode, module_getNativeContext, module_isNativeOfflineAudioContext);\nconst module_mediaStreamAudioDestinationNodeConstructor = createMediaStreamAudioDestinationNodeConstructor(module_audioNodeConstructor, createNativeMediaStreamAudioDestinationNode, module_getNativeContext, module_isNativeOfflineAudioContext);\nconst module_mediaStreamAudioSourceNodeConstructor = createMediaStreamAudioSourceNodeConstructor(module_audioNodeConstructor, createNativeMediaStreamAudioSourceNode, module_getNativeContext, module_isNativeOfflineAudioContext);\nconst createNativeMediaStreamTrackAudioSourceNode = createNativeMediaStreamTrackAudioSourceNodeFactory(invalid_state_error_createInvalidStateError, module_isNativeOfflineAudioContext);\nconst module_mediaStreamTrackAudioSourceNodeConstructor = createMediaStreamTrackAudioSourceNodeConstructor(module_audioNodeConstructor, createNativeMediaStreamTrackAudioSourceNode, module_getNativeContext);\nconst audioContextConstructor = createAudioContextConstructor(module_baseAudioContextConstructor, invalid_state_error_createInvalidStateError, not_supported_error_createNotSupportedError, unknown_error_createUnknownError, module_mediaElementAudioSourceNodeConstructor, module_mediaStreamAudioDestinationNodeConstructor, module_mediaStreamAudioSourceNodeConstructor, module_mediaStreamTrackAudioSourceNodeConstructor, module_nativeAudioContextConstructor);\n\nconst module_getUnrenderedAudioWorkletNodes = createGetUnrenderedAudioWorkletNodes(module_unrenderedAudioWorkletNodeStore);\nconst module_addUnrenderedAudioWorkletNode = createAddUnrenderedAudioWorkletNode(module_getUnrenderedAudioWorkletNodes);\nconst module_connectMultipleOutputs = createConnectMultipleOutputs(index_size_error_createIndexSizeError);\nconst module_deleteUnrenderedAudioWorkletNode = createDeleteUnrenderedAudioWorkletNode(module_getUnrenderedAudioWorkletNodes);\nconst module_disconnectMultipleOutputs = createDisconnectMultipleOutputs(index_size_error_createIndexSizeError);\nconst activeAudioWorkletNodeInputsStore = new WeakMap();\nconst module_getActiveAudioWorkletNodeInputs = createGetActiveAudioWorkletNodeInputs(activeAudioWorkletNodeInputsStore, get_value_for_key_getValueForKey);\nconst module_createNativeAudioWorkletNodeFaker = createNativeAudioWorkletNodeFakerFactory(module_connectMultipleOutputs, index_size_error_createIndexSizeError, invalid_state_error_createInvalidStateError, module_createNativeChannelMergerNode, native_channel_splitter_node_createNativeChannelSplitterNode, module_createNativeConstantSourceNode, native_gain_node_createNativeGainNode, native_script_processor_node_createNativeScriptProcessorNode, not_supported_error_createNotSupportedError, module_disconnectMultipleOutputs, module_exposeCurrentFrameAndCurrentTime, module_getActiveAudioWorkletNodeInputs, module_monitorConnections);\nconst module_createNativeAudioWorkletNode = createNativeAudioWorkletNodeFactory(invalid_state_error_createInvalidStateError, module_createNativeAudioWorkletNodeFaker, native_gain_node_createNativeGainNode, not_supported_error_createNotSupportedError, module_monitorConnections);\nconst module_createAudioWorkletNodeRenderer = createAudioWorkletNodeRendererFactory(module_connectAudioParam, module_connectMultipleOutputs, module_createNativeAudioBufferSourceNode, module_createNativeChannelMergerNode, native_channel_splitter_node_createNativeChannelSplitterNode, module_createNativeConstantSourceNode, native_gain_node_createNativeGainNode, module_deleteUnrenderedAudioWorkletNode, module_disconnectMultipleOutputs, module_exposeCurrentFrameAndCurrentTime, get_native_audio_node_getNativeAudioNode, module_nativeAudioWorkletNodeConstructor, module_nativeOfflineAudioContextConstructor, module_renderAutomation, module_renderInputsOfAudioNode, module_renderNativeOfflineAudioContext);\nconst module_getBackupOfflineAudioContext = createGetBackupOfflineAudioContext(backupOfflineAudioContextStore);\nconst module_setActiveAudioWorkletNodeInputs = createSetActiveAudioWorkletNodeInputs(activeAudioWorkletNodeInputsStore);\n// The AudioWorkletNode constructor is only available in a SecureContext.\nconst audioWorkletNodeConstructor = isSecureContext\n    ? createAudioWorkletNodeConstructor(module_addUnrenderedAudioWorkletNode, module_audioNodeConstructor, module_createAudioParam, module_createAudioWorkletNodeRenderer, module_createNativeAudioWorkletNode, get_audio_node_connections_getAudioNodeConnections, module_getBackupOfflineAudioContext, module_getNativeContext, module_isNativeOfflineAudioContext, module_nativeAudioWorkletNodeConstructor, sanitize_audio_worklet_node_options_sanitizeAudioWorkletNodeOptions, module_setActiveAudioWorkletNodeInputs, wrap_event_listener_wrapEventListener)\n    : undefined;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst minimalAudioContextConstructor = createMinimalAudioContextConstructor(invalid_state_error_createInvalidStateError, not_supported_error_createNotSupportedError, unknown_error_createUnknownError, module_minimalBaseAudioContextConstructor, module_nativeAudioContextConstructor);\n\nconst module_createNativeOfflineAudioContext = createCreateNativeOfflineAudioContext(not_supported_error_createNotSupportedError, module_nativeOfflineAudioContextConstructor);\nconst module_startRendering = createStartRendering(module_audioBufferStore, module_cacheTestResult, module_getAudioNodeRenderer, module_getUnrenderedAudioWorkletNodes, module_renderNativeOfflineAudioContext, test_audio_buffer_copy_channel_methods_out_of_bounds_support_testAudioBufferCopyChannelMethodsOutOfBoundsSupport, module_wrapAudioBufferCopyChannelMethods, module_wrapAudioBufferCopyChannelMethodsOutOfBounds);\nconst minimalOfflineAudioContextConstructor = createMinimalOfflineAudioContextConstructor(module_cacheTestResult, invalid_state_error_createInvalidStateError, module_createNativeOfflineAudioContext, module_minimalBaseAudioContextConstructor, module_startRendering);\n\nconst offlineAudioContextConstructor = createOfflineAudioContextConstructor(module_baseAudioContextConstructor, module_cacheTestResult, invalid_state_error_createInvalidStateError, module_createNativeOfflineAudioContext, module_startRendering);\n\n\n\n\n\n\nconst isAnyAudioContext = createIsAnyAudioContext(CONTEXT_STORE, module_isNativeAudioContext);\nconst isAnyAudioNode = createIsAnyAudioNode(AUDIO_NODE_STORE, module_isNativeAudioNode);\nconst isAnyAudioParam = createIsAnyAudioParam(AUDIO_PARAM_STORE, module_isNativeAudioParam);\nconst isAnyOfflineAudioContext = createIsAnyOfflineAudioContext(CONTEXT_STORE, module_isNativeOfflineAudioContext);\nconst isSupported = () => createIsSupportedPromise(module_cacheTestResult, createTestAudioBufferCopyChannelMethodsSubarraySupport(module_nativeOfflineAudioContextConstructor), createTestAudioContextCloseMethodSupport(module_nativeAudioContextConstructor), createTestAudioContextDecodeAudioDataMethodTypeErrorSupport(module_nativeOfflineAudioContextConstructor), createTestAudioContextOptionsSupport(module_nativeAudioContextConstructor), createTestAudioNodeConnectMethodSupport(module_nativeOfflineAudioContextConstructor), createTestAudioWorkletProcessorNoOutputsSupport(module_nativeAudioWorkletNodeConstructor, module_nativeOfflineAudioContextConstructor), createTestChannelMergerNodeChannelCountSupport(module_nativeOfflineAudioContextConstructor), createTestConstantSourceNodeAccurateSchedulingSupport(module_nativeOfflineAudioContextConstructor), createTestConvolverNodeBufferReassignabilitySupport(module_nativeOfflineAudioContextConstructor), createTestConvolverNodeChannelCountSupport(module_nativeOfflineAudioContextConstructor), testDomExceptionConstructorSupport, createTestIsSecureContextSupport(module_window), createTestMediaStreamAudioSourceNodeMediaStreamWithoutAudioTrackSupport(module_nativeAudioContextConstructor), createTestStereoPannerNodeDefaultValueSupport(module_nativeOfflineAudioContextConstructor), testTransferablesSupport);\n//# sourceMappingURL=module.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Debug.js\n/**\n * Assert that the statement is true, otherwise invoke the error.\n * @param statement\n * @param error The message which is passed into an Error\n */\nfunction assert(statement, error) {\n    if (!statement) {\n        throw new Error(error);\n    }\n}\n/**\n * Make sure that the given value is within the range\n */\nfunction assertRange(value, gte, lte = Infinity) {\n    if (!(gte <= value && value <= lte)) {\n        throw new RangeError(`Value must be within [${gte}, ${lte}], got: ${value}`);\n    }\n}\n/**\n * Make sure that the given value is within the range\n */\nfunction assertContextRunning(context) {\n    // add a warning if the context is not started\n    if (!context.isOffline && context.state !== "running") {\n        warn("The AudioContext is \\"suspended\\". Invoke Tone.start() from a user action to start the audio.");\n    }\n}\n/**\n * The default logger is the console\n */\nlet defaultLogger = console;\n/**\n * Set the logging interface\n */\nfunction setLogger(logger) {\n    defaultLogger = logger;\n}\n/**\n * Log anything\n */\nfunction log(...args) {\n    defaultLogger.log(...args);\n}\n/**\n * Warn anything\n */\nfunction warn(...args) {\n    defaultLogger.warn(...args);\n}\n//# sourceMappingURL=Debug.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/TypeCheck.js\n/**\n * Test if the arg is undefined\n */\nfunction isUndef(arg) {\n    return typeof arg === "undefined";\n}\n/**\n * Test if the arg is not undefined\n */\nfunction isDefined(arg) {\n    return !isUndef(arg);\n}\n/**\n * Test if the arg is a function\n */\nfunction isFunction(arg) {\n    return typeof arg === "function";\n}\n/**\n * Test if the argument is a number.\n */\nfunction isNumber(arg) {\n    return (typeof arg === "number");\n}\n/**\n * Test if the given argument is an object literal (i.e. `{}`);\n */\nfunction isObject(arg) {\n    return (Object.prototype.toString.call(arg) === "[object Object]" && arg.constructor === Object);\n}\n/**\n * Test if the argument is a boolean.\n */\nfunction isBoolean(arg) {\n    return (typeof arg === "boolean");\n}\n/**\n * Test if the argument is an Array\n */\nfunction isArray(arg) {\n    return (Array.isArray(arg));\n}\n/**\n * Test if the argument is a string.\n */\nfunction isString(arg) {\n    return (typeof arg === "string");\n}\n/**\n * Test if the argument is in the form of a note in scientific pitch notation.\n * e.g. "C4"\n */\nfunction isNote(arg) {\n    return isString(arg) && /^([a-g]{1}(?:b|#|x|bb)?)(-?[0-9]+)/i.test(arg);\n}\n//# sourceMappingURL=TypeCheck.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/AudioContext.js\n\n\n\n/**\n * Create a new AudioContext\n */\nfunction createAudioContext(options) {\n    return new audioContextConstructor(options);\n}\n/**\n * Create a new OfflineAudioContext\n */\nfunction createOfflineAudioContext(channels, length, sampleRate) {\n    return new offlineAudioContextConstructor(channels, length, sampleRate);\n}\n/**\n * A reference to the window object\n * @hidden\n */\nconst theWindow = typeof self === "object" ? self : null;\n/**\n * If the browser has a window object which has an AudioContext\n * @hidden\n */\nconst hasAudioContext = theWindow &&\n    (theWindow.hasOwnProperty("AudioContext") || theWindow.hasOwnProperty("webkitAudioContext"));\nfunction createAudioWorkletNode(context, name, options) {\n    assert(isDefined(audioWorkletNodeConstructor), "This node only works in a secure context (https or localhost)");\n    // @ts-ignore\n    return new audioWorkletNodeConstructor(context, name, options);\n}\n/**\n * This promise resolves to a boolean which indicates if the\n * functionality is supported within the currently used browse.\n * Taken from [standardized-audio-context](https://github.com/chrisguttandin/standardized-audio-context#issupported)\n */\n\n//# sourceMappingURL=AudioContext.js.map\n// CONCATENATED MODULE: ./node_modules/tone/node_modules/tslib/tslib.es6.js\n/*! *****************************************************************************\r\nCopyright (c) Microsoft Corporation.\r\n\r\nPermission to use, copy, modify, and/or distribute this software for any\r\npurpose with or without fee is hereby granted.\r\n\r\nTHE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH\r\nREGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY\r\nAND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,\r\nINDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\r\nLOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR\r\nOTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\r\nPERFORMANCE OF THIS SOFTWARE.\r\n***************************************************************************** */\r\n/* global Reflect, Promise */\r\n\r\nvar extendStatics = function(d, b) {\r\n    extendStatics = Object.setPrototypeOf ||\r\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\r\n        function (d, b) { for (var p in b) if (Object.prototype.hasOwnProperty.call(b, p)) d[p] = b[p]; };\r\n    return extendStatics(d, b);\r\n};\r\n\r\nfunction __extends(d, b) {\r\n    if (typeof b !== "function" && b !== null)\r\n        throw new TypeError("Class extends value " + String(b) + " is not a constructor or null");\r\n    extendStatics(d, b);\r\n    function __() { this.constructor = d; }\r\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\r\n}\r\n\r\nvar __assign = function() {\r\n    __assign = Object.assign || function __assign(t) {\r\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\r\n            s = arguments[i];\r\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\r\n        }\r\n        return t;\r\n    }\r\n    return __assign.apply(this, arguments);\r\n}\r\n\r\nfunction __rest(s, e) {\r\n    var t = {};\r\n    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)\r\n        t[p] = s[p];\r\n    if (s != null && typeof Object.getOwnPropertySymbols === "function")\r\n        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {\r\n            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))\r\n                t[p[i]] = s[p[i]];\r\n        }\r\n    return t;\r\n}\r\n\r\nfunction __decorate(decorators, target, key, desc) {\r\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\r\n    if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);\r\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\r\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\r\n}\r\n\r\nfunction __param(paramIndex, decorator) {\r\n    return function (target, key) { decorator(target, key, paramIndex); }\r\n}\r\n\r\nfunction __metadata(metadataKey, metadataValue) {\r\n    if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(metadataKey, metadataValue);\r\n}\r\n\r\nfunction __awaiter(thisArg, _arguments, P, generator) {\r\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\r\n    return new (P || (P = Promise))(function (resolve, reject) {\r\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\r\n        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }\r\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\r\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\r\n    });\r\n}\r\n\r\nfunction __generator(thisArg, body) {\r\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\r\n    return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;\r\n    function verb(n) { return function (v) { return step([n, v]); }; }\r\n    function step(op) {\r\n        if (f) throw new TypeError("Generator is already executing.");\r\n        while (_) try {\r\n            if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\r\n            if (y = 0, t) op = [op[0] & 2, t.value];\r\n            switch (op[0]) {\r\n                case 0: case 1: t = op; break;\r\n                case 4: _.label++; return { value: op[1], done: false };\r\n                case 5: _.label++; y = op[1]; op = [0]; continue;\r\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\r\n                default:\r\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\r\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\r\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\r\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\r\n                    if (t[2]) _.ops.pop();\r\n                    _.trys.pop(); continue;\r\n            }\r\n            op = body.call(thisArg, _);\r\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\r\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\r\n    }\r\n}\r\n\r\nvar __createBinding = Object.create ? (function(o, m, k, k2) {\r\n    if (k2 === undefined) k2 = k;\r\n    Object.defineProperty(o, k2, { enumerable: true, get: function() { return m[k]; } });\r\n}) : (function(o, m, k, k2) {\r\n    if (k2 === undefined) k2 = k;\r\n    o[k2] = m[k];\r\n});\r\n\r\nfunction __exportStar(m, o) {\r\n    for (var p in m) if (p !== "default" && !Object.prototype.hasOwnProperty.call(o, p)) __createBinding(o, m, p);\r\n}\r\n\r\nfunction __values(o) {\r\n    var s = typeof Symbol === "function" && Symbol.iterator, m = s && o[s], i = 0;\r\n    if (m) return m.call(o);\r\n    if (o && typeof o.length === "number") return {\r\n        next: function () {\r\n            if (o && i >= o.length) o = void 0;\r\n            return { value: o && o[i++], done: !o };\r\n        }\r\n    };\r\n    throw new TypeError(s ? "Object is not iterable." : "Symbol.iterator is not defined.");\r\n}\r\n\r\nfunction __read(o, n) {\r\n    var m = typeof Symbol === "function" && o[Symbol.iterator];\r\n    if (!m) return o;\r\n    var i = m.call(o), r, ar = [], e;\r\n    try {\r\n        while ((n === void 0 || n-- > 0) && !(r = i.next()).done) ar.push(r.value);\r\n    }\r\n    catch (error) { e = { error: error }; }\r\n    finally {\r\n        try {\r\n            if (r && !r.done && (m = i["return"])) m.call(i);\r\n        }\r\n        finally { if (e) throw e.error; }\r\n    }\r\n    return ar;\r\n}\r\n\r\n/** @deprecated */\r\nfunction __spread() {\r\n    for (var ar = [], i = 0; i < arguments.length; i++)\r\n        ar = ar.concat(__read(arguments[i]));\r\n    return ar;\r\n}\r\n\r\n/** @deprecated */\r\nfunction __spreadArrays() {\r\n    for (var s = 0, i = 0, il = arguments.length; i < il; i++) s += arguments[i].length;\r\n    for (var r = Array(s), k = 0, i = 0; i < il; i++)\r\n        for (var a = arguments[i], j = 0, jl = a.length; j < jl; j++, k++)\r\n            r[k] = a[j];\r\n    return r;\r\n}\r\n\r\nfunction __spreadArray(to, from) {\r\n    for (var i = 0, il = from.length, j = to.length; i < il; i++, j++)\r\n        to[j] = from[i];\r\n    return to;\r\n}\r\n\r\nfunction __await(v) {\r\n    return this instanceof __await ? (this.v = v, this) : new __await(v);\r\n}\r\n\r\nfunction __asyncGenerator(thisArg, _arguments, generator) {\r\n    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");\r\n    var g = generator.apply(thisArg, _arguments || []), i, q = [];\r\n    return i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i;\r\n    function verb(n) { if (g[n]) i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; }\r\n    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }\r\n    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }\r\n    function fulfill(value) { resume("next", value); }\r\n    function reject(value) { resume("throw", value); }\r\n    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }\r\n}\r\n\r\nfunction __asyncDelegator(o) {\r\n    var i, p;\r\n    return i = {}, verb("next"), verb("throw", function (e) { throw e; }), verb("return"), i[Symbol.iterator] = function () { return this; }, i;\r\n    function verb(n, f) { i[n] = o[n] ? function (v) { return (p = !p) ? { value: __await(o[n](v)), done: n === "return" } : f ? f(v) : v; } : f; }\r\n}\r\n\r\nfunction __asyncValues(o) {\r\n    if (!Symbol.asyncIterator) throw new TypeError("Symbol.asyncIterator is not defined.");\r\n    var m = o[Symbol.asyncIterator], i;\r\n    return m ? m.call(o) : (o = typeof __values === "function" ? __values(o) : o[Symbol.iterator](), i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function () { return this; }, i);\r\n    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }\r\n    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }\r\n}\r\n\r\nfunction __makeTemplateObject(cooked, raw) {\r\n    if (Object.defineProperty) { Object.defineProperty(cooked, "raw", { value: raw }); } else { cooked.raw = raw; }\r\n    return cooked;\r\n};\r\n\r\nvar __setModuleDefault = Object.create ? (function(o, v) {\r\n    Object.defineProperty(o, "default", { enumerable: true, value: v });\r\n}) : function(o, v) {\r\n    o["default"] = v;\r\n};\r\n\r\nfunction __importStar(mod) {\r\n    if (mod && mod.__esModule) return mod;\r\n    var result = {};\r\n    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\r\n    __setModuleDefault(result, mod);\r\n    return result;\r\n}\r\n\r\nfunction __importDefault(mod) {\r\n    return (mod && mod.__esModule) ? mod : { default: mod };\r\n}\r\n\r\nfunction __classPrivateFieldGet(receiver, privateMap) {\r\n    if (!privateMap.has(receiver)) {\r\n        throw new TypeError("attempted to get private field on non-instance");\r\n    }\r\n    return privateMap.get(receiver);\r\n}\r\n\r\nfunction __classPrivateFieldSet(receiver, privateMap, value) {\r\n    if (!privateMap.has(receiver)) {\r\n        throw new TypeError("attempted to set private field on non-instance");\r\n    }\r\n    privateMap.set(receiver, value);\r\n    return value;\r\n}\r\n\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/Ticker.js\n/**\n * A class which provides a reliable callback using either\n * a Web Worker, or if that isn\'t supported, falls back to setTimeout.\n */\nclass Ticker {\n    constructor(callback, type, updateInterval) {\n        this._callback = callback;\n        this._type = type;\n        this._updateInterval = updateInterval;\n        // create the clock source for the first time\n        this._createClock();\n    }\n    /**\n     * Generate a web worker\n     */\n    _createWorker() {\n        const blob = new Blob([\n            /* javascript */ `\n\t\t\t// the initial timeout time\n\t\t\tlet timeoutTime =  ${(this._updateInterval * 1000).toFixed(1)};\n\t\t\t// onmessage callback\n\t\t\tself.onmessage = function(msg){\n\t\t\t\ttimeoutTime = parseInt(msg.data);\n\t\t\t};\n\t\t\t// the tick function which posts a message\n\t\t\t// and schedules a new tick\n\t\t\tfunction tick(){\n\t\t\t\tsetTimeout(tick, timeoutTime);\n\t\t\t\tself.postMessage(\'tick\');\n\t\t\t}\n\t\t\t// call tick initially\n\t\t\ttick();\n\t\t\t`\n        ], { type: "text/javascript" });\n        const blobUrl = URL.createObjectURL(blob);\n        const worker = new Worker(blobUrl);\n        worker.onmessage = this._callback.bind(this);\n        this._worker = worker;\n    }\n    /**\n     * Create a timeout loop\n     */\n    _createTimeout() {\n        this._timeout = setTimeout(() => {\n            this._createTimeout();\n            this._callback();\n        }, this._updateInterval * 1000);\n    }\n    /**\n     * Create the clock source.\n     */\n    _createClock() {\n        if (this._type === "worker") {\n            try {\n                this._createWorker();\n            }\n            catch (e) {\n                // workers not supported, fallback to timeout\n                this._type = "timeout";\n                this._createClock();\n            }\n        }\n        else if (this._type === "timeout") {\n            this._createTimeout();\n        }\n    }\n    /**\n     * Clean up the current clock source\n     */\n    _disposeClock() {\n        if (this._timeout) {\n            clearTimeout(this._timeout);\n            this._timeout = 0;\n        }\n        if (this._worker) {\n            this._worker.terminate();\n            this._worker.onmessage = null;\n        }\n    }\n    /**\n     * The rate in seconds the ticker will update\n     */\n    get updateInterval() {\n        return this._updateInterval;\n    }\n    set updateInterval(interval) {\n        this._updateInterval = Math.max(interval, 128 / 44100);\n        if (this._type === "worker") {\n            this._worker.postMessage(Math.max(interval * 1000, 1));\n        }\n    }\n    /**\n     * The type of the ticker, either a worker or a timeout\n     */\n    get type() {\n        return this._type;\n    }\n    set type(type) {\n        this._disposeClock();\n        this._type = type;\n        this._createClock();\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        this._disposeClock();\n    }\n}\n//# sourceMappingURL=Ticker.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/AdvancedTypeCheck.js\n\n/**\n * Test if the given value is an instanceof AudioParam\n */\nfunction isAudioParam(arg) {\n    return isAnyAudioParam(arg);\n}\n/**\n * Test if the given value is an instanceof AudioNode\n */\nfunction AdvancedTypeCheck_isAudioNode(arg) {\n    return isAnyAudioNode(arg);\n}\n/**\n * Test if the arg is instanceof an OfflineAudioContext\n */\nfunction isOfflineAudioContext(arg) {\n    return isAnyOfflineAudioContext(arg);\n}\n/**\n * Test if the arg is an instanceof AudioContext\n */\nfunction isAudioContext(arg) {\n    return isAnyAudioContext(arg);\n}\n/**\n * Test if the arg is instanceof an AudioBuffer\n */\nfunction isAudioBuffer(arg) {\n    return arg instanceof AudioBuffer;\n}\n//# sourceMappingURL=AdvancedTypeCheck.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Defaults.js\n\n\n/**\n * Some objects should not be merged\n */\nfunction noCopy(key, arg) {\n    return key === "value" || isAudioParam(arg) || AdvancedTypeCheck_isAudioNode(arg) || isAudioBuffer(arg);\n}\nfunction deepMerge(target, ...sources) {\n    if (!sources.length) {\n        return target;\n    }\n    const source = sources.shift();\n    if (isObject(target) && isObject(source)) {\n        for (const key in source) {\n            if (noCopy(key, source[key])) {\n                target[key] = source[key];\n            }\n            else if (isObject(source[key])) {\n                if (!target[key]) {\n                    Object.assign(target, { [key]: {} });\n                }\n                deepMerge(target[key], source[key]);\n            }\n            else {\n                Object.assign(target, { [key]: source[key] });\n            }\n        }\n    }\n    // @ts-ignore\n    return deepMerge(target, ...sources);\n}\n/**\n * Returns true if the two arrays have the same value for each of the elements\n */\nfunction deepEquals(arrayA, arrayB) {\n    return arrayA.length === arrayB.length && arrayA.every((element, index) => arrayB[index] === element);\n}\n/**\n * Convert an args array into an object.\n */\nfunction optionsFromArguments(defaults, argsArray, keys = [], objKey) {\n    const opts = {};\n    const args = Array.from(argsArray);\n    // if the first argument is an object and has an object key\n    if (isObject(args[0]) && objKey && !Reflect.has(args[0], objKey)) {\n        // if it\'s not part of the defaults\n        const partOfDefaults = Object.keys(args[0]).some(key => Reflect.has(defaults, key));\n        if (!partOfDefaults) {\n            // merge that key\n            deepMerge(opts, { [objKey]: args[0] });\n            // remove the obj key from the keys\n            keys.splice(keys.indexOf(objKey), 1);\n            // shift the first argument off\n            args.shift();\n        }\n    }\n    if (args.length === 1 && isObject(args[0])) {\n        deepMerge(opts, args[0]);\n    }\n    else {\n        for (let i = 0; i < keys.length; i++) {\n            if (isDefined(args[i])) {\n                opts[keys[i]] = args[i];\n            }\n        }\n    }\n    return deepMerge(defaults, opts);\n}\n/**\n * Return this instances default values by calling Constructor.getDefaults()\n */\nfunction getDefaultsFromInstance(instance) {\n    return instance.constructor.getDefaults();\n}\n/**\n * Returns the fallback if the given object is undefined.\n * Take an array of arguments and return a formatted options object.\n */\nfunction defaultArg(given, fallback) {\n    if (isUndef(given)) {\n        return fallback;\n    }\n    else {\n        return given;\n    }\n}\n/**\n * Remove all of the properties belonging to omit from obj.\n */\nfunction omitFromObject(obj, omit) {\n    omit.forEach(prop => {\n        if (Reflect.has(obj, prop)) {\n            delete obj[prop];\n        }\n    });\n    return obj;\n}\n//# sourceMappingURL=Defaults.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/Tone.js\n/**\n * Tone.js\n * @author Yotam Mann\n * @license http://opensource.org/licenses/MIT MIT License\n * @copyright 2014-2019 Yotam Mann\n */\n\n\n\n/**\n * @class  Tone is the base class of all other classes.\n * @category Core\n * @constructor\n */\nclass Tone_Tone {\n    constructor() {\n        //-------------------------------------\n        // \tDEBUGGING\n        //-------------------------------------\n        /**\n         * Set this debug flag to log all events that happen in this class.\n         */\n        this.debug = false;\n        //-------------------------------------\n        // \tDISPOSING\n        //-------------------------------------\n        /**\n         * Indicates if the instance was disposed\n         */\n        this._wasDisposed = false;\n    }\n    /**\n     * Returns all of the default options belonging to the class.\n     */\n    static getDefaults() {\n        return {};\n    }\n    /**\n     * Prints the outputs to the console log for debugging purposes.\n     * Prints the contents only if either the object has a property\n     * called `debug` set to true, or a variable called TONE_DEBUG_CLASS\n     * is set to the name of the class.\n     * @example\n     * const osc = new Tone.Oscillator();\n     * // prints all logs originating from this oscillator\n     * osc.debug = true;\n     * // calls to start/stop will print in the console\n     * osc.start();\n     */\n    log(...args) {\n        // if the object is either set to debug = true\n        // or if there is a string on the Tone.global.with the class name\n        if (this.debug || (theWindow && this.toString() === theWindow.TONE_DEBUG_CLASS)) {\n            log(this, ...args);\n        }\n    }\n    /**\n     * disconnect and dispose.\n     */\n    dispose() {\n        this._wasDisposed = true;\n        return this;\n    }\n    /**\n     * Indicates if the instance was disposed. \'Disposing\' an\n     * instance means that all of the Web Audio nodes that were\n     * created for the instance are disconnected and freed for garbage collection.\n     */\n    get disposed() {\n        return this._wasDisposed;\n    }\n    /**\n     * Convert the class to a string\n     * @example\n     * const osc = new Tone.Oscillator();\n     * console.log(osc.toString());\n     */\n    toString() {\n        return this.name;\n    }\n}\n/**\n * The version number semver\n */\nTone_Tone.version = version;\n//# sourceMappingURL=Tone.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Math.js\n/**\n * The threshold for correctness for operators. Less than one sample even\n * at very high sampling rates (e.g. `1e-6 < 1 / 192000`).\n */\nconst EPSILON = 1e-6;\n/**\n * Test if A is greater than B\n */\nfunction GT(a, b) {\n    return a > b + EPSILON;\n}\n/**\n * Test if A is greater than or equal to B\n */\nfunction GTE(a, b) {\n    return GT(a, b) || EQ(a, b);\n}\n/**\n * Test if A is less than B\n */\nfunction LT(a, b) {\n    return a + EPSILON < b;\n}\n/**\n * Test if A is less than B\n */\nfunction EQ(a, b) {\n    return Math.abs(a - b) < EPSILON;\n}\n/**\n * Clamp the value within the given range\n */\nfunction clamp(value, min, max) {\n    return Math.max(Math.min(value, max), min);\n}\n//# sourceMappingURL=Math.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Timeline.js\n\n\n\n\n/**\n * A Timeline class for scheduling and maintaining state\n * along a timeline. All events must have a "time" property.\n * Internally, events are stored in time order for fast\n * retrieval.\n */\nclass Timeline_Timeline extends Tone_Tone {\n    constructor() {\n        super();\n        this.name = "Timeline";\n        /**\n         * The array of scheduled timeline events\n         */\n        this._timeline = [];\n        const options = optionsFromArguments(Timeline_Timeline.getDefaults(), arguments, ["memory"]);\n        this.memory = options.memory;\n        this.increasing = options.increasing;\n    }\n    static getDefaults() {\n        return {\n            memory: Infinity,\n            increasing: false,\n        };\n    }\n    /**\n     * The number of items in the timeline.\n     */\n    get length() {\n        return this._timeline.length;\n    }\n    /**\n     * Insert an event object onto the timeline. Events must have a "time" attribute.\n     * @param event  The event object to insert into the timeline.\n     */\n    add(event) {\n        // the event needs to have a time attribute\n        assert(Reflect.has(event, "time"), "Timeline: events must have a time attribute");\n        event.time = event.time.valueOf();\n        if (this.increasing && this.length) {\n            const lastValue = this._timeline[this.length - 1];\n            assert(GTE(event.time, lastValue.time), "The time must be greater than or equal to the last scheduled time");\n            this._timeline.push(event);\n        }\n        else {\n            const index = this._search(event.time);\n            this._timeline.splice(index + 1, 0, event);\n        }\n        // if the length is more than the memory, remove the previous ones\n        if (this.length > this.memory) {\n            const diff = this.length - this.memory;\n            this._timeline.splice(0, diff);\n        }\n        return this;\n    }\n    /**\n     * Remove an event from the timeline.\n     * @param  {Object}  event  The event object to remove from the list.\n     * @returns {Timeline} this\n     */\n    remove(event) {\n        const index = this._timeline.indexOf(event);\n        if (index !== -1) {\n            this._timeline.splice(index, 1);\n        }\n        return this;\n    }\n    /**\n     * Get the nearest event whose time is less than or equal to the given time.\n     * @param  time  The time to query.\n     */\n    get(time, param = "time") {\n        const index = this._search(time, param);\n        if (index !== -1) {\n            return this._timeline[index];\n        }\n        else {\n            return null;\n        }\n    }\n    /**\n     * Return the first event in the timeline without removing it\n     * @returns {Object} The first event object\n     */\n    peek() {\n        return this._timeline[0];\n    }\n    /**\n     * Return the first event in the timeline and remove it\n     */\n    shift() {\n        return this._timeline.shift();\n    }\n    /**\n     * Get the event which is scheduled after the given time.\n     * @param  time  The time to query.\n     */\n    getAfter(time, param = "time") {\n        const index = this._search(time, param);\n        if (index + 1 < this._timeline.length) {\n            return this._timeline[index + 1];\n        }\n        else {\n            return null;\n        }\n    }\n    /**\n     * Get the event before the event at the given time.\n     * @param  time  The time to query.\n     */\n    getBefore(time) {\n        const len = this._timeline.length;\n        // if it\'s after the last item, return the last item\n        if (len > 0 && this._timeline[len - 1].time < time) {\n            return this._timeline[len - 1];\n        }\n        const index = this._search(time);\n        if (index - 1 >= 0) {\n            return this._timeline[index - 1];\n        }\n        else {\n            return null;\n        }\n    }\n    /**\n     * Cancel events at and after the given time\n     * @param  after  The time to query.\n     */\n    cancel(after) {\n        if (this._timeline.length > 1) {\n            let index = this._search(after);\n            if (index >= 0) {\n                if (EQ(this._timeline[index].time, after)) {\n                    // get the first item with that time\n                    for (let i = index; i >= 0; i--) {\n                        if (EQ(this._timeline[i].time, after)) {\n                            index = i;\n                        }\n                        else {\n                            break;\n                        }\n                    }\n                    this._timeline = this._timeline.slice(0, index);\n                }\n                else {\n                    this._timeline = this._timeline.slice(0, index + 1);\n                }\n            }\n            else {\n                this._timeline = [];\n            }\n        }\n        else if (this._timeline.length === 1) {\n            // the first item\'s time\n            if (GTE(this._timeline[0].time, after)) {\n                this._timeline = [];\n            }\n        }\n        return this;\n    }\n    /**\n     * Cancel events before or equal to the given time.\n     * @param  time  The time to cancel before.\n     */\n    cancelBefore(time) {\n        const index = this._search(time);\n        if (index >= 0) {\n            this._timeline = this._timeline.slice(index + 1);\n        }\n        return this;\n    }\n    /**\n     * Returns the previous event if there is one. null otherwise\n     * @param  event The event to find the previous one of\n     * @return The event right before the given event\n     */\n    previousEvent(event) {\n        const index = this._timeline.indexOf(event);\n        if (index > 0) {\n            return this._timeline[index - 1];\n        }\n        else {\n            return null;\n        }\n    }\n    /**\n     * Does a binary search on the timeline array and returns the\n     * nearest event index whose time is after or equal to the given time.\n     * If a time is searched before the first index in the timeline, -1 is returned.\n     * If the time is after the end, the index of the last item is returned.\n     */\n    _search(time, param = "time") {\n        if (this._timeline.length === 0) {\n            return -1;\n        }\n        let beginning = 0;\n        const len = this._timeline.length;\n        let end = len;\n        if (len > 0 && this._timeline[len - 1][param] <= time) {\n            return len - 1;\n        }\n        while (beginning < end) {\n            // calculate the midpoint for roughly equal partition\n            let midPoint = Math.floor(beginning + (end - beginning) / 2);\n            const event = this._timeline[midPoint];\n            const nextEvent = this._timeline[midPoint + 1];\n            if (EQ(event[param], time)) {\n                // choose the last one that has the same time\n                for (let i = midPoint; i < this._timeline.length; i++) {\n                    const testEvent = this._timeline[i];\n                    if (EQ(testEvent[param], time)) {\n                        midPoint = i;\n                    }\n                    else {\n                        break;\n                    }\n                }\n                return midPoint;\n            }\n            else if (LT(event[param], time) && GT(nextEvent[param], time)) {\n                return midPoint;\n            }\n            else if (GT(event[param], time)) {\n                // search lower\n                end = midPoint;\n            }\n            else {\n                // search upper\n                beginning = midPoint + 1;\n            }\n        }\n        return -1;\n    }\n    /**\n     * Internal iterator. Applies extra safety checks for\n     * removing items from the array.\n     */\n    _iterate(callback, lowerBound = 0, upperBound = this._timeline.length - 1) {\n        this._timeline.slice(lowerBound, upperBound + 1).forEach(callback);\n    }\n    /**\n     * Iterate over everything in the array\n     * @param  callback The callback to invoke with every item\n     */\n    forEach(callback) {\n        this._iterate(callback);\n        return this;\n    }\n    /**\n     * Iterate over everything in the array at or before the given time.\n     * @param  time The time to check if items are before\n     * @param  callback The callback to invoke with every item\n     */\n    forEachBefore(time, callback) {\n        // iterate over the items in reverse so that removing an item doesn\'t break things\n        const upperBound = this._search(time);\n        if (upperBound !== -1) {\n            this._iterate(callback, 0, upperBound);\n        }\n        return this;\n    }\n    /**\n     * Iterate over everything in the array after the given time.\n     * @param  time The time to check if items are before\n     * @param  callback The callback to invoke with every item\n     */\n    forEachAfter(time, callback) {\n        // iterate over the items in reverse so that removing an item doesn\'t break things\n        const lowerBound = this._search(time);\n        this._iterate(callback, lowerBound + 1);\n        return this;\n    }\n    /**\n     * Iterate over everything in the array between the startTime and endTime.\n     * The timerange is inclusive of the startTime, but exclusive of the endTime.\n     * range = [startTime, endTime).\n     * @param  startTime The time to check if items are before\n     * @param  endTime The end of the test interval.\n     * @param  callback The callback to invoke with every item\n     */\n    forEachBetween(startTime, endTime, callback) {\n        let lowerBound = this._search(startTime);\n        let upperBound = this._search(endTime);\n        if (lowerBound !== -1 && upperBound !== -1) {\n            if (this._timeline[lowerBound].time !== startTime) {\n                lowerBound += 1;\n            }\n            // exclusive of the end time\n            if (this._timeline[upperBound].time === endTime) {\n                upperBound -= 1;\n            }\n            this._iterate(callback, lowerBound, upperBound);\n        }\n        else if (lowerBound === -1) {\n            this._iterate(callback, 0, upperBound);\n        }\n        return this;\n    }\n    /**\n     * Iterate over everything in the array at or after the given time. Similar to\n     * forEachAfter, but includes the item(s) at the given time.\n     * @param  time The time to check if items are before\n     * @param  callback The callback to invoke with every item\n     */\n    forEachFrom(time, callback) {\n        // iterate over the items in reverse so that removing an item doesn\'t break things\n        let lowerBound = this._search(time);\n        // work backwards until the event time is less than time\n        while (lowerBound >= 0 && this._timeline[lowerBound].time >= time) {\n            lowerBound--;\n        }\n        this._iterate(callback, lowerBound + 1);\n        return this;\n    }\n    /**\n     * Iterate over everything in the array at the given time\n     * @param  time The time to check if items are before\n     * @param  callback The callback to invoke with every item\n     */\n    forEachAtTime(time, callback) {\n        // iterate over the items in reverse so that removing an item doesn\'t break things\n        const upperBound = this._search(time);\n        if (upperBound !== -1 && EQ(this._timeline[upperBound].time, time)) {\n            let lowerBound = upperBound;\n            for (let i = upperBound; i >= 0; i--) {\n                if (EQ(this._timeline[i].time, time)) {\n                    lowerBound = i;\n                }\n                else {\n                    break;\n                }\n            }\n            this._iterate(event => {\n                callback(event);\n            }, lowerBound, upperBound);\n        }\n        return this;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._timeline = [];\n        return this;\n    }\n}\n//# sourceMappingURL=Timeline.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/ContextInitialization.js\n//-------------------------------------\n// INITIALIZING NEW CONTEXT\n//-------------------------------------\n/**\n * Array of callbacks to invoke when a new context is created\n */\nconst notifyNewContext = [];\n/**\n * Used internally to setup a new Context\n */\nfunction onContextInit(cb) {\n    notifyNewContext.push(cb);\n}\n/**\n * Invoke any classes which need to also be initialized when a new context is created.\n */\nfunction initializeContext(ctx) {\n    // add any additional modules\n    notifyNewContext.forEach(cb => cb(ctx));\n}\n/**\n * Array of callbacks to invoke when a new context is created\n */\nconst notifyCloseContext = [];\n/**\n * Used internally to tear down a Context\n */\nfunction onContextClose(cb) {\n    notifyCloseContext.push(cb);\n}\nfunction closeContext(ctx) {\n    // add any additional modules\n    notifyCloseContext.forEach(cb => cb(ctx));\n}\n//# sourceMappingURL=ContextInitialization.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Emitter.js\n\n\n/**\n * Emitter gives classes which extend it\n * the ability to listen for and emit events.\n * Inspiration and reference from Jerome Etienne\'s [MicroEvent](https://github.com/jeromeetienne/microevent.js).\n * MIT (c) 2011 Jerome Etienne.\n * @category Core\n */\nclass Emitter_Emitter extends Tone_Tone {\n    constructor() {\n        super(...arguments);\n        this.name = "Emitter";\n    }\n    /**\n     * Bind a callback to a specific event.\n     * @param  event     The name of the event to listen for.\n     * @param  callback  The callback to invoke when the event is emitted\n     */\n    on(event, callback) {\n        // split the event\n        const events = event.split(/\\W+/);\n        events.forEach(eventName => {\n            if (isUndef(this._events)) {\n                this._events = {};\n            }\n            if (!this._events.hasOwnProperty(eventName)) {\n                this._events[eventName] = [];\n            }\n            this._events[eventName].push(callback);\n        });\n        return this;\n    }\n    /**\n     * Bind a callback which is only invoked once\n     * @param  event     The name of the event to listen for.\n     * @param  callback  The callback to invoke when the event is emitted\n     */\n    once(event, callback) {\n        const boundCallback = (...args) => {\n            // invoke the callback\n            callback(...args);\n            // remove the event\n            this.off(event, boundCallback);\n        };\n        this.on(event, boundCallback);\n        return this;\n    }\n    /**\n     * Remove the event listener.\n     * @param  event     The event to stop listening to.\n     * @param  callback  The callback which was bound to the event with Emitter.on.\n     *                   If no callback is given, all callbacks events are removed.\n     */\n    off(event, callback) {\n        const events = event.split(/\\W+/);\n        events.forEach(eventName => {\n            if (isUndef(this._events)) {\n                this._events = {};\n            }\n            if (this._events.hasOwnProperty(event)) {\n                if (isUndef(callback)) {\n                    this._events[event] = [];\n                }\n                else {\n                    const eventList = this._events[event];\n                    for (let i = eventList.length - 1; i >= 0; i--) {\n                        if (eventList[i] === callback) {\n                            eventList.splice(i, 1);\n                        }\n                    }\n                }\n            }\n        });\n        return this;\n    }\n    /**\n     * Invoke all of the callbacks bound to the event\n     * with any arguments passed in.\n     * @param  event  The name of the event.\n     * @param args The arguments to pass to the functions listening.\n     */\n    emit(event, ...args) {\n        if (this._events) {\n            if (this._events.hasOwnProperty(event)) {\n                const eventList = this._events[event].slice(0);\n                for (let i = 0, len = eventList.length; i < len; i++) {\n                    eventList[i].apply(this, args);\n                }\n            }\n        }\n        return this;\n    }\n    /**\n     * Add Emitter functions (on/off/emit) to the object\n     */\n    static mixin(constr) {\n        // instance._events = {};\n        ["on", "once", "off", "emit"].forEach(name => {\n            const property = Object.getOwnPropertyDescriptor(Emitter_Emitter.prototype, name);\n            Object.defineProperty(constr.prototype, name, property);\n        });\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this._events = undefined;\n        return this;\n    }\n}\n//# sourceMappingURL=Emitter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/BaseContext.js\n\nclass BaseContext_BaseContext extends Emitter_Emitter {\n    constructor() {\n        super(...arguments);\n        this.isOffline = false;\n    }\n    /*\n     * This is a placeholder so that JSON.stringify does not throw an error\n     * This matches what JSON.stringify(audioContext) returns on a native\n     * audioContext instance.\n     */\n    toJSON() {\n        return {};\n    }\n}\n//# sourceMappingURL=BaseContext.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Context.js\n\n\n\n\n\n\n\n\n\n\n/**\n * Wrapper around the native AudioContext.\n * @category Core\n */\nclass Context_Context extends BaseContext_BaseContext {\n    constructor() {\n        super();\n        this.name = "Context";\n        /**\n         * An object containing all of the constants AudioBufferSourceNodes\n         */\n        this._constants = new Map();\n        /**\n         * All of the setTimeout events.\n         */\n        this._timeouts = new Timeline_Timeline();\n        /**\n         * The timeout id counter\n         */\n        this._timeoutIds = 0;\n        /**\n         * Private indicator if the context has been initialized\n         */\n        this._initialized = false;\n        /**\n         * Indicates if the context is an OfflineAudioContext or an AudioContext\n         */\n        this.isOffline = false;\n        //--------------------------------------------\n        // AUDIO WORKLET\n        //--------------------------------------------\n        /**\n         * Maps a module name to promise of the addModule method\n         */\n        this._workletModules = new Map();\n        const options = optionsFromArguments(Context_Context.getDefaults(), arguments, [\n            "context",\n        ]);\n        if (options.context) {\n            this._context = options.context;\n        }\n        else {\n            this._context = createAudioContext({\n                latencyHint: options.latencyHint,\n            });\n        }\n        this._ticker = new Ticker(this.emit.bind(this, "tick"), options.clockSource, options.updateInterval);\n        this.on("tick", this._timeoutLoop.bind(this));\n        // fwd events from the context\n        this._context.onstatechange = () => {\n            this.emit("statechange", this.state);\n        };\n        this._setLatencyHint(options.latencyHint);\n        this.lookAhead = options.lookAhead;\n    }\n    static getDefaults() {\n        return {\n            clockSource: "worker",\n            latencyHint: "interactive",\n            lookAhead: 0.1,\n            updateInterval: 0.05,\n        };\n    }\n    /**\n     * Finish setting up the context. **You usually do not need to do this manually.**\n     */\n    initialize() {\n        if (!this._initialized) {\n            // add any additional modules\n            initializeContext(this);\n            this._initialized = true;\n        }\n        return this;\n    }\n    //---------------------------\n    // BASE AUDIO CONTEXT METHODS\n    //---------------------------\n    createAnalyser() {\n        return this._context.createAnalyser();\n    }\n    createOscillator() {\n        return this._context.createOscillator();\n    }\n    createBufferSource() {\n        return this._context.createBufferSource();\n    }\n    createBiquadFilter() {\n        return this._context.createBiquadFilter();\n    }\n    createBuffer(numberOfChannels, length, sampleRate) {\n        return this._context.createBuffer(numberOfChannels, length, sampleRate);\n    }\n    createChannelMerger(numberOfInputs) {\n        return this._context.createChannelMerger(numberOfInputs);\n    }\n    createChannelSplitter(numberOfOutputs) {\n        return this._context.createChannelSplitter(numberOfOutputs);\n    }\n    createConstantSource() {\n        return this._context.createConstantSource();\n    }\n    createConvolver() {\n        return this._context.createConvolver();\n    }\n    createDelay(maxDelayTime) {\n        return this._context.createDelay(maxDelayTime);\n    }\n    createDynamicsCompressor() {\n        return this._context.createDynamicsCompressor();\n    }\n    createGain() {\n        return this._context.createGain();\n    }\n    createIIRFilter(feedForward, feedback) {\n        // @ts-ignore\n        return this._context.createIIRFilter(feedForward, feedback);\n    }\n    createPanner() {\n        return this._context.createPanner();\n    }\n    createPeriodicWave(real, imag, constraints) {\n        return this._context.createPeriodicWave(real, imag, constraints);\n    }\n    createStereoPanner() {\n        return this._context.createStereoPanner();\n    }\n    createWaveShaper() {\n        return this._context.createWaveShaper();\n    }\n    createMediaStreamSource(stream) {\n        assert(isAudioContext(this._context), "Not available if OfflineAudioContext");\n        const context = this._context;\n        return context.createMediaStreamSource(stream);\n    }\n    createMediaElementSource(element) {\n        assert(isAudioContext(this._context), "Not available if OfflineAudioContext");\n        const context = this._context;\n        return context.createMediaElementSource(element);\n    }\n    createMediaStreamDestination() {\n        assert(isAudioContext(this._context), "Not available if OfflineAudioContext");\n        const context = this._context;\n        return context.createMediaStreamDestination();\n    }\n    decodeAudioData(audioData) {\n        return this._context.decodeAudioData(audioData);\n    }\n    /**\n     * The current time in seconds of the AudioContext.\n     */\n    get currentTime() {\n        return this._context.currentTime;\n    }\n    /**\n     * The current time in seconds of the AudioContext.\n     */\n    get state() {\n        return this._context.state;\n    }\n    /**\n     * The current time in seconds of the AudioContext.\n     */\n    get sampleRate() {\n        return this._context.sampleRate;\n    }\n    /**\n     * The listener\n     */\n    get listener() {\n        this.initialize();\n        return this._listener;\n    }\n    set listener(l) {\n        assert(!this._initialized, "The listener cannot be set after initialization.");\n        this._listener = l;\n    }\n    /**\n     * There is only one Transport per Context. It is created on initialization.\n     */\n    get transport() {\n        this.initialize();\n        return this._transport;\n    }\n    set transport(t) {\n        assert(!this._initialized, "The transport cannot be set after initialization.");\n        this._transport = t;\n    }\n    /**\n     * This is the Draw object for the context which is useful for synchronizing the draw frame with the Tone.js clock.\n     */\n    get draw() {\n        this.initialize();\n        return this._draw;\n    }\n    set draw(d) {\n        assert(!this._initialized, "Draw cannot be set after initialization.");\n        this._draw = d;\n    }\n    /**\n     * A reference to the Context\'s destination node.\n     */\n    get destination() {\n        this.initialize();\n        return this._destination;\n    }\n    set destination(d) {\n        assert(!this._initialized, "The destination cannot be set after initialization.");\n        this._destination = d;\n    }\n    /**\n     * Create an audio worklet node from a name and options. The module\n     * must first be loaded using [[addAudioWorkletModule]].\n     */\n    createAudioWorkletNode(name, options) {\n        return createAudioWorkletNode(this.rawContext, name, options);\n    }\n    /**\n     * Add an AudioWorkletProcessor module\n     * @param url The url of the module\n     * @param name The name of the module\n     */\n    addAudioWorkletModule(url, name) {\n        return __awaiter(this, void 0, void 0, function* () {\n            assert(isDefined(this.rawContext.audioWorklet), "AudioWorkletNode is only available in a secure context (https or localhost)");\n            if (!this._workletModules.has(name)) {\n                this._workletModules.set(name, this.rawContext.audioWorklet.addModule(url));\n            }\n            yield this._workletModules.get(name);\n        });\n    }\n    /**\n     * Returns a promise which resolves when all of the worklets have been loaded on this context\n     */\n    workletsAreReady() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const promises = [];\n            this._workletModules.forEach((promise) => promises.push(promise));\n            yield Promise.all(promises);\n        });\n    }\n    //---------------------------\n    // TICKER\n    //---------------------------\n    /**\n     * How often the interval callback is invoked.\n     * This number corresponds to how responsive the scheduling\n     * can be. context.updateInterval + context.lookAhead gives you the\n     * total latency between scheduling an event and hearing it.\n     */\n    get updateInterval() {\n        return this._ticker.updateInterval;\n    }\n    set updateInterval(interval) {\n        this._ticker.updateInterval = interval;\n    }\n    /**\n     * What the source of the clock is, either "worker" (default),\n     * "timeout", or "offline" (none).\n     */\n    get clockSource() {\n        return this._ticker.type;\n    }\n    set clockSource(type) {\n        this._ticker.type = type;\n    }\n    /**\n     * The type of playback, which affects tradeoffs between audio\n     * output latency and responsiveness.\n     * In addition to setting the value in seconds, the latencyHint also\n     * accepts the strings "interactive" (prioritizes low latency),\n     * "playback" (prioritizes sustained playback), "balanced" (balances\n     * latency and performance).\n     * @example\n     * // prioritize sustained playback\n     * const context = new Tone.Context({ latencyHint: "playback" });\n     * // set this context as the global Context\n     * Tone.setContext(context);\n     * // the global context is gettable with Tone.getContext()\n     * console.log(Tone.getContext().latencyHint);\n     */\n    get latencyHint() {\n        return this._latencyHint;\n    }\n    /**\n     * Update the lookAhead and updateInterval based on the latencyHint\n     */\n    _setLatencyHint(hint) {\n        let lookAheadValue = 0;\n        this._latencyHint = hint;\n        if (isString(hint)) {\n            switch (hint) {\n                case "interactive":\n                    lookAheadValue = 0.1;\n                    break;\n                case "playback":\n                    lookAheadValue = 0.5;\n                    break;\n                case "balanced":\n                    lookAheadValue = 0.25;\n                    break;\n            }\n        }\n        this.lookAhead = lookAheadValue;\n        this.updateInterval = lookAheadValue / 2;\n    }\n    /**\n     * The unwrapped AudioContext or OfflineAudioContext\n     */\n    get rawContext() {\n        return this._context;\n    }\n    /**\n     * The current audio context time plus a short [[lookAhead]].\n     */\n    now() {\n        return this._context.currentTime + this.lookAhead;\n    }\n    /**\n     * The current audio context time without the [[lookAhead]].\n     * In most cases it is better to use [[now]] instead of [[immediate]] since\n     * with [[now]] the [[lookAhead]] is applied equally to _all_ components including internal components,\n     * to making sure that everything is scheduled in sync. Mixing [[now]] and [[immediate]]\n     * can cause some timing issues. If no lookAhead is desired, you can set the [[lookAhead]] to `0`.\n     */\n    immediate() {\n        return this._context.currentTime;\n    }\n    /**\n     * Starts the audio context from a suspended state. This is required\n     * to initially start the AudioContext. See [[Tone.start]]\n     */\n    resume() {\n        if (isAudioContext(this._context)) {\n            return this._context.resume();\n        }\n        else {\n            return Promise.resolve();\n        }\n    }\n    /**\n     * Close the context. Once closed, the context can no longer be used and\n     * any AudioNodes created from the context will be silent.\n     */\n    close() {\n        return __awaiter(this, void 0, void 0, function* () {\n            if (isAudioContext(this._context)) {\n                yield this._context.close();\n            }\n            if (this._initialized) {\n                closeContext(this);\n            }\n        });\n    }\n    /**\n     * **Internal** Generate a looped buffer at some constant value.\n     */\n    getConstant(val) {\n        if (this._constants.has(val)) {\n            return this._constants.get(val);\n        }\n        else {\n            const buffer = this._context.createBuffer(1, 128, this._context.sampleRate);\n            const arr = buffer.getChannelData(0);\n            for (let i = 0; i < arr.length; i++) {\n                arr[i] = val;\n            }\n            const constant = this._context.createBufferSource();\n            constant.channelCount = 1;\n            constant.channelCountMode = "explicit";\n            constant.buffer = buffer;\n            constant.loop = true;\n            constant.start(0);\n            this._constants.set(val, constant);\n            return constant;\n        }\n    }\n    /**\n     * Clean up. Also closes the audio context.\n     */\n    dispose() {\n        super.dispose();\n        this._ticker.dispose();\n        this._timeouts.dispose();\n        Object.keys(this._constants).map((val) => this._constants[val].disconnect());\n        return this;\n    }\n    //---------------------------\n    // TIMEOUTS\n    //---------------------------\n    /**\n     * The private loop which keeps track of the context scheduled timeouts\n     * Is invoked from the clock source\n     */\n    _timeoutLoop() {\n        const now = this.now();\n        let firstEvent = this._timeouts.peek();\n        while (this._timeouts.length && firstEvent && firstEvent.time <= now) {\n            // invoke the callback\n            firstEvent.callback();\n            // shift the first event off\n            this._timeouts.shift();\n            // get the next one\n            firstEvent = this._timeouts.peek();\n        }\n    }\n    /**\n     * A setTimeout which is guaranteed by the clock source.\n     * Also runs in the offline context.\n     * @param  fn       The callback to invoke\n     * @param  timeout  The timeout in seconds\n     * @returns ID to use when invoking Context.clearTimeout\n     */\n    setTimeout(fn, timeout) {\n        this._timeoutIds++;\n        const now = this.now();\n        this._timeouts.add({\n            callback: fn,\n            id: this._timeoutIds,\n            time: now + timeout,\n        });\n        return this._timeoutIds;\n    }\n    /**\n     * Clears a previously scheduled timeout with Tone.context.setTimeout\n     * @param  id  The ID returned from setTimeout\n     */\n    clearTimeout(id) {\n        this._timeouts.forEach((event) => {\n            if (event.id === id) {\n                this._timeouts.remove(event);\n            }\n        });\n        return this;\n    }\n    /**\n     * Clear the function scheduled by [[setInterval]]\n     */\n    clearInterval(id) {\n        return this.clearTimeout(id);\n    }\n    /**\n     * Adds a repeating event to the context\'s callback clock\n     */\n    setInterval(fn, interval) {\n        const id = ++this._timeoutIds;\n        const intervalFn = () => {\n            const now = this.now();\n            this._timeouts.add({\n                callback: () => {\n                    // invoke the callback\n                    fn();\n                    // invoke the event to repeat it\n                    intervalFn();\n                },\n                id,\n                time: now + interval,\n            });\n        };\n        // kick it off\n        intervalFn();\n        return id;\n    }\n}\n//# sourceMappingURL=Context.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/DummyContext.js\n\n\nclass DummyContext_DummyContext extends BaseContext_BaseContext {\n    constructor() {\n        super(...arguments);\n        this.lookAhead = 0;\n        this.latencyHint = 0;\n        this.isOffline = false;\n    }\n    //---------------------------\n    // BASE AUDIO CONTEXT METHODS\n    //---------------------------\n    createAnalyser() {\n        return {};\n    }\n    createOscillator() {\n        return {};\n    }\n    createBufferSource() {\n        return {};\n    }\n    createBiquadFilter() {\n        return {};\n    }\n    createBuffer(_numberOfChannels, _length, _sampleRate) {\n        return {};\n    }\n    createChannelMerger(_numberOfInputs) {\n        return {};\n    }\n    createChannelSplitter(_numberOfOutputs) {\n        return {};\n    }\n    createConstantSource() {\n        return {};\n    }\n    createConvolver() {\n        return {};\n    }\n    createDelay(_maxDelayTime) {\n        return {};\n    }\n    createDynamicsCompressor() {\n        return {};\n    }\n    createGain() {\n        return {};\n    }\n    createIIRFilter(_feedForward, _feedback) {\n        return {};\n    }\n    createPanner() {\n        return {};\n    }\n    createPeriodicWave(_real, _imag, _constraints) {\n        return {};\n    }\n    createStereoPanner() {\n        return {};\n    }\n    createWaveShaper() {\n        return {};\n    }\n    createMediaStreamSource(_stream) {\n        return {};\n    }\n    createMediaElementSource(_element) {\n        return {};\n    }\n    createMediaStreamDestination() {\n        return {};\n    }\n    decodeAudioData(_audioData) {\n        return Promise.resolve({});\n    }\n    //---------------------------\n    // TONE AUDIO CONTEXT METHODS\n    //---------------------------\n    createAudioWorkletNode(_name, _options) {\n        return {};\n    }\n    get rawContext() {\n        return {};\n    }\n    addAudioWorkletModule(_url, _name) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return Promise.resolve();\n        });\n    }\n    resume() {\n        return Promise.resolve();\n    }\n    setTimeout(_fn, _timeout) {\n        return 0;\n    }\n    clearTimeout(_id) {\n        return this;\n    }\n    setInterval(_fn, _interval) {\n        return 0;\n    }\n    clearInterval(_id) {\n        return this;\n    }\n    getConstant(_val) {\n        return {};\n    }\n    get currentTime() {\n        return 0;\n    }\n    get state() {\n        return {};\n    }\n    get sampleRate() {\n        return 0;\n    }\n    get listener() {\n        return {};\n    }\n    get transport() {\n        return {};\n    }\n    get draw() {\n        return {};\n    }\n    set draw(_d) { }\n    get destination() {\n        return {};\n    }\n    set destination(_d) { }\n    now() {\n        return 0;\n    }\n    immediate() {\n        return 0;\n    }\n}\n//# sourceMappingURL=DummyContext.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Interface.js\n\n/**\n * Make the property not writable using `defineProperty`. Internal use only.\n */\nfunction readOnly(target, property) {\n    if (isArray(property)) {\n        property.forEach(str => readOnly(target, str));\n    }\n    else {\n        Object.defineProperty(target, property, {\n            enumerable: true,\n            writable: false,\n        });\n    }\n}\n/**\n * Make an attribute writeable. Internal use only.\n */\nfunction writable(target, property) {\n    if (isArray(property)) {\n        property.forEach(str => writable(target, str));\n    }\n    else {\n        Object.defineProperty(target, property, {\n            writable: true,\n        });\n    }\n}\nconst noOp = () => {\n    // no operation here!\n};\n//# sourceMappingURL=Interface.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/ToneAudioBuffer.js\n\n\n\n\n\n\n\n\n/**\n * AudioBuffer loading and storage. ToneAudioBuffer is used internally by all\n * classes that make requests for audio files such as Tone.Player,\n * Tone.Sampler and Tone.Convolver.\n * @example\n * const buffer = new Tone.ToneAudioBuffer("https://tonejs.github.io/audio/casio/A1.mp3", () => {\n * \tconsole.log("loaded");\n * });\n * @category Core\n */\nclass ToneAudioBuffer_ToneAudioBuffer extends Tone_Tone {\n    constructor() {\n        super();\n        this.name = "ToneAudioBuffer";\n        /**\n         * Callback when the buffer is loaded.\n         */\n        this.onload = noOp;\n        const options = optionsFromArguments(ToneAudioBuffer_ToneAudioBuffer.getDefaults(), arguments, ["url", "onload", "onerror"]);\n        this.reverse = options.reverse;\n        this.onload = options.onload;\n        if (options.url && isAudioBuffer(options.url) || options.url instanceof ToneAudioBuffer_ToneAudioBuffer) {\n            this.set(options.url);\n        }\n        else if (isString(options.url)) {\n            // initiate the download\n            this.load(options.url).catch(options.onerror);\n        }\n    }\n    static getDefaults() {\n        return {\n            onerror: noOp,\n            onload: noOp,\n            reverse: false,\n        };\n    }\n    /**\n     * The sample rate of the AudioBuffer\n     */\n    get sampleRate() {\n        if (this._buffer) {\n            return this._buffer.sampleRate;\n        }\n        else {\n            return getContext().sampleRate;\n        }\n    }\n    /**\n     * Pass in an AudioBuffer or ToneAudioBuffer to set the value of this buffer.\n     */\n    set(buffer) {\n        if (buffer instanceof ToneAudioBuffer_ToneAudioBuffer) {\n            // if it\'s loaded, set it\n            if (buffer.loaded) {\n                this._buffer = buffer.get();\n            }\n            else {\n                // otherwise when it\'s loaded, invoke it\'s callback\n                buffer.onload = () => {\n                    this.set(buffer);\n                    this.onload(this);\n                };\n            }\n        }\n        else {\n            this._buffer = buffer;\n        }\n        // reverse it initially\n        if (this._reversed) {\n            this._reverse();\n        }\n        return this;\n    }\n    /**\n     * The audio buffer stored in the object.\n     */\n    get() {\n        return this._buffer;\n    }\n    /**\n     * Makes an fetch request for the selected url then decodes the file as an audio buffer.\n     * Invokes the callback once the audio buffer loads.\n     * @param url The url of the buffer to load. filetype support depends on the browser.\n     * @returns A Promise which resolves with this ToneAudioBuffer\n     */\n    load(url) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const doneLoading = ToneAudioBuffer_ToneAudioBuffer.load(url).then(audioBuffer => {\n                this.set(audioBuffer);\n                // invoke the onload method\n                this.onload(this);\n            });\n            ToneAudioBuffer_ToneAudioBuffer.downloads.push(doneLoading);\n            try {\n                yield doneLoading;\n            }\n            finally {\n                // remove the downloaded file\n                const index = ToneAudioBuffer_ToneAudioBuffer.downloads.indexOf(doneLoading);\n                ToneAudioBuffer_ToneAudioBuffer.downloads.splice(index, 1);\n            }\n            return this;\n        });\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        this._buffer = undefined;\n        return this;\n    }\n    /**\n     * Set the audio buffer from the array.\n     * To create a multichannel AudioBuffer, pass in a multidimensional array.\n     * @param array The array to fill the audio buffer\n     */\n    fromArray(array) {\n        const isMultidimensional = isArray(array) && array[0].length > 0;\n        const channels = isMultidimensional ? array.length : 1;\n        const len = isMultidimensional ? array[0].length : array.length;\n        const context = getContext();\n        const buffer = context.createBuffer(channels, len, context.sampleRate);\n        const multiChannelArray = !isMultidimensional && channels === 1 ?\n            [array] : array;\n        for (let c = 0; c < channels; c++) {\n            buffer.copyToChannel(multiChannelArray[c], c);\n        }\n        this._buffer = buffer;\n        return this;\n    }\n    /**\n     * Sums multiple channels into 1 channel\n     * @param chanNum Optionally only copy a single channel from the array.\n     */\n    toMono(chanNum) {\n        if (isNumber(chanNum)) {\n            this.fromArray(this.toArray(chanNum));\n        }\n        else {\n            let outputArray = new Float32Array(this.length);\n            const numChannels = this.numberOfChannels;\n            for (let channel = 0; channel < numChannels; channel++) {\n                const channelArray = this.toArray(channel);\n                for (let i = 0; i < channelArray.length; i++) {\n                    outputArray[i] += channelArray[i];\n                }\n            }\n            // divide by the number of channels\n            outputArray = outputArray.map(sample => sample / numChannels);\n            this.fromArray(outputArray);\n        }\n        return this;\n    }\n    /**\n     * Get the buffer as an array. Single channel buffers will return a 1-dimensional\n     * Float32Array, and multichannel buffers will return multidimensional arrays.\n     * @param channel Optionally only copy a single channel from the array.\n     */\n    toArray(channel) {\n        if (isNumber(channel)) {\n            return this.getChannelData(channel);\n        }\n        else if (this.numberOfChannels === 1) {\n            return this.toArray(0);\n        }\n        else {\n            const ret = [];\n            for (let c = 0; c < this.numberOfChannels; c++) {\n                ret[c] = this.getChannelData(c);\n            }\n            return ret;\n        }\n    }\n    /**\n     * Returns the Float32Array representing the PCM audio data for the specific channel.\n     * @param  channel  The channel number to return\n     * @return The audio as a TypedArray\n     */\n    getChannelData(channel) {\n        if (this._buffer) {\n            return this._buffer.getChannelData(channel);\n        }\n        else {\n            return new Float32Array(0);\n        }\n    }\n    /**\n     * Cut a subsection of the array and return a buffer of the\n     * subsection. Does not modify the original buffer\n     * @param start The time to start the slice\n     * @param end The end time to slice. If none is given will default to the end of the buffer\n     */\n    slice(start, end = this.duration) {\n        const startSamples = Math.floor(start * this.sampleRate);\n        const endSamples = Math.floor(end * this.sampleRate);\n        assert(startSamples < endSamples, "The start time must be less than the end time");\n        const length = endSamples - startSamples;\n        const retBuffer = getContext().createBuffer(this.numberOfChannels, length, this.sampleRate);\n        for (let channel = 0; channel < this.numberOfChannels; channel++) {\n            retBuffer.copyToChannel(this.getChannelData(channel).subarray(startSamples, endSamples), channel);\n        }\n        return new ToneAudioBuffer_ToneAudioBuffer(retBuffer);\n    }\n    /**\n     * Reverse the buffer.\n     */\n    _reverse() {\n        if (this.loaded) {\n            for (let i = 0; i < this.numberOfChannels; i++) {\n                this.getChannelData(i).reverse();\n            }\n        }\n        return this;\n    }\n    /**\n     * If the buffer is loaded or not\n     */\n    get loaded() {\n        return this.length > 0;\n    }\n    /**\n     * The duration of the buffer in seconds.\n     */\n    get duration() {\n        if (this._buffer) {\n            return this._buffer.duration;\n        }\n        else {\n            return 0;\n        }\n    }\n    /**\n     * The length of the buffer in samples\n     */\n    get length() {\n        if (this._buffer) {\n            return this._buffer.length;\n        }\n        else {\n            return 0;\n        }\n    }\n    /**\n     * The number of discrete audio channels. Returns 0 if no buffer is loaded.\n     */\n    get numberOfChannels() {\n        if (this._buffer) {\n            return this._buffer.numberOfChannels;\n        }\n        else {\n            return 0;\n        }\n    }\n    /**\n     * Reverse the buffer.\n     */\n    get reverse() {\n        return this._reversed;\n    }\n    set reverse(rev) {\n        if (this._reversed !== rev) {\n            this._reversed = rev;\n            this._reverse();\n        }\n    }\n    /**\n     * Create a ToneAudioBuffer from the array. To create a multichannel AudioBuffer,\n     * pass in a multidimensional array.\n     * @param array The array to fill the audio buffer\n     * @return A ToneAudioBuffer created from the array\n     */\n    static fromArray(array) {\n        return (new ToneAudioBuffer_ToneAudioBuffer()).fromArray(array);\n    }\n    /**\n     * Creates a ToneAudioBuffer from a URL, returns a promise which resolves to a ToneAudioBuffer\n     * @param  url The url to load.\n     * @return A promise which resolves to a ToneAudioBuffer\n     */\n    static fromUrl(url) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const buffer = new ToneAudioBuffer_ToneAudioBuffer();\n            return yield buffer.load(url);\n        });\n    }\n    /**\n     * Loads a url using fetch and returns the AudioBuffer.\n     */\n    static load(url) {\n        return __awaiter(this, void 0, void 0, function* () {\n            // test if the url contains multiple extensions\n            const matches = url.match(/\\[([^\\]\\[]+\\|.+)\\]$/);\n            if (matches) {\n                const extensions = matches[1].split("|");\n                let extension = extensions[0];\n                for (const ext of extensions) {\n                    if (ToneAudioBuffer_ToneAudioBuffer.supportsType(ext)) {\n                        extension = ext;\n                        break;\n                    }\n                }\n                url = url.replace(matches[0], extension);\n            }\n            // make sure there is a slash between the baseUrl and the url\n            const baseUrl = ToneAudioBuffer_ToneAudioBuffer.baseUrl === "" || ToneAudioBuffer_ToneAudioBuffer.baseUrl.endsWith("/") ? ToneAudioBuffer_ToneAudioBuffer.baseUrl : ToneAudioBuffer_ToneAudioBuffer.baseUrl + "/";\n            const response = yield fetch(baseUrl + url);\n            if (!response.ok) {\n                throw new Error(`could not load url: ${url}`);\n            }\n            const arrayBuffer = yield response.arrayBuffer();\n            const audioBuffer = yield getContext().decodeAudioData(arrayBuffer);\n            return audioBuffer;\n        });\n    }\n    /**\n     * Checks a url\'s extension to see if the current browser can play that file type.\n     * @param url The url/extension to test\n     * @return If the file extension can be played\n     * @static\n     * @example\n     * Tone.ToneAudioBuffer.supportsType("wav"); // returns true\n     * Tone.ToneAudioBuffer.supportsType("path/to/file.wav"); // returns true\n     */\n    static supportsType(url) {\n        const extensions = url.split(".");\n        const extension = extensions[extensions.length - 1];\n        const response = document.createElement("audio").canPlayType("audio/" + extension);\n        return response !== "";\n    }\n    /**\n     * Returns a Promise which resolves when all of the buffers have loaded\n     */\n    static loaded() {\n        return __awaiter(this, void 0, void 0, function* () {\n            // this makes sure that the function is always async\n            yield Promise.resolve();\n            while (ToneAudioBuffer_ToneAudioBuffer.downloads.length) {\n                yield ToneAudioBuffer_ToneAudioBuffer.downloads[0];\n            }\n        });\n    }\n}\n//-------------------------------------\n// STATIC METHODS\n//-------------------------------------\n/**\n * A path which is prefixed before every url.\n */\nToneAudioBuffer_ToneAudioBuffer.baseUrl = "";\n/**\n * All of the downloads\n */\nToneAudioBuffer_ToneAudioBuffer.downloads = [];\n//# sourceMappingURL=ToneAudioBuffer.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/OfflineContext.js\n\n\n\n\n\n/**\n * Wrapper around the OfflineAudioContext\n * @category Core\n * @example\n * // generate a single channel, 0.5 second buffer\n * const context = new Tone.OfflineContext(1, 0.5, 44100);\n * const osc = new Tone.Oscillator({ context });\n * context.render().then(buffer => {\n * \tconsole.log(buffer.numberOfChannels, buffer.duration);\n * });\n */\nclass OfflineContext_OfflineContext extends Context_Context {\n    constructor() {\n        super({\n            clockSource: "offline",\n            context: isOfflineAudioContext(arguments[0]) ?\n                arguments[0] : createOfflineAudioContext(arguments[0], arguments[1] * arguments[2], arguments[2]),\n            lookAhead: 0,\n            updateInterval: isOfflineAudioContext(arguments[0]) ?\n                128 / arguments[0].sampleRate : 128 / arguments[2],\n        });\n        this.name = "OfflineContext";\n        /**\n         * An artificial clock source\n         */\n        this._currentTime = 0;\n        this.isOffline = true;\n        this._duration = isOfflineAudioContext(arguments[0]) ?\n            arguments[0].length / arguments[0].sampleRate : arguments[1];\n    }\n    /**\n     * Override the now method to point to the internal clock time\n     */\n    now() {\n        return this._currentTime;\n    }\n    /**\n     * Same as this.now()\n     */\n    get currentTime() {\n        return this._currentTime;\n    }\n    /**\n     * Render just the clock portion of the audio context.\n     */\n    _renderClock(asynchronous) {\n        return __awaiter(this, void 0, void 0, function* () {\n            let index = 0;\n            while (this._duration - this._currentTime >= 0) {\n                // invoke all the callbacks on that time\n                this.emit("tick");\n                // increment the clock in block-sized chunks\n                this._currentTime += 128 / this.sampleRate;\n                // yield once a second of audio\n                index++;\n                const yieldEvery = Math.floor(this.sampleRate / 128);\n                if (asynchronous && index % yieldEvery === 0) {\n                    yield new Promise(done => setTimeout(done, 1));\n                }\n            }\n        });\n    }\n    /**\n     * Render the output of the OfflineContext\n     * @param asynchronous If the clock should be rendered asynchronously, which will not block the main thread, but be slightly slower.\n     */\n    render(asynchronous = true) {\n        return __awaiter(this, void 0, void 0, function* () {\n            yield this.workletsAreReady();\n            yield this._renderClock(asynchronous);\n            const buffer = yield this._context.startRendering();\n            return new ToneAudioBuffer_ToneAudioBuffer(buffer);\n        });\n    }\n    /**\n     * Close the context\n     */\n    close() {\n        return Promise.resolve();\n    }\n}\n//# sourceMappingURL=OfflineContext.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/Global.js\n\n\n\n\n\n\n/**\n * This dummy context is used to avoid throwing immediate errors when importing in Node.js\n */\nconst dummyContext = new DummyContext_DummyContext();\n/**\n * The global audio context which is getable and assignable through\n * getContext and setContext\n */\nlet globalContext = dummyContext;\n/**\n * Returns the default system-wide [[Context]]\n * @category Core\n */\nfunction getContext() {\n    if (globalContext === dummyContext && hasAudioContext) {\n        setContext(new Context_Context());\n    }\n    return globalContext;\n}\n/**\n * Set the default audio context\n * @category Core\n */\nfunction setContext(context) {\n    if (isAudioContext(context)) {\n        globalContext = new Context_Context(context);\n    }\n    else if (isOfflineAudioContext(context)) {\n        globalContext = new OfflineContext_OfflineContext(context);\n    }\n    else {\n        globalContext = context;\n    }\n}\n/**\n * Most browsers will not play _any_ audio until a user\n * clicks something (like a play button). Invoke this method\n * on a click or keypress event handler to start the audio context.\n * More about the Autoplay policy\n * [here](https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio)\n * @example\n * document.querySelector("button").addEventListener("click", async () => {\n * \tawait Tone.start();\n * \tconsole.log("context started");\n * });\n * @category Core\n */\nfunction Global_start() {\n    return globalContext.resume();\n}\n/**\n * Log Tone.js + version in the console.\n */\nif (theWindow && !theWindow.TONE_SILENCE_LOGGING) {\n    let prefix = "v";\n    if (version === "dev") {\n        prefix = "";\n    }\n    const printString = ` * Tone.js ${prefix}${version} * `;\n    // eslint-disable-next-line no-console\n    console.log(`%c${printString}`, "background: #000; color: #fff");\n}\n//# sourceMappingURL=Global.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/Conversions.js\n/**\n * Equal power gain scale. Good for cross-fading.\n * @param  percent (0-1)\n */\nfunction equalPowerScale(percent) {\n    const piFactor = 0.5 * Math.PI;\n    return Math.sin(percent * piFactor);\n}\n/**\n * Convert decibels into gain.\n */\nfunction dbToGain(db) {\n    return Math.pow(10, db / 20);\n}\n/**\n * Convert gain to decibels.\n */\nfunction gainToDb(gain) {\n    return 20 * (Math.log(gain) / Math.LN10);\n}\n/**\n * Convert an interval (in semitones) to a frequency ratio.\n * @param interval the number of semitones above the base note\n * @example\n * Tone.intervalToFrequencyRatio(0); // 1\n * Tone.intervalToFrequencyRatio(12); // 2\n * Tone.intervalToFrequencyRatio(-12); // 0.5\n */\nfunction intervalToFrequencyRatio(interval) {\n    return Math.pow(2, (interval / 12));\n}\n/**\n * The Global [concert tuning pitch](https://en.wikipedia.org/wiki/Concert_pitch) which is used\n * to generate all the other pitch values from notes. A4\'s values in Hertz.\n */\nlet A4 = 440;\nfunction getA4() {\n    return A4;\n}\nfunction setA4(freq) {\n    A4 = freq;\n}\n/**\n * Convert a frequency value to a MIDI note.\n * @param frequency The value to frequency value to convert.\n * @example\n * Tone.ftom(440); // returns 69\n */\nfunction ftom(frequency) {\n    return Math.round(ftomf(frequency));\n}\n/**\n * Convert a frequency to a floating point midi value\n */\nfunction ftomf(frequency) {\n    return 69 + 12 * Math.log2(frequency / A4);\n}\n/**\n * Convert a MIDI note to frequency value.\n * @param  midi The midi number to convert.\n * @return The corresponding frequency value\n * @example\n * Tone.mtof(69); // 440\n */\nfunction mtof(midi) {\n    return A4 * Math.pow(2, (midi - 69) / 12);\n}\n//# sourceMappingURL=Conversions.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/TimeBase.js\n\n\n/**\n * TimeBase is a flexible encoding of time which can be evaluated to and from a string.\n */\nclass TimeBase_TimeBaseClass extends Tone_Tone {\n    /**\n     * @param context The context associated with the time value. Used to compute\n     * Transport and context-relative timing.\n     * @param  value  The time value as a number, string or object\n     * @param  units  Unit values\n     */\n    constructor(context, value, units) {\n        super();\n        /**\n         * The default units\n         */\n        this.defaultUnits = "s";\n        this._val = value;\n        this._units = units;\n        this.context = context;\n        this._expressions = this._getExpressions();\n    }\n    /**\n     * All of the time encoding expressions\n     */\n    _getExpressions() {\n        return {\n            hz: {\n                method: (value) => {\n                    return this._frequencyToUnits(parseFloat(value));\n                },\n                regexp: /^(\\d+(?:\\.\\d+)?)hz$/i,\n            },\n            i: {\n                method: (value) => {\n                    return this._ticksToUnits(parseInt(value, 10));\n                },\n                regexp: /^(\\d+)i$/i,\n            },\n            m: {\n                method: (value) => {\n                    return this._beatsToUnits(parseInt(value, 10) * this._getTimeSignature());\n                },\n                regexp: /^(\\d+)m$/i,\n            },\n            n: {\n                method: (value, dot) => {\n                    const numericValue = parseInt(value, 10);\n                    const scalar = dot === "." ? 1.5 : 1;\n                    if (numericValue === 1) {\n                        return this._beatsToUnits(this._getTimeSignature()) * scalar;\n                    }\n                    else {\n                        return this._beatsToUnits(4 / numericValue) * scalar;\n                    }\n                },\n                regexp: /^(\\d+)n(\\.?)$/i,\n            },\n            number: {\n                method: (value) => {\n                    return this._expressions[this.defaultUnits].method.call(this, value);\n                },\n                regexp: /^(\\d+(?:\\.\\d+)?)$/,\n            },\n            s: {\n                method: (value) => {\n                    return this._secondsToUnits(parseFloat(value));\n                },\n                regexp: /^(\\d+(?:\\.\\d+)?)s$/,\n            },\n            samples: {\n                method: (value) => {\n                    return parseInt(value, 10) / this.context.sampleRate;\n                },\n                regexp: /^(\\d+)samples$/,\n            },\n            t: {\n                method: (value) => {\n                    const numericValue = parseInt(value, 10);\n                    return this._beatsToUnits(8 / (Math.floor(numericValue) * 3));\n                },\n                regexp: /^(\\d+)t$/i,\n            },\n            tr: {\n                method: (m, q, s) => {\n                    let total = 0;\n                    if (m && m !== "0") {\n                        total += this._beatsToUnits(this._getTimeSignature() * parseFloat(m));\n                    }\n                    if (q && q !== "0") {\n                        total += this._beatsToUnits(parseFloat(q));\n                    }\n                    if (s && s !== "0") {\n                        total += this._beatsToUnits(parseFloat(s) / 4);\n                    }\n                    return total;\n                },\n                regexp: /^(\\d+(?:\\.\\d+)?):(\\d+(?:\\.\\d+)?):?(\\d+(?:\\.\\d+)?)?$/,\n            },\n        };\n    }\n    //-------------------------------------\n    // \tVALUE OF\n    //-------------------------------------\n    /**\n     * Evaluate the time value. Returns the time in seconds.\n     */\n    valueOf() {\n        if (this._val instanceof TimeBase_TimeBaseClass) {\n            this.fromType(this._val);\n        }\n        if (isUndef(this._val)) {\n            return this._noArg();\n        }\n        else if (isString(this._val) && isUndef(this._units)) {\n            for (const units in this._expressions) {\n                if (this._expressions[units].regexp.test(this._val.trim())) {\n                    this._units = units;\n                    break;\n                }\n            }\n        }\n        else if (isObject(this._val)) {\n            let total = 0;\n            for (const typeName in this._val) {\n                if (isDefined(this._val[typeName])) {\n                    const quantity = this._val[typeName];\n                    // @ts-ignore\n                    const time = (new this.constructor(this.context, typeName)).valueOf() * quantity;\n                    total += time;\n                }\n            }\n            return total;\n        }\n        if (isDefined(this._units)) {\n            const expr = this._expressions[this._units];\n            const matching = this._val.toString().trim().match(expr.regexp);\n            if (matching) {\n                return expr.method.apply(this, matching.slice(1));\n            }\n            else {\n                return expr.method.call(this, this._val);\n            }\n        }\n        else if (isString(this._val)) {\n            return parseFloat(this._val);\n        }\n        else {\n            return this._val;\n        }\n    }\n    //-------------------------------------\n    // \tUNIT CONVERSIONS\n    //-------------------------------------\n    /**\n     * Returns the value of a frequency in the current units\n     */\n    _frequencyToUnits(freq) {\n        return 1 / freq;\n    }\n    /**\n     * Return the value of the beats in the current units\n     */\n    _beatsToUnits(beats) {\n        return (60 / this._getBpm()) * beats;\n    }\n    /**\n     * Returns the value of a second in the current units\n     */\n    _secondsToUnits(seconds) {\n        return seconds;\n    }\n    /**\n     * Returns the value of a tick in the current time units\n     */\n    _ticksToUnits(ticks) {\n        return (ticks * (this._beatsToUnits(1)) / this._getPPQ());\n    }\n    /**\n     * With no arguments, return \'now\'\n     */\n    _noArg() {\n        return this._now();\n    }\n    //-------------------------------------\n    // \tTEMPO CONVERSIONS\n    //-------------------------------------\n    /**\n     * Return the bpm\n     */\n    _getBpm() {\n        return this.context.transport.bpm.value;\n    }\n    /**\n     * Return the timeSignature\n     */\n    _getTimeSignature() {\n        return this.context.transport.timeSignature;\n    }\n    /**\n     * Return the PPQ or 192 if Transport is not available\n     */\n    _getPPQ() {\n        return this.context.transport.PPQ;\n    }\n    //-------------------------------------\n    // \tCONVERSION INTERFACE\n    //-------------------------------------\n    /**\n     * Coerce a time type into this units type.\n     * @param type Any time type units\n     */\n    fromType(type) {\n        this._units = undefined;\n        switch (this.defaultUnits) {\n            case "s":\n                this._val = type.toSeconds();\n                break;\n            case "i":\n                this._val = type.toTicks();\n                break;\n            case "hz":\n                this._val = type.toFrequency();\n                break;\n            case "midi":\n                this._val = type.toMidi();\n                break;\n        }\n        return this;\n    }\n    /**\n     * Return the value in hertz\n     */\n    toFrequency() {\n        return 1 / this.toSeconds();\n    }\n    /**\n     * Return the time in samples\n     */\n    toSamples() {\n        return this.toSeconds() * this.context.sampleRate;\n    }\n    /**\n     * Return the time in milliseconds.\n     */\n    toMilliseconds() {\n        return this.toSeconds() * 1000;\n    }\n}\n//# sourceMappingURL=TimeBase.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/Time.js\n\n\n\n/**\n * TimeClass is a primitive type for encoding and decoding Time values.\n * TimeClass can be passed into the parameter of any method which takes time as an argument.\n * @param  val    The time value.\n * @param  units  The units of the value.\n * @example\n * const time = Tone.Time("4n"); // a quarter note\n * @category Unit\n */\nclass Time_TimeClass extends TimeBase_TimeBaseClass {\n    constructor() {\n        super(...arguments);\n        this.name = "TimeClass";\n    }\n    _getExpressions() {\n        return Object.assign(super._getExpressions(), {\n            now: {\n                method: (capture) => {\n                    return this._now() + new this.constructor(this.context, capture).valueOf();\n                },\n                regexp: /^\\+(.+)/,\n            },\n            quantize: {\n                method: (capture) => {\n                    const quantTo = new Time_TimeClass(this.context, capture).valueOf();\n                    return this._secondsToUnits(this.context.transport.nextSubdivision(quantTo));\n                },\n                regexp: /^@(.+)/,\n            },\n        });\n    }\n    /**\n     * Quantize the time by the given subdivision. Optionally add a\n     * percentage which will move the time value towards the ideal\n     * quantized value by that percentage.\n     * @param  subdiv    The subdivision to quantize to\n     * @param  percent  Move the time value towards the quantized value by a percentage.\n     * @example\n     * Tone.Time(21).quantize(2); // returns 22\n     * Tone.Time(0.6).quantize("4n", 0.5); // returns 0.55\n     */\n    quantize(subdiv, percent = 1) {\n        const subdivision = new this.constructor(this.context, subdiv).valueOf();\n        const value = this.valueOf();\n        const multiple = Math.round(value / subdivision);\n        const ideal = multiple * subdivision;\n        const diff = ideal - value;\n        return value + diff * percent;\n    }\n    //-------------------------------------\n    // CONVERSIONS\n    //-------------------------------------\n    /**\n     * Convert a Time to Notation. The notation values are will be the\n     * closest representation between 1m to 128th note.\n     * @return {Notation}\n     * @example\n     * // if the Transport is at 120bpm:\n     * Tone.Time(2).toNotation(); // returns "1m"\n     */\n    toNotation() {\n        const time = this.toSeconds();\n        const testNotations = ["1m"];\n        for (let power = 1; power < 9; power++) {\n            const subdiv = Math.pow(2, power);\n            testNotations.push(subdiv + "n.");\n            testNotations.push(subdiv + "n");\n            testNotations.push(subdiv + "t");\n        }\n        testNotations.push("0");\n        // find the closets notation representation\n        let closest = testNotations[0];\n        let closestSeconds = new Time_TimeClass(this.context, testNotations[0]).toSeconds();\n        testNotations.forEach(notation => {\n            const notationSeconds = new Time_TimeClass(this.context, notation).toSeconds();\n            if (Math.abs(notationSeconds - time) < Math.abs(closestSeconds - time)) {\n                closest = notation;\n                closestSeconds = notationSeconds;\n            }\n        });\n        return closest;\n    }\n    /**\n     * Return the time encoded as Bars:Beats:Sixteenths.\n     */\n    toBarsBeatsSixteenths() {\n        const quarterTime = this._beatsToUnits(1);\n        let quarters = this.valueOf() / quarterTime;\n        quarters = parseFloat(quarters.toFixed(4));\n        const measures = Math.floor(quarters / this._getTimeSignature());\n        let sixteenths = (quarters % 1) * 4;\n        quarters = Math.floor(quarters) % this._getTimeSignature();\n        const sixteenthString = sixteenths.toString();\n        if (sixteenthString.length > 3) {\n            // the additional parseFloat removes insignificant trailing zeroes\n            sixteenths = parseFloat(parseFloat(sixteenthString).toFixed(3));\n        }\n        const progress = [measures, quarters, sixteenths];\n        return progress.join(":");\n    }\n    /**\n     * Return the time in ticks.\n     */\n    toTicks() {\n        const quarterTime = this._beatsToUnits(1);\n        const quarters = this.valueOf() / quarterTime;\n        return Math.round(quarters * this._getPPQ());\n    }\n    /**\n     * Return the time in seconds.\n     */\n    toSeconds() {\n        return this.valueOf();\n    }\n    /**\n     * Return the value as a midi note.\n     */\n    toMidi() {\n        return ftom(this.toFrequency());\n    }\n    _now() {\n        return this.context.now();\n    }\n}\n/**\n * Create a TimeClass from a time string or number. The time is computed against the\n * global Tone.Context. To use a specific context, use [[TimeClass]]\n * @param value A value which represents time\n * @param units The value\'s units if they can\'t be inferred by the value.\n * @category Unit\n * @example\n * const time = Tone.Time("4n").toSeconds();\n * console.log(time);\n * @example\n * const note = Tone.Time(1).toNotation();\n * console.log(note);\n * @example\n * const freq = Tone.Time(0.5).toFrequency();\n * console.log(freq);\n */\nfunction Time(value, units) {\n    return new Time_TimeClass(getContext(), value, units);\n}\n//# sourceMappingURL=Time.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/Frequency.js\n\n\n\n\n/**\n * Frequency is a primitive type for encoding Frequency values.\n * Eventually all time values are evaluated to hertz using the `valueOf` method.\n * @example\n * Tone.Frequency("C3"); // 261\n * Tone.Frequency(38, "midi");\n * Tone.Frequency("C3").transpose(4);\n * @category Unit\n */\nclass Frequency_FrequencyClass extends Time_TimeClass {\n    constructor() {\n        super(...arguments);\n        this.name = "Frequency";\n        this.defaultUnits = "hz";\n    }\n    /**\n     * The [concert tuning pitch](https://en.wikipedia.org/wiki/Concert_pitch) which is used\n     * to generate all the other pitch values from notes. A4\'s values in Hertz.\n     */\n    static get A4() {\n        return getA4();\n    }\n    static set A4(freq) {\n        setA4(freq);\n    }\n    //-------------------------------------\n    // \tAUGMENT BASE EXPRESSIONS\n    //-------------------------------------\n    _getExpressions() {\n        return Object.assign({}, super._getExpressions(), {\n            midi: {\n                regexp: /^(\\d+(?:\\.\\d+)?midi)/,\n                method(value) {\n                    if (this.defaultUnits === "midi") {\n                        return value;\n                    }\n                    else {\n                        return Frequency_FrequencyClass.mtof(value);\n                    }\n                },\n            },\n            note: {\n                regexp: /^([a-g]{1}(?:b|#|x|bb)?)(-?[0-9]+)/i,\n                method(pitch, octave) {\n                    const index = noteToScaleIndex[pitch.toLowerCase()];\n                    const noteNumber = index + (parseInt(octave, 10) + 1) * 12;\n                    if (this.defaultUnits === "midi") {\n                        return noteNumber;\n                    }\n                    else {\n                        return Frequency_FrequencyClass.mtof(noteNumber);\n                    }\n                },\n            },\n            tr: {\n                regexp: /^(\\d+(?:\\.\\d+)?):(\\d+(?:\\.\\d+)?):?(\\d+(?:\\.\\d+)?)?/,\n                method(m, q, s) {\n                    let total = 1;\n                    if (m && m !== "0") {\n                        total *= this._beatsToUnits(this._getTimeSignature() * parseFloat(m));\n                    }\n                    if (q && q !== "0") {\n                        total *= this._beatsToUnits(parseFloat(q));\n                    }\n                    if (s && s !== "0") {\n                        total *= this._beatsToUnits(parseFloat(s) / 4);\n                    }\n                    return total;\n                },\n            },\n        });\n    }\n    //-------------------------------------\n    // \tEXPRESSIONS\n    //-------------------------------------\n    /**\n     * Transposes the frequency by the given number of semitones.\n     * @return  A new transposed frequency\n     * @example\n     * Tone.Frequency("A4").transpose(3); // "C5"\n     */\n    transpose(interval) {\n        return new Frequency_FrequencyClass(this.context, this.valueOf() * intervalToFrequencyRatio(interval));\n    }\n    /**\n     * Takes an array of semitone intervals and returns\n     * an array of frequencies transposed by those intervals.\n     * @return  Returns an array of Frequencies\n     * @example\n     * Tone.Frequency("A4").harmonize([0, 3, 7]); // ["A4", "C5", "E5"]\n     */\n    harmonize(intervals) {\n        return intervals.map(interval => {\n            return this.transpose(interval);\n        });\n    }\n    //-------------------------------------\n    // \tUNIT CONVERSIONS\n    //-------------------------------------\n    /**\n     * Return the value of the frequency as a MIDI note\n     * @example\n     * Tone.Frequency("C4").toMidi(); // 60\n     */\n    toMidi() {\n        return ftom(this.valueOf());\n    }\n    /**\n     * Return the value of the frequency in Scientific Pitch Notation\n     * @example\n     * Tone.Frequency(69, "midi").toNote(); // "A4"\n     */\n    toNote() {\n        const freq = this.toFrequency();\n        const log = Math.log2(freq / Frequency_FrequencyClass.A4);\n        let noteNumber = Math.round(12 * log) + 57;\n        const octave = Math.floor(noteNumber / 12);\n        if (octave < 0) {\n            noteNumber += -12 * octave;\n        }\n        const noteName = scaleIndexToNote[noteNumber % 12];\n        return noteName + octave.toString();\n    }\n    /**\n     * Return the duration of one cycle in seconds.\n     */\n    toSeconds() {\n        return 1 / super.toSeconds();\n    }\n    /**\n     * Return the duration of one cycle in ticks\n     */\n    toTicks() {\n        const quarterTime = this._beatsToUnits(1);\n        const quarters = this.valueOf() / quarterTime;\n        return Math.floor(quarters * this._getPPQ());\n    }\n    //-------------------------------------\n    // \tUNIT CONVERSIONS HELPERS\n    //-------------------------------------\n    /**\n     * With no arguments, return 0\n     */\n    _noArg() {\n        return 0;\n    }\n    /**\n     * Returns the value of a frequency in the current units\n     */\n    _frequencyToUnits(freq) {\n        return freq;\n    }\n    /**\n     * Returns the value of a tick in the current time units\n     */\n    _ticksToUnits(ticks) {\n        return 1 / ((ticks * 60) / (this._getBpm() * this._getPPQ()));\n    }\n    /**\n     * Return the value of the beats in the current units\n     */\n    _beatsToUnits(beats) {\n        return 1 / super._beatsToUnits(beats);\n    }\n    /**\n     * Returns the value of a second in the current units\n     */\n    _secondsToUnits(seconds) {\n        return 1 / seconds;\n    }\n    /**\n     * Convert a MIDI note to frequency value.\n     * @param  midi The midi number to convert.\n     * @return The corresponding frequency value\n     */\n    static mtof(midi) {\n        return mtof(midi);\n    }\n    /**\n     * Convert a frequency value to a MIDI note.\n     * @param frequency The value to frequency value to convert.\n     */\n    static ftom(frequency) {\n        return ftom(frequency);\n    }\n}\n//-------------------------------------\n// \tFREQUENCY CONVERSIONS\n//-------------------------------------\n/**\n * Note to scale index.\n * @hidden\n */\nconst noteToScaleIndex = {\n    cbb: -2, cb: -1, c: 0, "c#": 1, cx: 2,\n    dbb: 0, db: 1, d: 2, "d#": 3, dx: 4,\n    ebb: 2, eb: 3, e: 4, "e#": 5, ex: 6,\n    fbb: 3, fb: 4, f: 5, "f#": 6, fx: 7,\n    gbb: 5, gb: 6, g: 7, "g#": 8, gx: 9,\n    abb: 7, ab: 8, a: 9, "a#": 10, ax: 11,\n    bbb: 9, bb: 10, b: 11, "b#": 12, bx: 13,\n};\n/**\n * scale index to note (sharps)\n * @hidden\n */\nconst scaleIndexToNote = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"];\n/**\n * Convert a value into a FrequencyClass object.\n * @category Unit\n * @example\n * const midi = Tone.Frequency("C3").toMidi();\n * console.log(midi);\n * @example\n * const hertz = Tone.Frequency(38, "midi").toFrequency();\n * console.log(hertz);\n */\nfunction Frequency(value, units) {\n    return new Frequency_FrequencyClass(getContext(), value, units);\n}\n//# sourceMappingURL=Frequency.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/TransportTime.js\n\n\n/**\n * TransportTime is a the time along the Transport\'s\n * timeline. It is similar to Tone.Time, but instead of evaluating\n * against the AudioContext\'s clock, it is evaluated against\n * the Transport\'s position. See [TransportTime wiki](https://github.com/Tonejs/Tone.js/wiki/TransportTime).\n * @category Unit\n */\nclass TransportTime_TransportTimeClass extends Time_TimeClass {\n    constructor() {\n        super(...arguments);\n        this.name = "TransportTime";\n    }\n    /**\n     * Return the current time in whichever context is relevant\n     */\n    _now() {\n        return this.context.transport.seconds;\n    }\n}\n/**\n * TransportTime is a the time along the Transport\'s\n * timeline. It is similar to [[Time]], but instead of evaluating\n * against the AudioContext\'s clock, it is evaluated against\n * the Transport\'s position. See [TransportTime wiki](https://github.com/Tonejs/Tone.js/wiki/TransportTime).\n * @category Unit\n */\nfunction TransportTime(value, units) {\n    return new TransportTime_TransportTimeClass(getContext(), value, units);\n}\n//# sourceMappingURL=TransportTime.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/ToneWithContext.js\n\n\n\n\n\n\n\n/**\n * The Base class for all nodes that have an AudioContext.\n */\nclass ToneWithContext_ToneWithContext extends Tone_Tone {\n    constructor() {\n        super();\n        const options = optionsFromArguments(ToneWithContext_ToneWithContext.getDefaults(), arguments, ["context"]);\n        if (this.defaultContext) {\n            this.context = this.defaultContext;\n        }\n        else {\n            this.context = options.context;\n        }\n    }\n    static getDefaults() {\n        return {\n            context: getContext(),\n        };\n    }\n    /**\n     * Return the current time of the Context clock plus the lookAhead.\n     * @example\n     * setInterval(() => {\n     * \tconsole.log(Tone.now());\n     * }, 100);\n     */\n    now() {\n        return this.context.currentTime + this.context.lookAhead;\n    }\n    /**\n     * Return the current time of the Context clock without any lookAhead.\n     * @example\n     * setInterval(() => {\n     * \tconsole.log(Tone.immediate());\n     * }, 100);\n     */\n    immediate() {\n        return this.context.currentTime;\n    }\n    /**\n     * The duration in seconds of one sample.\n     * @example\n     * console.log(Tone.Transport.sampleTime);\n     */\n    get sampleTime() {\n        return 1 / this.context.sampleRate;\n    }\n    /**\n     * The number of seconds of 1 processing block (128 samples)\n     * @example\n     * console.log(Tone.Destination.blockTime);\n     */\n    get blockTime() {\n        return 128 / this.context.sampleRate;\n    }\n    /**\n     * Convert the incoming time to seconds.\n     * This is calculated against the current [[Tone.Transport]] bpm\n     * @example\n     * const gain = new Tone.Gain();\n     * setInterval(() => console.log(gain.toSeconds("4n")), 100);\n     * // ramp the tempo to 60 bpm over 30 seconds\n     * Tone.getTransport().bpm.rampTo(60, 30);\n     */\n    toSeconds(time) {\n        return new Time_TimeClass(this.context, time).toSeconds();\n    }\n    /**\n     * Convert the input to a frequency number\n     * @example\n     * const gain = new Tone.Gain();\n     * console.log(gain.toFrequency("4n"));\n     */\n    toFrequency(freq) {\n        return new Frequency_FrequencyClass(this.context, freq).toFrequency();\n    }\n    /**\n     * Convert the input time into ticks\n     * @example\n     * const gain = new Tone.Gain();\n     * console.log(gain.toTicks("4n"));\n     */\n    toTicks(time) {\n        return new TransportTime_TransportTimeClass(this.context, time).toTicks();\n    }\n    //-------------------------------------\n    // \tGET/SET\n    //-------------------------------------\n    /**\n     * Get a subset of the properties which are in the partial props\n     */\n    _getPartialProperties(props) {\n        const options = this.get();\n        // remove attributes from the prop that are not in the partial\n        Object.keys(options).forEach(name => {\n            if (isUndef(props[name])) {\n                delete options[name];\n            }\n        });\n        return options;\n    }\n    /**\n     * Get the object\'s attributes.\n     * @example\n     * const osc = new Tone.Oscillator();\n     * console.log(osc.get());\n     */\n    get() {\n        const defaults = getDefaultsFromInstance(this);\n        Object.keys(defaults).forEach(attribute => {\n            if (Reflect.has(this, attribute)) {\n                const member = this[attribute];\n                if (isDefined(member) && isDefined(member.value) && isDefined(member.setValueAtTime)) {\n                    defaults[attribute] = member.value;\n                }\n                else if (member instanceof ToneWithContext_ToneWithContext) {\n                    defaults[attribute] = member._getPartialProperties(defaults[attribute]);\n                    // otherwise make sure it\'s a serializable type\n                }\n                else if (isArray(member) || isNumber(member) || isString(member) || isBoolean(member)) {\n                    defaults[attribute] = member;\n                }\n                else {\n                    // remove all undefined and unserializable attributes\n                    delete defaults[attribute];\n                }\n            }\n        });\n        return defaults;\n    }\n    /**\n     * Set multiple properties at once with an object.\n     * @example\n     * const filter = new Tone.Filter().toDestination();\n     * // set values using an object\n     * filter.set({\n     * \tfrequency: "C6",\n     * \ttype: "highpass"\n     * });\n     * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/Analogsynth_octaves_highmid.mp3").connect(filter);\n     * player.autostart = true;\n     */\n    set(props) {\n        Object.keys(props).forEach(attribute => {\n            if (Reflect.has(this, attribute) && isDefined(this[attribute])) {\n                if (this[attribute] && isDefined(this[attribute].value) && isDefined(this[attribute].setValueAtTime)) {\n                    // small optimization\n                    if (this[attribute].value !== props[attribute]) {\n                        this[attribute].value = props[attribute];\n                    }\n                }\n                else if (this[attribute] instanceof ToneWithContext_ToneWithContext) {\n                    this[attribute].set(props[attribute]);\n                }\n                else {\n                    this[attribute] = props[attribute];\n                }\n            }\n        });\n        return this;\n    }\n}\n//# sourceMappingURL=ToneWithContext.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/StateTimeline.js\n\n\n/**\n * A Timeline State. Provides the methods: `setStateAtTime("state", time)` and `getValueAtTime(time)`\n * @param initial The initial state of the StateTimeline.  Defaults to `undefined`\n */\nclass StateTimeline_StateTimeline extends Timeline_Timeline {\n    constructor(initial = "stopped") {\n        super();\n        this.name = "StateTimeline";\n        this._initial = initial;\n        this.setStateAtTime(this._initial, 0);\n    }\n    /**\n     * Returns the scheduled state scheduled before or at\n     * the given time.\n     * @param  time  The time to query.\n     * @return  The name of the state input in setStateAtTime.\n     */\n    getValueAtTime(time) {\n        const event = this.get(time);\n        if (event !== null) {\n            return event.state;\n        }\n        else {\n            return this._initial;\n        }\n    }\n    /**\n     * Add a state to the timeline.\n     * @param  state The name of the state to set.\n     * @param  time  The time to query.\n     * @param options Any additional options that are needed in the timeline.\n     */\n    setStateAtTime(state, time, options) {\n        assertRange(time, 0);\n        this.add(Object.assign({}, options, {\n            state,\n            time,\n        }));\n        return this;\n    }\n    /**\n     * Return the event before the time with the given state\n     * @param  state The state to look for\n     * @param  time  When to check before\n     * @return  The event with the given state before the time\n     */\n    getLastState(state, time) {\n        // time = this.toSeconds(time);\n        const index = this._search(time);\n        for (let i = index; i >= 0; i--) {\n            const event = this._timeline[i];\n            if (event.state === state) {\n                return event;\n            }\n        }\n    }\n    /**\n     * Return the event after the time with the given state\n     * @param  state The state to look for\n     * @param  time  When to check from\n     * @return  The event with the given state after the time\n     */\n    getNextState(state, time) {\n        // time = this.toSeconds(time);\n        const index = this._search(time);\n        if (index !== -1) {\n            for (let i = index; i < this._timeline.length; i++) {\n                const event = this._timeline[i];\n                if (event.state === state) {\n                    return event;\n                }\n            }\n        }\n    }\n}\n//# sourceMappingURL=StateTimeline.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Param.js\n\n\n\n\n\n\n\n\n/**\n * Param wraps the native Web Audio\'s AudioParam to provide\n * additional unit conversion functionality. It also\n * serves as a base-class for classes which have a single,\n * automatable parameter.\n * @category Core\n */\nclass Param_Param extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(optionsFromArguments(Param_Param.getDefaults(), arguments, ["param", "units", "convert"]));\n        this.name = "Param";\n        this.overridden = false;\n        /**\n         * The minimum output value\n         */\n        this._minOutput = 1e-7;\n        const options = optionsFromArguments(Param_Param.getDefaults(), arguments, ["param", "units", "convert"]);\n        assert(isDefined(options.param) &&\n            (isAudioParam(options.param) || options.param instanceof Param_Param), "param must be an AudioParam");\n        while (!isAudioParam(options.param)) {\n            options.param = options.param._param;\n        }\n        this._swappable = isDefined(options.swappable) ? options.swappable : false;\n        if (this._swappable) {\n            this.input = this.context.createGain();\n            // initialize\n            this._param = options.param;\n            this.input.connect(this._param);\n        }\n        else {\n            this._param = this.input = options.param;\n        }\n        this._events = new Timeline_Timeline(1000);\n        this._initialValue = this._param.defaultValue;\n        this.units = options.units;\n        this.convert = options.convert;\n        this._minValue = options.minValue;\n        this._maxValue = options.maxValue;\n        // if the value is defined, set it immediately\n        if (isDefined(options.value) && options.value !== this._toType(this._initialValue)) {\n            this.setValueAtTime(options.value, 0);\n        }\n    }\n    static getDefaults() {\n        return Object.assign(ToneWithContext_ToneWithContext.getDefaults(), {\n            convert: true,\n            units: "number",\n        });\n    }\n    get value() {\n        const now = this.now();\n        return this.getValueAtTime(now);\n    }\n    set value(value) {\n        this.cancelScheduledValues(this.now());\n        this.setValueAtTime(value, this.now());\n    }\n    get minValue() {\n        // if it\'s not the default minValue, return it\n        if (isDefined(this._minValue)) {\n            return this._minValue;\n        }\n        else if (this.units === "time" || this.units === "frequency" ||\n            this.units === "normalRange" || this.units === "positive" ||\n            this.units === "transportTime" || this.units === "ticks" ||\n            this.units === "bpm" || this.units === "hertz" || this.units === "samples") {\n            return 0;\n        }\n        else if (this.units === "audioRange") {\n            return -1;\n        }\n        else if (this.units === "decibels") {\n            return -Infinity;\n        }\n        else {\n            return this._param.minValue;\n        }\n    }\n    get maxValue() {\n        if (isDefined(this._maxValue)) {\n            return this._maxValue;\n        }\n        else if (this.units === "normalRange" ||\n            this.units === "audioRange") {\n            return 1;\n        }\n        else {\n            return this._param.maxValue;\n        }\n    }\n    /**\n     * Type guard based on the unit name\n     */\n    _is(arg, type) {\n        return this.units === type;\n    }\n    /**\n     * Make sure the value is always in the defined range\n     */\n    _assertRange(value) {\n        if (isDefined(this.maxValue) && isDefined(this.minValue)) {\n            assertRange(value, this._fromType(this.minValue), this._fromType(this.maxValue));\n        }\n        return value;\n    }\n    /**\n     * Convert the given value from the type specified by Param.units\n     * into the destination value (such as Gain or Frequency).\n     */\n    _fromType(val) {\n        if (this.convert && !this.overridden) {\n            if (this._is(val, "time")) {\n                return this.toSeconds(val);\n            }\n            else if (this._is(val, "decibels")) {\n                return dbToGain(val);\n            }\n            else if (this._is(val, "frequency")) {\n                return this.toFrequency(val);\n            }\n            else {\n                return val;\n            }\n        }\n        else if (this.overridden) {\n            // if it\'s overridden, should only schedule 0s\n            return 0;\n        }\n        else {\n            return val;\n        }\n    }\n    /**\n     * Convert the parameters value into the units specified by Param.units.\n     */\n    _toType(val) {\n        if (this.convert && this.units === "decibels") {\n            return gainToDb(val);\n        }\n        else {\n            return val;\n        }\n    }\n    //-------------------------------------\n    // ABSTRACT PARAM INTERFACE\n    // all docs are generated from ParamInterface.ts\n    //-------------------------------------\n    setValueAtTime(value, time) {\n        const computedTime = this.toSeconds(time);\n        const numericValue = this._fromType(value);\n        assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to setValueAtTime: ${JSON.stringify(value)}, ${JSON.stringify(time)}`);\n        this._assertRange(numericValue);\n        this.log(this.units, "setValueAtTime", value, computedTime);\n        this._events.add({\n            time: computedTime,\n            type: "setValueAtTime",\n            value: numericValue,\n        });\n        this._param.setValueAtTime(numericValue, computedTime);\n        return this;\n    }\n    getValueAtTime(time) {\n        const computedTime = Math.max(this.toSeconds(time), 0);\n        const after = this._events.getAfter(computedTime);\n        const before = this._events.get(computedTime);\n        let value = this._initialValue;\n        // if it was set by\n        if (before === null) {\n            value = this._initialValue;\n        }\n        else if (before.type === "setTargetAtTime" && (after === null || after.type === "setValueAtTime")) {\n            const previous = this._events.getBefore(before.time);\n            let previousVal;\n            if (previous === null) {\n                previousVal = this._initialValue;\n            }\n            else {\n                previousVal = previous.value;\n            }\n            if (before.type === "setTargetAtTime") {\n                value = this._exponentialApproach(before.time, previousVal, before.value, before.constant, computedTime);\n            }\n        }\n        else if (after === null) {\n            value = before.value;\n        }\n        else if (after.type === "linearRampToValueAtTime" || after.type === "exponentialRampToValueAtTime") {\n            let beforeValue = before.value;\n            if (before.type === "setTargetAtTime") {\n                const previous = this._events.getBefore(before.time);\n                if (previous === null) {\n                    beforeValue = this._initialValue;\n                }\n                else {\n                    beforeValue = previous.value;\n                }\n            }\n            if (after.type === "linearRampToValueAtTime") {\n                value = this._linearInterpolate(before.time, beforeValue, after.time, after.value, computedTime);\n            }\n            else {\n                value = this._exponentialInterpolate(before.time, beforeValue, after.time, after.value, computedTime);\n            }\n        }\n        else {\n            value = before.value;\n        }\n        return this._toType(value);\n    }\n    setRampPoint(time) {\n        time = this.toSeconds(time);\n        let currentVal = this.getValueAtTime(time);\n        this.cancelAndHoldAtTime(time);\n        if (this._fromType(currentVal) === 0) {\n            currentVal = this._toType(this._minOutput);\n        }\n        this.setValueAtTime(currentVal, time);\n        return this;\n    }\n    linearRampToValueAtTime(value, endTime) {\n        const numericValue = this._fromType(value);\n        const computedTime = this.toSeconds(endTime);\n        assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to linearRampToValueAtTime: ${JSON.stringify(value)}, ${JSON.stringify(endTime)}`);\n        this._assertRange(numericValue);\n        this._events.add({\n            time: computedTime,\n            type: "linearRampToValueAtTime",\n            value: numericValue,\n        });\n        this.log(this.units, "linearRampToValueAtTime", value, computedTime);\n        this._param.linearRampToValueAtTime(numericValue, computedTime);\n        return this;\n    }\n    exponentialRampToValueAtTime(value, endTime) {\n        let numericValue = this._fromType(value);\n        // the value can\'t be 0\n        numericValue = EQ(numericValue, 0) ? this._minOutput : numericValue;\n        this._assertRange(numericValue);\n        const computedTime = this.toSeconds(endTime);\n        assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to exponentialRampToValueAtTime: ${JSON.stringify(value)}, ${JSON.stringify(endTime)}`);\n        // store the event\n        this._events.add({\n            time: computedTime,\n            type: "exponentialRampToValueAtTime",\n            value: numericValue,\n        });\n        this.log(this.units, "exponentialRampToValueAtTime", value, computedTime);\n        this._param.exponentialRampToValueAtTime(numericValue, computedTime);\n        return this;\n    }\n    exponentialRampTo(value, rampTime, startTime) {\n        startTime = this.toSeconds(startTime);\n        this.setRampPoint(startTime);\n        this.exponentialRampToValueAtTime(value, startTime + this.toSeconds(rampTime));\n        return this;\n    }\n    linearRampTo(value, rampTime, startTime) {\n        startTime = this.toSeconds(startTime);\n        this.setRampPoint(startTime);\n        this.linearRampToValueAtTime(value, startTime + this.toSeconds(rampTime));\n        return this;\n    }\n    targetRampTo(value, rampTime, startTime) {\n        startTime = this.toSeconds(startTime);\n        this.setRampPoint(startTime);\n        this.exponentialApproachValueAtTime(value, startTime, rampTime);\n        return this;\n    }\n    exponentialApproachValueAtTime(value, time, rampTime) {\n        time = this.toSeconds(time);\n        rampTime = this.toSeconds(rampTime);\n        const timeConstant = Math.log(rampTime + 1) / Math.log(200);\n        this.setTargetAtTime(value, time, timeConstant);\n        // at 90% start a linear ramp to the final value\n        this.cancelAndHoldAtTime(time + rampTime * 0.9);\n        this.linearRampToValueAtTime(value, time + rampTime);\n        return this;\n    }\n    setTargetAtTime(value, startTime, timeConstant) {\n        const numericValue = this._fromType(value);\n        // The value will never be able to approach without timeConstant > 0.\n        assert(isFinite(timeConstant) && timeConstant > 0, "timeConstant must be a number greater than 0");\n        const computedTime = this.toSeconds(startTime);\n        this._assertRange(numericValue);\n        assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to setTargetAtTime: ${JSON.stringify(value)}, ${JSON.stringify(startTime)}`);\n        this._events.add({\n            constant: timeConstant,\n            time: computedTime,\n            type: "setTargetAtTime",\n            value: numericValue,\n        });\n        this.log(this.units, "setTargetAtTime", value, computedTime, timeConstant);\n        this._param.setTargetAtTime(numericValue, computedTime, timeConstant);\n        return this;\n    }\n    setValueCurveAtTime(values, startTime, duration, scaling = 1) {\n        duration = this.toSeconds(duration);\n        startTime = this.toSeconds(startTime);\n        const startingValue = this._fromType(values[0]) * scaling;\n        this.setValueAtTime(this._toType(startingValue), startTime);\n        const segTime = duration / (values.length - 1);\n        for (let i = 1; i < values.length; i++) {\n            const numericValue = this._fromType(values[i]) * scaling;\n            this.linearRampToValueAtTime(this._toType(numericValue), startTime + i * segTime);\n        }\n        return this;\n    }\n    cancelScheduledValues(time) {\n        const computedTime = this.toSeconds(time);\n        assert(isFinite(computedTime), `Invalid argument to cancelScheduledValues: ${JSON.stringify(time)}`);\n        this._events.cancel(computedTime);\n        this._param.cancelScheduledValues(computedTime);\n        this.log(this.units, "cancelScheduledValues", computedTime);\n        return this;\n    }\n    cancelAndHoldAtTime(time) {\n        const computedTime = this.toSeconds(time);\n        const valueAtTime = this._fromType(this.getValueAtTime(computedTime));\n        // remove the schedule events\n        assert(isFinite(computedTime), `Invalid argument to cancelAndHoldAtTime: ${JSON.stringify(time)}`);\n        this.log(this.units, "cancelAndHoldAtTime", computedTime, "value=" + valueAtTime);\n        // if there is an event at the given computedTime\n        // and that even is not a "set"\n        const before = this._events.get(computedTime);\n        const after = this._events.getAfter(computedTime);\n        if (before && EQ(before.time, computedTime)) {\n            // remove everything after\n            if (after) {\n                this._param.cancelScheduledValues(after.time);\n                this._events.cancel(after.time);\n            }\n            else {\n                this._param.cancelAndHoldAtTime(computedTime);\n                this._events.cancel(computedTime + this.sampleTime);\n            }\n        }\n        else if (after) {\n            this._param.cancelScheduledValues(after.time);\n            // cancel the next event(s)\n            this._events.cancel(after.time);\n            if (after.type === "linearRampToValueAtTime") {\n                this.linearRampToValueAtTime(this._toType(valueAtTime), computedTime);\n            }\n            else if (after.type === "exponentialRampToValueAtTime") {\n                this.exponentialRampToValueAtTime(this._toType(valueAtTime), computedTime);\n            }\n        }\n        // set the value at the given time\n        this._events.add({\n            time: computedTime,\n            type: "setValueAtTime",\n            value: valueAtTime,\n        });\n        this._param.setValueAtTime(valueAtTime, computedTime);\n        return this;\n    }\n    rampTo(value, rampTime = 0.1, startTime) {\n        if (this.units === "frequency" || this.units === "bpm" || this.units === "decibels") {\n            this.exponentialRampTo(value, rampTime, startTime);\n        }\n        else {\n            this.linearRampTo(value, rampTime, startTime);\n        }\n        return this;\n    }\n    /**\n     * Apply all of the previously scheduled events to the passed in Param or AudioParam.\n     * The applied values will start at the context\'s current time and schedule\n     * all of the events which are scheduled on this Param onto the passed in param.\n     */\n    apply(param) {\n        const now = this.context.currentTime;\n        // set the param\'s value at the current time and schedule everything else\n        param.setValueAtTime(this.getValueAtTime(now), now);\n        // if the previous event was a curve, then set the rest of it\n        const previousEvent = this._events.get(now);\n        if (previousEvent && previousEvent.type === "setTargetAtTime") {\n            // approx it until the next event with linear ramps\n            const nextEvent = this._events.getAfter(previousEvent.time);\n            // or for 2 seconds if there is no event\n            const endTime = nextEvent ? nextEvent.time : now + 2;\n            const subdivisions = (endTime - now) / 10;\n            for (let i = now; i < endTime; i += subdivisions) {\n                param.linearRampToValueAtTime(this.getValueAtTime(i), i);\n            }\n        }\n        this._events.forEachAfter(this.context.currentTime, event => {\n            if (event.type === "cancelScheduledValues") {\n                param.cancelScheduledValues(event.time);\n            }\n            else if (event.type === "setTargetAtTime") {\n                param.setTargetAtTime(event.value, event.time, event.constant);\n            }\n            else {\n                param[event.type](event.value, event.time);\n            }\n        });\n        return this;\n    }\n    /**\n     * Replace the Param\'s internal AudioParam. Will apply scheduled curves\n     * onto the parameter and replace the connections.\n     */\n    setParam(param) {\n        assert(this._swappable, "The Param must be assigned as \'swappable\' in the constructor");\n        const input = this.input;\n        input.disconnect(this._param);\n        this.apply(param);\n        this._param = param;\n        input.connect(this._param);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._events.dispose();\n        return this;\n    }\n    get defaultValue() {\n        return this._toType(this._param.defaultValue);\n    }\n    //-------------------------------------\n    // \tAUTOMATION CURVE CALCULATIONS\n    // \tMIT License, copyright (c) 2014 Jordan Santell\n    //-------------------------------------\n    // Calculates the the value along the curve produced by setTargetAtTime\n    _exponentialApproach(t0, v0, v1, timeConstant, t) {\n        return v1 + (v0 - v1) * Math.exp(-(t - t0) / timeConstant);\n    }\n    // Calculates the the value along the curve produced by linearRampToValueAtTime\n    _linearInterpolate(t0, v0, t1, v1, t) {\n        return v0 + (v1 - v0) * ((t - t0) / (t1 - t0));\n    }\n    // Calculates the the value along the curve produced by exponentialRampToValueAtTime\n    _exponentialInterpolate(t0, v0, t1, v1, t) {\n        return v0 * Math.pow(v1 / v0, (t - t0) / (t1 - t0));\n    }\n}\n//# sourceMappingURL=Param.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/ToneAudioNode.js\n\n\n\n\n\n/**\n * ToneAudioNode is the base class for classes which process audio.\n */\nclass ToneAudioNode_ToneAudioNode extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(...arguments);\n        /**\n         * The name of the class\n         */\n        this.name = "ToneAudioNode";\n        /**\n         * List all of the node that must be set to match the ChannelProperties\n         */\n        this._internalChannels = [];\n    }\n    /**\n     * The number of inputs feeding into the AudioNode.\n     * For source nodes, this will be 0.\n     * @example\n     * const node = new Tone.Gain();\n     * console.log(node.numberOfInputs);\n     */\n    get numberOfInputs() {\n        if (isDefined(this.input)) {\n            if (isAudioParam(this.input) || this.input instanceof Param_Param) {\n                return 1;\n            }\n            else {\n                return this.input.numberOfInputs;\n            }\n        }\n        else {\n            return 0;\n        }\n    }\n    /**\n     * The number of outputs of the AudioNode.\n     * @example\n     * const node = new Tone.Gain();\n     * console.log(node.numberOfOutputs);\n     */\n    get numberOfOutputs() {\n        if (isDefined(this.output)) {\n            return this.output.numberOfOutputs;\n        }\n        else {\n            return 0;\n        }\n    }\n    //-------------------------------------\n    // AUDIO PROPERTIES\n    //-------------------------------------\n    /**\n     * Used to decide which nodes to get/set properties on\n     */\n    _isAudioNode(node) {\n        return isDefined(node) && (node instanceof ToneAudioNode_ToneAudioNode || AdvancedTypeCheck_isAudioNode(node));\n    }\n    /**\n     * Get all of the audio nodes (either internal or input/output) which together\n     * make up how the class node responds to channel input/output\n     */\n    _getInternalNodes() {\n        const nodeList = this._internalChannels.slice(0);\n        if (this._isAudioNode(this.input)) {\n            nodeList.push(this.input);\n        }\n        if (this._isAudioNode(this.output)) {\n            if (this.input !== this.output) {\n                nodeList.push(this.output);\n            }\n        }\n        return nodeList;\n    }\n    /**\n     * Set the audio options for this node such as channelInterpretation\n     * channelCount, etc.\n     * @param options\n     */\n    _setChannelProperties(options) {\n        const nodeList = this._getInternalNodes();\n        nodeList.forEach(node => {\n            node.channelCount = options.channelCount;\n            node.channelCountMode = options.channelCountMode;\n            node.channelInterpretation = options.channelInterpretation;\n        });\n    }\n    /**\n     * Get the current audio options for this node such as channelInterpretation\n     * channelCount, etc.\n     */\n    _getChannelProperties() {\n        const nodeList = this._getInternalNodes();\n        assert(nodeList.length > 0, "ToneAudioNode does not have any internal nodes");\n        // use the first node to get properties\n        // they should all be the same\n        const node = nodeList[0];\n        return {\n            channelCount: node.channelCount,\n            channelCountMode: node.channelCountMode,\n            channelInterpretation: node.channelInterpretation,\n        };\n    }\n    /**\n     * channelCount is the number of channels used when up-mixing and down-mixing\n     * connections to any inputs to the node. The default value is 2 except for\n     * specific nodes where its value is specially determined.\n     */\n    get channelCount() {\n        return this._getChannelProperties().channelCount;\n    }\n    set channelCount(channelCount) {\n        const props = this._getChannelProperties();\n        // merge it with the other properties\n        this._setChannelProperties(Object.assign(props, { channelCount }));\n    }\n    /**\n     * channelCountMode determines how channels will be counted when up-mixing and\n     * down-mixing connections to any inputs to the node.\n     * The default value is "max". This attribute has no effect for nodes with no inputs.\n     * * "max" - computedNumberOfChannels is the maximum of the number of channels of all connections to an input. In this mode channelCount is ignored.\n     * * "clamped-max" - computedNumberOfChannels is determined as for "max" and then clamped to a maximum value of the given channelCount.\n     * * "explicit" - computedNumberOfChannels is the exact value as specified by the channelCount.\n     */\n    get channelCountMode() {\n        return this._getChannelProperties().channelCountMode;\n    }\n    set channelCountMode(channelCountMode) {\n        const props = this._getChannelProperties();\n        // merge it with the other properties\n        this._setChannelProperties(Object.assign(props, { channelCountMode }));\n    }\n    /**\n     * channelInterpretation determines how individual channels will be treated\n     * when up-mixing and down-mixing connections to any inputs to the node.\n     * The default value is "speakers".\n     */\n    get channelInterpretation() {\n        return this._getChannelProperties().channelInterpretation;\n    }\n    set channelInterpretation(channelInterpretation) {\n        const props = this._getChannelProperties();\n        // merge it with the other properties\n        this._setChannelProperties(Object.assign(props, { channelInterpretation }));\n    }\n    //-------------------------------------\n    // CONNECTIONS\n    //-------------------------------------\n    /**\n     * connect the output of a ToneAudioNode to an AudioParam, AudioNode, or ToneAudioNode\n     * @param destination The output to connect to\n     * @param outputNum The output to connect from\n     * @param inputNum The input to connect to\n     */\n    connect(destination, outputNum = 0, inputNum = 0) {\n        ToneAudioNode_connect(this, destination, outputNum, inputNum);\n        return this;\n    }\n    /**\n     * Connect the output to the context\'s destination node.\n     * @example\n     * const osc = new Tone.Oscillator("C2").start();\n     * osc.toDestination();\n     */\n    toDestination() {\n        this.connect(this.context.destination);\n        return this;\n    }\n    /**\n     * Connect the output to the context\'s destination node.\n     * See [[toDestination]]\n     * @deprecated\n     */\n    toMaster() {\n        warn("toMaster() has been renamed toDestination()");\n        return this.toDestination();\n    }\n    /**\n     * disconnect the output\n     */\n    disconnect(destination, outputNum = 0, inputNum = 0) {\n        ToneAudioNode_disconnect(this, destination, outputNum, inputNum);\n        return this;\n    }\n    /**\n     * Connect the output of this node to the rest of the nodes in series.\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/drum-samples/handdrum-loop.mp3");\n     * player.autostart = true;\n     * const filter = new Tone.AutoFilter(4).start();\n     * const distortion = new Tone.Distortion(0.5);\n     * // connect the player to the filter, distortion and then to the master output\n     * player.chain(filter, distortion, Tone.Destination);\n     */\n    chain(...nodes) {\n        connectSeries(this, ...nodes);\n        return this;\n    }\n    /**\n     * connect the output of this node to the rest of the nodes in parallel.\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/drum-samples/conga-rhythm.mp3");\n     * player.autostart = true;\n     * const pitchShift = new Tone.PitchShift(4).toDestination();\n     * const filter = new Tone.Filter("G5").toDestination();\n     * // connect a node to the pitch shift and filter in parallel\n     * player.fan(pitchShift, filter);\n     */\n    fan(...nodes) {\n        nodes.forEach(node => this.connect(node));\n        return this;\n    }\n    /**\n     * Dispose and disconnect\n     */\n    dispose() {\n        super.dispose();\n        if (isDefined(this.input)) {\n            if (this.input instanceof ToneAudioNode_ToneAudioNode) {\n                this.input.dispose();\n            }\n            else if (AdvancedTypeCheck_isAudioNode(this.input)) {\n                this.input.disconnect();\n            }\n        }\n        if (isDefined(this.output)) {\n            if (this.output instanceof ToneAudioNode_ToneAudioNode) {\n                this.output.dispose();\n            }\n            else if (AdvancedTypeCheck_isAudioNode(this.output)) {\n                this.output.disconnect();\n            }\n        }\n        this._internalChannels = [];\n        return this;\n    }\n}\n//-------------------------------------\n// CONNECTIONS\n//-------------------------------------\n/**\n * connect together all of the arguments in series\n * @param nodes\n */\nfunction connectSeries(...nodes) {\n    const first = nodes.shift();\n    nodes.reduce((prev, current) => {\n        if (prev instanceof ToneAudioNode_ToneAudioNode) {\n            prev.connect(current);\n        }\n        else if (AdvancedTypeCheck_isAudioNode(prev)) {\n            ToneAudioNode_connect(prev, current);\n        }\n        return current;\n    }, first);\n}\n/**\n * Connect two nodes together so that signal flows from the\n * first node to the second. Optionally specify the input and output channels.\n * @param srcNode The source node\n * @param dstNode The destination node\n * @param outputNumber The output channel of the srcNode\n * @param inputNumber The input channel of the dstNode\n */\nfunction ToneAudioNode_connect(srcNode, dstNode, outputNumber = 0, inputNumber = 0) {\n    assert(isDefined(srcNode), "Cannot connect from undefined node");\n    assert(isDefined(dstNode), "Cannot connect to undefined node");\n    if (dstNode instanceof ToneAudioNode_ToneAudioNode || AdvancedTypeCheck_isAudioNode(dstNode)) {\n        assert(dstNode.numberOfInputs > 0, "Cannot connect to node with no inputs");\n    }\n    assert(srcNode.numberOfOutputs > 0, "Cannot connect from node with no outputs");\n    // resolve the input of the dstNode\n    while ((dstNode instanceof ToneAudioNode_ToneAudioNode || dstNode instanceof Param_Param)) {\n        if (isDefined(dstNode.input)) {\n            dstNode = dstNode.input;\n        }\n    }\n    while (srcNode instanceof ToneAudioNode_ToneAudioNode) {\n        if (isDefined(srcNode.output)) {\n            srcNode = srcNode.output;\n        }\n    }\n    // make the connection\n    if (isAudioParam(dstNode)) {\n        srcNode.connect(dstNode, outputNumber);\n    }\n    else {\n        srcNode.connect(dstNode, outputNumber, inputNumber);\n    }\n}\n/**\n * Disconnect a node from all nodes or optionally include a destination node and input/output channels.\n * @param srcNode The source node\n * @param dstNode The destination node\n * @param outputNumber The output channel of the srcNode\n * @param inputNumber The input channel of the dstNode\n */\nfunction ToneAudioNode_disconnect(srcNode, dstNode, outputNumber = 0, inputNumber = 0) {\n    // resolve the destination node\n    if (isDefined(dstNode)) {\n        while (dstNode instanceof ToneAudioNode_ToneAudioNode) {\n            dstNode = dstNode.input;\n        }\n    }\n    // resolve the src node\n    while (!(AdvancedTypeCheck_isAudioNode(srcNode))) {\n        if (isDefined(srcNode.output)) {\n            srcNode = srcNode.output;\n        }\n    }\n    if (isAudioParam(dstNode)) {\n        srcNode.disconnect(dstNode, outputNumber);\n    }\n    else if (AdvancedTypeCheck_isAudioNode(dstNode)) {\n        srcNode.disconnect(dstNode, outputNumber, inputNumber);\n    }\n    else {\n        srcNode.disconnect();\n    }\n}\n//# sourceMappingURL=ToneAudioNode.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Gain.js\n\n\n\n\n/**\n * A thin wrapper around the Native Web Audio GainNode.\n * The GainNode is a basic building block of the Web Audio\n * API and is useful for routing audio and adjusting gains.\n * @category Core\n * @example\n * return Tone.Offline(() => {\n * \tconst gainNode = new Tone.Gain(0).toDestination();\n * \tconst osc = new Tone.Oscillator(30).connect(gainNode).start();\n * \tgainNode.gain.rampTo(1, 0.1);\n * \tgainNode.gain.rampTo(0, 0.4, 0.2);\n * }, 0.7, 1);\n */\nclass Gain_Gain extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Gain_Gain.getDefaults(), arguments, ["gain", "units"]));\n        this.name = "Gain";\n        /**\n         * The wrapped GainNode.\n         */\n        this._gainNode = this.context.createGain();\n        // input = output\n        this.input = this._gainNode;\n        this.output = this._gainNode;\n        const options = optionsFromArguments(Gain_Gain.getDefaults(), arguments, ["gain", "units"]);\n        this.gain = new Param_Param({\n            context: this.context,\n            convert: options.convert,\n            param: this._gainNode.gain,\n            units: options.units,\n            value: options.gain,\n            minValue: options.minValue,\n            maxValue: options.maxValue,\n        });\n        readOnly(this, "gain");\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            convert: true,\n            gain: 1,\n            units: "gain",\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._gainNode.disconnect();\n        this.gain.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Gain.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/OneShotSource.js\n\n\n\n\n/**\n * Base class for fire-and-forget nodes\n */\nclass OneShotSource_OneShotSource extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        /**\n         * The callback to invoke after the\n         * source is done playing.\n         */\n        this.onended = noOp;\n        /**\n         * The start time\n         */\n        this._startTime = -1;\n        /**\n         * The stop time\n         */\n        this._stopTime = -1;\n        /**\n         * The id of the timeout\n         */\n        this._timeout = -1;\n        /**\n         * The public output node\n         */\n        this.output = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        /**\n         * The output gain node.\n         */\n        this._gainNode = this.output;\n        /**\n         * Get the playback state at the given time\n         */\n        this.getStateAtTime = function (time) {\n            const computedTime = this.toSeconds(time);\n            if (this._startTime !== -1 &&\n                computedTime >= this._startTime &&\n                (this._stopTime === -1 || computedTime <= this._stopTime)) {\n                return "started";\n            }\n            else {\n                return "stopped";\n            }\n        };\n        this._fadeIn = options.fadeIn;\n        this._fadeOut = options.fadeOut;\n        this._curve = options.curve;\n        this.onended = options.onended;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            curve: "linear",\n            fadeIn: 0,\n            fadeOut: 0,\n            onended: noOp,\n        });\n    }\n    /**\n     * Start the source at the given time\n     * @param  time When to start the source\n     */\n    _startGain(time, gain = 1) {\n        assert(this._startTime === -1, "Source cannot be started more than once");\n        // apply a fade in envelope\n        const fadeInTime = this.toSeconds(this._fadeIn);\n        // record the start time\n        this._startTime = time + fadeInTime;\n        this._startTime = Math.max(this._startTime, this.context.currentTime);\n        // schedule the envelope\n        if (fadeInTime > 0) {\n            this._gainNode.gain.setValueAtTime(0, time);\n            if (this._curve === "linear") {\n                this._gainNode.gain.linearRampToValueAtTime(gain, time + fadeInTime);\n            }\n            else {\n                this._gainNode.gain.exponentialApproachValueAtTime(gain, time, fadeInTime);\n            }\n        }\n        else {\n            this._gainNode.gain.setValueAtTime(gain, time);\n        }\n        return this;\n    }\n    /**\n     * Stop the source node at the given time.\n     * @param time When to stop the source\n     */\n    stop(time) {\n        this.log("stop", time);\n        this._stopGain(this.toSeconds(time));\n        return this;\n    }\n    /**\n     * Stop the source at the given time\n     * @param  time When to stop the source\n     */\n    _stopGain(time) {\n        assert(this._startTime !== -1, "\'start\' must be called before \'stop\'");\n        // cancel the previous stop\n        this.cancelStop();\n        // the fadeOut time\n        const fadeOutTime = this.toSeconds(this._fadeOut);\n        // schedule the stop callback\n        this._stopTime = this.toSeconds(time) + fadeOutTime;\n        this._stopTime = Math.max(this._stopTime, this.context.currentTime);\n        if (fadeOutTime > 0) {\n            // start the fade out curve at the given time\n            if (this._curve === "linear") {\n                this._gainNode.gain.linearRampTo(0, fadeOutTime, time);\n            }\n            else {\n                this._gainNode.gain.targetRampTo(0, fadeOutTime, time);\n            }\n        }\n        else {\n            // stop any ongoing ramps, and set the value to 0\n            this._gainNode.gain.cancelAndHoldAtTime(time);\n            this._gainNode.gain.setValueAtTime(0, time);\n        }\n        this.context.clearTimeout(this._timeout);\n        this._timeout = this.context.setTimeout(() => {\n            // allow additional time for the exponential curve to fully decay\n            const additionalTail = this._curve === "exponential" ? fadeOutTime * 2 : 0;\n            this._stopSource(this.now() + additionalTail);\n            this._onended();\n        }, this._stopTime - this.context.currentTime);\n        return this;\n    }\n    /**\n     * Invoke the onended callback\n     */\n    _onended() {\n        if (this.onended !== noOp) {\n            this.onended(this);\n            // overwrite onended to make sure it only is called once\n            this.onended = noOp;\n            // dispose when it\'s ended to free up for garbage collection only in the online context\n            if (!this.context.isOffline) {\n                const disposeCallback = () => this.dispose();\n                // @ts-ignore\n                if (typeof window.requestIdleCallback !== "undefined") {\n                    // @ts-ignore\n                    window.requestIdleCallback(disposeCallback);\n                }\n                else {\n                    setTimeout(disposeCallback, 1000);\n                }\n            }\n        }\n    }\n    /**\n     * Get the playback state at the current time\n     */\n    get state() {\n        return this.getStateAtTime(this.now());\n    }\n    /**\n     * Cancel a scheduled stop event\n     */\n    cancelStop() {\n        this.log("cancelStop");\n        assert(this._startTime !== -1, "Source is not started");\n        // cancel the stop envelope\n        this._gainNode.gain.cancelScheduledValues(this._startTime + this.sampleTime);\n        this.context.clearTimeout(this._timeout);\n        this._stopTime = -1;\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._gainNode.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=OneShotSource.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/ToneConstantSource.js\n\n\n\n\n/**\n * Wrapper around the native fire-and-forget ConstantSource.\n * Adds the ability to reschedule the stop method.\n * @category Signal\n */\nclass ToneConstantSource_ToneConstantSource extends OneShotSource_OneShotSource {\n    constructor() {\n        super(optionsFromArguments(ToneConstantSource_ToneConstantSource.getDefaults(), arguments, ["offset"]));\n        this.name = "ToneConstantSource";\n        /**\n         * The signal generator\n         */\n        this._source = this.context.createConstantSource();\n        const options = optionsFromArguments(ToneConstantSource_ToneConstantSource.getDefaults(), arguments, ["offset"]);\n        ToneAudioNode_connect(this._source, this._gainNode);\n        this.offset = new Param_Param({\n            context: this.context,\n            convert: options.convert,\n            param: this._source.offset,\n            units: options.units,\n            value: options.offset,\n            minValue: options.minValue,\n            maxValue: options.maxValue,\n        });\n    }\n    static getDefaults() {\n        return Object.assign(OneShotSource_OneShotSource.getDefaults(), {\n            convert: true,\n            offset: 1,\n            units: "number",\n        });\n    }\n    /**\n     * Start the source node at the given time\n     * @param  time When to start the source\n     */\n    start(time) {\n        const computedTime = this.toSeconds(time);\n        this.log("start", computedTime);\n        this._startGain(computedTime);\n        this._source.start(computedTime);\n        return this;\n    }\n    _stopSource(time) {\n        this._source.stop(time);\n    }\n    dispose() {\n        super.dispose();\n        if (this.state === "started") {\n            this.stop();\n        }\n        this._source.disconnect();\n        this.offset.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=ToneConstantSource.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Signal.js\n\n\n\n\n\n\n/**\n * A signal is an audio-rate value. Tone.Signal is a core component of the library.\n * Unlike a number, Signals can be scheduled with sample-level accuracy. Tone.Signal\n * has all of the methods available to native Web Audio\n * [AudioParam](http://webaudio.github.io/web-audio-api/#the-audioparam-interface)\n * as well as additional conveniences. Read more about working with signals\n * [here](https://github.com/Tonejs/Tone.js/wiki/Signals).\n *\n * @example\n * const osc = new Tone.Oscillator().toDestination().start();\n * // a scheduleable signal which can be connected to control an AudioParam or another Signal\n * const signal = new Tone.Signal({\n * \tvalue: "C4",\n * \tunits: "frequency"\n * }).connect(osc.frequency);\n * // the scheduled ramp controls the connected signal\n * signal.rampTo("C2", 4, "+0.5");\n * @category Signal\n */\nclass Signal_Signal extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Signal_Signal.getDefaults(), arguments, ["value", "units"]));\n        this.name = "Signal";\n        /**\n         * Indicates if the value should be overridden on connection.\n         */\n        this.override = true;\n        const options = optionsFromArguments(Signal_Signal.getDefaults(), arguments, ["value", "units"]);\n        this.output = this._constantSource = new ToneConstantSource_ToneConstantSource({\n            context: this.context,\n            convert: options.convert,\n            offset: options.value,\n            units: options.units,\n            minValue: options.minValue,\n            maxValue: options.maxValue,\n        });\n        this._constantSource.start(0);\n        this.input = this._param = this._constantSource.offset;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            convert: true,\n            units: "number",\n            value: 0,\n        });\n    }\n    connect(destination, outputNum = 0, inputNum = 0) {\n        // start it only when connected to something\n        connectSignal(this, destination, outputNum, inputNum);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._param.dispose();\n        this._constantSource.dispose();\n        return this;\n    }\n    //-------------------------------------\n    // ABSTRACT PARAM INTERFACE\n    // just a proxy for the ConstantSourceNode\'s offset AudioParam\n    // all docs are generated from AbstractParam.ts\n    //-------------------------------------\n    setValueAtTime(value, time) {\n        this._param.setValueAtTime(value, time);\n        return this;\n    }\n    getValueAtTime(time) {\n        return this._param.getValueAtTime(time);\n    }\n    setRampPoint(time) {\n        this._param.setRampPoint(time);\n        return this;\n    }\n    linearRampToValueAtTime(value, time) {\n        this._param.linearRampToValueAtTime(value, time);\n        return this;\n    }\n    exponentialRampToValueAtTime(value, time) {\n        this._param.exponentialRampToValueAtTime(value, time);\n        return this;\n    }\n    exponentialRampTo(value, rampTime, startTime) {\n        this._param.exponentialRampTo(value, rampTime, startTime);\n        return this;\n    }\n    linearRampTo(value, rampTime, startTime) {\n        this._param.linearRampTo(value, rampTime, startTime);\n        return this;\n    }\n    targetRampTo(value, rampTime, startTime) {\n        this._param.targetRampTo(value, rampTime, startTime);\n        return this;\n    }\n    exponentialApproachValueAtTime(value, time, rampTime) {\n        this._param.exponentialApproachValueAtTime(value, time, rampTime);\n        return this;\n    }\n    setTargetAtTime(value, startTime, timeConstant) {\n        this._param.setTargetAtTime(value, startTime, timeConstant);\n        return this;\n    }\n    setValueCurveAtTime(values, startTime, duration, scaling) {\n        this._param.setValueCurveAtTime(values, startTime, duration, scaling);\n        return this;\n    }\n    cancelScheduledValues(time) {\n        this._param.cancelScheduledValues(time);\n        return this;\n    }\n    cancelAndHoldAtTime(time) {\n        this._param.cancelAndHoldAtTime(time);\n        return this;\n    }\n    rampTo(value, rampTime, startTime) {\n        this._param.rampTo(value, rampTime, startTime);\n        return this;\n    }\n    get value() {\n        return this._param.value;\n    }\n    set value(value) {\n        this._param.value = value;\n    }\n    get convert() {\n        return this._param.convert;\n    }\n    set convert(convert) {\n        this._param.convert = convert;\n    }\n    get units() {\n        return this._param.units;\n    }\n    get overridden() {\n        return this._param.overridden;\n    }\n    set overridden(overridden) {\n        this._param.overridden = overridden;\n    }\n    get maxValue() {\n        return this._param.maxValue;\n    }\n    get minValue() {\n        return this._param.minValue;\n    }\n    /**\n     * See [[Param.apply]].\n     */\n    apply(param) {\n        this._param.apply(param);\n        return this;\n    }\n}\n/**\n * When connecting from a signal, it\'s necessary to zero out the node destination\n * node if that node is also a signal. If the destination is not 0, then the values\n * will be summed. This method insures that the output of the destination signal will\n * be the same as the source signal, making the destination signal a pass through node.\n * @param signal The output signal to connect from\n * @param destination the destination to connect to\n * @param outputNum the optional output number\n * @param inputNum the input number\n */\nfunction connectSignal(signal, destination, outputNum, inputNum) {\n    if (destination instanceof Param_Param || isAudioParam(destination) ||\n        (destination instanceof Signal_Signal && destination.override)) {\n        // cancel changes\n        destination.cancelScheduledValues(0);\n        // reset the value\n        destination.setValueAtTime(0, 0);\n        // mark the value as overridden\n        if (destination instanceof Signal_Signal) {\n            destination.overridden = true;\n        }\n    }\n    ToneAudioNode_connect(signal, destination, outputNum, inputNum);\n}\n//# sourceMappingURL=Signal.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/TickParam.js\n\n\n\n\n/**\n * A Param class just for computing ticks. Similar to the [[Param]] class,\n * but offers conversion to BPM values as well as ability to compute tick\n * duration and elapsed ticks\n */\nclass TickParam_TickParam extends Param_Param {\n    constructor() {\n        super(optionsFromArguments(TickParam_TickParam.getDefaults(), arguments, ["value"]));\n        this.name = "TickParam";\n        /**\n         * The timeline which tracks all of the automations.\n         */\n        this._events = new Timeline_Timeline(Infinity);\n        /**\n         * The internal holder for the multiplier value\n         */\n        this._multiplier = 1;\n        const options = optionsFromArguments(TickParam_TickParam.getDefaults(), arguments, ["value"]);\n        // set the multiplier\n        this._multiplier = options.multiplier;\n        // clear the ticks from the beginning\n        this._events.cancel(0);\n        // set an initial event\n        this._events.add({\n            ticks: 0,\n            time: 0,\n            type: "setValueAtTime",\n            value: this._fromType(options.value),\n        });\n        this.setValueAtTime(options.value, 0);\n    }\n    static getDefaults() {\n        return Object.assign(Param_Param.getDefaults(), {\n            multiplier: 1,\n            units: "hertz",\n            value: 1,\n        });\n    }\n    setTargetAtTime(value, time, constant) {\n        // approximate it with multiple linear ramps\n        time = this.toSeconds(time);\n        this.setRampPoint(time);\n        const computedValue = this._fromType(value);\n        // start from previously scheduled value\n        const prevEvent = this._events.get(time);\n        const segments = Math.round(Math.max(1 / constant, 1));\n        for (let i = 0; i <= segments; i++) {\n            const segTime = constant * i + time;\n            const rampVal = this._exponentialApproach(prevEvent.time, prevEvent.value, computedValue, constant, segTime);\n            this.linearRampToValueAtTime(this._toType(rampVal), segTime);\n        }\n        return this;\n    }\n    setValueAtTime(value, time) {\n        const computedTime = this.toSeconds(time);\n        super.setValueAtTime(value, time);\n        const event = this._events.get(computedTime);\n        const previousEvent = this._events.previousEvent(event);\n        const ticksUntilTime = this._getTicksUntilEvent(previousEvent, computedTime);\n        event.ticks = Math.max(ticksUntilTime, 0);\n        return this;\n    }\n    linearRampToValueAtTime(value, time) {\n        const computedTime = this.toSeconds(time);\n        super.linearRampToValueAtTime(value, time);\n        const event = this._events.get(computedTime);\n        const previousEvent = this._events.previousEvent(event);\n        const ticksUntilTime = this._getTicksUntilEvent(previousEvent, computedTime);\n        event.ticks = Math.max(ticksUntilTime, 0);\n        return this;\n    }\n    exponentialRampToValueAtTime(value, time) {\n        // aproximate it with multiple linear ramps\n        time = this.toSeconds(time);\n        const computedVal = this._fromType(value);\n        // start from previously scheduled value\n        const prevEvent = this._events.get(time);\n        // approx 10 segments per second\n        const segments = Math.round(Math.max((time - prevEvent.time) * 10, 1));\n        const segmentDur = ((time - prevEvent.time) / segments);\n        for (let i = 0; i <= segments; i++) {\n            const segTime = segmentDur * i + prevEvent.time;\n            const rampVal = this._exponentialInterpolate(prevEvent.time, prevEvent.value, time, computedVal, segTime);\n            this.linearRampToValueAtTime(this._toType(rampVal), segTime);\n        }\n        return this;\n    }\n    /**\n     * Returns the tick value at the time. Takes into account\n     * any automation curves scheduled on the signal.\n     * @param  event The time to get the tick count at\n     * @return The number of ticks which have elapsed at the time given any automations.\n     */\n    _getTicksUntilEvent(event, time) {\n        if (event === null) {\n            event = {\n                ticks: 0,\n                time: 0,\n                type: "setValueAtTime",\n                value: 0,\n            };\n        }\n        else if (isUndef(event.ticks)) {\n            const previousEvent = this._events.previousEvent(event);\n            event.ticks = this._getTicksUntilEvent(previousEvent, event.time);\n        }\n        const val0 = this._fromType(this.getValueAtTime(event.time));\n        let val1 = this._fromType(this.getValueAtTime(time));\n        // if it\'s right on the line, take the previous value\n        const onTheLineEvent = this._events.get(time);\n        if (onTheLineEvent && onTheLineEvent.time === time && onTheLineEvent.type === "setValueAtTime") {\n            val1 = this._fromType(this.getValueAtTime(time - this.sampleTime));\n        }\n        return 0.5 * (time - event.time) * (val0 + val1) + event.ticks;\n    }\n    /**\n     * Returns the tick value at the time. Takes into account\n     * any automation curves scheduled on the signal.\n     * @param  time The time to get the tick count at\n     * @return The number of ticks which have elapsed at the time given any automations.\n     */\n    getTicksAtTime(time) {\n        const computedTime = this.toSeconds(time);\n        const event = this._events.get(computedTime);\n        return Math.max(this._getTicksUntilEvent(event, computedTime), 0);\n    }\n    /**\n     * Return the elapsed time of the number of ticks from the given time\n     * @param ticks The number of ticks to calculate\n     * @param  time The time to get the next tick from\n     * @return The duration of the number of ticks from the given time in seconds\n     */\n    getDurationOfTicks(ticks, time) {\n        const computedTime = this.toSeconds(time);\n        const currentTick = this.getTicksAtTime(time);\n        return this.getTimeOfTick(currentTick + ticks) - computedTime;\n    }\n    /**\n     * Given a tick, returns the time that tick occurs at.\n     * @return The time that the tick occurs.\n     */\n    getTimeOfTick(tick) {\n        const before = this._events.get(tick, "ticks");\n        const after = this._events.getAfter(tick, "ticks");\n        if (before && before.ticks === tick) {\n            return before.time;\n        }\n        else if (before && after &&\n            after.type === "linearRampToValueAtTime" &&\n            before.value !== after.value) {\n            const val0 = this._fromType(this.getValueAtTime(before.time));\n            const val1 = this._fromType(this.getValueAtTime(after.time));\n            const delta = (val1 - val0) / (after.time - before.time);\n            const k = Math.sqrt(Math.pow(val0, 2) - 2 * delta * (before.ticks - tick));\n            const sol1 = (-val0 + k) / delta;\n            const sol2 = (-val0 - k) / delta;\n            return (sol1 > 0 ? sol1 : sol2) + before.time;\n        }\n        else if (before) {\n            if (before.value === 0) {\n                return Infinity;\n            }\n            else {\n                return before.time + (tick - before.ticks) / before.value;\n            }\n        }\n        else {\n            return tick / this._initialValue;\n        }\n    }\n    /**\n     * Convert some number of ticks their the duration in seconds accounting\n     * for any automation curves starting at the given time.\n     * @param  ticks The number of ticks to convert to seconds.\n     * @param  when  When along the automation timeline to convert the ticks.\n     * @return The duration in seconds of the ticks.\n     */\n    ticksToTime(ticks, when) {\n        return this.getDurationOfTicks(ticks, when);\n    }\n    /**\n     * The inverse of [[ticksToTime]]. Convert a duration in\n     * seconds to the corresponding number of ticks accounting for any\n     * automation curves starting at the given time.\n     * @param  duration The time interval to convert to ticks.\n     * @param  when When along the automation timeline to convert the ticks.\n     * @return The duration in ticks.\n     */\n    timeToTicks(duration, when) {\n        const computedTime = this.toSeconds(when);\n        const computedDuration = this.toSeconds(duration);\n        const startTicks = this.getTicksAtTime(computedTime);\n        const endTicks = this.getTicksAtTime(computedTime + computedDuration);\n        return endTicks - startTicks;\n    }\n    /**\n     * Convert from the type when the unit value is BPM\n     */\n    _fromType(val) {\n        if (this.units === "bpm" && this.multiplier) {\n            return 1 / (60 / val / this.multiplier);\n        }\n        else {\n            return super._fromType(val);\n        }\n    }\n    /**\n     * Special case of type conversion where the units === "bpm"\n     */\n    _toType(val) {\n        if (this.units === "bpm" && this.multiplier) {\n            return (val / this.multiplier) * 60;\n        }\n        else {\n            return super._toType(val);\n        }\n    }\n    /**\n     * A multiplier on the bpm value. Useful for setting a PPQ relative to the base frequency value.\n     */\n    get multiplier() {\n        return this._multiplier;\n    }\n    set multiplier(m) {\n        // get and reset the current value with the new multiplier\n        // might be necessary to clear all the previous values\n        const currentVal = this.value;\n        this._multiplier = m;\n        this.cancelScheduledValues(0);\n        this.setValueAtTime(currentVal, 0);\n    }\n}\n//# sourceMappingURL=TickParam.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/TickSignal.js\n\n\n\n/**\n * TickSignal extends Tone.Signal, but adds the capability\n * to calculate the number of elapsed ticks. exponential and target curves\n * are approximated with multiple linear ramps.\n *\n * Thank you Bruno Dias, H. Sofia Pinto, and David M. Matos,\n * for your [WAC paper](https://smartech.gatech.edu/bitstream/handle/1853/54588/WAC2016-49.pdf)\n * describing integrating timing functions for tempo calculations.\n */\nclass TickSignal_TickSignal extends Signal_Signal {\n    constructor() {\n        super(optionsFromArguments(TickSignal_TickSignal.getDefaults(), arguments, ["value"]));\n        this.name = "TickSignal";\n        const options = optionsFromArguments(TickSignal_TickSignal.getDefaults(), arguments, ["value"]);\n        this.input = this._param = new TickParam_TickParam({\n            context: this.context,\n            convert: options.convert,\n            multiplier: options.multiplier,\n            param: this._constantSource.offset,\n            units: options.units,\n            value: options.value,\n        });\n    }\n    static getDefaults() {\n        return Object.assign(Signal_Signal.getDefaults(), {\n            multiplier: 1,\n            units: "hertz",\n            value: 1,\n        });\n    }\n    ticksToTime(ticks, when) {\n        return this._param.ticksToTime(ticks, when);\n    }\n    timeToTicks(duration, when) {\n        return this._param.timeToTicks(duration, when);\n    }\n    getTimeOfTick(tick) {\n        return this._param.getTimeOfTick(tick);\n    }\n    getDurationOfTicks(ticks, time) {\n        return this._param.getDurationOfTicks(ticks, time);\n    }\n    getTicksAtTime(time) {\n        return this._param.getTicksAtTime(time);\n    }\n    /**\n     * A multiplier on the bpm value. Useful for setting a PPQ relative to the base frequency value.\n     */\n    get multiplier() {\n        return this._param.multiplier;\n    }\n    set multiplier(m) {\n        this._param.multiplier = m;\n    }\n    dispose() {\n        super.dispose();\n        this._param.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=TickSignal.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/TickSource.js\n\n\n\n\n\n\n\n\n/**\n * Uses [TickSignal](TickSignal) to track elapsed ticks with complex automation curves.\n */\nclass TickSource_TickSource extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(optionsFromArguments(TickSource_TickSource.getDefaults(), arguments, ["frequency"]));\n        this.name = "TickSource";\n        /**\n         * The state timeline\n         */\n        this._state = new StateTimeline_StateTimeline();\n        /**\n         * The offset values of the ticks\n         */\n        this._tickOffset = new Timeline_Timeline();\n        const options = optionsFromArguments(TickSource_TickSource.getDefaults(), arguments, ["frequency"]);\n        this.frequency = new TickSignal_TickSignal({\n            context: this.context,\n            units: options.units,\n            value: options.frequency,\n        });\n        readOnly(this, "frequency");\n        // set the initial state\n        this._state.setStateAtTime("stopped", 0);\n        // add the first event\n        this.setTicksAtTime(0, 0);\n    }\n    static getDefaults() {\n        return Object.assign({\n            frequency: 1,\n            units: "hertz",\n        }, ToneWithContext_ToneWithContext.getDefaults());\n    }\n    /**\n     * Returns the playback state of the source, either "started", "stopped" or "paused".\n     */\n    get state() {\n        return this.getStateAtTime(this.now());\n    }\n    /**\n     * Start the clock at the given time. Optionally pass in an offset\n     * of where to start the tick counter from.\n     * @param  time    The time the clock should start\n     * @param offset The number of ticks to start the source at\n     */\n    start(time, offset) {\n        const computedTime = this.toSeconds(time);\n        if (this._state.getValueAtTime(computedTime) !== "started") {\n            this._state.setStateAtTime("started", computedTime);\n            if (isDefined(offset)) {\n                this.setTicksAtTime(offset, computedTime);\n            }\n        }\n        return this;\n    }\n    /**\n     * Stop the clock. Stopping the clock resets the tick counter to 0.\n     * @param time The time when the clock should stop.\n     */\n    stop(time) {\n        const computedTime = this.toSeconds(time);\n        // cancel the previous stop\n        if (this._state.getValueAtTime(computedTime) === "stopped") {\n            const event = this._state.get(computedTime);\n            if (event && event.time > 0) {\n                this._tickOffset.cancel(event.time);\n                this._state.cancel(event.time);\n            }\n        }\n        this._state.cancel(computedTime);\n        this._state.setStateAtTime("stopped", computedTime);\n        this.setTicksAtTime(0, computedTime);\n        return this;\n    }\n    /**\n     * Pause the clock. Pausing does not reset the tick counter.\n     * @param time The time when the clock should stop.\n     */\n    pause(time) {\n        const computedTime = this.toSeconds(time);\n        if (this._state.getValueAtTime(computedTime) === "started") {\n            this._state.setStateAtTime("paused", computedTime);\n        }\n        return this;\n    }\n    /**\n     * Cancel start/stop/pause and setTickAtTime events scheduled after the given time.\n     * @param time When to clear the events after\n     */\n    cancel(time) {\n        time = this.toSeconds(time);\n        this._state.cancel(time);\n        this._tickOffset.cancel(time);\n        return this;\n    }\n    /**\n     * Get the elapsed ticks at the given time\n     * @param  time  When to get the tick value\n     * @return The number of ticks\n     */\n    getTicksAtTime(time) {\n        const computedTime = this.toSeconds(time);\n        const stopEvent = this._state.getLastState("stopped", computedTime);\n        // this event allows forEachBetween to iterate until the current time\n        const tmpEvent = { state: "paused", time: computedTime };\n        this._state.add(tmpEvent);\n        // keep track of the previous offset event\n        let lastState = stopEvent;\n        let elapsedTicks = 0;\n        // iterate through all the events since the last stop\n        this._state.forEachBetween(stopEvent.time, computedTime + this.sampleTime, e => {\n            let periodStartTime = lastState.time;\n            // if there is an offset event in this period use that\n            const offsetEvent = this._tickOffset.get(e.time);\n            if (offsetEvent && offsetEvent.time >= lastState.time) {\n                elapsedTicks = offsetEvent.ticks;\n                periodStartTime = offsetEvent.time;\n            }\n            if (lastState.state === "started" && e.state !== "started") {\n                elapsedTicks += this.frequency.getTicksAtTime(e.time) - this.frequency.getTicksAtTime(periodStartTime);\n            }\n            lastState = e;\n        });\n        // remove the temporary event\n        this._state.remove(tmpEvent);\n        // return the ticks\n        return elapsedTicks;\n    }\n    /**\n     * The number of times the callback was invoked. Starts counting at 0\n     * and increments after the callback was invoked. Returns -1 when stopped.\n     */\n    get ticks() {\n        return this.getTicksAtTime(this.now());\n    }\n    set ticks(t) {\n        this.setTicksAtTime(t, this.now());\n    }\n    /**\n     * The time since ticks=0 that the TickSource has been running. Accounts\n     * for tempo curves\n     */\n    get seconds() {\n        return this.getSecondsAtTime(this.now());\n    }\n    set seconds(s) {\n        const now = this.now();\n        const ticks = this.frequency.timeToTicks(s, now);\n        this.setTicksAtTime(ticks, now);\n    }\n    /**\n     * Return the elapsed seconds at the given time.\n     * @param  time  When to get the elapsed seconds\n     * @return  The number of elapsed seconds\n     */\n    getSecondsAtTime(time) {\n        time = this.toSeconds(time);\n        const stopEvent = this._state.getLastState("stopped", time);\n        // this event allows forEachBetween to iterate until the current time\n        const tmpEvent = { state: "paused", time };\n        this._state.add(tmpEvent);\n        // keep track of the previous offset event\n        let lastState = stopEvent;\n        let elapsedSeconds = 0;\n        // iterate through all the events since the last stop\n        this._state.forEachBetween(stopEvent.time, time + this.sampleTime, e => {\n            let periodStartTime = lastState.time;\n            // if there is an offset event in this period use that\n            const offsetEvent = this._tickOffset.get(e.time);\n            if (offsetEvent && offsetEvent.time >= lastState.time) {\n                elapsedSeconds = offsetEvent.seconds;\n                periodStartTime = offsetEvent.time;\n            }\n            if (lastState.state === "started" && e.state !== "started") {\n                elapsedSeconds += e.time - periodStartTime;\n            }\n            lastState = e;\n        });\n        // remove the temporary event\n        this._state.remove(tmpEvent);\n        // return the ticks\n        return elapsedSeconds;\n    }\n    /**\n     * Set the clock\'s ticks at the given time.\n     * @param  ticks The tick value to set\n     * @param  time  When to set the tick value\n     */\n    setTicksAtTime(ticks, time) {\n        time = this.toSeconds(time);\n        this._tickOffset.cancel(time);\n        this._tickOffset.add({\n            seconds: this.frequency.getDurationOfTicks(ticks, time),\n            ticks,\n            time,\n        });\n        return this;\n    }\n    /**\n     * Returns the scheduled state at the given time.\n     * @param  time  The time to query.\n     */\n    getStateAtTime(time) {\n        time = this.toSeconds(time);\n        return this._state.getValueAtTime(time);\n    }\n    /**\n     * Get the time of the given tick. The second argument\n     * is when to test before. Since ticks can be set (with setTicksAtTime)\n     * there may be multiple times for a given tick value.\n     * @param  tick The tick number.\n     * @param  before When to measure the tick value from.\n     * @return The time of the tick\n     */\n    getTimeOfTick(tick, before = this.now()) {\n        const offset = this._tickOffset.get(before);\n        const event = this._state.get(before);\n        const startTime = Math.max(offset.time, event.time);\n        const absoluteTicks = this.frequency.getTicksAtTime(startTime) + tick - offset.ticks;\n        return this.frequency.getTimeOfTick(absoluteTicks);\n    }\n    /**\n     * Invoke the callback event at all scheduled ticks between the\n     * start time and the end time\n     * @param  startTime  The beginning of the search range\n     * @param  endTime    The end of the search range\n     * @param  callback   The callback to invoke with each tick\n     */\n    forEachTickBetween(startTime, endTime, callback) {\n        // only iterate through the sections where it is "started"\n        let lastStateEvent = this._state.get(startTime);\n        this._state.forEachBetween(startTime, endTime, event => {\n            if (lastStateEvent && lastStateEvent.state === "started" && event.state !== "started") {\n                this.forEachTickBetween(Math.max(lastStateEvent.time, startTime), event.time - this.sampleTime, callback);\n            }\n            lastStateEvent = event;\n        });\n        let error = null;\n        if (lastStateEvent && lastStateEvent.state === "started") {\n            const maxStartTime = Math.max(lastStateEvent.time, startTime);\n            // figure out the difference between the frequency ticks and the\n            const startTicks = this.frequency.getTicksAtTime(maxStartTime);\n            const ticksAtStart = this.frequency.getTicksAtTime(lastStateEvent.time);\n            const diff = startTicks - ticksAtStart;\n            let offset = Math.ceil(diff) - diff;\n            // guard against floating point issues\n            offset = EQ(offset, 1) ? 0 : offset;\n            let nextTickTime = this.frequency.getTimeOfTick(startTicks + offset);\n            while (nextTickTime < endTime) {\n                try {\n                    callback(nextTickTime, Math.round(this.getTicksAtTime(nextTickTime)));\n                }\n                catch (e) {\n                    error = e;\n                    break;\n                }\n                nextTickTime += this.frequency.getDurationOfTicks(1, nextTickTime);\n            }\n        }\n        if (error) {\n            throw error;\n        }\n        return this;\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this._state.dispose();\n        this._tickOffset.dispose();\n        this.frequency.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=TickSource.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/Clock.js\n\n\n\n\n\n\n\n/**\n * A sample accurate clock which provides a callback at the given rate.\n * While the callback is not sample-accurate (it is still susceptible to\n * loose JS timing), the time passed in as the argument to the callback\n * is precise. For most applications, it is better to use Tone.Transport\n * instead of the Clock by itself since you can synchronize multiple callbacks.\n * @example\n * // the callback will be invoked approximately once a second\n * // and will print the time exactly once a second apart.\n * const clock = new Tone.Clock(time => {\n * \tconsole.log(time);\n * }, 1);\n * clock.start();\n * @category Core\n */\nclass Clock_Clock extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(optionsFromArguments(Clock_Clock.getDefaults(), arguments, ["callback", "frequency"]));\n        this.name = "Clock";\n        /**\n         * The callback function to invoke at the scheduled tick.\n         */\n        this.callback = noOp;\n        /**\n         * The last time the loop callback was invoked\n         */\n        this._lastUpdate = 0;\n        /**\n         * Keep track of the playback state\n         */\n        this._state = new StateTimeline_StateTimeline("stopped");\n        /**\n         * Context bound reference to the _loop method\n         * This is necessary to remove the event in the end.\n         */\n        this._boundLoop = this._loop.bind(this);\n        const options = optionsFromArguments(Clock_Clock.getDefaults(), arguments, ["callback", "frequency"]);\n        this.callback = options.callback;\n        this._tickSource = new TickSource_TickSource({\n            context: this.context,\n            frequency: options.frequency,\n            units: options.units,\n        });\n        this._lastUpdate = 0;\n        this.frequency = this._tickSource.frequency;\n        readOnly(this, "frequency");\n        // add an initial state\n        this._state.setStateAtTime("stopped", 0);\n        // bind a callback to the worker thread\n        this.context.on("tick", this._boundLoop);\n    }\n    static getDefaults() {\n        return Object.assign(ToneWithContext_ToneWithContext.getDefaults(), {\n            callback: noOp,\n            frequency: 1,\n            units: "hertz",\n        });\n    }\n    /**\n     * Returns the playback state of the source, either "started", "stopped" or "paused".\n     */\n    get state() {\n        return this._state.getValueAtTime(this.now());\n    }\n    /**\n     * Start the clock at the given time. Optionally pass in an offset\n     * of where to start the tick counter from.\n     * @param  time    The time the clock should start\n     * @param offset  Where the tick counter starts counting from.\n     */\n    start(time, offset) {\n        // make sure the context is running\n        assertContextRunning(this.context);\n        // start the loop\n        const computedTime = this.toSeconds(time);\n        this.log("start", computedTime);\n        if (this._state.getValueAtTime(computedTime) !== "started") {\n            this._state.setStateAtTime("started", computedTime);\n            this._tickSource.start(computedTime, offset);\n            if (computedTime < this._lastUpdate) {\n                this.emit("start", computedTime, offset);\n            }\n        }\n        return this;\n    }\n    /**\n     * Stop the clock. Stopping the clock resets the tick counter to 0.\n     * @param time The time when the clock should stop.\n     * @example\n     * const clock = new Tone.Clock(time => {\n     * \tconsole.log(time);\n     * }, 1);\n     * clock.start();\n     * // stop the clock after 10 seconds\n     * clock.stop("+10");\n     */\n    stop(time) {\n        const computedTime = this.toSeconds(time);\n        this.log("stop", computedTime);\n        this._state.cancel(computedTime);\n        this._state.setStateAtTime("stopped", computedTime);\n        this._tickSource.stop(computedTime);\n        if (computedTime < this._lastUpdate) {\n            this.emit("stop", computedTime);\n        }\n        return this;\n    }\n    /**\n     * Pause the clock. Pausing does not reset the tick counter.\n     * @param time The time when the clock should stop.\n     */\n    pause(time) {\n        const computedTime = this.toSeconds(time);\n        if (this._state.getValueAtTime(computedTime) === "started") {\n            this._state.setStateAtTime("paused", computedTime);\n            this._tickSource.pause(computedTime);\n            if (computedTime < this._lastUpdate) {\n                this.emit("pause", computedTime);\n            }\n        }\n        return this;\n    }\n    /**\n     * The number of times the callback was invoked. Starts counting at 0\n     * and increments after the callback was invoked.\n     */\n    get ticks() {\n        return Math.ceil(this.getTicksAtTime(this.now()));\n    }\n    set ticks(t) {\n        this._tickSource.ticks = t;\n    }\n    /**\n     * The time since ticks=0 that the Clock has been running. Accounts for tempo curves\n     */\n    get seconds() {\n        return this._tickSource.seconds;\n    }\n    set seconds(s) {\n        this._tickSource.seconds = s;\n    }\n    /**\n     * Return the elapsed seconds at the given time.\n     * @param  time  When to get the elapsed seconds\n     * @return  The number of elapsed seconds\n     */\n    getSecondsAtTime(time) {\n        return this._tickSource.getSecondsAtTime(time);\n    }\n    /**\n     * Set the clock\'s ticks at the given time.\n     * @param  ticks The tick value to set\n     * @param  time  When to set the tick value\n     */\n    setTicksAtTime(ticks, time) {\n        this._tickSource.setTicksAtTime(ticks, time);\n        return this;\n    }\n    /**\n     * Get the time of the given tick. The second argument\n     * is when to test before. Since ticks can be set (with setTicksAtTime)\n     * there may be multiple times for a given tick value.\n     * @param  tick The tick number.\n     * @param  before When to measure the tick value from.\n     * @return The time of the tick\n     */\n    getTimeOfTick(tick, before = this.now()) {\n        return this._tickSource.getTimeOfTick(tick, before);\n    }\n    /**\n     * Get the clock\'s ticks at the given time.\n     * @param  time  When to get the tick value\n     * @return The tick value at the given time.\n     */\n    getTicksAtTime(time) {\n        return this._tickSource.getTicksAtTime(time);\n    }\n    /**\n     * Get the time of the next tick\n     * @param  offset The tick number.\n     */\n    nextTickTime(offset, when) {\n        const computedTime = this.toSeconds(when);\n        const currentTick = this.getTicksAtTime(computedTime);\n        return this._tickSource.getTimeOfTick(currentTick + offset, computedTime);\n    }\n    /**\n     * The scheduling loop.\n     */\n    _loop() {\n        const startTime = this._lastUpdate;\n        const endTime = this.now();\n        this._lastUpdate = endTime;\n        this.log("loop", startTime, endTime);\n        if (startTime !== endTime) {\n            // the state change events\n            this._state.forEachBetween(startTime, endTime, e => {\n                switch (e.state) {\n                    case "started":\n                        const offset = this._tickSource.getTicksAtTime(e.time);\n                        this.emit("start", e.time, offset);\n                        break;\n                    case "stopped":\n                        if (e.time !== 0) {\n                            this.emit("stop", e.time);\n                        }\n                        break;\n                    case "paused":\n                        this.emit("pause", e.time);\n                        break;\n                }\n            });\n            // the tick callbacks\n            this._tickSource.forEachTickBetween(startTime, endTime, (time, ticks) => {\n                this.callback(time, ticks);\n            });\n        }\n    }\n    /**\n     * Returns the scheduled state at the given time.\n     * @param  time  The time to query.\n     * @return  The name of the state input in setStateAtTime.\n     * @example\n     * const clock = new Tone.Clock();\n     * clock.start("+0.1");\n     * clock.getStateAtTime("+0.1"); // returns "started"\n     */\n    getStateAtTime(time) {\n        const computedTime = this.toSeconds(time);\n        return this._state.getValueAtTime(computedTime);\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this.context.off("tick", this._boundLoop);\n        this._tickSource.dispose();\n        this._state.dispose();\n        return this;\n    }\n}\nEmitter_Emitter.mixin(Clock_Clock);\n//# sourceMappingURL=Clock.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Delay.js\n\n\n\n\n/**\n * Wrapper around Web Audio\'s native [DelayNode](http://webaudio.github.io/web-audio-api/#the-delaynode-interface).\n * @category Core\n * @example\n * return Tone.Offline(() => {\n * \tconst delay = new Tone.Delay(0.1).toDestination();\n * \t// connect the signal to both the delay and the destination\n * \tconst pulse = new Tone.PulseOscillator().connect(delay).toDestination();\n * \t// start and stop the pulse\n * \tpulse.start(0).stop(0.01);\n * }, 0.5, 1);\n */\nclass Delay_Delay extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Delay_Delay.getDefaults(), arguments, ["delayTime", "maxDelay"]));\n        this.name = "Delay";\n        const options = optionsFromArguments(Delay_Delay.getDefaults(), arguments, ["delayTime", "maxDelay"]);\n        const maxDelayInSeconds = this.toSeconds(options.maxDelay);\n        this._maxDelay = Math.max(maxDelayInSeconds, this.toSeconds(options.delayTime));\n        this._delayNode = this.input = this.output = this.context.createDelay(maxDelayInSeconds);\n        this.delayTime = new Param_Param({\n            context: this.context,\n            param: this._delayNode.delayTime,\n            units: "time",\n            value: options.delayTime,\n            minValue: 0,\n            maxValue: this.maxDelay,\n        });\n        readOnly(this, "delayTime");\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            delayTime: 0,\n            maxDelay: 1,\n        });\n    }\n    /**\n     * The maximum delay time. This cannot be changed after\n     * the value is passed into the constructor.\n     */\n    get maxDelay() {\n        return this._maxDelay;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._delayNode.disconnect();\n        this.delayTime.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Delay.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Offline.js\n\n\n\n\n/**\n * Generate a buffer by rendering all of the Tone.js code within the callback using the OfflineAudioContext.\n * The OfflineAudioContext is capable of rendering much faster than real time in many cases.\n * The callback function also passes in an offline instance of [[Context]] which can be used\n * to schedule events along the Transport.\n * @param  callback  All Tone.js nodes which are created and scheduled within this callback are recorded into the output Buffer.\n * @param  duration     the amount of time to record for.\n * @return  The promise which is invoked with the ToneAudioBuffer of the recorded output.\n * @example\n * // render 2 seconds of the oscillator\n * Tone.Offline(() => {\n * \t// only nodes created in this callback will be recorded\n * \tconst oscillator = new Tone.Oscillator().toDestination().start(0);\n * }, 2).then((buffer) => {\n * \t// do something with the output buffer\n * \tconsole.log(buffer);\n * });\n * @example\n * // can also schedule events along the Transport\n * // using the passed in Offline Transport\n * Tone.Offline(({ transport }) => {\n * \tconst osc = new Tone.Oscillator().toDestination();\n * \ttransport.schedule(time => {\n * \t\tosc.start(time).stop(time + 0.1);\n * \t}, 1);\n * \t// make sure to start the transport\n * \ttransport.start(0.2);\n * }, 4).then((buffer) => {\n * \t// do something with the output buffer\n * \tconsole.log(buffer);\n * });\n * @category Core\n */\nfunction Offline(callback, duration, channels = 2, sampleRate = getContext().sampleRate) {\n    return __awaiter(this, void 0, void 0, function* () {\n        // set the OfflineAudioContext based on the current context\n        const originalContext = getContext();\n        const context = new OfflineContext_OfflineContext(channels, duration, sampleRate);\n        setContext(context);\n        // invoke the callback/scheduling\n        yield callback(context);\n        // then render the audio\n        const bufferPromise = context.render();\n        // return the original AudioContext\n        setContext(originalContext);\n        // await the rendering\n        const buffer = yield bufferPromise;\n        // return the audio\n        return new ToneAudioBuffer_ToneAudioBuffer(buffer);\n    });\n}\n//# sourceMappingURL=Offline.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/ToneAudioBuffers.js\n\n\n\n\n\n\n/**\n * A data structure for holding multiple buffers in a Map-like datastructure.\n *\n * @example\n * const pianoSamples = new Tone.ToneAudioBuffers({\n * \tA1: "https://tonejs.github.io/audio/casio/A1.mp3",\n * \tA2: "https://tonejs.github.io/audio/casio/A2.mp3",\n * }, () => {\n * \tconst player = new Tone.Player().toDestination();\n * \t// play one of the samples when they all load\n * \tplayer.buffer = pianoSamples.get("A2");\n * \tplayer.start();\n * });\n * @example\n * // To pass in additional parameters in the second parameter\n * const buffers = new Tone.ToneAudioBuffers({\n * \t urls: {\n * \t\t A1: "A1.mp3",\n * \t\t A2: "A2.mp3",\n * \t },\n * \t onload: () => console.log("loaded"),\n * \t baseUrl: "https://tonejs.github.io/audio/casio/"\n * });\n * @category Core\n */\nclass ToneAudioBuffers_ToneAudioBuffers extends Tone_Tone {\n    constructor() {\n        super();\n        this.name = "ToneAudioBuffers";\n        /**\n         * All of the buffers\n         */\n        this._buffers = new Map();\n        /**\n         * Keep track of the number of loaded buffers\n         */\n        this._loadingCount = 0;\n        const options = optionsFromArguments(ToneAudioBuffers_ToneAudioBuffers.getDefaults(), arguments, ["urls", "onload", "baseUrl"], "urls");\n        this.baseUrl = options.baseUrl;\n        // add each one\n        Object.keys(options.urls).forEach(name => {\n            this._loadingCount++;\n            const url = options.urls[name];\n            this.add(name, url, this._bufferLoaded.bind(this, options.onload), options.onerror);\n        });\n    }\n    static getDefaults() {\n        return {\n            baseUrl: "",\n            onerror: noOp,\n            onload: noOp,\n            urls: {},\n        };\n    }\n    /**\n     * True if the buffers object has a buffer by that name.\n     * @param  name  The key or index of the buffer.\n     */\n    has(name) {\n        return this._buffers.has(name.toString());\n    }\n    /**\n     * Get a buffer by name. If an array was loaded,\n     * then use the array index.\n     * @param  name  The key or index of the buffer.\n     */\n    get(name) {\n        assert(this.has(name), `ToneAudioBuffers has no buffer named: ${name}`);\n        return this._buffers.get(name.toString());\n    }\n    /**\n     * A buffer was loaded. decrement the counter.\n     */\n    _bufferLoaded(callback) {\n        this._loadingCount--;\n        if (this._loadingCount === 0 && callback) {\n            callback();\n        }\n    }\n    /**\n     * If the buffers are loaded or not\n     */\n    get loaded() {\n        return Array.from(this._buffers).every(([_, buffer]) => buffer.loaded);\n    }\n    /**\n     * Add a buffer by name and url to the Buffers\n     * @param  name      A unique name to give the buffer\n     * @param  url  Either the url of the bufer, or a buffer which will be added with the given name.\n     * @param  callback  The callback to invoke when the url is loaded.\n     * @param  onerror  Invoked if the buffer can\'t be loaded\n     */\n    add(name, url, callback = noOp, onerror = noOp) {\n        if (isString(url)) {\n            this._buffers.set(name.toString(), new ToneAudioBuffer_ToneAudioBuffer(this.baseUrl + url, callback, onerror));\n        }\n        else {\n            this._buffers.set(name.toString(), new ToneAudioBuffer_ToneAudioBuffer(url, callback, onerror));\n        }\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._buffers.forEach(buffer => buffer.dispose());\n        this._buffers.clear();\n        return this;\n    }\n}\n//# sourceMappingURL=ToneAudioBuffers.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/Midi.js\n\n\n\n/**\n * Midi is a primitive type for encoding Time values.\n * Midi can be constructed with or without the `new` keyword. Midi can be passed\n * into the parameter of any method which takes time as an argument.\n * @category Unit\n */\nclass Midi_MidiClass extends Frequency_FrequencyClass {\n    constructor() {\n        super(...arguments);\n        this.name = "MidiClass";\n        this.defaultUnits = "midi";\n    }\n    /**\n     * Returns the value of a frequency in the current units\n     */\n    _frequencyToUnits(freq) {\n        return ftom(super._frequencyToUnits(freq));\n    }\n    /**\n     * Returns the value of a tick in the current time units\n     */\n    _ticksToUnits(ticks) {\n        return ftom(super._ticksToUnits(ticks));\n    }\n    /**\n     * Return the value of the beats in the current units\n     */\n    _beatsToUnits(beats) {\n        return ftom(super._beatsToUnits(beats));\n    }\n    /**\n     * Returns the value of a second in the current units\n     */\n    _secondsToUnits(seconds) {\n        return ftom(super._secondsToUnits(seconds));\n    }\n    /**\n     * Return the value of the frequency as a MIDI note\n     * @example\n     * Tone.Midi(60).toMidi(); // 60\n     */\n    toMidi() {\n        return this.valueOf();\n    }\n    /**\n     * Return the value of the frequency as a MIDI note\n     * @example\n     * Tone.Midi(60).toFrequency(); // 261.6255653005986\n     */\n    toFrequency() {\n        return mtof(this.toMidi());\n    }\n    /**\n     * Transposes the frequency by the given number of semitones.\n     * @return A new transposed MidiClass\n     * @example\n     * Tone.Midi("A4").transpose(3); // "C5"\n     */\n    transpose(interval) {\n        return new Midi_MidiClass(this.context, this.toMidi() + interval);\n    }\n}\n/**\n * Convert a value into a FrequencyClass object.\n * @category Unit\n */\nfunction Midi(value, units) {\n    return new Midi_MidiClass(getContext(), value, units);\n}\n//# sourceMappingURL=Midi.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/Ticks.js\n\n\n/**\n * Ticks is a primitive type for encoding Time values.\n * Ticks can be constructed with or without the `new` keyword. Ticks can be passed\n * into the parameter of any method which takes time as an argument.\n * @example\n * const t = Tone.Ticks("4n"); // a quarter note as ticks\n * @category Unit\n */\nclass Ticks_TicksClass extends TransportTime_TransportTimeClass {\n    constructor() {\n        super(...arguments);\n        this.name = "Ticks";\n        this.defaultUnits = "i";\n    }\n    /**\n     * Get the current time in the given units\n     */\n    _now() {\n        return this.context.transport.ticks;\n    }\n    /**\n     * Return the value of the beats in the current units\n     */\n    _beatsToUnits(beats) {\n        return this._getPPQ() * beats;\n    }\n    /**\n     * Returns the value of a second in the current units\n     */\n    _secondsToUnits(seconds) {\n        return Math.floor(seconds / (60 / this._getBpm()) * this._getPPQ());\n    }\n    /**\n     * Returns the value of a tick in the current time units\n     */\n    _ticksToUnits(ticks) {\n        return ticks;\n    }\n    /**\n     * Return the time in ticks\n     */\n    toTicks() {\n        return this.valueOf();\n    }\n    /**\n     * Return the time in seconds\n     */\n    toSeconds() {\n        return (this.valueOf() / this._getPPQ()) * (60 / this._getBpm());\n    }\n}\n/**\n * Convert a time representation to ticks\n * @category Unit\n */\nfunction Ticks(value, units) {\n    return new Ticks_TicksClass(getContext(), value, units);\n}\n//# sourceMappingURL=Ticks.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Draw.js\n\n\n\n/**\n * Draw is useful for synchronizing visuals and audio events.\n * Callbacks from Tone.Transport or any of the Tone.Event classes\n * always happen _before_ the scheduled time and are not synchronized\n * to the animation frame so they are not good for triggering tightly\n * synchronized visuals and sound. Draw makes it easy to schedule\n * callbacks using the AudioContext time and uses requestAnimationFrame.\n * @example\n * Tone.Transport.schedule((time) => {\n * \t// use the time argument to schedule a callback with Draw\n * \tTone.Draw.schedule(() => {\n * \t\t// do drawing or DOM manipulation here\n * \t\tconsole.log(time);\n * \t}, time);\n * }, "+0.5");\n * Tone.Transport.start();\n * @category Core\n */\nclass Draw_Draw extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(...arguments);\n        this.name = "Draw";\n        /**\n         * The duration after which events are not invoked.\n         */\n        this.expiration = 0.25;\n        /**\n         * The amount of time before the scheduled time\n         * that the callback can be invoked. Default is\n         * half the time of an animation frame (0.008 seconds).\n         */\n        this.anticipation = 0.008;\n        /**\n         * All of the events.\n         */\n        this._events = new Timeline_Timeline();\n        /**\n         * The draw loop\n         */\n        this._boundDrawLoop = this._drawLoop.bind(this);\n        /**\n         * The animation frame id\n         */\n        this._animationFrame = -1;\n    }\n    /**\n     * Schedule a function at the given time to be invoked\n     * on the nearest animation frame.\n     * @param  callback  Callback is invoked at the given time.\n     * @param  time      The time relative to the AudioContext time to invoke the callback.\n     * @example\n     * Tone.Transport.scheduleRepeat(time => {\n     * \tTone.Draw.schedule(() => console.log(time), time);\n     * }, 1);\n     * Tone.Transport.start();\n     */\n    schedule(callback, time) {\n        this._events.add({\n            callback,\n            time: this.toSeconds(time),\n        });\n        // start the draw loop on the first event\n        if (this._events.length === 1) {\n            this._animationFrame = requestAnimationFrame(this._boundDrawLoop);\n        }\n        return this;\n    }\n    /**\n     * Cancel events scheduled after the given time\n     * @param  after  Time after which scheduled events will be removed from the scheduling timeline.\n     */\n    cancel(after) {\n        this._events.cancel(this.toSeconds(after));\n        return this;\n    }\n    /**\n     * The draw loop\n     */\n    _drawLoop() {\n        const now = this.context.currentTime;\n        while (this._events.length && this._events.peek().time - this.anticipation <= now) {\n            const event = this._events.shift();\n            if (event && now - event.time <= this.expiration) {\n                event.callback();\n            }\n        }\n        if (this._events.length > 0) {\n            this._animationFrame = requestAnimationFrame(this._boundDrawLoop);\n        }\n    }\n    dispose() {\n        super.dispose();\n        this._events.dispose();\n        cancelAnimationFrame(this._animationFrame);\n        return this;\n    }\n}\n//-------------------------------------\n// \tINITIALIZATION\n//-------------------------------------\nonContextInit(context => {\n    context.draw = new Draw_Draw({ context });\n});\nonContextClose(context => {\n    context.draw.dispose();\n});\n//# sourceMappingURL=Draw.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/IntervalTimeline.js\n\n\n\n/**\n * Similar to Tone.Timeline, but all events represent\n * intervals with both "time" and "duration" times. The\n * events are placed in a tree structure optimized\n * for querying an intersection point with the timeline\n * events. Internally uses an [Interval Tree](https://en.wikipedia.org/wiki/Interval_tree)\n * to represent the data.\n */\nclass IntervalTimeline_IntervalTimeline extends Tone_Tone {\n    constructor() {\n        super(...arguments);\n        this.name = "IntervalTimeline";\n        /**\n         * The root node of the inteval tree\n         */\n        this._root = null;\n        /**\n         * Keep track of the length of the timeline.\n         */\n        this._length = 0;\n    }\n    /**\n     * The event to add to the timeline. All events must\n     * have a time and duration value\n     * @param  event  The event to add to the timeline\n     */\n    add(event) {\n        assert(isDefined(event.time), "Events must have a time property");\n        assert(isDefined(event.duration), "Events must have a duration parameter");\n        event.time = event.time.valueOf();\n        let node = new IntervalNode(event.time, event.time + event.duration, event);\n        if (this._root === null) {\n            this._root = node;\n        }\n        else {\n            this._root.insert(node);\n        }\n        this._length++;\n        // Restructure tree to be balanced\n        while (node !== null) {\n            node.updateHeight();\n            node.updateMax();\n            this._rebalance(node);\n            node = node.parent;\n        }\n        return this;\n    }\n    /**\n     * Remove an event from the timeline.\n     * @param  event  The event to remove from the timeline\n     */\n    remove(event) {\n        if (this._root !== null) {\n            const results = [];\n            this._root.search(event.time, results);\n            for (const node of results) {\n                if (node.event === event) {\n                    this._removeNode(node);\n                    this._length--;\n                    break;\n                }\n            }\n        }\n        return this;\n    }\n    /**\n     * The number of items in the timeline.\n     * @readOnly\n     */\n    get length() {\n        return this._length;\n    }\n    /**\n     * Remove events whose time time is after the given time\n     * @param  after  The time to query.\n     */\n    cancel(after) {\n        this.forEachFrom(after, event => this.remove(event));\n        return this;\n    }\n    /**\n     * Set the root node as the given node\n     */\n    _setRoot(node) {\n        this._root = node;\n        if (this._root !== null) {\n            this._root.parent = null;\n        }\n    }\n    /**\n     * Replace the references to the node in the node\'s parent\n     * with the replacement node.\n     */\n    _replaceNodeInParent(node, replacement) {\n        if (node.parent !== null) {\n            if (node.isLeftChild()) {\n                node.parent.left = replacement;\n            }\n            else {\n                node.parent.right = replacement;\n            }\n            this._rebalance(node.parent);\n        }\n        else {\n            this._setRoot(replacement);\n        }\n    }\n    /**\n     * Remove the node from the tree and replace it with\n     * a successor which follows the schema.\n     */\n    _removeNode(node) {\n        if (node.left === null && node.right === null) {\n            this._replaceNodeInParent(node, null);\n        }\n        else if (node.right === null) {\n            this._replaceNodeInParent(node, node.left);\n        }\n        else if (node.left === null) {\n            this._replaceNodeInParent(node, node.right);\n        }\n        else {\n            const balance = node.getBalance();\n            let replacement;\n            let temp = null;\n            if (balance > 0) {\n                if (node.left.right === null) {\n                    replacement = node.left;\n                    replacement.right = node.right;\n                    temp = replacement;\n                }\n                else {\n                    replacement = node.left.right;\n                    while (replacement.right !== null) {\n                        replacement = replacement.right;\n                    }\n                    if (replacement.parent) {\n                        replacement.parent.right = replacement.left;\n                        temp = replacement.parent;\n                        replacement.left = node.left;\n                        replacement.right = node.right;\n                    }\n                }\n            }\n            else if (node.right.left === null) {\n                replacement = node.right;\n                replacement.left = node.left;\n                temp = replacement;\n            }\n            else {\n                replacement = node.right.left;\n                while (replacement.left !== null) {\n                    replacement = replacement.left;\n                }\n                if (replacement.parent) {\n                    replacement.parent.left = replacement.right;\n                    temp = replacement.parent;\n                    replacement.left = node.left;\n                    replacement.right = node.right;\n                }\n            }\n            if (node.parent !== null) {\n                if (node.isLeftChild()) {\n                    node.parent.left = replacement;\n                }\n                else {\n                    node.parent.right = replacement;\n                }\n            }\n            else {\n                this._setRoot(replacement);\n            }\n            if (temp) {\n                this._rebalance(temp);\n            }\n        }\n        node.dispose();\n    }\n    /**\n     * Rotate the tree to the left\n     */\n    _rotateLeft(node) {\n        const parent = node.parent;\n        const isLeftChild = node.isLeftChild();\n        // Make node.right the new root of this sub tree (instead of node)\n        const pivotNode = node.right;\n        if (pivotNode) {\n            node.right = pivotNode.left;\n            pivotNode.left = node;\n        }\n        if (parent !== null) {\n            if (isLeftChild) {\n                parent.left = pivotNode;\n            }\n            else {\n                parent.right = pivotNode;\n            }\n        }\n        else {\n            this._setRoot(pivotNode);\n        }\n    }\n    /**\n     * Rotate the tree to the right\n     */\n    _rotateRight(node) {\n        const parent = node.parent;\n        const isLeftChild = node.isLeftChild();\n        // Make node.left the new root of this sub tree (instead of node)\n        const pivotNode = node.left;\n        if (pivotNode) {\n            node.left = pivotNode.right;\n            pivotNode.right = node;\n        }\n        if (parent !== null) {\n            if (isLeftChild) {\n                parent.left = pivotNode;\n            }\n            else {\n                parent.right = pivotNode;\n            }\n        }\n        else {\n            this._setRoot(pivotNode);\n        }\n    }\n    /**\n     * Balance the BST\n     */\n    _rebalance(node) {\n        const balance = node.getBalance();\n        if (balance > 1 && node.left) {\n            if (node.left.getBalance() < 0) {\n                this._rotateLeft(node.left);\n            }\n            else {\n                this._rotateRight(node);\n            }\n        }\n        else if (balance < -1 && node.right) {\n            if (node.right.getBalance() > 0) {\n                this._rotateRight(node.right);\n            }\n            else {\n                this._rotateLeft(node);\n            }\n        }\n    }\n    /**\n     * Get an event whose time and duration span the give time. Will\n     * return the match whose "time" value is closest to the given time.\n     * @return  The event which spans the desired time\n     */\n    get(time) {\n        if (this._root !== null) {\n            const results = [];\n            this._root.search(time, results);\n            if (results.length > 0) {\n                let max = results[0];\n                for (let i = 1; i < results.length; i++) {\n                    if (results[i].low > max.low) {\n                        max = results[i];\n                    }\n                }\n                return max.event;\n            }\n        }\n        return null;\n    }\n    /**\n     * Iterate over everything in the timeline.\n     * @param  callback The callback to invoke with every item\n     */\n    forEach(callback) {\n        if (this._root !== null) {\n            const allNodes = [];\n            this._root.traverse(node => allNodes.push(node));\n            allNodes.forEach(node => {\n                if (node.event) {\n                    callback(node.event);\n                }\n            });\n        }\n        return this;\n    }\n    /**\n     * Iterate over everything in the array in which the given time\n     * overlaps with the time and duration time of the event.\n     * @param  time The time to check if items are overlapping\n     * @param  callback The callback to invoke with every item\n     */\n    forEachAtTime(time, callback) {\n        if (this._root !== null) {\n            const results = [];\n            this._root.search(time, results);\n            results.forEach(node => {\n                if (node.event) {\n                    callback(node.event);\n                }\n            });\n        }\n        return this;\n    }\n    /**\n     * Iterate over everything in the array in which the time is greater\n     * than or equal to the given time.\n     * @param  time The time to check if items are before\n     * @param  callback The callback to invoke with every item\n     */\n    forEachFrom(time, callback) {\n        if (this._root !== null) {\n            const results = [];\n            this._root.searchAfter(time, results);\n            results.forEach(node => {\n                if (node.event) {\n                    callback(node.event);\n                }\n            });\n        }\n        return this;\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        if (this._root !== null) {\n            this._root.traverse(node => node.dispose());\n        }\n        this._root = null;\n        return this;\n    }\n}\n//-------------------------------------\n// \tINTERVAL NODE HELPER\n//-------------------------------------\n/**\n * Represents a node in the binary search tree, with the addition\n * of a "high" value which keeps track of the highest value of\n * its children.\n * References:\n * https://brooknovak.wordpress.com/2013/12/07/augmented-interval-tree-in-c/\n * http://www.mif.vu.lt/~valdas/ALGORITMAI/LITERATURA/Cormen/Cormen.pdf\n * @param low\n * @param high\n */\nclass IntervalNode {\n    constructor(low, high, event) {\n        // the nodes to the left\n        this._left = null;\n        // the nodes to the right\n        this._right = null;\n        // the parent node\n        this.parent = null;\n        // the number of child nodes\n        this.height = 0;\n        this.event = event;\n        // the low value\n        this.low = low;\n        // the high value\n        this.high = high;\n        // the high value for this and all child nodes\n        this.max = this.high;\n    }\n    /**\n     * Insert a node into the correct spot in the tree\n     */\n    insert(node) {\n        if (node.low <= this.low) {\n            if (this.left === null) {\n                this.left = node;\n            }\n            else {\n                this.left.insert(node);\n            }\n        }\n        else if (this.right === null) {\n            this.right = node;\n        }\n        else {\n            this.right.insert(node);\n        }\n    }\n    /**\n     * Search the tree for nodes which overlap\n     * with the given point\n     * @param  point  The point to query\n     * @param  results  The array to put the results\n     */\n    search(point, results) {\n        // If p is to the right of the rightmost point of any interval\n        // in this node and all children, there won\'t be any matches.\n        if (point > this.max) {\n            return;\n        }\n        // Search left children\n        if (this.left !== null) {\n            this.left.search(point, results);\n        }\n        // Check this node\n        if (this.low <= point && this.high > point) {\n            results.push(this);\n        }\n        // If p is to the left of the time of this interval,\n        // then it can\'t be in any child to the right.\n        if (this.low > point) {\n            return;\n        }\n        // Search right children\n        if (this.right !== null) {\n            this.right.search(point, results);\n        }\n    }\n    /**\n     * Search the tree for nodes which are less\n     * than the given point\n     * @param  point  The point to query\n     * @param  results  The array to put the results\n     */\n    searchAfter(point, results) {\n        // Check this node\n        if (this.low >= point) {\n            results.push(this);\n            if (this.left !== null) {\n                this.left.searchAfter(point, results);\n            }\n        }\n        // search the right side\n        if (this.right !== null) {\n            this.right.searchAfter(point, results);\n        }\n    }\n    /**\n     * Invoke the callback on this element and both it\'s branches\n     * @param  {Function}  callback\n     */\n    traverse(callback) {\n        callback(this);\n        if (this.left !== null) {\n            this.left.traverse(callback);\n        }\n        if (this.right !== null) {\n            this.right.traverse(callback);\n        }\n    }\n    /**\n     * Update the height of the node\n     */\n    updateHeight() {\n        if (this.left !== null && this.right !== null) {\n            this.height = Math.max(this.left.height, this.right.height) + 1;\n        }\n        else if (this.right !== null) {\n            this.height = this.right.height + 1;\n        }\n        else if (this.left !== null) {\n            this.height = this.left.height + 1;\n        }\n        else {\n            this.height = 0;\n        }\n    }\n    /**\n     * Update the height of the node\n     */\n    updateMax() {\n        this.max = this.high;\n        if (this.left !== null) {\n            this.max = Math.max(this.max, this.left.max);\n        }\n        if (this.right !== null) {\n            this.max = Math.max(this.max, this.right.max);\n        }\n    }\n    /**\n     * The balance is how the leafs are distributed on the node\n     * @return  Negative numbers are balanced to the right\n     */\n    getBalance() {\n        let balance = 0;\n        if (this.left !== null && this.right !== null) {\n            balance = this.left.height - this.right.height;\n        }\n        else if (this.left !== null) {\n            balance = this.left.height + 1;\n        }\n        else if (this.right !== null) {\n            balance = -(this.right.height + 1);\n        }\n        return balance;\n    }\n    /**\n     * @returns true if this node is the left child of its parent\n     */\n    isLeftChild() {\n        return this.parent !== null && this.parent.left === this;\n    }\n    /**\n     * get/set the left node\n     */\n    get left() {\n        return this._left;\n    }\n    set left(node) {\n        this._left = node;\n        if (node !== null) {\n            node.parent = this;\n        }\n        this.updateHeight();\n        this.updateMax();\n    }\n    /**\n     * get/set the right node\n     */\n    get right() {\n        return this._right;\n    }\n    set right(node) {\n        this._right = node;\n        if (node !== null) {\n            node.parent = this;\n        }\n        this.updateHeight();\n        this.updateMax();\n    }\n    /**\n     * null out references.\n     */\n    dispose() {\n        this.parent = null;\n        this._left = null;\n        this._right = null;\n        this.event = null;\n    }\n}\n//# sourceMappingURL=IntervalTimeline.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/NoteUnits.js\n// this file contains all of the valid note names for all pitches between C-4 and C11\n\n//# sourceMappingURL=NoteUnits.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/type/Units.js\n\n//# sourceMappingURL=Units.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/index.js\n\n// export * from "./clock/Transport";\n\n\n\n// export * from "./context/Destination";\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// get the units and export them under the "Unit" namespace\n\n\n// export the debug stuff as Debug\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Volume.js\n\n\n\n\n/**\n * Volume is a simple volume node, useful for creating a volume fader.\n *\n * @example\n * const vol = new Tone.Volume(-12).toDestination();\n * const osc = new Tone.Oscillator().connect(vol).start();\n * @category Component\n */\nclass Volume_Volume extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Volume_Volume.getDefaults(), arguments, ["volume"]));\n        this.name = "Volume";\n        const options = optionsFromArguments(Volume_Volume.getDefaults(), arguments, ["volume"]);\n        this.input = this.output = new Gain_Gain({\n            context: this.context,\n            gain: options.volume,\n            units: "decibels",\n        });\n        this.volume = this.output.gain;\n        readOnly(this, "volume");\n        this._unmutedVolume = options.volume;\n        // set the mute initially\n        this.mute = options.mute;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            mute: false,\n            volume: 0,\n        });\n    }\n    /**\n     * Mute the output.\n     * @example\n     * const vol = new Tone.Volume(-12).toDestination();\n     * const osc = new Tone.Oscillator().connect(vol).start();\n     * // mute the output\n     * vol.mute = true;\n     */\n    get mute() {\n        return this.volume.value === -Infinity;\n    }\n    set mute(mute) {\n        if (!this.mute && mute) {\n            this._unmutedVolume = this.volume.value;\n            // maybe it should ramp here?\n            this.volume.value = -Infinity;\n        }\n        else if (this.mute && !mute) {\n            this.volume.value = this._unmutedVolume;\n        }\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this.volume.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Volume.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Destination.js\n\n\n\n\n\n/**\n * A single master output which is connected to the\n * AudioDestinationNode (aka your speakers).\n * It provides useful conveniences such as the ability\n * to set the volume and mute the entire application.\n * It also gives you the ability to apply master effects to your application.\n *\n * @example\n * const oscillator = new Tone.Oscillator().start();\n * // the audio will go from the oscillator to the speakers\n * oscillator.connect(Tone.getDestination());\n * // a convenience for connecting to the master output is also provided:\n * oscillator.toDestination();\n * @category Core\n */\nclass Destination_Destination extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Destination_Destination.getDefaults(), arguments));\n        this.name = "Destination";\n        this.input = new Volume_Volume({ context: this.context });\n        this.output = new Gain_Gain({ context: this.context });\n        /**\n         * The volume of the master output in decibels. -Infinity is silent, and 0 is no change.\n         * @example\n         * const osc = new Tone.Oscillator().toDestination();\n         * osc.start();\n         * // ramp the volume down to silent over 10 seconds\n         * Tone.getDestination().volume.rampTo(-Infinity, 10);\n         */\n        this.volume = this.input.volume;\n        const options = optionsFromArguments(Destination_Destination.getDefaults(), arguments);\n        connectSeries(this.input, this.output, this.context.rawContext.destination);\n        this.mute = options.mute;\n        this._internalChannels = [this.input, this.context.rawContext.destination, this.output];\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            mute: false,\n            volume: 0,\n        });\n    }\n    /**\n     * Mute the output.\n     * @example\n     * const oscillator = new Tone.Oscillator().start().toDestination();\n     * setTimeout(() => {\n     * \t// mute the output\n     * \tTone.Destination.mute = true;\n     * }, 1000);\n     */\n    get mute() {\n        return this.input.mute;\n    }\n    set mute(mute) {\n        this.input.mute = mute;\n    }\n    /**\n     * Add a master effects chain. NOTE: this will disconnect any nodes which were previously\n     * chained in the master effects chain.\n     * @param args All arguments will be connected in a row and the Master will be routed through it.\n     * @example\n     * // route all audio through a filter and compressor\n     * const lowpass = new Tone.Filter(800, "lowpass");\n     * const compressor = new Tone.Compressor(-18);\n     * Tone.Destination.chain(lowpass, compressor);\n     */\n    chain(...args) {\n        this.input.disconnect();\n        args.unshift(this.input);\n        args.push(this.output);\n        connectSeries(...args);\n        return this;\n    }\n    /**\n     * The maximum number of channels the system can output\n     * @example\n     * console.log(Tone.Destination.maxChannelCount);\n     */\n    get maxChannelCount() {\n        return this.context.rawContext.destination.maxChannelCount;\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this.volume.dispose();\n        return this;\n    }\n}\n//-------------------------------------\n// \tINITIALIZATION\n//-------------------------------------\nonContextInit(context => {\n    context.destination = new Destination_Destination({ context });\n});\nonContextClose(context => {\n    context.destination.dispose();\n});\n//# sourceMappingURL=Destination.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/TimelineValue.js\n\n\n/**\n * Represents a single value which is gettable and settable in a timed way\n */\nclass TimelineValue_TimelineValue extends Tone_Tone {\n    /**\n     * @param initialValue The value to return if there is no scheduled values\n     */\n    constructor(initialValue) {\n        super();\n        this.name = "TimelineValue";\n        /**\n         * The timeline which stores the values\n         */\n        this._timeline = new Timeline_Timeline({ memory: 10 });\n        this._initialValue = initialValue;\n    }\n    /**\n     * Set the value at the given time\n     */\n    set(value, time) {\n        this._timeline.add({\n            value, time\n        });\n        return this;\n    }\n    /**\n     * Get the value at the given time\n     */\n    get(time) {\n        const event = this._timeline.get(time);\n        if (event) {\n            return event.value;\n        }\n        else {\n            return this._initialValue;\n        }\n    }\n}\n//# sourceMappingURL=TimelineValue.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/TransportEvent.js\n\n/**\n * TransportEvent is an internal class used by [[Transport]]\n * to schedule events. Do no invoke this class directly, it is\n * handled from within Tone.Transport.\n */\nclass TransportEvent_TransportEvent {\n    /**\n     * @param transport The transport object which the event belongs to\n     */\n    constructor(transport, opts) {\n        /**\n         * The unique id of the event\n         */\n        this.id = TransportEvent_TransportEvent._eventId++;\n        const options = Object.assign(TransportEvent_TransportEvent.getDefaults(), opts);\n        this.transport = transport;\n        this.callback = options.callback;\n        this._once = options.once;\n        this.time = options.time;\n    }\n    static getDefaults() {\n        return {\n            callback: noOp,\n            once: false,\n            time: 0,\n        };\n    }\n    /**\n     * Invoke the event callback.\n     * @param  time  The AudioContext time in seconds of the event\n     */\n    invoke(time) {\n        if (this.callback) {\n            this.callback(time);\n            if (this._once) {\n                this.transport.clear(this.id);\n            }\n        }\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        this.callback = undefined;\n        return this;\n    }\n}\n/**\n * Current ID counter\n */\nTransportEvent_TransportEvent._eventId = 0;\n//# sourceMappingURL=TransportEvent.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/TransportRepeatEvent.js\n\n\n/**\n * TransportRepeatEvent is an internal class used by Tone.Transport\n * to schedule repeat events. This class should not be instantiated directly.\n */\nclass TransportRepeatEvent_TransportRepeatEvent extends TransportEvent_TransportEvent {\n    /**\n     * @param transport The transport object which the event belongs to\n     */\n    constructor(transport, opts) {\n        super(transport, opts);\n        /**\n         * The ID of the current timeline event\n         */\n        this._currentId = -1;\n        /**\n         * The ID of the next timeline event\n         */\n        this._nextId = -1;\n        /**\n         * The time of the next event\n         */\n        this._nextTick = this.time;\n        /**\n         * a reference to the bound start method\n         */\n        this._boundRestart = this._restart.bind(this);\n        const options = Object.assign(TransportRepeatEvent_TransportRepeatEvent.getDefaults(), opts);\n        this.duration = new Ticks_TicksClass(transport.context, options.duration).valueOf();\n        this._interval = new Ticks_TicksClass(transport.context, options.interval).valueOf();\n        this._nextTick = options.time;\n        this.transport.on("start", this._boundRestart);\n        this.transport.on("loopStart", this._boundRestart);\n        this.context = this.transport.context;\n        this._restart();\n    }\n    static getDefaults() {\n        return Object.assign({}, TransportEvent_TransportEvent.getDefaults(), {\n            duration: Infinity,\n            interval: 1,\n            once: false,\n        });\n    }\n    /**\n     * Invoke the callback. Returns the tick time which\n     * the next event should be scheduled at.\n     * @param  time  The AudioContext time in seconds of the event\n     */\n    invoke(time) {\n        // create more events if necessary\n        this._createEvents(time);\n        // call the super class\n        super.invoke(time);\n    }\n    /**\n     * Push more events onto the timeline to keep up with the position of the timeline\n     */\n    _createEvents(time) {\n        // schedule the next event\n        const ticks = this.transport.getTicksAtTime(time);\n        if (ticks >= this.time && ticks >= this._nextTick && this._nextTick + this._interval < this.time + this.duration) {\n            this._nextTick += this._interval;\n            this._currentId = this._nextId;\n            this._nextId = this.transport.scheduleOnce(this.invoke.bind(this), new Ticks_TicksClass(this.context, this._nextTick).toSeconds());\n        }\n    }\n    /**\n     * Push more events onto the timeline to keep up with the position of the timeline\n     */\n    _restart(time) {\n        this.transport.clear(this._currentId);\n        this.transport.clear(this._nextId);\n        this._nextTick = this.time;\n        const ticks = this.transport.getTicksAtTime(time);\n        if (ticks > this.time) {\n            this._nextTick = this.time + Math.ceil((ticks - this.time) / this._interval) * this._interval;\n        }\n        this._currentId = this.transport.scheduleOnce(this.invoke.bind(this), new Ticks_TicksClass(this.context, this._nextTick).toSeconds());\n        this._nextTick += this._interval;\n        this._nextId = this.transport.scheduleOnce(this.invoke.bind(this), new Ticks_TicksClass(this.context, this._nextTick).toSeconds());\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this.transport.clear(this._currentId);\n        this.transport.clear(this._nextId);\n        this.transport.off("start", this._boundRestart);\n        this.transport.off("loopStart", this._boundRestart);\n        return this;\n    }\n}\n//# sourceMappingURL=TransportRepeatEvent.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/clock/Transport.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/**\n * Transport for timing musical events.\n * Supports tempo curves and time changes. Unlike browser-based timing (setInterval, requestAnimationFrame)\n * Transport timing events pass in the exact time of the scheduled event\n * in the argument of the callback function. Pass that time value to the object\n * you\'re scheduling. <br><br>\n * A single transport is created for you when the library is initialized.\n * <br><br>\n * The transport emits the events: "start", "stop", "pause", and "loop" which are\n * called with the time of that event as the argument.\n *\n * @example\n * const osc = new Tone.Oscillator().toDestination();\n * // repeated event every 8th note\n * Tone.Transport.scheduleRepeat((time) => {\n * \t// use the callback time to schedule events\n * \tosc.start(time).stop(time + 0.1);\n * }, "8n");\n * // transport must be started before it starts invoking events\n * Tone.Transport.start();\n * @category Core\n */\nclass Transport_Transport extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(optionsFromArguments(Transport_Transport.getDefaults(), arguments));\n        this.name = "Transport";\n        //-------------------------------------\n        // \tLOOPING\n        //-------------------------------------\n        /**\n         * If the transport loops or not.\n         */\n        this._loop = new TimelineValue_TimelineValue(false);\n        /**\n         * The loop start position in ticks\n         */\n        this._loopStart = 0;\n        /**\n         * The loop end position in ticks\n         */\n        this._loopEnd = 0;\n        //-------------------------------------\n        // \tTIMELINE EVENTS\n        //-------------------------------------\n        /**\n         * All the events in an object to keep track by ID\n         */\n        this._scheduledEvents = {};\n        /**\n         * The scheduled events.\n         */\n        this._timeline = new Timeline_Timeline();\n        /**\n         * Repeated events\n         */\n        this._repeatedEvents = new IntervalTimeline_IntervalTimeline();\n        /**\n         * All of the synced Signals\n         */\n        this._syncedSignals = [];\n        /**\n         * The swing amount\n         */\n        this._swingAmount = 0;\n        const options = optionsFromArguments(Transport_Transport.getDefaults(), arguments);\n        // CLOCK/TEMPO\n        this._ppq = options.ppq;\n        this._clock = new Clock_Clock({\n            callback: this._processTick.bind(this),\n            context: this.context,\n            frequency: 0,\n            units: "bpm",\n        });\n        this._bindClockEvents();\n        this.bpm = this._clock.frequency;\n        this._clock.frequency.multiplier = options.ppq;\n        this.bpm.setValueAtTime(options.bpm, 0);\n        readOnly(this, "bpm");\n        this._timeSignature = options.timeSignature;\n        // SWING\n        this._swingTicks = options.ppq / 2; // 8n\n    }\n    static getDefaults() {\n        return Object.assign(ToneWithContext_ToneWithContext.getDefaults(), {\n            bpm: 120,\n            loopEnd: "4m",\n            loopStart: 0,\n            ppq: 192,\n            swing: 0,\n            swingSubdivision: "8n",\n            timeSignature: 4,\n        });\n    }\n    //-------------------------------------\n    // \tTICKS\n    //-------------------------------------\n    /**\n     * called on every tick\n     * @param  tickTime clock relative tick time\n     */\n    _processTick(tickTime, ticks) {\n        // do the loop test\n        if (this._loop.get(tickTime)) {\n            if (ticks >= this._loopEnd) {\n                this.emit("loopEnd", tickTime);\n                this._clock.setTicksAtTime(this._loopStart, tickTime);\n                ticks = this._loopStart;\n                this.emit("loopStart", tickTime, this._clock.getSecondsAtTime(tickTime));\n                this.emit("loop", tickTime);\n            }\n        }\n        // handle swing\n        if (this._swingAmount > 0 &&\n            ticks % this._ppq !== 0 && // not on a downbeat\n            ticks % (this._swingTicks * 2) !== 0) {\n            // add some swing\n            const progress = (ticks % (this._swingTicks * 2)) / (this._swingTicks * 2);\n            const amount = Math.sin((progress) * Math.PI) * this._swingAmount;\n            tickTime += new Ticks_TicksClass(this.context, this._swingTicks * 2 / 3).toSeconds() * amount;\n        }\n        // invoke the timeline events scheduled on this tick\n        this._timeline.forEachAtTime(ticks, event => event.invoke(tickTime));\n    }\n    //-------------------------------------\n    // \tSCHEDULABLE EVENTS\n    //-------------------------------------\n    /**\n     * Schedule an event along the timeline.\n     * @param callback The callback to be invoked at the time.\n     * @param time The time to invoke the callback at.\n     * @return The id of the event which can be used for canceling the event.\n     * @example\n     * // schedule an event on the 16th measure\n     * Tone.Transport.schedule((time) => {\n     * \t// invoked on measure 16\n     * \tconsole.log("measure 16!");\n     * }, "16:0:0");\n     */\n    schedule(callback, time) {\n        const event = new TransportEvent_TransportEvent(this, {\n            callback,\n            time: new TransportTime_TransportTimeClass(this.context, time).toTicks(),\n        });\n        return this._addEvent(event, this._timeline);\n    }\n    /**\n     * Schedule a repeated event along the timeline. The event will fire\n     * at the `interval` starting at the `startTime` and for the specified\n     * `duration`.\n     * @param  callback   The callback to invoke.\n     * @param  interval   The duration between successive callbacks. Must be a positive number.\n     * @param  startTime  When along the timeline the events should start being invoked.\n     * @param  duration How long the event should repeat.\n     * @return  The ID of the scheduled event. Use this to cancel the event.\n     * @example\n     * const osc = new Tone.Oscillator().toDestination().start();\n     * // a callback invoked every eighth note after the first measure\n     * Tone.Transport.scheduleRepeat((time) => {\n     * \tosc.start(time).stop(time + 0.1);\n     * }, "8n", "1m");\n     */\n    scheduleRepeat(callback, interval, startTime, duration = Infinity) {\n        const event = new TransportRepeatEvent_TransportRepeatEvent(this, {\n            callback,\n            duration: new Time_TimeClass(this.context, duration).toTicks(),\n            interval: new Time_TimeClass(this.context, interval).toTicks(),\n            time: new TransportTime_TransportTimeClass(this.context, startTime).toTicks(),\n        });\n        // kick it off if the Transport is started\n        // @ts-ignore\n        return this._addEvent(event, this._repeatedEvents);\n    }\n    /**\n     * Schedule an event that will be removed after it is invoked.\n     * @param callback The callback to invoke once.\n     * @param time The time the callback should be invoked.\n     * @returns The ID of the scheduled event.\n     */\n    scheduleOnce(callback, time) {\n        const event = new TransportEvent_TransportEvent(this, {\n            callback,\n            once: true,\n            time: new TransportTime_TransportTimeClass(this.context, time).toTicks(),\n        });\n        return this._addEvent(event, this._timeline);\n    }\n    /**\n     * Clear the passed in event id from the timeline\n     * @param eventId The id of the event.\n     */\n    clear(eventId) {\n        if (this._scheduledEvents.hasOwnProperty(eventId)) {\n            const item = this._scheduledEvents[eventId.toString()];\n            item.timeline.remove(item.event);\n            item.event.dispose();\n            delete this._scheduledEvents[eventId.toString()];\n        }\n        return this;\n    }\n    /**\n     * Add an event to the correct timeline. Keep track of the\n     * timeline it was added to.\n     * @returns the event id which was just added\n     */\n    _addEvent(event, timeline) {\n        this._scheduledEvents[event.id.toString()] = {\n            event,\n            timeline,\n        };\n        timeline.add(event);\n        return event.id;\n    }\n    /**\n     * Remove scheduled events from the timeline after\n     * the given time. Repeated events will be removed\n     * if their startTime is after the given time\n     * @param after Clear all events after this time.\n     */\n    cancel(after = 0) {\n        const computedAfter = this.toTicks(after);\n        this._timeline.forEachFrom(computedAfter, event => this.clear(event.id));\n        this._repeatedEvents.forEachFrom(computedAfter, event => this.clear(event.id));\n        return this;\n    }\n    //-------------------------------------\n    // \tSTART/STOP/PAUSE\n    //-------------------------------------\n    /**\n     * Bind start/stop/pause events from the clock and emit them.\n     */\n    _bindClockEvents() {\n        this._clock.on("start", (time, offset) => {\n            offset = new Ticks_TicksClass(this.context, offset).toSeconds();\n            this.emit("start", time, offset);\n        });\n        this._clock.on("stop", (time) => {\n            this.emit("stop", time);\n        });\n        this._clock.on("pause", (time) => {\n            this.emit("pause", time);\n        });\n    }\n    /**\n     * Returns the playback state of the source, either "started", "stopped", or "paused"\n     */\n    get state() {\n        return this._clock.getStateAtTime(this.now());\n    }\n    /**\n     * Start the transport and all sources synced to the transport.\n     * @param  time The time when the transport should start.\n     * @param  offset The timeline offset to start the transport.\n     * @example\n     * // start the transport in one second starting at beginning of the 5th measure.\n     * Tone.Transport.start("+1", "4:0:0");\n     */\n    start(time, offset) {\n        let offsetTicks;\n        if (isDefined(offset)) {\n            offsetTicks = this.toTicks(offset);\n        }\n        // start the clock\n        this._clock.start(time, offsetTicks);\n        return this;\n    }\n    /**\n     * Stop the transport and all sources synced to the transport.\n     * @param time The time when the transport should stop.\n     * @example\n     * Tone.Transport.stop();\n     */\n    stop(time) {\n        this._clock.stop(time);\n        return this;\n    }\n    /**\n     * Pause the transport and all sources synced to the transport.\n     */\n    pause(time) {\n        this._clock.pause(time);\n        return this;\n    }\n    /**\n     * Toggle the current state of the transport. If it is\n     * started, it will stop it, otherwise it will start the Transport.\n     * @param  time The time of the event\n     */\n    toggle(time) {\n        time = this.toSeconds(time);\n        if (this._clock.getStateAtTime(time) !== "started") {\n            this.start(time);\n        }\n        else {\n            this.stop(time);\n        }\n        return this;\n    }\n    //-------------------------------------\n    // \tSETTERS/GETTERS\n    //-------------------------------------\n    /**\n     * The time signature as just the numerator over 4.\n     * For example 4/4 would be just 4 and 6/8 would be 3.\n     * @example\n     * // common time\n     * Tone.Transport.timeSignature = 4;\n     * // 7/8\n     * Tone.Transport.timeSignature = [7, 8];\n     * // this will be reduced to a single number\n     * Tone.Transport.timeSignature; // returns 3.5\n     */\n    get timeSignature() {\n        return this._timeSignature;\n    }\n    set timeSignature(timeSig) {\n        if (isArray(timeSig)) {\n            timeSig = (timeSig[0] / timeSig[1]) * 4;\n        }\n        this._timeSignature = timeSig;\n    }\n    /**\n     * When the Transport.loop = true, this is the starting position of the loop.\n     */\n    get loopStart() {\n        return new Time_TimeClass(this.context, this._loopStart, "i").toSeconds();\n    }\n    set loopStart(startPosition) {\n        this._loopStart = this.toTicks(startPosition);\n    }\n    /**\n     * When the Transport.loop = true, this is the ending position of the loop.\n     */\n    get loopEnd() {\n        return new Time_TimeClass(this.context, this._loopEnd, "i").toSeconds();\n    }\n    set loopEnd(endPosition) {\n        this._loopEnd = this.toTicks(endPosition);\n    }\n    /**\n     * If the transport loops or not.\n     */\n    get loop() {\n        return this._loop.get(this.now());\n    }\n    set loop(loop) {\n        this._loop.set(loop, this.now());\n    }\n    /**\n     * Set the loop start and stop at the same time.\n     * @example\n     * // loop over the first measure\n     * Tone.Transport.setLoopPoints(0, "1m");\n     * Tone.Transport.loop = true;\n     */\n    setLoopPoints(startPosition, endPosition) {\n        this.loopStart = startPosition;\n        this.loopEnd = endPosition;\n        return this;\n    }\n    /**\n     * The swing value. Between 0-1 where 1 equal to the note + half the subdivision.\n     */\n    get swing() {\n        return this._swingAmount;\n    }\n    set swing(amount) {\n        // scale the values to a normal range\n        this._swingAmount = amount;\n    }\n    /**\n     * Set the subdivision which the swing will be applied to.\n     * The default value is an 8th note. Value must be less\n     * than a quarter note.\n     */\n    get swingSubdivision() {\n        return new Ticks_TicksClass(this.context, this._swingTicks).toNotation();\n    }\n    set swingSubdivision(subdivision) {\n        this._swingTicks = this.toTicks(subdivision);\n    }\n    /**\n     * The Transport\'s position in Bars:Beats:Sixteenths.\n     * Setting the value will jump to that position right away.\n     */\n    get position() {\n        const now = this.now();\n        const ticks = this._clock.getTicksAtTime(now);\n        return new Ticks_TicksClass(this.context, ticks).toBarsBeatsSixteenths();\n    }\n    set position(progress) {\n        const ticks = this.toTicks(progress);\n        this.ticks = ticks;\n    }\n    /**\n     * The Transport\'s position in seconds\n     * Setting the value will jump to that position right away.\n     */\n    get seconds() {\n        return this._clock.seconds;\n    }\n    set seconds(s) {\n        const now = this.now();\n        const ticks = this._clock.frequency.timeToTicks(s, now);\n        this.ticks = ticks;\n    }\n    /**\n     * The Transport\'s loop position as a normalized value. Always\n     * returns 0 if the transport if loop is not true.\n     */\n    get progress() {\n        if (this.loop) {\n            const now = this.now();\n            const ticks = this._clock.getTicksAtTime(now);\n            return (ticks - this._loopStart) / (this._loopEnd - this._loopStart);\n        }\n        else {\n            return 0;\n        }\n    }\n    /**\n     * The transports current tick position.\n     */\n    get ticks() {\n        return this._clock.ticks;\n    }\n    set ticks(t) {\n        if (this._clock.ticks !== t) {\n            const now = this.now();\n            // stop everything synced to the transport\n            if (this.state === "started") {\n                const ticks = this._clock.getTicksAtTime(now);\n                // schedule to start on the next tick, #573\n                const remainingTick = this._clock.frequency.getDurationOfTicks(Math.ceil(ticks) - ticks, now);\n                const time = now + remainingTick;\n                this.emit("stop", time);\n                this._clock.setTicksAtTime(t, time);\n                // restart it with the new time\n                this.emit("start", time, this._clock.getSecondsAtTime(time));\n            }\n            else {\n                this._clock.setTicksAtTime(t, now);\n            }\n        }\n    }\n    /**\n     * Get the clock\'s ticks at the given time.\n     * @param  time  When to get the tick value\n     * @return The tick value at the given time.\n     */\n    getTicksAtTime(time) {\n        return Math.round(this._clock.getTicksAtTime(time));\n    }\n    /**\n     * Return the elapsed seconds at the given time.\n     * @param  time  When to get the elapsed seconds\n     * @return  The number of elapsed seconds\n     */\n    getSecondsAtTime(time) {\n        return this._clock.getSecondsAtTime(time);\n    }\n    /**\n     * Pulses Per Quarter note. This is the smallest resolution\n     * the Transport timing supports. This should be set once\n     * on initialization and not set again. Changing this value\n     * after other objects have been created can cause problems.\n     */\n    get PPQ() {\n        return this._clock.frequency.multiplier;\n    }\n    set PPQ(ppq) {\n        this._clock.frequency.multiplier = ppq;\n    }\n    //-------------------------------------\n    // \tSYNCING\n    //-------------------------------------\n    /**\n     * Returns the time aligned to the next subdivision\n     * of the Transport. If the Transport is not started,\n     * it will return 0.\n     * Note: this will not work precisely during tempo ramps.\n     * @param  subdivision  The subdivision to quantize to\n     * @return  The context time of the next subdivision.\n     * @example\n     * // the transport must be started, otherwise returns 0\n     * Tone.Transport.start();\n     * Tone.Transport.nextSubdivision("4n");\n     */\n    nextSubdivision(subdivision) {\n        subdivision = this.toTicks(subdivision);\n        if (this.state !== "started") {\n            // if the transport\'s not started, return 0\n            return 0;\n        }\n        else {\n            const now = this.now();\n            // the remainder of the current ticks and the subdivision\n            const transportPos = this.getTicksAtTime(now);\n            const remainingTicks = subdivision - transportPos % subdivision;\n            return this._clock.nextTickTime(remainingTicks, now);\n        }\n    }\n    /**\n     * Attaches the signal to the tempo control signal so that\n     * any changes in the tempo will change the signal in the same\n     * ratio.\n     *\n     * @param signal\n     * @param ratio Optionally pass in the ratio between the two signals.\n     * \t\t\tOtherwise it will be computed based on their current values.\n     */\n    syncSignal(signal, ratio) {\n        if (!ratio) {\n            // get the sync ratio\n            const now = this.now();\n            if (signal.getValueAtTime(now) !== 0) {\n                const bpm = this.bpm.getValueAtTime(now);\n                const computedFreq = 1 / (60 / bpm / this.PPQ);\n                ratio = signal.getValueAtTime(now) / computedFreq;\n            }\n            else {\n                ratio = 0;\n            }\n        }\n        const ratioSignal = new Gain_Gain(ratio);\n        // @ts-ignore\n        this.bpm.connect(ratioSignal);\n        // @ts-ignore\n        ratioSignal.connect(signal._param);\n        this._syncedSignals.push({\n            initial: signal.value,\n            ratio: ratioSignal,\n            signal,\n        });\n        signal.value = 0;\n        return this;\n    }\n    /**\n     * Unsyncs a previously synced signal from the transport\'s control.\n     * See Transport.syncSignal.\n     */\n    unsyncSignal(signal) {\n        for (let i = this._syncedSignals.length - 1; i >= 0; i--) {\n            const syncedSignal = this._syncedSignals[i];\n            if (syncedSignal.signal === signal) {\n                syncedSignal.ratio.dispose();\n                syncedSignal.signal.value = syncedSignal.initial;\n                this._syncedSignals.splice(i, 1);\n            }\n        }\n        return this;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._clock.dispose();\n        writable(this, "bpm");\n        this._timeline.dispose();\n        this._repeatedEvents.dispose();\n        return this;\n    }\n}\nEmitter_Emitter.mixin(Transport_Transport);\n//-------------------------------------\n// \tINITIALIZATION\n//-------------------------------------\nonContextInit(context => {\n    context.transport = new Transport_Transport({ context });\n});\nonContextClose(context => {\n    context.transport.dispose();\n});\n//# sourceMappingURL=Transport.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/Source.js\n\n\n\n\n\n\n\n\n\n\n/**\n * Base class for sources.\n * start/stop of this.context.transport.\n *\n * ```\n * // Multiple state change events can be chained together,\n * // but must be set in the correct order and with ascending times\n * // OK\n * state.start().stop("+0.2");\n * // OK\n * state.start().stop("+0.2").start("+0.4").stop("+0.7")\n * // BAD\n * state.stop("+0.2").start();\n * // BAD\n * state.start("+0.3").stop("+0.2");\n * ```\n */\nclass Source_Source extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        /**\n         * Sources have no inputs\n         */\n        this.input = undefined;\n        /**\n         * Keep track of the scheduled state.\n         */\n        this._state = new StateTimeline_StateTimeline("stopped");\n        /**\n         * The synced `start` callback function from the transport\n         */\n        this._synced = false;\n        /**\n         * Keep track of all of the scheduled event ids\n         */\n        this._scheduled = [];\n        /**\n         * Placeholder functions for syncing/unsyncing to transport\n         */\n        this._syncedStart = noOp;\n        this._syncedStop = noOp;\n        this._state.memory = 100;\n        this._state.increasing = true;\n        this._volume = this.output = new Volume_Volume({\n            context: this.context,\n            mute: options.mute,\n            volume: options.volume,\n        });\n        this.volume = this._volume.volume;\n        readOnly(this, "volume");\n        this.onstop = options.onstop;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            mute: false,\n            onstop: noOp,\n            volume: 0,\n        });\n    }\n    /**\n     * Returns the playback state of the source, either "started" or "stopped".\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/ahntone_c3.mp3", () => {\n     * \tplayer.start();\n     * \tconsole.log(player.state);\n     * }).toDestination();\n     */\n    get state() {\n        if (this._synced) {\n            if (this.context.transport.state === "started") {\n                return this._state.getValueAtTime(this.context.transport.seconds);\n            }\n            else {\n                return "stopped";\n            }\n        }\n        else {\n            return this._state.getValueAtTime(this.now());\n        }\n    }\n    /**\n     * Mute the output.\n     * @example\n     * const osc = new Tone.Oscillator().toDestination().start();\n     * // mute the output\n     * osc.mute = true;\n     */\n    get mute() {\n        return this._volume.mute;\n    }\n    set mute(mute) {\n        this._volume.mute = mute;\n    }\n    /**\n     * Ensure that the scheduled time is not before the current time.\n     * Should only be used when scheduled unsynced.\n     */\n    _clampToCurrentTime(time) {\n        if (this._synced) {\n            return time;\n        }\n        else {\n            return Math.max(time, this.context.currentTime);\n        }\n    }\n    /**\n     * Start the source at the specified time. If no time is given,\n     * start the source now.\n     * @param  time When the source should be started.\n     * @example\n     * const source = new Tone.Oscillator().toDestination();\n     * source.start("+0.5"); // starts the source 0.5 seconds from now\n     */\n    start(time, offset, duration) {\n        let computedTime = isUndef(time) && this._synced ? this.context.transport.seconds : this.toSeconds(time);\n        computedTime = this._clampToCurrentTime(computedTime);\n        // if it\'s started, stop it and restart it\n        if (!this._synced && this._state.getValueAtTime(computedTime) === "started") {\n            // time should be strictly greater than the previous start time\n            assert(GT(computedTime, this._state.get(computedTime).time), "Start time must be strictly greater than previous start time");\n            this._state.cancel(computedTime);\n            this._state.setStateAtTime("started", computedTime);\n            this.log("restart", computedTime);\n            this.restart(computedTime, offset, duration);\n        }\n        else {\n            this.log("start", computedTime);\n            this._state.setStateAtTime("started", computedTime);\n            if (this._synced) {\n                // add the offset time to the event\n                const event = this._state.get(computedTime);\n                if (event) {\n                    event.offset = this.toSeconds(defaultArg(offset, 0));\n                    event.duration = duration ? this.toSeconds(duration) : undefined;\n                }\n                const sched = this.context.transport.schedule(t => {\n                    this._start(t, offset, duration);\n                }, computedTime);\n                this._scheduled.push(sched);\n                // if the transport is already started\n                // and the time is greater than where the transport is\n                if (this.context.transport.state === "started" &&\n                    this.context.transport.getSecondsAtTime(this.immediate()) > computedTime) {\n                    this._syncedStart(this.now(), this.context.transport.seconds);\n                }\n            }\n            else {\n                assertContextRunning(this.context);\n                this._start(computedTime, offset, duration);\n            }\n        }\n        return this;\n    }\n    /**\n     * Stop the source at the specified time. If no time is given,\n     * stop the source now.\n     * @param  time When the source should be stopped.\n     * @example\n     * const source = new Tone.Oscillator().toDestination();\n     * source.start();\n     * source.stop("+0.5"); // stops the source 0.5 seconds from now\n     */\n    stop(time) {\n        let computedTime = isUndef(time) && this._synced ? this.context.transport.seconds : this.toSeconds(time);\n        computedTime = this._clampToCurrentTime(computedTime);\n        if (this._state.getValueAtTime(computedTime) === "started" || isDefined(this._state.getNextState("started", computedTime))) {\n            this.log("stop", computedTime);\n            if (!this._synced) {\n                this._stop(computedTime);\n            }\n            else {\n                const sched = this.context.transport.schedule(this._stop.bind(this), computedTime);\n                this._scheduled.push(sched);\n            }\n            this._state.cancel(computedTime);\n            this._state.setStateAtTime("stopped", computedTime);\n        }\n        return this;\n    }\n    /**\n     * Restart the source.\n     */\n    restart(time, offset, duration) {\n        time = this.toSeconds(time);\n        if (this._state.getValueAtTime(time) === "started") {\n            this._state.cancel(time);\n            this._restart(time, offset, duration);\n        }\n        return this;\n    }\n    /**\n     * Sync the source to the Transport so that all subsequent\n     * calls to `start` and `stop` are synced to the TransportTime\n     * instead of the AudioContext time.\n     *\n     * @example\n     * const osc = new Tone.Oscillator().toDestination();\n     * // sync the source so that it plays between 0 and 0.3 on the Transport\'s timeline\n     * osc.sync().start(0).stop(0.3);\n     * // start the transport.\n     * Tone.Transport.start();\n     * // set it to loop once a second\n     * Tone.Transport.loop = true;\n     * Tone.Transport.loopEnd = 1;\n     */\n    sync() {\n        if (!this._synced) {\n            this._synced = true;\n            this._syncedStart = (time, offset) => {\n                if (offset > 0) {\n                    // get the playback state at that time\n                    const stateEvent = this._state.get(offset);\n                    // listen for start events which may occur in the middle of the sync\'ed time\n                    if (stateEvent && stateEvent.state === "started" && stateEvent.time !== offset) {\n                        // get the offset\n                        const startOffset = offset - this.toSeconds(stateEvent.time);\n                        let duration;\n                        if (stateEvent.duration) {\n                            duration = this.toSeconds(stateEvent.duration) - startOffset;\n                        }\n                        this._start(time, this.toSeconds(stateEvent.offset) + startOffset, duration);\n                    }\n                }\n            };\n            this._syncedStop = time => {\n                const seconds = this.context.transport.getSecondsAtTime(Math.max(time - this.sampleTime, 0));\n                if (this._state.getValueAtTime(seconds) === "started") {\n                    this._stop(time);\n                }\n            };\n            this.context.transport.on("start", this._syncedStart);\n            this.context.transport.on("loopStart", this._syncedStart);\n            this.context.transport.on("stop", this._syncedStop);\n            this.context.transport.on("pause", this._syncedStop);\n            this.context.transport.on("loopEnd", this._syncedStop);\n        }\n        return this;\n    }\n    /**\n     * Unsync the source to the Transport. See Source.sync\n     */\n    unsync() {\n        if (this._synced) {\n            this.context.transport.off("stop", this._syncedStop);\n            this.context.transport.off("pause", this._syncedStop);\n            this.context.transport.off("loopEnd", this._syncedStop);\n            this.context.transport.off("start", this._syncedStart);\n            this.context.transport.off("loopStart", this._syncedStart);\n        }\n        this._synced = false;\n        // clear all of the scheduled ids\n        this._scheduled.forEach(id => this.context.transport.clear(id));\n        this._scheduled = [];\n        this._state.cancel(0);\n        // stop it also\n        this._stop(0);\n        return this;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this.onstop = noOp;\n        this.unsync();\n        this._volume.dispose();\n        this._state.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Source.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/buffer/ToneBufferSource.js\n\n\n\n\n\n\n\n\n\n/**\n * Wrapper around the native BufferSourceNode.\n * @category Source\n */\nclass ToneBufferSource_ToneBufferSource extends OneShotSource_OneShotSource {\n    constructor() {\n        super(optionsFromArguments(ToneBufferSource_ToneBufferSource.getDefaults(), arguments, ["url", "onload"]));\n        this.name = "ToneBufferSource";\n        /**\n         * The oscillator\n         */\n        this._source = this.context.createBufferSource();\n        this._internalChannels = [this._source];\n        /**\n         * indicators if the source has started/stopped\n         */\n        this._sourceStarted = false;\n        this._sourceStopped = false;\n        const options = optionsFromArguments(ToneBufferSource_ToneBufferSource.getDefaults(), arguments, ["url", "onload"]);\n        ToneAudioNode_connect(this._source, this._gainNode);\n        this._source.onended = () => this._stopSource();\n        /**\n         * The playbackRate of the buffer\n         */\n        this.playbackRate = new Param_Param({\n            context: this.context,\n            param: this._source.playbackRate,\n            units: "positive",\n            value: options.playbackRate,\n        });\n        // set some values initially\n        this.loop = options.loop;\n        this.loopStart = options.loopStart;\n        this.loopEnd = options.loopEnd;\n        this._buffer = new ToneAudioBuffer_ToneAudioBuffer(options.url, options.onload, options.onerror);\n        this._internalChannels.push(this._source);\n    }\n    static getDefaults() {\n        return Object.assign(OneShotSource_OneShotSource.getDefaults(), {\n            url: new ToneAudioBuffer_ToneAudioBuffer(),\n            loop: false,\n            loopEnd: 0,\n            loopStart: 0,\n            onload: noOp,\n            onerror: noOp,\n            playbackRate: 1,\n        });\n    }\n    /**\n     * The fadeIn time of the amplitude envelope.\n     */\n    get fadeIn() {\n        return this._fadeIn;\n    }\n    set fadeIn(t) {\n        this._fadeIn = t;\n    }\n    /**\n     * The fadeOut time of the amplitude envelope.\n     */\n    get fadeOut() {\n        return this._fadeOut;\n    }\n    set fadeOut(t) {\n        this._fadeOut = t;\n    }\n    /**\n     * The curve applied to the fades, either "linear" or "exponential"\n     */\n    get curve() {\n        return this._curve;\n    }\n    set curve(t) {\n        this._curve = t;\n    }\n    /**\n     * Start the buffer\n     * @param  time When the player should start.\n     * @param  offset The offset from the beginning of the sample to start at.\n     * @param  duration How long the sample should play. If no duration is given, it will default to the full length of the sample (minus any offset)\n     * @param  gain  The gain to play the buffer back at.\n     */\n    start(time, offset, duration, gain = 1) {\n        assert(this.buffer.loaded, "buffer is either not set or not loaded");\n        const computedTime = this.toSeconds(time);\n        // apply the gain envelope\n        this._startGain(computedTime, gain);\n        // if it\'s a loop the default offset is the loopstart point\n        if (this.loop) {\n            offset = defaultArg(offset, this.loopStart);\n        }\n        else {\n            // otherwise the default offset is 0\n            offset = defaultArg(offset, 0);\n        }\n        // make sure the offset is not less than 0\n        let computedOffset = Math.max(this.toSeconds(offset), 0);\n        // start the buffer source\n        if (this.loop) {\n            // modify the offset if it\'s greater than the loop time\n            const loopEnd = this.toSeconds(this.loopEnd) || this.buffer.duration;\n            const loopStart = this.toSeconds(this.loopStart);\n            const loopDuration = loopEnd - loopStart;\n            // move the offset back\n            if (GTE(computedOffset, loopEnd)) {\n                computedOffset = ((computedOffset - loopStart) % loopDuration) + loopStart;\n            }\n            // when the offset is very close to the duration, set it to 0\n            if (EQ(computedOffset, this.buffer.duration)) {\n                computedOffset = 0;\n            }\n        }\n        // this.buffer.loaded would have return false if the AudioBuffer was undefined\n        this._source.buffer = this.buffer.get();\n        this._source.loopEnd = this.toSeconds(this.loopEnd) || this.buffer.duration;\n        if (LT(computedOffset, this.buffer.duration)) {\n            this._sourceStarted = true;\n            this._source.start(computedTime, computedOffset);\n        }\n        // if a duration is given, schedule a stop\n        if (isDefined(duration)) {\n            let computedDur = this.toSeconds(duration);\n            // make sure it\'s never negative\n            computedDur = Math.max(computedDur, 0);\n            this.stop(computedTime + computedDur);\n        }\n        return this;\n    }\n    _stopSource(time) {\n        if (!this._sourceStopped && this._sourceStarted) {\n            this._sourceStopped = true;\n            this._source.stop(this.toSeconds(time));\n            this._onended();\n        }\n    }\n    /**\n     * If loop is true, the loop will start at this position.\n     */\n    get loopStart() {\n        return this._source.loopStart;\n    }\n    set loopStart(loopStart) {\n        this._source.loopStart = this.toSeconds(loopStart);\n    }\n    /**\n     * If loop is true, the loop will end at this position.\n     */\n    get loopEnd() {\n        return this._source.loopEnd;\n    }\n    set loopEnd(loopEnd) {\n        this._source.loopEnd = this.toSeconds(loopEnd);\n    }\n    /**\n     * The audio buffer belonging to the player.\n     */\n    get buffer() {\n        return this._buffer;\n    }\n    set buffer(buffer) {\n        this._buffer.set(buffer);\n    }\n    /**\n     * If the buffer should loop once it\'s over.\n     */\n    get loop() {\n        return this._source.loop;\n    }\n    set loop(loop) {\n        this._source.loop = loop;\n        if (this._sourceStarted) {\n            this.cancelStop();\n        }\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._source.onended = null;\n        this._source.disconnect();\n        this._buffer.dispose();\n        this.playbackRate.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=ToneBufferSource.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/Noise.js\n\n\n\n\n\n/**\n * Noise is a noise generator. It uses looped noise buffers to save on performance.\n * Noise supports the noise types: "pink", "white", and "brown". Read more about\n * colors of noise on [Wikipedia](https://en.wikipedia.org/wiki/Colors_of_noise).\n *\n * @example\n * // initialize the noise and start\n * const noise = new Tone.Noise("pink").start();\n * // make an autofilter to shape the noise\n * const autoFilter = new Tone.AutoFilter({\n * \tfrequency: "8n",\n * \tbaseFrequency: 200,\n * \toctaves: 8\n * }).toDestination().start();\n * // connect the noise\n * noise.connect(autoFilter);\n * // start the autofilter LFO\n * autoFilter.start();\n * @category Source\n */\nclass Noise_Noise extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(Noise_Noise.getDefaults(), arguments, ["type"]));\n        this.name = "Noise";\n        /**\n         * Private reference to the source\n         */\n        this._source = null;\n        const options = optionsFromArguments(Noise_Noise.getDefaults(), arguments, ["type"]);\n        this._playbackRate = options.playbackRate;\n        this.type = options.type;\n        this._fadeIn = options.fadeIn;\n        this._fadeOut = options.fadeOut;\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            fadeIn: 0,\n            fadeOut: 0,\n            playbackRate: 1,\n            type: "white",\n        });\n    }\n    /**\n     * The type of the noise. Can be "white", "brown", or "pink".\n     * @example\n     * const noise = new Tone.Noise().toDestination().start();\n     * noise.type = "brown";\n     */\n    get type() {\n        return this._type;\n    }\n    set type(type) {\n        assert(type in _noiseBuffers, "Noise: invalid type: " + type);\n        if (this._type !== type) {\n            this._type = type;\n            // if it\'s playing, stop and restart it\n            if (this.state === "started") {\n                const now = this.now();\n                this._stop(now);\n                this._start(now);\n            }\n        }\n    }\n    /**\n     * The playback rate of the noise. Affects\n     * the "frequency" of the noise.\n     */\n    get playbackRate() {\n        return this._playbackRate;\n    }\n    set playbackRate(rate) {\n        this._playbackRate = rate;\n        if (this._source) {\n            this._source.playbackRate.value = rate;\n        }\n    }\n    /**\n     * internal start method\n     */\n    _start(time) {\n        const buffer = _noiseBuffers[this._type];\n        this._source = new ToneBufferSource_ToneBufferSource({\n            url: buffer,\n            context: this.context,\n            fadeIn: this._fadeIn,\n            fadeOut: this._fadeOut,\n            loop: true,\n            onended: () => this.onstop(this),\n            playbackRate: this._playbackRate,\n        }).connect(this.output);\n        this._source.start(this.toSeconds(time), Math.random() * (buffer.duration - 0.001));\n    }\n    /**\n     * internal stop method\n     */\n    _stop(time) {\n        if (this._source) {\n            this._source.stop(this.toSeconds(time));\n            this._source = null;\n        }\n    }\n    /**\n     * The fadeIn time of the amplitude envelope.\n     */\n    get fadeIn() {\n        return this._fadeIn;\n    }\n    set fadeIn(time) {\n        this._fadeIn = time;\n        if (this._source) {\n            this._source.fadeIn = this._fadeIn;\n        }\n    }\n    /**\n     * The fadeOut time of the amplitude envelope.\n     */\n    get fadeOut() {\n        return this._fadeOut;\n    }\n    set fadeOut(time) {\n        this._fadeOut = time;\n        if (this._source) {\n            this._source.fadeOut = this._fadeOut;\n        }\n    }\n    _restart(time) {\n        // TODO could be optimized by cancelling the buffer source \'stop\'\n        this._stop(time);\n        this._start(time);\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        if (this._source) {\n            this._source.disconnect();\n        }\n        return this;\n    }\n}\n//--------------------\n// THE NOISE BUFFERS\n//--------------------\n// Noise buffer stats\nconst BUFFER_LENGTH = 44100 * 5;\nconst NUM_CHANNELS = 2;\n/**\n * Cache the noise buffers\n */\nconst _noiseCache = {\n    brown: null,\n    pink: null,\n    white: null,\n};\n/**\n * The noise arrays. Generated on initialization.\n * borrowed heavily from https://github.com/zacharydenton/noise.js\n * (c) 2013 Zach Denton (MIT)\n */\nconst _noiseBuffers = {\n    get brown() {\n        if (!_noiseCache.brown) {\n            const buffer = [];\n            for (let channelNum = 0; channelNum < NUM_CHANNELS; channelNum++) {\n                const channel = new Float32Array(BUFFER_LENGTH);\n                buffer[channelNum] = channel;\n                let lastOut = 0.0;\n                for (let i = 0; i < BUFFER_LENGTH; i++) {\n                    const white = Math.random() * 2 - 1;\n                    channel[i] = (lastOut + (0.02 * white)) / 1.02;\n                    lastOut = channel[i];\n                    channel[i] *= 3.5; // (roughly) compensate for gain\n                }\n            }\n            _noiseCache.brown = new ToneAudioBuffer_ToneAudioBuffer().fromArray(buffer);\n        }\n        return _noiseCache.brown;\n    },\n    get pink() {\n        if (!_noiseCache.pink) {\n            const buffer = [];\n            for (let channelNum = 0; channelNum < NUM_CHANNELS; channelNum++) {\n                const channel = new Float32Array(BUFFER_LENGTH);\n                buffer[channelNum] = channel;\n                let b0, b1, b2, b3, b4, b5, b6;\n                b0 = b1 = b2 = b3 = b4 = b5 = b6 = 0.0;\n                for (let i = 0; i < BUFFER_LENGTH; i++) {\n                    const white = Math.random() * 2 - 1;\n                    b0 = 0.99886 * b0 + white * 0.0555179;\n                    b1 = 0.99332 * b1 + white * 0.0750759;\n                    b2 = 0.96900 * b2 + white * 0.1538520;\n                    b3 = 0.86650 * b3 + white * 0.3104856;\n                    b4 = 0.55000 * b4 + white * 0.5329522;\n                    b5 = -0.7616 * b5 - white * 0.0168980;\n                    channel[i] = b0 + b1 + b2 + b3 + b4 + b5 + b6 + white * 0.5362;\n                    channel[i] *= 0.11; // (roughly) compensate for gain\n                    b6 = white * 0.115926;\n                }\n            }\n            _noiseCache.pink = new ToneAudioBuffer_ToneAudioBuffer().fromArray(buffer);\n        }\n        return _noiseCache.pink;\n    },\n    get white() {\n        if (!_noiseCache.white) {\n            const buffer = [];\n            for (let channelNum = 0; channelNum < NUM_CHANNELS; channelNum++) {\n                const channel = new Float32Array(BUFFER_LENGTH);\n                buffer[channelNum] = channel;\n                for (let i = 0; i < BUFFER_LENGTH; i++) {\n                    channel[i] = Math.random() * 2 - 1;\n                }\n            }\n            _noiseCache.white = new ToneAudioBuffer_ToneAudioBuffer().fromArray(buffer);\n        }\n        return _noiseCache.white;\n    },\n};\n//# sourceMappingURL=Noise.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/UserMedia.js\n\n\n\n\n\n\n\n/**\n * UserMedia uses MediaDevices.getUserMedia to open up and external microphone or audio input.\n * Check [MediaDevices API Support](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)\n * to see which browsers are supported. Access to an external input\n * is limited to secure (HTTPS) connections.\n * @example\n * const meter = new Tone.Meter();\n * const mic = new Tone.UserMedia().connect(meter);\n * mic.open().then(() => {\n * \t// promise resolves when input is available\n * \tconsole.log("mic open");\n * \t// print the incoming mic levels in decibels\n * \tsetInterval(() => console.log(meter.getValue()), 100);\n * }).catch(e => {\n * \t// promise is rejected when the user doesn\'t have or allow mic access\n * \tconsole.log("mic not open");\n * });\n * @category Source\n */\nclass UserMedia_UserMedia extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(UserMedia_UserMedia.getDefaults(), arguments, ["volume"]));\n        this.name = "UserMedia";\n        const options = optionsFromArguments(UserMedia_UserMedia.getDefaults(), arguments, ["volume"]);\n        this._volume = this.output = new Volume_Volume({\n            context: this.context,\n            volume: options.volume,\n        });\n        this.volume = this._volume.volume;\n        readOnly(this, "volume");\n        this.mute = options.mute;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            mute: false,\n            volume: 0\n        });\n    }\n    /**\n     * Open the media stream. If a string is passed in, it is assumed\n     * to be the label or id of the stream, if a number is passed in,\n     * it is the input number of the stream.\n     * @param  labelOrId The label or id of the audio input media device.\n     *                   With no argument, the default stream is opened.\n     * @return The promise is resolved when the stream is open.\n     */\n    open(labelOrId) {\n        return __awaiter(this, void 0, void 0, function* () {\n            assert(UserMedia_UserMedia.supported, "UserMedia is not supported");\n            // close the previous stream\n            if (this.state === "started") {\n                this.close();\n            }\n            const devices = yield UserMedia_UserMedia.enumerateDevices();\n            if (isNumber(labelOrId)) {\n                this._device = devices[labelOrId];\n            }\n            else {\n                this._device = devices.find((device) => {\n                    return device.label === labelOrId || device.deviceId === labelOrId;\n                });\n                // didn\'t find a matching device\n                if (!this._device && devices.length > 0) {\n                    this._device = devices[0];\n                }\n                assert(isDefined(this._device), `No matching device ${labelOrId}`);\n            }\n            // do getUserMedia\n            const constraints = {\n                audio: {\n                    echoCancellation: false,\n                    sampleRate: this.context.sampleRate,\n                    noiseSuppression: false,\n                    mozNoiseSuppression: false,\n                }\n            };\n            if (this._device) {\n                // @ts-ignore\n                constraints.audio.deviceId = this._device.deviceId;\n            }\n            const stream = yield navigator.mediaDevices.getUserMedia(constraints);\n            // start a new source only if the previous one is closed\n            if (!this._stream) {\n                this._stream = stream;\n                // Wrap a MediaStreamSourceNode around the live input stream.\n                const mediaStreamNode = this.context.createMediaStreamSource(stream);\n                // Connect the MediaStreamSourceNode to a gate gain node\n                ToneAudioNode_connect(mediaStreamNode, this.output);\n                this._mediaStream = mediaStreamNode;\n            }\n            return this;\n        });\n    }\n    /**\n     * Close the media stream\n     */\n    close() {\n        if (this._stream && this._mediaStream) {\n            this._stream.getAudioTracks().forEach((track) => {\n                track.stop();\n            });\n            this._stream = undefined;\n            // remove the old media stream\n            this._mediaStream.disconnect();\n            this._mediaStream = undefined;\n        }\n        this._device = undefined;\n        return this;\n    }\n    /**\n     * Returns a promise which resolves with the list of audio input devices available.\n     * @return The promise that is resolved with the devices\n     * @example\n     * Tone.UserMedia.enumerateDevices().then((devices) => {\n     * \t// print the device labels\n     * \tconsole.log(devices.map(device => device.label));\n     * });\n     */\n    static enumerateDevices() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const allDevices = yield navigator.mediaDevices.enumerateDevices();\n            return allDevices.filter(device => {\n                return device.kind === "audioinput";\n            });\n        });\n    }\n    /**\n     * Returns the playback state of the source, "started" when the microphone is open\n     * and "stopped" when the mic is closed.\n     */\n    get state() {\n        return this._stream && this._stream.active ? "started" : "stopped";\n    }\n    /**\n     * Returns an identifier for the represented device that is\n     * persisted across sessions. It is un-guessable by other applications and\n     * unique to the origin of the calling application. It is reset when the\n     * user clears cookies (for Private Browsing, a different identifier is\n     * used that is not persisted across sessions). Returns undefined when the\n     * device is not open.\n     */\n    get deviceId() {\n        if (this._device) {\n            return this._device.deviceId;\n        }\n        else {\n            return undefined;\n        }\n    }\n    /**\n     * Returns a group identifier. Two devices have the\n     * same group identifier if they belong to the same physical device.\n     * Returns null  when the device is not open.\n     */\n    get groupId() {\n        if (this._device) {\n            return this._device.groupId;\n        }\n        else {\n            return undefined;\n        }\n    }\n    /**\n     * Returns a label describing this device (for example "Built-in Microphone").\n     * Returns undefined when the device is not open or label is not available\n     * because of permissions.\n     */\n    get label() {\n        if (this._device) {\n            return this._device.label;\n        }\n        else {\n            return undefined;\n        }\n    }\n    /**\n     * Mute the output.\n     * @example\n     * const mic = new Tone.UserMedia();\n     * mic.open().then(() => {\n     * \t// promise resolves when input is available\n     * });\n     * // mute the output\n     * mic.mute = true;\n     */\n    get mute() {\n        return this._volume.mute;\n    }\n    set mute(mute) {\n        this._volume.mute = mute;\n    }\n    dispose() {\n        super.dispose();\n        this.close();\n        this._volume.dispose();\n        this.volume.dispose();\n        return this;\n    }\n    /**\n     * If getUserMedia is supported by the browser.\n     */\n    static get supported() {\n        return isDefined(navigator.mediaDevices) &&\n            isDefined(navigator.mediaDevices.getUserMedia);\n    }\n}\n//# sourceMappingURL=UserMedia.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/OscillatorInterface.js\n\n\n/**\n * Render a segment of the oscillator to an offline context and return the results as an array\n */\nfunction generateWaveform(instance, length) {\n    return __awaiter(this, void 0, void 0, function* () {\n        const duration = length / instance.context.sampleRate;\n        const context = new OfflineContext_OfflineContext(1, duration, instance.context.sampleRate);\n        const clone = new instance.constructor(Object.assign(instance.get(), {\n            // should do 2 iterations\n            frequency: 2 / duration,\n            // zero out the detune\n            detune: 0,\n            context\n        })).toDestination();\n        clone.start(0);\n        const buffer = yield context.render();\n        return buffer.getChannelData(0);\n    });\n}\n//# sourceMappingURL=OscillatorInterface.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/ToneOscillatorNode.js\n\n\n\n\n\n/**\n * Wrapper around the native fire-and-forget OscillatorNode.\n * Adds the ability to reschedule the stop method.\n * ***[[Oscillator]] is better for most use-cases***\n * @category Source\n */\nclass ToneOscillatorNode_ToneOscillatorNode extends OneShotSource_OneShotSource {\n    constructor() {\n        super(optionsFromArguments(ToneOscillatorNode_ToneOscillatorNode.getDefaults(), arguments, ["frequency", "type"]));\n        this.name = "ToneOscillatorNode";\n        /**\n         * The oscillator\n         */\n        this._oscillator = this.context.createOscillator();\n        this._internalChannels = [this._oscillator];\n        const options = optionsFromArguments(ToneOscillatorNode_ToneOscillatorNode.getDefaults(), arguments, ["frequency", "type"]);\n        ToneAudioNode_connect(this._oscillator, this._gainNode);\n        this.type = options.type;\n        this.frequency = new Param_Param({\n            context: this.context,\n            param: this._oscillator.frequency,\n            units: "frequency",\n            value: options.frequency,\n        });\n        this.detune = new Param_Param({\n            context: this.context,\n            param: this._oscillator.detune,\n            units: "cents",\n            value: options.detune,\n        });\n        readOnly(this, ["frequency", "detune"]);\n    }\n    static getDefaults() {\n        return Object.assign(OneShotSource_OneShotSource.getDefaults(), {\n            detune: 0,\n            frequency: 440,\n            type: "sine",\n        });\n    }\n    /**\n     * Start the oscillator node at the given time\n     * @param  time When to start the oscillator\n     */\n    start(time) {\n        const computedTime = this.toSeconds(time);\n        this.log("start", computedTime);\n        this._startGain(computedTime);\n        this._oscillator.start(computedTime);\n        return this;\n    }\n    _stopSource(time) {\n        this._oscillator.stop(time);\n    }\n    /**\n     * Sets an arbitrary custom periodic waveform given a PeriodicWave.\n     * @param  periodicWave PeriodicWave should be created with context.createPeriodicWave\n     */\n    setPeriodicWave(periodicWave) {\n        this._oscillator.setPeriodicWave(periodicWave);\n        return this;\n    }\n    /**\n     * The oscillator type. Either \'sine\', \'sawtooth\', \'square\', or \'triangle\'\n     */\n    get type() {\n        return this._oscillator.type;\n    }\n    set type(type) {\n        this._oscillator.type = type;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        if (this.state === "started") {\n            this.stop();\n        }\n        this._oscillator.disconnect();\n        this.frequency.dispose();\n        this.detune.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=ToneOscillatorNode.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/Oscillator.js\n\n\n\n\n\n\n\n\n\n\n/**\n * Oscillator supports a number of features including\n * phase rotation, multiple oscillator types (see Oscillator.type),\n * and Transport syncing (see Oscillator.syncFrequency).\n *\n * @example\n * // make and start a 440hz sine tone\n * const osc = new Tone.Oscillator(440, "sine").toDestination().start();\n * @category Source\n */\nclass Oscillator_Oscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(Oscillator_Oscillator.getDefaults(), arguments, ["frequency", "type"]));\n        this.name = "Oscillator";\n        /**\n         * the main oscillator\n         */\n        this._oscillator = null;\n        const options = optionsFromArguments(Oscillator_Oscillator.getDefaults(), arguments, ["frequency", "type"]);\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n        });\n        readOnly(this, "frequency");\n        this.detune = new Signal_Signal({\n            context: this.context,\n            units: "cents",\n            value: options.detune,\n        });\n        readOnly(this, "detune");\n        this._partials = options.partials;\n        this._partialCount = options.partialCount;\n        this._type = options.type;\n        if (options.partialCount && options.type !== "custom") {\n            this._type = this.baseType + options.partialCount.toString();\n        }\n        this.phase = options.phase;\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            detune: 0,\n            frequency: 440,\n            partialCount: 0,\n            partials: [],\n            phase: 0,\n            type: "sine",\n        });\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        const computedTime = this.toSeconds(time);\n        // new oscillator with previous values\n        const oscillator = new ToneOscillatorNode_ToneOscillatorNode({\n            context: this.context,\n            onended: () => this.onstop(this),\n        });\n        this._oscillator = oscillator;\n        if (this._wave) {\n            this._oscillator.setPeriodicWave(this._wave);\n        }\n        else {\n            this._oscillator.type = this._type;\n        }\n        // connect the control signal to the oscillator frequency & detune\n        this._oscillator.connect(this.output);\n        this.frequency.connect(this._oscillator.frequency);\n        this.detune.connect(this._oscillator.detune);\n        // start the oscillator\n        this._oscillator.start(computedTime);\n    }\n    /**\n     * stop the oscillator\n     */\n    _stop(time) {\n        const computedTime = this.toSeconds(time);\n        if (this._oscillator) {\n            this._oscillator.stop(computedTime);\n        }\n    }\n    /**\n     * Restart the oscillator. Does not stop the oscillator, but instead\n     * just cancels any scheduled \'stop\' from being invoked.\n     */\n    _restart(time) {\n        const computedTime = this.toSeconds(time);\n        this.log("restart", computedTime);\n        if (this._oscillator) {\n            this._oscillator.cancelStop();\n        }\n        this._state.cancel(computedTime);\n        return this;\n    }\n    /**\n     * Sync the signal to the Transport\'s bpm. Any changes to the transports bpm,\n     * will also affect the oscillators frequency.\n     * @example\n     * const osc = new Tone.Oscillator().toDestination().start();\n     * osc.frequency.value = 440;\n     * // the ratio between the bpm and the frequency will be maintained\n     * osc.syncFrequency();\n     * // double the tempo\n     * Tone.Transport.bpm.value *= 2;\n     * // the frequency of the oscillator is doubled to 880\n     */\n    syncFrequency() {\n        this.context.transport.syncSignal(this.frequency);\n        return this;\n    }\n    /**\n     * Unsync the oscillator\'s frequency from the Transport.\n     * See Oscillator.syncFrequency\n     */\n    unsyncFrequency() {\n        this.context.transport.unsyncSignal(this.frequency);\n        return this;\n    }\n    /**\n     * Get a cached periodic wave. Avoids having to recompute\n     * the oscillator values when they have already been computed\n     * with the same values.\n     */\n    _getCachedPeriodicWave() {\n        if (this._type === "custom") {\n            const oscProps = Oscillator_Oscillator._periodicWaveCache.find(description => {\n                return description.phase === this._phase &&\n                    deepEquals(description.partials, this._partials);\n            });\n            return oscProps;\n        }\n        else {\n            const oscProps = Oscillator_Oscillator._periodicWaveCache.find(description => {\n                return description.type === this._type &&\n                    description.phase === this._phase;\n            });\n            this._partialCount = oscProps ? oscProps.partialCount : this._partialCount;\n            return oscProps;\n        }\n    }\n    get type() {\n        return this._type;\n    }\n    set type(type) {\n        this._type = type;\n        const isBasicType = ["sine", "square", "sawtooth", "triangle"].indexOf(type) !== -1;\n        if (this._phase === 0 && isBasicType) {\n            this._wave = undefined;\n            this._partialCount = 0;\n            // just go with the basic approach\n            if (this._oscillator !== null) {\n                // already tested that it\'s a basic type\n                this._oscillator.type = type;\n            }\n        }\n        else {\n            // first check if the value is cached\n            const cache = this._getCachedPeriodicWave();\n            if (isDefined(cache)) {\n                const { partials, wave } = cache;\n                this._wave = wave;\n                this._partials = partials;\n                if (this._oscillator !== null) {\n                    this._oscillator.setPeriodicWave(this._wave);\n                }\n            }\n            else {\n                const [real, imag] = this._getRealImaginary(type, this._phase);\n                const periodicWave = this.context.createPeriodicWave(real, imag);\n                this._wave = periodicWave;\n                if (this._oscillator !== null) {\n                    this._oscillator.setPeriodicWave(this._wave);\n                }\n                // set the cache\n                Oscillator_Oscillator._periodicWaveCache.push({\n                    imag,\n                    partialCount: this._partialCount,\n                    partials: this._partials,\n                    phase: this._phase,\n                    real,\n                    type: this._type,\n                    wave: this._wave,\n                });\n                if (Oscillator_Oscillator._periodicWaveCache.length > 100) {\n                    Oscillator_Oscillator._periodicWaveCache.shift();\n                }\n            }\n        }\n    }\n    get baseType() {\n        return this._type.replace(this.partialCount.toString(), "");\n    }\n    set baseType(baseType) {\n        if (this.partialCount && this._type !== "custom" && baseType !== "custom") {\n            this.type = baseType + this.partialCount;\n        }\n        else {\n            this.type = baseType;\n        }\n    }\n    get partialCount() {\n        return this._partialCount;\n    }\n    set partialCount(p) {\n        assertRange(p, 0);\n        let type = this._type;\n        const partial = /^(sine|triangle|square|sawtooth)(\\d+)$/.exec(this._type);\n        if (partial) {\n            type = partial[1];\n        }\n        if (this._type !== "custom") {\n            if (p === 0) {\n                this.type = type;\n            }\n            else {\n                this.type = type + p.toString();\n            }\n        }\n        else {\n            // extend or shorten the partials array\n            const fullPartials = new Float32Array(p);\n            // copy over the partials array\n            this._partials.forEach((v, i) => fullPartials[i] = v);\n            this._partials = Array.from(fullPartials);\n            this.type = this._type;\n        }\n    }\n    /**\n     * Returns the real and imaginary components based\n     * on the oscillator type.\n     * @returns [real: Float32Array, imaginary: Float32Array]\n     */\n    _getRealImaginary(type, phase) {\n        const fftSize = 4096;\n        let periodicWaveSize = fftSize / 2;\n        const real = new Float32Array(periodicWaveSize);\n        const imag = new Float32Array(periodicWaveSize);\n        let partialCount = 1;\n        if (type === "custom") {\n            partialCount = this._partials.length + 1;\n            this._partialCount = this._partials.length;\n            periodicWaveSize = partialCount;\n            // if the partial count is 0, don\'t bother doing any computation\n            if (this._partials.length === 0) {\n                return [real, imag];\n            }\n        }\n        else {\n            const partial = /^(sine|triangle|square|sawtooth)(\\d+)$/.exec(type);\n            if (partial) {\n                partialCount = parseInt(partial[2], 10) + 1;\n                this._partialCount = parseInt(partial[2], 10);\n                type = partial[1];\n                partialCount = Math.max(partialCount, 2);\n                periodicWaveSize = partialCount;\n            }\n            else {\n                this._partialCount = 0;\n            }\n            this._partials = [];\n        }\n        for (let n = 1; n < periodicWaveSize; ++n) {\n            const piFactor = 2 / (n * Math.PI);\n            let b;\n            switch (type) {\n                case "sine":\n                    b = (n <= partialCount) ? 1 : 0;\n                    this._partials[n - 1] = b;\n                    break;\n                case "square":\n                    b = (n & 1) ? 2 * piFactor : 0;\n                    this._partials[n - 1] = b;\n                    break;\n                case "sawtooth":\n                    b = piFactor * ((n & 1) ? 1 : -1);\n                    this._partials[n - 1] = b;\n                    break;\n                case "triangle":\n                    if (n & 1) {\n                        b = 2 * (piFactor * piFactor) * ((((n - 1) >> 1) & 1) ? -1 : 1);\n                    }\n                    else {\n                        b = 0;\n                    }\n                    this._partials[n - 1] = b;\n                    break;\n                case "custom":\n                    b = this._partials[n - 1];\n                    break;\n                default:\n                    throw new TypeError("Oscillator: invalid type: " + type);\n            }\n            if (b !== 0) {\n                real[n] = -b * Math.sin(phase * n);\n                imag[n] = b * Math.cos(phase * n);\n            }\n            else {\n                real[n] = 0;\n                imag[n] = 0;\n            }\n        }\n        return [real, imag];\n    }\n    /**\n     * Compute the inverse FFT for a given phase.\n     */\n    _inverseFFT(real, imag, phase) {\n        let sum = 0;\n        const len = real.length;\n        for (let i = 0; i < len; i++) {\n            sum += real[i] * Math.cos(i * phase) + imag[i] * Math.sin(i * phase);\n        }\n        return sum;\n    }\n    /**\n     * Returns the initial value of the oscillator when stopped.\n     * E.g. a "sine" oscillator with phase = 90 would return an initial value of -1.\n     */\n    getInitialValue() {\n        const [real, imag] = this._getRealImaginary(this._type, 0);\n        let maxValue = 0;\n        const twoPi = Math.PI * 2;\n        const testPositions = 32;\n        // check for peaks in 16 places\n        for (let i = 0; i < testPositions; i++) {\n            maxValue = Math.max(this._inverseFFT(real, imag, (i / testPositions) * twoPi), maxValue);\n        }\n        return clamp(-this._inverseFFT(real, imag, this._phase) / maxValue, -1, 1);\n    }\n    get partials() {\n        return this._partials.slice(0, this.partialCount);\n    }\n    set partials(partials) {\n        this._partials = partials;\n        this._partialCount = this._partials.length;\n        if (partials.length) {\n            this.type = "custom";\n        }\n    }\n    get phase() {\n        return this._phase * (180 / Math.PI);\n    }\n    set phase(phase) {\n        this._phase = phase * Math.PI / 180;\n        // reset the type\n        this.type = this._type;\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    dispose() {\n        super.dispose();\n        if (this._oscillator !== null) {\n            this._oscillator.dispose();\n        }\n        this._wave = undefined;\n        this.frequency.dispose();\n        this.detune.dispose();\n        return this;\n    }\n}\n/**\n * Cache the periodic waves to avoid having to redo computations\n */\nOscillator_Oscillator._periodicWaveCache = [];\n//# sourceMappingURL=Oscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/SignalOperator.js\n\n\n\n/**\n * A signal operator has an input and output and modifies the signal.\n */\nclass SignalOperator_SignalOperator extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(SignalOperator_SignalOperator.getDefaults(), arguments, ["context"])));\n    }\n    connect(destination, outputNum = 0, inputNum = 0) {\n        connectSignal(this, destination, outputNum, inputNum);\n        return this;\n    }\n}\n//# sourceMappingURL=SignalOperator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/WaveShaper.js\n\n\n\n\n\n/**\n * Wraps the native Web Audio API\n * [WaveShaperNode](http://webaudio.github.io/web-audio-api/#the-waveshapernode-interface).\n *\n * @example\n * const osc = new Tone.Oscillator().toDestination().start();\n * // multiply the output of the signal by 2 using the waveshaper\'s function\n * const timesTwo = new Tone.WaveShaper((val) => val * 2, 2048).connect(osc.frequency);\n * const signal = new Tone.Signal(440).connect(timesTwo);\n * @category Signal\n */\nclass WaveShaper_WaveShaper extends SignalOperator_SignalOperator {\n    constructor() {\n        super(Object.assign(optionsFromArguments(WaveShaper_WaveShaper.getDefaults(), arguments, ["mapping", "length"])));\n        this.name = "WaveShaper";\n        /**\n         * the waveshaper node\n         */\n        this._shaper = this.context.createWaveShaper();\n        /**\n         * The input to the waveshaper node.\n         */\n        this.input = this._shaper;\n        /**\n         * The output from the waveshaper node\n         */\n        this.output = this._shaper;\n        const options = optionsFromArguments(WaveShaper_WaveShaper.getDefaults(), arguments, ["mapping", "length"]);\n        if (isArray(options.mapping) || options.mapping instanceof Float32Array) {\n            this.curve = Float32Array.from(options.mapping);\n        }\n        else if (isFunction(options.mapping)) {\n            this.setMap(options.mapping, options.length);\n        }\n    }\n    static getDefaults() {\n        return Object.assign(Signal_Signal.getDefaults(), {\n            length: 1024,\n        });\n    }\n    /**\n     * Uses a mapping function to set the value of the curve.\n     * @param mapping The function used to define the values.\n     *                The mapping function take two arguments:\n     *                the first is the value at the current position\n     *                which goes from -1 to 1 over the number of elements\n     *                in the curve array. The second argument is the array position.\n     * @example\n     * const shaper = new Tone.WaveShaper();\n     * // map the input signal from [-1, 1] to [0, 10]\n     * shaper.setMap((val, index) => (val + 1) * 5);\n     */\n    setMap(mapping, length = 1024) {\n        const array = new Float32Array(length);\n        for (let i = 0, len = length; i < len; i++) {\n            const normalized = (i / (len - 1)) * 2 - 1;\n            array[i] = mapping(normalized, i);\n        }\n        this.curve = array;\n        return this;\n    }\n    /**\n     * The array to set as the waveshaper curve. For linear curves\n     * array length does not make much difference, but for complex curves\n     * longer arrays will provide smoother interpolation.\n     */\n    get curve() {\n        return this._shaper.curve;\n    }\n    set curve(mapping) {\n        this._shaper.curve = mapping;\n    }\n    /**\n     * Specifies what type of oversampling (if any) should be used when\n     * applying the shaping curve. Can either be "none", "2x" or "4x".\n     */\n    get oversample() {\n        return this._shaper.oversample;\n    }\n    set oversample(oversampling) {\n        const isOverSampleType = ["none", "2x", "4x"].some(str => str.includes(oversampling));\n        assert(isOverSampleType, "oversampling must be either \'none\', \'2x\', or \'4x\'");\n        this._shaper.oversample = oversampling;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._shaper.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=WaveShaper.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/AudioToGain.js\n\n\n/**\n * AudioToGain converts an input in AudioRange [-1,1] to NormalRange [0,1].\n * See [[GainToAudio]].\n * @category Signal\n */\nclass AudioToGain_AudioToGain extends SignalOperator_SignalOperator {\n    constructor() {\n        super(...arguments);\n        this.name = "AudioToGain";\n        /**\n         * The node which converts the audio ranges\n         */\n        this._norm = new WaveShaper_WaveShaper({\n            context: this.context,\n            mapping: x => (x + 1) / 2,\n        });\n        /**\n         * The AudioRange input [-1, 1]\n         */\n        this.input = this._norm;\n        /**\n         * The GainRange output [0, 1]\n         */\n        this.output = this._norm;\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        this._norm.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AudioToGain.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Multiply.js\n\n\n\n/**\n * Multiply two incoming signals. Or, if a number is given in the constructor,\n * multiplies the incoming signal by that value.\n *\n * @example\n * // multiply two signals\n * const mult = new Tone.Multiply();\n * const sigA = new Tone.Signal(3);\n * const sigB = new Tone.Signal(4);\n * sigA.connect(mult);\n * sigB.connect(mult.factor);\n * // output of mult is 12.\n * @example\n * // multiply a signal and a number\n * const mult = new Tone.Multiply(10);\n * const sig = new Tone.Signal(2).connect(mult);\n * // the output of mult is 20.\n * @category Signal\n */\nclass Multiply_Multiply extends Signal_Signal {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Multiply_Multiply.getDefaults(), arguments, ["value"])));\n        this.name = "Multiply";\n        /**\n         * Indicates if the value should be overridden on connection\n         */\n        this.override = false;\n        const options = optionsFromArguments(Multiply_Multiply.getDefaults(), arguments, ["value"]);\n        this._mult = this.input = this.output = new Gain_Gain({\n            context: this.context,\n            minValue: options.minValue,\n            maxValue: options.maxValue,\n        });\n        this.factor = this._param = this._mult.gain;\n        this.factor.setValueAtTime(options.value, 0);\n    }\n    static getDefaults() {\n        return Object.assign(Signal_Signal.getDefaults(), {\n            value: 0,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._mult.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Multiply.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/AMOscillator.js\n\n\n\n\n\n\n\n\n\n/**\n * An amplitude modulated oscillator node. It is implemented with\n * two oscillators, one which modulators the other\'s amplitude\n * through a gain node.\n * ```\n *    +-------------+       +----------+\n *    | Carrier Osc +>------\x3e GainNode |\n *    +-------------+       |          +---\x3eOutput\n *                      +---\x3e gain     |\n * +---------------+    |   +----------+\n * | Modulator Osc +>---+\n * +---------------+\n * ```\n * @example\n * return Tone.Offline(() => {\n * \tconst amOsc = new Tone.AMOscillator(30, "sine", "square").toDestination().start();\n * }, 0.2, 1);\n * @category Source\n */\nclass AMOscillator_AMOscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(AMOscillator_AMOscillator.getDefaults(), arguments, ["frequency", "type", "modulationType"]));\n        this.name = "AMOscillator";\n        /**\n         * convert the -1,1 output to 0,1\n         */\n        this._modulationScale = new AudioToGain_AudioToGain({ context: this.context });\n        /**\n         * the node where the modulation happens\n         */\n        this._modulationNode = new Gain_Gain({\n            context: this.context,\n        });\n        const options = optionsFromArguments(AMOscillator_AMOscillator.getDefaults(), arguments, ["frequency", "type", "modulationType"]);\n        this._carrier = new Oscillator_Oscillator({\n            context: this.context,\n            detune: options.detune,\n            frequency: options.frequency,\n            onstop: () => this.onstop(this),\n            phase: options.phase,\n            type: options.type,\n        });\n        this.frequency = this._carrier.frequency,\n            this.detune = this._carrier.detune;\n        this._modulator = new Oscillator_Oscillator({\n            context: this.context,\n            phase: options.phase,\n            type: options.modulationType,\n        });\n        this.harmonicity = new Multiply_Multiply({\n            context: this.context,\n            units: "positive",\n            value: options.harmonicity,\n        });\n        // connections\n        this.frequency.chain(this.harmonicity, this._modulator.frequency);\n        this._modulator.chain(this._modulationScale, this._modulationNode.gain);\n        this._carrier.chain(this._modulationNode, this.output);\n        readOnly(this, ["frequency", "detune", "harmonicity"]);\n    }\n    static getDefaults() {\n        return Object.assign(Oscillator_Oscillator.getDefaults(), {\n            harmonicity: 1,\n            modulationType: "square",\n        });\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        this._modulator.start(time);\n        this._carrier.start(time);\n    }\n    /**\n     * stop the oscillator\n     */\n    _stop(time) {\n        this._modulator.stop(time);\n        this._carrier.stop(time);\n    }\n    _restart(time) {\n        this._modulator.restart(time);\n        this._carrier.restart(time);\n    }\n    /**\n     * The type of the carrier oscillator\n     */\n    get type() {\n        return this._carrier.type;\n    }\n    set type(type) {\n        this._carrier.type = type;\n    }\n    get baseType() {\n        return this._carrier.baseType;\n    }\n    set baseType(baseType) {\n        this._carrier.baseType = baseType;\n    }\n    get partialCount() {\n        return this._carrier.partialCount;\n    }\n    set partialCount(partialCount) {\n        this._carrier.partialCount = partialCount;\n    }\n    /**\n     * The type of the modulator oscillator\n     */\n    get modulationType() {\n        return this._modulator.type;\n    }\n    set modulationType(type) {\n        this._modulator.type = type;\n    }\n    get phase() {\n        return this._carrier.phase;\n    }\n    set phase(phase) {\n        this._carrier.phase = phase;\n        this._modulator.phase = phase;\n    }\n    get partials() {\n        return this._carrier.partials;\n    }\n    set partials(partials) {\n        this._carrier.partials = partials;\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this.frequency.dispose();\n        this.detune.dispose();\n        this.harmonicity.dispose();\n        this._carrier.dispose();\n        this._modulator.dispose();\n        this._modulationNode.dispose();\n        this._modulationScale.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AMOscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/FMOscillator.js\n\n\n\n\n\n\n\n\n\n/**\n * FMOscillator implements a frequency modulation synthesis\n * ```\n *                                              +-------------+\n * +---------------+        +-------------+     | Carrier Osc |\n * | Modulator Osc +>-------\x3e GainNode    |     |             +---\x3eOutput\n * +---------------+        |             +>----\x3e frequency   |\n *                       +--\x3e gain        |     +-------------+\n *                       |  +-------------+\n * +-----------------+   |\n * | modulationIndex +>--+\n * +-----------------+\n * ```\n *\n * @example\n * return Tone.Offline(() => {\n * \tconst fmOsc = new Tone.FMOscillator({\n * \t\tfrequency: 200,\n * \t\ttype: "square",\n * \t\tmodulationType: "triangle",\n * \t\tharmonicity: 0.2,\n * \t\tmodulationIndex: 3\n * \t}).toDestination().start();\n * }, 0.1, 1);\n * @category Source\n */\nclass FMOscillator_FMOscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(FMOscillator_FMOscillator.getDefaults(), arguments, ["frequency", "type", "modulationType"]));\n        this.name = "FMOscillator";\n        /**\n         * the node where the modulation happens\n         */\n        this._modulationNode = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        const options = optionsFromArguments(FMOscillator_FMOscillator.getDefaults(), arguments, ["frequency", "type", "modulationType"]);\n        this._carrier = new Oscillator_Oscillator({\n            context: this.context,\n            detune: options.detune,\n            frequency: 0,\n            onstop: () => this.onstop(this),\n            phase: options.phase,\n            type: options.type,\n        });\n        this.detune = this._carrier.detune;\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n        });\n        this._modulator = new Oscillator_Oscillator({\n            context: this.context,\n            phase: options.phase,\n            type: options.modulationType,\n        });\n        this.harmonicity = new Multiply_Multiply({\n            context: this.context,\n            units: "positive",\n            value: options.harmonicity,\n        });\n        this.modulationIndex = new Multiply_Multiply({\n            context: this.context,\n            units: "positive",\n            value: options.modulationIndex,\n        });\n        // connections\n        this.frequency.connect(this._carrier.frequency);\n        this.frequency.chain(this.harmonicity, this._modulator.frequency);\n        this.frequency.chain(this.modulationIndex, this._modulationNode);\n        this._modulator.connect(this._modulationNode.gain);\n        this._modulationNode.connect(this._carrier.frequency);\n        this._carrier.connect(this.output);\n        this.detune.connect(this._modulator.detune);\n        readOnly(this, ["modulationIndex", "frequency", "detune", "harmonicity"]);\n    }\n    static getDefaults() {\n        return Object.assign(Oscillator_Oscillator.getDefaults(), {\n            harmonicity: 1,\n            modulationIndex: 2,\n            modulationType: "square",\n        });\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        this._modulator.start(time);\n        this._carrier.start(time);\n    }\n    /**\n     * stop the oscillator\n     */\n    _stop(time) {\n        this._modulator.stop(time);\n        this._carrier.stop(time);\n    }\n    _restart(time) {\n        this._modulator.restart(time);\n        this._carrier.restart(time);\n        return this;\n    }\n    get type() {\n        return this._carrier.type;\n    }\n    set type(type) {\n        this._carrier.type = type;\n    }\n    get baseType() {\n        return this._carrier.baseType;\n    }\n    set baseType(baseType) {\n        this._carrier.baseType = baseType;\n    }\n    get partialCount() {\n        return this._carrier.partialCount;\n    }\n    set partialCount(partialCount) {\n        this._carrier.partialCount = partialCount;\n    }\n    /**\n     * The type of the modulator oscillator\n     */\n    get modulationType() {\n        return this._modulator.type;\n    }\n    set modulationType(type) {\n        this._modulator.type = type;\n    }\n    get phase() {\n        return this._carrier.phase;\n    }\n    set phase(phase) {\n        this._carrier.phase = phase;\n        this._modulator.phase = phase;\n    }\n    get partials() {\n        return this._carrier.partials;\n    }\n    set partials(partials) {\n        this._carrier.partials = partials;\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this.frequency.dispose();\n        this.harmonicity.dispose();\n        this._carrier.dispose();\n        this._modulator.dispose();\n        this._modulationNode.dispose();\n        this.modulationIndex.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FMOscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/PulseOscillator.js\n\n\n\n\n\n\n\n\n\n/**\n * PulseOscillator is an oscillator with control over pulse width,\n * also known as the duty cycle. At 50% duty cycle (width = 0) the wave is\n * a square wave.\n * [Read more](https://wigglewave.wordpress.com/2014/08/16/pulse-waveforms-and-harmonics/).\n * ```\n *    width = -0.25        width = 0.0          width = 0.25\n *\n *   +-----+            +-------+       +    +-------+     +-+\n *   |     |            |       |       |            |     |\n *   |     |            |       |       |            |     |\n * +-+     +-------+    +       +-------+            +-----+\n *\n *\n *    width = -0.5                              width = 0.5\n *\n *     +---+                                 +-------+   +---+\n *     |   |                                         |   |\n *     |   |                                         |   |\n * +---+   +-------+                                 +---+\n *\n *\n *    width = -0.75                             width = 0.75\n *\n *       +-+                                 +-------+ +-----+\n *       | |                                         | |\n *       | |                                         | |\n * +-----+ +-------+                                 +-+\n * ```\n * @example\n * return Tone.Offline(() => {\n * \tconst pulse = new Tone.PulseOscillator(50, 0.4).toDestination().start();\n * }, 0.1, 1);\n * @category Source\n */\nclass PulseOscillator_PulseOscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(PulseOscillator_PulseOscillator.getDefaults(), arguments, ["frequency", "width"]));\n        this.name = "PulseOscillator";\n        /**\n         * gate the width amount\n         */\n        this._widthGate = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        /**\n         * Threshold the signal to turn it into a square\n         */\n        this._thresh = new WaveShaper_WaveShaper({\n            context: this.context,\n            mapping: val => val <= 0 ? -1 : 1,\n        });\n        const options = optionsFromArguments(PulseOscillator_PulseOscillator.getDefaults(), arguments, ["frequency", "width"]);\n        this.width = new Signal_Signal({\n            context: this.context,\n            units: "audioRange",\n            value: options.width,\n        });\n        this._triangle = new Oscillator_Oscillator({\n            context: this.context,\n            detune: options.detune,\n            frequency: options.frequency,\n            onstop: () => this.onstop(this),\n            phase: options.phase,\n            type: "triangle",\n        });\n        this.frequency = this._triangle.frequency;\n        this.detune = this._triangle.detune;\n        // connections\n        this._triangle.chain(this._thresh, this.output);\n        this.width.chain(this._widthGate, this._thresh);\n        readOnly(this, ["width", "frequency", "detune"]);\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            detune: 0,\n            frequency: 440,\n            phase: 0,\n            type: "pulse",\n            width: 0.2,\n        });\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        time = this.toSeconds(time);\n        this._triangle.start(time);\n        this._widthGate.gain.setValueAtTime(1, time);\n    }\n    /**\n     * stop the oscillator\n     */\n    _stop(time) {\n        time = this.toSeconds(time);\n        this._triangle.stop(time);\n        // the width is still connected to the output.\n        // that needs to be stopped also\n        this._widthGate.gain.cancelScheduledValues(time);\n        this._widthGate.gain.setValueAtTime(0, time);\n    }\n    _restart(time) {\n        this._triangle.restart(time);\n        this._widthGate.gain.cancelScheduledValues(time);\n        this._widthGate.gain.setValueAtTime(1, time);\n    }\n    /**\n     * The phase of the oscillator in degrees.\n     */\n    get phase() {\n        return this._triangle.phase;\n    }\n    set phase(phase) {\n        this._triangle.phase = phase;\n    }\n    /**\n     * The type of the oscillator. Always returns "pulse".\n     */\n    get type() {\n        return "pulse";\n    }\n    /**\n     * The baseType of the oscillator. Always returns "pulse".\n     */\n    get baseType() {\n        return "pulse";\n    }\n    /**\n     * The partials of the waveform. Cannot set partials for this waveform type\n     */\n    get partials() {\n        return [];\n    }\n    /**\n     * No partials for this waveform type.\n     */\n    get partialCount() {\n        return 0;\n    }\n    /**\n     * *Internal use* The carrier oscillator type is fed through the\n     * waveshaper node to create the pulse. Using different carrier oscillators\n     * changes oscillator\'s behavior.\n     */\n    set carrierType(type) {\n        this._triangle.type = type;\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    /**\n     * Clean up method.\n     */\n    dispose() {\n        super.dispose();\n        this._triangle.dispose();\n        this.width.dispose();\n        this._widthGate.dispose();\n        this._thresh.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=PulseOscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/FatOscillator.js\n\n\n\n\n\n\n\n\n/**\n * FatOscillator is an array of oscillators with detune spread between the oscillators\n * @example\n * const fatOsc = new Tone.FatOscillator("Ab3", "sawtooth", 40).toDestination().start();\n * @category Source\n */\nclass FatOscillator_FatOscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(FatOscillator_FatOscillator.getDefaults(), arguments, ["frequency", "type", "spread"]));\n        this.name = "FatOscillator";\n        /**\n         * The array of oscillators\n         */\n        this._oscillators = [];\n        const options = optionsFromArguments(FatOscillator_FatOscillator.getDefaults(), arguments, ["frequency", "type", "spread"]);\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n        });\n        this.detune = new Signal_Signal({\n            context: this.context,\n            units: "cents",\n            value: options.detune,\n        });\n        this._spread = options.spread;\n        this._type = options.type;\n        this._phase = options.phase;\n        this._partials = options.partials;\n        this._partialCount = options.partialCount;\n        // set the count initially\n        this.count = options.count;\n        readOnly(this, ["frequency", "detune"]);\n    }\n    static getDefaults() {\n        return Object.assign(Oscillator_Oscillator.getDefaults(), {\n            count: 3,\n            spread: 20,\n            type: "sawtooth",\n        });\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        time = this.toSeconds(time);\n        this._forEach(osc => osc.start(time));\n    }\n    /**\n     * stop the oscillator\n     */\n    _stop(time) {\n        time = this.toSeconds(time);\n        this._forEach(osc => osc.stop(time));\n    }\n    _restart(time) {\n        this._forEach(osc => osc.restart(time));\n    }\n    /**\n     * Iterate over all of the oscillators\n     */\n    _forEach(iterator) {\n        for (let i = 0; i < this._oscillators.length; i++) {\n            iterator(this._oscillators[i], i);\n        }\n    }\n    /**\n     * The type of the oscillator\n     */\n    get type() {\n        return this._type;\n    }\n    set type(type) {\n        this._type = type;\n        this._forEach(osc => osc.type = type);\n    }\n    /**\n     * The detune spread between the oscillators. If "count" is\n     * set to 3 oscillators and the "spread" is set to 40,\n     * the three oscillators would be detuned like this: [-20, 0, 20]\n     * for a total detune spread of 40 cents.\n     * @example\n     * const fatOsc = new Tone.FatOscillator().toDestination().start();\n     * fatOsc.spread = 70;\n     */\n    get spread() {\n        return this._spread;\n    }\n    set spread(spread) {\n        this._spread = spread;\n        if (this._oscillators.length > 1) {\n            const start = -spread / 2;\n            const step = spread / (this._oscillators.length - 1);\n            this._forEach((osc, i) => osc.detune.value = start + step * i);\n        }\n    }\n    /**\n     * The number of detuned oscillators. Must be an integer greater than 1.\n     * @example\n     * const fatOsc = new Tone.FatOscillator("C#3", "sawtooth").toDestination().start();\n     * // use 4 sawtooth oscillators\n     * fatOsc.count = 4;\n     */\n    get count() {\n        return this._oscillators.length;\n    }\n    set count(count) {\n        assertRange(count, 1);\n        if (this._oscillators.length !== count) {\n            // dispose the previous oscillators\n            this._forEach(osc => osc.dispose());\n            this._oscillators = [];\n            for (let i = 0; i < count; i++) {\n                const osc = new Oscillator_Oscillator({\n                    context: this.context,\n                    volume: -6 - count * 1.1,\n                    type: this._type,\n                    phase: this._phase + (i / count) * 360,\n                    partialCount: this._partialCount,\n                    onstop: i === 0 ? () => this.onstop(this) : noOp,\n                });\n                if (this.type === "custom") {\n                    osc.partials = this._partials;\n                }\n                this.frequency.connect(osc.frequency);\n                this.detune.connect(osc.detune);\n                osc.detune.overridden = false;\n                osc.connect(this.output);\n                this._oscillators[i] = osc;\n            }\n            // set the spread\n            this.spread = this._spread;\n            if (this.state === "started") {\n                this._forEach(osc => osc.start());\n            }\n        }\n    }\n    get phase() {\n        return this._phase;\n    }\n    set phase(phase) {\n        this._phase = phase;\n        this._forEach((osc, i) => osc.phase = this._phase + (i / this.count) * 360);\n    }\n    get baseType() {\n        return this._oscillators[0].baseType;\n    }\n    set baseType(baseType) {\n        this._forEach(osc => osc.baseType = baseType);\n        this._type = this._oscillators[0].type;\n    }\n    get partials() {\n        return this._oscillators[0].partials;\n    }\n    set partials(partials) {\n        this._partials = partials;\n        this._partialCount = this._partials.length;\n        if (partials.length) {\n            this._type = "custom";\n            this._forEach(osc => osc.partials = partials);\n        }\n    }\n    get partialCount() {\n        return this._oscillators[0].partialCount;\n    }\n    set partialCount(partialCount) {\n        this._partialCount = partialCount;\n        this._forEach(osc => osc.partialCount = partialCount);\n        this._type = this._oscillators[0].type;\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this.frequency.dispose();\n        this.detune.dispose();\n        this._forEach(osc => osc.dispose());\n        return this;\n    }\n}\n//# sourceMappingURL=FatOscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/PWMOscillator.js\n\n\n\n\n\n\n\n\n/**\n * PWMOscillator modulates the width of a Tone.PulseOscillator\n * at the modulationFrequency. This has the effect of continuously\n * changing the timbre of the oscillator by altering the harmonics\n * generated.\n * @example\n * return Tone.Offline(() => {\n * \tconst pwm = new Tone.PWMOscillator(60, 0.3).toDestination().start();\n * }, 0.1, 1);\n * @category Source\n */\nclass PWMOscillator_PWMOscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(PWMOscillator_PWMOscillator.getDefaults(), arguments, ["frequency", "modulationFrequency"]));\n        this.name = "PWMOscillator";\n        this.sourceType = "pwm";\n        /**\n         * Scale the oscillator so it doesn\'t go silent\n         * at the extreme values.\n         */\n        this._scale = new Multiply_Multiply({\n            context: this.context,\n            value: 2,\n        });\n        const options = optionsFromArguments(PWMOscillator_PWMOscillator.getDefaults(), arguments, ["frequency", "modulationFrequency"]);\n        this._pulse = new PulseOscillator_PulseOscillator({\n            context: this.context,\n            frequency: options.modulationFrequency,\n        });\n        // change the pulse oscillator type\n        this._pulse.carrierType = "sine";\n        this.modulationFrequency = this._pulse.frequency;\n        this._modulator = new Oscillator_Oscillator({\n            context: this.context,\n            detune: options.detune,\n            frequency: options.frequency,\n            onstop: () => this.onstop(this),\n            phase: options.phase,\n        });\n        this.frequency = this._modulator.frequency;\n        this.detune = this._modulator.detune;\n        // connections\n        this._modulator.chain(this._scale, this._pulse.width);\n        this._pulse.connect(this.output);\n        readOnly(this, ["modulationFrequency", "frequency", "detune"]);\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            detune: 0,\n            frequency: 440,\n            modulationFrequency: 0.4,\n            phase: 0,\n            type: "pwm",\n        });\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        time = this.toSeconds(time);\n        this._modulator.start(time);\n        this._pulse.start(time);\n    }\n    /**\n     * stop the oscillator\n     */\n    _stop(time) {\n        time = this.toSeconds(time);\n        this._modulator.stop(time);\n        this._pulse.stop(time);\n    }\n    /**\n     * restart the oscillator\n     */\n    _restart(time) {\n        this._modulator.restart(time);\n        this._pulse.restart(time);\n    }\n    /**\n     * The type of the oscillator. Always returns "pwm".\n     */\n    get type() {\n        return "pwm";\n    }\n    /**\n     * The baseType of the oscillator. Always returns "pwm".\n     */\n    get baseType() {\n        return "pwm";\n    }\n    /**\n     * The partials of the waveform. Cannot set partials for this waveform type\n     */\n    get partials() {\n        return [];\n    }\n    /**\n     * No partials for this waveform type.\n     */\n    get partialCount() {\n        return 0;\n    }\n    /**\n     * The phase of the oscillator in degrees.\n     */\n    get phase() {\n        return this._modulator.phase;\n    }\n    set phase(phase) {\n        this._modulator.phase = phase;\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._pulse.dispose();\n        this._scale.dispose();\n        this._modulator.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=PWMOscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/OmniOscillator.js\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst OmniOscillatorSourceMap = {\n    am: AMOscillator_AMOscillator,\n    fat: FatOscillator_FatOscillator,\n    fm: FMOscillator_FMOscillator,\n    oscillator: Oscillator_Oscillator,\n    pulse: PulseOscillator_PulseOscillator,\n    pwm: PWMOscillator_PWMOscillator,\n};\n/**\n * OmniOscillator aggregates all of the oscillator types into one.\n * @example\n * return Tone.Offline(() => {\n * \tconst omniOsc = new Tone.OmniOscillator("C#4", "pwm").toDestination().start();\n * }, 0.1, 1);\n * @category Source\n */\nclass OmniOscillator_OmniOscillator extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(OmniOscillator_OmniOscillator.getDefaults(), arguments, ["frequency", "type"]));\n        this.name = "OmniOscillator";\n        const options = optionsFromArguments(OmniOscillator_OmniOscillator.getDefaults(), arguments, ["frequency", "type"]);\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n        });\n        this.detune = new Signal_Signal({\n            context: this.context,\n            units: "cents",\n            value: options.detune,\n        });\n        readOnly(this, ["frequency", "detune"]);\n        // set the options\n        this.set(options);\n    }\n    static getDefaults() {\n        return Object.assign(Oscillator_Oscillator.getDefaults(), FMOscillator_FMOscillator.getDefaults(), AMOscillator_AMOscillator.getDefaults(), FatOscillator_FatOscillator.getDefaults(), PulseOscillator_PulseOscillator.getDefaults(), PWMOscillator_PWMOscillator.getDefaults());\n    }\n    /**\n     * start the oscillator\n     */\n    _start(time) {\n        this._oscillator.start(time);\n    }\n    /**\n     * start the oscillator\n     */\n    _stop(time) {\n        this._oscillator.stop(time);\n    }\n    _restart(time) {\n        this._oscillator.restart(time);\n        return this;\n    }\n    /**\n     * The type of the oscillator. Can be any of the basic types: sine, square, triangle, sawtooth. Or\n     * prefix the basic types with "fm", "am", or "fat" to use the FMOscillator, AMOscillator or FatOscillator\n     * types. The oscillator could also be set to "pwm" or "pulse". All of the parameters of the\n     * oscillator\'s class are accessible when the oscillator is set to that type, but throws an error\n     * when it\'s not.\n     * @example\n     * const omniOsc = new Tone.OmniOscillator().toDestination().start();\n     * omniOsc.type = "pwm";\n     * // modulationFrequency is parameter which is available\n     * // only when the type is "pwm".\n     * omniOsc.modulationFrequency.value = 0.5;\n     */\n    get type() {\n        let prefix = "";\n        if (["am", "fm", "fat"].some(p => this._sourceType === p)) {\n            prefix = this._sourceType;\n        }\n        return prefix + this._oscillator.type;\n    }\n    set type(type) {\n        if (type.substr(0, 2) === "fm") {\n            this._createNewOscillator("fm");\n            this._oscillator = this._oscillator;\n            this._oscillator.type = type.substr(2);\n        }\n        else if (type.substr(0, 2) === "am") {\n            this._createNewOscillator("am");\n            this._oscillator = this._oscillator;\n            this._oscillator.type = type.substr(2);\n        }\n        else if (type.substr(0, 3) === "fat") {\n            this._createNewOscillator("fat");\n            this._oscillator = this._oscillator;\n            this._oscillator.type = type.substr(3);\n        }\n        else if (type === "pwm") {\n            this._createNewOscillator("pwm");\n            this._oscillator = this._oscillator;\n        }\n        else if (type === "pulse") {\n            this._createNewOscillator("pulse");\n        }\n        else {\n            this._createNewOscillator("oscillator");\n            this._oscillator = this._oscillator;\n            this._oscillator.type = type;\n        }\n    }\n    /**\n     * The value is an empty array when the type is not "custom".\n     * This is not available on "pwm" and "pulse" oscillator types.\n     * See [[Oscillator.partials]]\n     */\n    get partials() {\n        return this._oscillator.partials;\n    }\n    set partials(partials) {\n        if (!this._getOscType(this._oscillator, "pulse") && !this._getOscType(this._oscillator, "pwm")) {\n            this._oscillator.partials = partials;\n        }\n    }\n    get partialCount() {\n        return this._oscillator.partialCount;\n    }\n    set partialCount(partialCount) {\n        if (!this._getOscType(this._oscillator, "pulse") && !this._getOscType(this._oscillator, "pwm")) {\n            this._oscillator.partialCount = partialCount;\n        }\n    }\n    set(props) {\n        // make sure the type is set first\n        if (Reflect.has(props, "type") && props.type) {\n            this.type = props.type;\n        }\n        // then set the rest\n        super.set(props);\n        return this;\n    }\n    /**\n     * connect the oscillator to the frequency and detune signals\n     */\n    _createNewOscillator(oscType) {\n        if (oscType !== this._sourceType) {\n            this._sourceType = oscType;\n            const OscConstructor = OmniOscillatorSourceMap[oscType];\n            // short delay to avoid clicks on the change\n            const now = this.now();\n            if (this._oscillator) {\n                const oldOsc = this._oscillator;\n                oldOsc.stop(now);\n                // dispose the old one\n                this.context.setTimeout(() => oldOsc.dispose(), this.blockTime);\n            }\n            this._oscillator = new OscConstructor({\n                context: this.context,\n            });\n            this.frequency.connect(this._oscillator.frequency);\n            this.detune.connect(this._oscillator.detune);\n            this._oscillator.connect(this.output);\n            this._oscillator.onstop = () => this.onstop(this);\n            if (this.state === "started") {\n                this._oscillator.start(now);\n            }\n        }\n    }\n    get phase() {\n        return this._oscillator.phase;\n    }\n    set phase(phase) {\n        this._oscillator.phase = phase;\n    }\n    /**\n     * The source type of the oscillator.\n     * @example\n     * const omniOsc = new Tone.OmniOscillator(440, "fmsquare");\n     * console.log(omniOsc.sourceType); // \'fm\'\n     */\n    get sourceType() {\n        return this._sourceType;\n    }\n    set sourceType(sType) {\n        // the basetype defaults to sine\n        let baseType = "sine";\n        if (this._oscillator.type !== "pwm" && this._oscillator.type !== "pulse") {\n            baseType = this._oscillator.type;\n        }\n        // set the type\n        if (sType === "fm") {\n            this.type = "fm" + baseType;\n        }\n        else if (sType === "am") {\n            this.type = "am" + baseType;\n        }\n        else if (sType === "fat") {\n            this.type = "fat" + baseType;\n        }\n        else if (sType === "oscillator") {\n            this.type = baseType;\n        }\n        else if (sType === "pulse") {\n            this.type = "pulse";\n        }\n        else if (sType === "pwm") {\n            this.type = "pwm";\n        }\n    }\n    _getOscType(osc, sourceType) {\n        return osc instanceof OmniOscillatorSourceMap[sourceType];\n    }\n    /**\n     * The base type of the oscillator. See [[Oscillator.baseType]]\n     * @example\n     * const omniOsc = new Tone.OmniOscillator(440, "fmsquare4");\n     * console.log(omniOsc.sourceType, omniOsc.baseType, omniOsc.partialCount);\n     */\n    get baseType() {\n        return this._oscillator.baseType;\n    }\n    set baseType(baseType) {\n        if (!this._getOscType(this._oscillator, "pulse") &&\n            !this._getOscType(this._oscillator, "pwm") &&\n            baseType !== "pulse" && baseType !== "pwm") {\n            this._oscillator.baseType = baseType;\n        }\n    }\n    /**\n     * The width of the oscillator when sourceType === "pulse".\n     * See [[PWMOscillator.width]]\n     */\n    get width() {\n        if (this._getOscType(this._oscillator, "pulse")) {\n            return this._oscillator.width;\n        }\n        else {\n            return undefined;\n        }\n    }\n    /**\n     * The number of detuned oscillators when sourceType === "fat".\n     * See [[FatOscillator.count]]\n     */\n    get count() {\n        if (this._getOscType(this._oscillator, "fat")) {\n            return this._oscillator.count;\n        }\n        else {\n            return undefined;\n        }\n    }\n    set count(count) {\n        if (this._getOscType(this._oscillator, "fat") && isNumber(count)) {\n            this._oscillator.count = count;\n        }\n    }\n    /**\n     * The detune spread between the oscillators when sourceType === "fat".\n     * See [[FatOscillator.count]]\n     */\n    get spread() {\n        if (this._getOscType(this._oscillator, "fat")) {\n            return this._oscillator.spread;\n        }\n        else {\n            return undefined;\n        }\n    }\n    set spread(spread) {\n        if (this._getOscType(this._oscillator, "fat") && isNumber(spread)) {\n            this._oscillator.spread = spread;\n        }\n    }\n    /**\n     * The type of the modulator oscillator. Only if the oscillator is set to "am" or "fm" types.\n     * See [[AMOscillator]] or [[FMOscillator]]\n     */\n    get modulationType() {\n        if (this._getOscType(this._oscillator, "fm") || this._getOscType(this._oscillator, "am")) {\n            return this._oscillator.modulationType;\n        }\n        else {\n            return undefined;\n        }\n    }\n    set modulationType(mType) {\n        if ((this._getOscType(this._oscillator, "fm") || this._getOscType(this._oscillator, "am")) && isString(mType)) {\n            this._oscillator.modulationType = mType;\n        }\n    }\n    /**\n     * The modulation index when the sourceType === "fm"\n     * See [[FMOscillator]].\n     */\n    get modulationIndex() {\n        if (this._getOscType(this._oscillator, "fm")) {\n            return this._oscillator.modulationIndex;\n        }\n        else {\n            return undefined;\n        }\n    }\n    /**\n     * Harmonicity is the frequency ratio between the carrier and the modulator oscillators.\n     * See [[AMOscillator]] or [[FMOscillator]]\n     */\n    get harmonicity() {\n        if (this._getOscType(this._oscillator, "fm") || this._getOscType(this._oscillator, "am")) {\n            return this._oscillator.harmonicity;\n        }\n        else {\n            return undefined;\n        }\n    }\n    /**\n     * The modulationFrequency Signal of the oscillator when sourceType === "pwm"\n     * see [[PWMOscillator]]\n     * @min 0.1\n     * @max 5\n     */\n    get modulationFrequency() {\n        if (this._getOscType(this._oscillator, "pwm")) {\n            return this._oscillator.modulationFrequency;\n        }\n        else {\n            return undefined;\n        }\n    }\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            return generateWaveform(this, length);\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.detune.dispose();\n        this.frequency.dispose();\n        this._oscillator.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=OmniOscillator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Add.js\n\n\n\n\n/**\n * Add a signal and a number or two signals. When no value is\n * passed into the constructor, Tone.Add will sum input and `addend`\n * If a value is passed into the constructor, the it will be added to the input.\n *\n * @example\n * return Tone.Offline(() => {\n * \tconst add = new Tone.Add(2).toDestination();\n * \tadd.addend.setValueAtTime(1, 0.2);\n * \tconst signal = new Tone.Signal(2);\n * \t// add a signal and a scalar\n * \tsignal.connect(add);\n * \tsignal.setValueAtTime(1, 0.1);\n * }, 0.5, 1);\n * @category Signal\n */\nclass Add_Add extends Signal_Signal {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Add_Add.getDefaults(), arguments, ["value"])));\n        this.override = false;\n        this.name = "Add";\n        /**\n         * the summing node\n         */\n        this._sum = new Gain_Gain({ context: this.context });\n        this.input = this._sum;\n        this.output = this._sum;\n        /**\n         * The value which is added to the input signal\n         */\n        this.addend = this._param;\n        connectSeries(this._constantSource, this._sum);\n    }\n    static getDefaults() {\n        return Object.assign(Signal_Signal.getDefaults(), {\n            value: 0,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._sum.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Add.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Scale.js\n\n\n\n\n/**\n * Performs a linear scaling on an input signal.\n * Scales a NormalRange input to between\n * outputMin and outputMax.\n *\n * @example\n * const scale = new Tone.Scale(50, 100);\n * const signal = new Tone.Signal(0.5).connect(scale);\n * // the output of scale equals 75\n * @category Signal\n */\nclass Scale_Scale extends SignalOperator_SignalOperator {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Scale_Scale.getDefaults(), arguments, ["min", "max"])));\n        this.name = "Scale";\n        const options = optionsFromArguments(Scale_Scale.getDefaults(), arguments, ["min", "max"]);\n        this._mult = this.input = new Multiply_Multiply({\n            context: this.context,\n            value: options.max - options.min,\n        });\n        this._add = this.output = new Add_Add({\n            context: this.context,\n            value: options.min,\n        });\n        this._min = options.min;\n        this._max = options.max;\n        this.input.connect(this.output);\n    }\n    static getDefaults() {\n        return Object.assign(SignalOperator_SignalOperator.getDefaults(), {\n            max: 1,\n            min: 0,\n        });\n    }\n    /**\n     * The minimum output value. This number is output when the value input value is 0.\n     */\n    get min() {\n        return this._min;\n    }\n    set min(min) {\n        this._min = min;\n        this._setRange();\n    }\n    /**\n     * The maximum output value. This number is output when the value input value is 1.\n     */\n    get max() {\n        return this._max;\n    }\n    set max(max) {\n        this._max = max;\n        this._setRange();\n    }\n    /**\n     * set the values\n     */\n    _setRange() {\n        this._add.value = this._min;\n        this._mult.value = this._max - this._min;\n    }\n    dispose() {\n        super.dispose();\n        this._add.dispose();\n        this._mult.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Scale.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Zero.js\n\n\n\n\n/**\n * Tone.Zero outputs 0\'s at audio-rate. The reason this has to be\n * it\'s own class is that many browsers optimize out Tone.Signal\n * with a value of 0 and will not process nodes further down the graph.\n * @category Signal\n */\nclass Zero_Zero extends SignalOperator_SignalOperator {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Zero_Zero.getDefaults(), arguments)));\n        this.name = "Zero";\n        /**\n         * The gain node which connects the constant source to the output\n         */\n        this._gain = new Gain_Gain({ context: this.context });\n        /**\n         * Only outputs 0\n         */\n        this.output = this._gain;\n        /**\n         * no input node\n         */\n        this.input = undefined;\n        ToneAudioNode_connect(this.context.getConstant(0), this._gain);\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        ToneAudioNode_disconnect(this.context.getConstant(0), this._gain);\n        return this;\n    }\n}\n//# sourceMappingURL=Zero.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/oscillator/LFO.js\n\n\n\n\n\n\n\n\n\n\n/**\n * LFO stands for low frequency oscillator. LFO produces an output signal\n * which can be attached to an AudioParam or Tone.Signal\n * in order to modulate that parameter with an oscillator. The LFO can\n * also be synced to the transport to start/stop and change when the tempo changes.\n * @example\n * return Tone.Offline(() => {\n * \tconst lfo = new Tone.LFO("4n", 400, 4000).start().toDestination();\n * }, 0.5, 1);\n * @category Source\n */\nclass LFO_LFO extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(LFO_LFO.getDefaults(), arguments, ["frequency", "min", "max"]));\n        this.name = "LFO";\n        /**\n         * The value that the LFO outputs when it\'s stopped\n         */\n        this._stoppedValue = 0;\n        /**\n         * A private placeholder for the units\n         */\n        this._units = "number";\n        /**\n         * If the input value is converted using the [[units]]\n         */\n        this.convert = true;\n        /**\n         * Private methods borrowed from Param\n         */\n        // @ts-ignore\n        this._fromType = Param_Param.prototype._fromType;\n        // @ts-ignore\n        this._toType = Param_Param.prototype._toType;\n        // @ts-ignore\n        this._is = Param_Param.prototype._is;\n        // @ts-ignore\n        this._clampValue = Param_Param.prototype._clampValue;\n        const options = optionsFromArguments(LFO_LFO.getDefaults(), arguments, ["frequency", "min", "max"]);\n        this._oscillator = new Oscillator_Oscillator(options);\n        this.frequency = this._oscillator.frequency;\n        this._amplitudeGain = new Gain_Gain({\n            context: this.context,\n            gain: options.amplitude,\n            units: "normalRange",\n        });\n        this.amplitude = this._amplitudeGain.gain;\n        this._stoppedSignal = new Signal_Signal({\n            context: this.context,\n            units: "audioRange",\n            value: 0,\n        });\n        this._zeros = new Zero_Zero({ context: this.context });\n        this._a2g = new AudioToGain_AudioToGain({ context: this.context });\n        this._scaler = this.output = new Scale_Scale({\n            context: this.context,\n            max: options.max,\n            min: options.min,\n        });\n        this.units = options.units;\n        this.min = options.min;\n        this.max = options.max;\n        // connect it up\n        this._oscillator.chain(this._amplitudeGain, this._a2g, this._scaler);\n        this._zeros.connect(this._a2g);\n        this._stoppedSignal.connect(this._a2g);\n        readOnly(this, ["amplitude", "frequency"]);\n        this.phase = options.phase;\n    }\n    static getDefaults() {\n        return Object.assign(Oscillator_Oscillator.getDefaults(), {\n            amplitude: 1,\n            frequency: "4n",\n            max: 1,\n            min: 0,\n            type: "sine",\n            units: "number",\n        });\n    }\n    /**\n     * Start the LFO.\n     * @param time The time the LFO will start\n     */\n    start(time) {\n        time = this.toSeconds(time);\n        this._stoppedSignal.setValueAtTime(0, time);\n        this._oscillator.start(time);\n        return this;\n    }\n    /**\n     * Stop the LFO.\n     * @param  time The time the LFO will stop\n     */\n    stop(time) {\n        time = this.toSeconds(time);\n        this._stoppedSignal.setValueAtTime(this._stoppedValue, time);\n        this._oscillator.stop(time);\n        return this;\n    }\n    /**\n     * Sync the start/stop/pause to the transport\n     * and the frequency to the bpm of the transport\n     * @example\n     * const lfo = new Tone.LFO("8n");\n     * lfo.sync().start(0);\n     * // the rate of the LFO will always be an eighth note, even as the tempo changes\n     */\n    sync() {\n        this._oscillator.sync();\n        this._oscillator.syncFrequency();\n        return this;\n    }\n    /**\n     * unsync the LFO from transport control\n     */\n    unsync() {\n        this._oscillator.unsync();\n        this._oscillator.unsyncFrequency();\n        return this;\n    }\n    /**\n     * After the oscillator waveform is updated, reset the `_stoppedSignal` value to match the updated waveform\n     */\n    _setStoppedValue() {\n        this._stoppedValue = this._oscillator.getInitialValue();\n        this._stoppedSignal.value = this._stoppedValue;\n    }\n    /**\n     * The minimum output of the LFO.\n     */\n    get min() {\n        return this._toType(this._scaler.min);\n    }\n    set min(min) {\n        min = this._fromType(min);\n        this._scaler.min = min;\n    }\n    /**\n     * The maximum output of the LFO.\n     */\n    get max() {\n        return this._toType(this._scaler.max);\n    }\n    set max(max) {\n        max = this._fromType(max);\n        this._scaler.max = max;\n    }\n    /**\n     * The type of the oscillator: See [[Oscillator.type]]\n     */\n    get type() {\n        return this._oscillator.type;\n    }\n    set type(type) {\n        this._oscillator.type = type;\n        this._setStoppedValue();\n    }\n    /**\n     * The oscillator\'s partials array: See [[Oscillator.partials]]\n     */\n    get partials() {\n        return this._oscillator.partials;\n    }\n    set partials(partials) {\n        this._oscillator.partials = partials;\n        this._setStoppedValue();\n    }\n    /**\n     * The phase of the LFO.\n     */\n    get phase() {\n        return this._oscillator.phase;\n    }\n    set phase(phase) {\n        this._oscillator.phase = phase;\n        this._setStoppedValue();\n    }\n    /**\n     * The output units of the LFO.\n     */\n    get units() {\n        return this._units;\n    }\n    set units(val) {\n        const currentMin = this.min;\n        const currentMax = this.max;\n        // convert the min and the max\n        this._units = val;\n        this.min = currentMin;\n        this.max = currentMax;\n    }\n    /**\n     * Returns the playback state of the source, either "started" or "stopped".\n     */\n    get state() {\n        return this._oscillator.state;\n    }\n    /**\n     * @param node the destination to connect to\n     * @param outputNum the optional output number\n     * @param inputNum the input number\n     */\n    connect(node, outputNum, inputNum) {\n        if (node instanceof Param_Param || node instanceof Signal_Signal) {\n            this.convert = node.convert;\n            this.units = node.units;\n        }\n        connectSignal(this, node, outputNum, inputNum);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._oscillator.dispose();\n        this._stoppedSignal.dispose();\n        this._zeros.dispose();\n        this._scaler.dispose();\n        this._a2g.dispose();\n        this._amplitudeGain.dispose();\n        this.amplitude.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=LFO.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/util/Decorator.js\n\n/**\n * Assert that the number is in the given range.\n */\nfunction Decorator_range(min, max = Infinity) {\n    const valueMap = new WeakMap();\n    return function (target, propertyKey) {\n        Reflect.defineProperty(target, propertyKey, {\n            configurable: true,\n            enumerable: true,\n            get: function () {\n                return valueMap.get(this);\n            },\n            set: function (newValue) {\n                assertRange(newValue, min, max);\n                valueMap.set(this, newValue);\n            }\n        });\n    };\n}\n/**\n * Convert the time to seconds and assert that the time is in between the two\n * values when being set.\n */\nfunction timeRange(min, max = Infinity) {\n    const valueMap = new WeakMap();\n    return function (target, propertyKey) {\n        Reflect.defineProperty(target, propertyKey, {\n            configurable: true,\n            enumerable: true,\n            get: function () {\n                return valueMap.get(this);\n            },\n            set: function (newValue) {\n                assertRange(this.toSeconds(newValue), min, max);\n                valueMap.set(this, newValue);\n            }\n        });\n    };\n}\n//# sourceMappingURL=Decorator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/buffer/Player.js\n\n\n\n\n\n\n\n\n\n/**\n * Player is an audio file player with start, loop, and stop functions.\n * @example\n * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/gong_1.mp3").toDestination();\n * // play as soon as the buffer is loaded\n * player.autostart = true;\n * @category Source\n */\nclass Player_Player extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(Player_Player.getDefaults(), arguments, ["url", "onload"]));\n        this.name = "Player";\n        /**\n         * All of the active buffer source nodes\n         */\n        this._activeSources = new Set();\n        const options = optionsFromArguments(Player_Player.getDefaults(), arguments, ["url", "onload"]);\n        this._buffer = new ToneAudioBuffer_ToneAudioBuffer({\n            onload: this._onload.bind(this, options.onload),\n            onerror: options.onerror,\n            reverse: options.reverse,\n            url: options.url,\n        });\n        this.autostart = options.autostart;\n        this._loop = options.loop;\n        this._loopStart = options.loopStart;\n        this._loopEnd = options.loopEnd;\n        this._playbackRate = options.playbackRate;\n        this.fadeIn = options.fadeIn;\n        this.fadeOut = options.fadeOut;\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            autostart: false,\n            fadeIn: 0,\n            fadeOut: 0,\n            loop: false,\n            loopEnd: 0,\n            loopStart: 0,\n            onload: noOp,\n            onerror: noOp,\n            playbackRate: 1,\n            reverse: false,\n        });\n    }\n    /**\n     * Load the audio file as an audio buffer.\n     * Decodes the audio asynchronously and invokes\n     * the callback once the audio buffer loads.\n     * Note: this does not need to be called if a url\n     * was passed in to the constructor. Only use this\n     * if you want to manually load a new url.\n     * @param url The url of the buffer to load. Filetype support depends on the browser.\n     */\n    load(url) {\n        return __awaiter(this, void 0, void 0, function* () {\n            yield this._buffer.load(url);\n            this._onload();\n            return this;\n        });\n    }\n    /**\n     * Internal callback when the buffer is loaded.\n     */\n    _onload(callback = noOp) {\n        callback();\n        if (this.autostart) {\n            this.start();\n        }\n    }\n    /**\n     * Internal callback when the buffer is done playing.\n     */\n    _onSourceEnd(source) {\n        // invoke the onstop function\n        this.onstop(this);\n        // delete the source from the active sources\n        this._activeSources.delete(source);\n        if (this._activeSources.size === 0 && !this._synced &&\n            this._state.getValueAtTime(this.now()) === "started") {\n            // remove the \'implicitEnd\' event and replace with an explicit end\n            this._state.cancel(this.now());\n            this._state.setStateAtTime("stopped", this.now());\n        }\n    }\n    /**\n     * Play the buffer at the given startTime. Optionally add an offset\n     * and/or duration which will play the buffer from a position\n     * within the buffer for the given duration.\n     *\n     * @param  time When the player should start.\n     * @param  offset The offset from the beginning of the sample to start at.\n     * @param  duration How long the sample should play. If no duration is given, it will default to the full length of the sample (minus any offset)\n     */\n    start(time, offset, duration) {\n        super.start(time, offset, duration);\n        return this;\n    }\n    /**\n     * Internal start method\n     */\n    _start(startTime, offset, duration) {\n        // if it\'s a loop the default offset is the loopStart point\n        if (this._loop) {\n            offset = defaultArg(offset, this._loopStart);\n        }\n        else {\n            // otherwise the default offset is 0\n            offset = defaultArg(offset, 0);\n        }\n        // compute the values in seconds\n        const computedOffset = this.toSeconds(offset);\n        // compute the duration which is either the passed in duration of the buffer.duration - offset\n        const origDuration = duration;\n        duration = defaultArg(duration, Math.max(this._buffer.duration - computedOffset, 0));\n        let computedDuration = this.toSeconds(duration);\n        // scale it by the playback rate\n        computedDuration = computedDuration / this._playbackRate;\n        // get the start time\n        startTime = this.toSeconds(startTime);\n        // make the source\n        const source = new ToneBufferSource_ToneBufferSource({\n            url: this._buffer,\n            context: this.context,\n            fadeIn: this.fadeIn,\n            fadeOut: this.fadeOut,\n            loop: this._loop,\n            loopEnd: this._loopEnd,\n            loopStart: this._loopStart,\n            onended: this._onSourceEnd.bind(this),\n            playbackRate: this._playbackRate,\n        }).connect(this.output);\n        // set the looping properties\n        if (!this._loop && !this._synced) {\n            // cancel the previous stop\n            this._state.cancel(startTime + computedDuration);\n            // if it\'s not looping, set the state change at the end of the sample\n            this._state.setStateAtTime("stopped", startTime + computedDuration, {\n                implicitEnd: true,\n            });\n        }\n        // add it to the array of active sources\n        this._activeSources.add(source);\n        // start it\n        if (this._loop && isUndef(origDuration)) {\n            source.start(startTime, computedOffset);\n        }\n        else {\n            // subtract the fade out time\n            source.start(startTime, computedOffset, computedDuration - this.toSeconds(this.fadeOut));\n        }\n    }\n    /**\n     * Stop playback.\n     */\n    _stop(time) {\n        const computedTime = this.toSeconds(time);\n        this._activeSources.forEach(source => source.stop(computedTime));\n    }\n    /**\n     * Stop and then restart the player from the beginning (or offset)\n     * @param  time When the player should start.\n     * @param  offset The offset from the beginning of the sample to start at.\n     * @param  duration How long the sample should play. If no duration is given,\n     * \t\t\t\t\tit will default to the full length of the sample (minus any offset)\n     */\n    restart(time, offset, duration) {\n        super.restart(time, offset, duration);\n        return this;\n    }\n    _restart(time, offset, duration) {\n        this._stop(time);\n        this._start(time, offset, duration);\n    }\n    /**\n     * Seek to a specific time in the player\'s buffer. If the\n     * source is no longer playing at that time, it will stop.\n     * @param offset The time to seek to.\n     * @param when The time for the seek event to occur.\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/gurgling_theremin_1.mp3", () => {\n     * \tplayer.start();\n     * \t// seek to the offset in 1 second from now\n     * \tplayer.seek(0.4, "+1");\n     * }).toDestination();\n     */\n    seek(offset, when) {\n        const computedTime = this.toSeconds(when);\n        if (this._state.getValueAtTime(computedTime) === "started") {\n            const computedOffset = this.toSeconds(offset);\n            // if it\'s currently playing, stop it\n            this._stop(computedTime);\n            // restart it at the given time\n            this._start(computedTime, computedOffset);\n        }\n        return this;\n    }\n    /**\n     * Set the loop start and end. Will only loop if loop is set to true.\n     * @param loopStart The loop start time\n     * @param loopEnd The loop end time\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/malevoices_aa2_F3.mp3").toDestination();\n     * // loop between the given points\n     * player.setLoopPoints(0.2, 0.3);\n     * player.loop = true;\n     * player.autostart = true;\n     */\n    setLoopPoints(loopStart, loopEnd) {\n        this.loopStart = loopStart;\n        this.loopEnd = loopEnd;\n        return this;\n    }\n    /**\n     * If loop is true, the loop will start at this position.\n     */\n    get loopStart() {\n        return this._loopStart;\n    }\n    set loopStart(loopStart) {\n        this._loopStart = loopStart;\n        if (this.buffer.loaded) {\n            assertRange(this.toSeconds(loopStart), 0, this.buffer.duration);\n        }\n        // get the current source\n        this._activeSources.forEach(source => {\n            source.loopStart = loopStart;\n        });\n    }\n    /**\n     * If loop is true, the loop will end at this position.\n     */\n    get loopEnd() {\n        return this._loopEnd;\n    }\n    set loopEnd(loopEnd) {\n        this._loopEnd = loopEnd;\n        if (this.buffer.loaded) {\n            assertRange(this.toSeconds(loopEnd), 0, this.buffer.duration);\n        }\n        // get the current source\n        this._activeSources.forEach(source => {\n            source.loopEnd = loopEnd;\n        });\n    }\n    /**\n     * The audio buffer belonging to the player.\n     */\n    get buffer() {\n        return this._buffer;\n    }\n    set buffer(buffer) {\n        this._buffer.set(buffer);\n    }\n    /**\n     * If the buffer should loop once it\'s over.\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/drum-samples/breakbeat.mp3").toDestination();\n     * player.loop = true;\n     * player.autostart = true;\n     */\n    get loop() {\n        return this._loop;\n    }\n    set loop(loop) {\n        // if no change, do nothing\n        if (this._loop === loop) {\n            return;\n        }\n        this._loop = loop;\n        // set the loop of all of the sources\n        this._activeSources.forEach(source => {\n            source.loop = loop;\n        });\n        if (loop) {\n            // remove the next stopEvent\n            const stopEvent = this._state.getNextState("stopped", this.now());\n            if (stopEvent) {\n                this._state.cancel(stopEvent.time);\n            }\n        }\n    }\n    /**\n     * Normal speed is 1. The pitch will change with the playback rate.\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/femalevoices_aa2_A5.mp3").toDestination();\n     * // play at 1/4 speed\n     * player.playbackRate = 0.25;\n     * // play as soon as the buffer is loaded\n     * player.autostart = true;\n     */\n    get playbackRate() {\n        return this._playbackRate;\n    }\n    set playbackRate(rate) {\n        this._playbackRate = rate;\n        const now = this.now();\n        // cancel the stop event since it\'s at a different time now\n        const stopEvent = this._state.getNextState("stopped", now);\n        if (stopEvent && stopEvent.implicitEnd) {\n            this._state.cancel(stopEvent.time);\n            this._activeSources.forEach(source => source.cancelStop());\n        }\n        // set all the sources\n        this._activeSources.forEach(source => {\n            source.playbackRate.setValueAtTime(rate, now);\n        });\n    }\n    /**\n     * If the buffer should be reversed\n     * @example\n     * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/chime_1.mp3").toDestination();\n     * player.autostart = true;\n     * player.reverse = true;\n     */\n    get reverse() {\n        return this._buffer.reverse;\n    }\n    set reverse(rev) {\n        this._buffer.reverse = rev;\n    }\n    /**\n     * If the buffer is loaded\n     */\n    get loaded() {\n        return this._buffer.loaded;\n    }\n    dispose() {\n        super.dispose();\n        // disconnect all of the players\n        this._activeSources.forEach(source => source.dispose());\n        this._activeSources.clear();\n        this._buffer.dispose();\n        return this;\n    }\n}\n__decorate([\n    timeRange(0)\n], Player_Player.prototype, "fadeIn", void 0);\n__decorate([\n    timeRange(0)\n], Player_Player.prototype, "fadeOut", void 0);\n//# sourceMappingURL=Player.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/buffer/Players.js\n\n\n\n\n\n\n\n\n/**\n * Players combines multiple [[Player]] objects.\n * @category Source\n */\nclass Players_Players extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Players_Players.getDefaults(), arguments, ["urls", "onload"], "urls"));\n        this.name = "Players";\n        /**\n         * Players has no input.\n         */\n        this.input = undefined;\n        /**\n         * The container of all of the players\n         */\n        this._players = new Map();\n        const options = optionsFromArguments(Players_Players.getDefaults(), arguments, ["urls", "onload"], "urls");\n        /**\n         * The output volume node\n         */\n        this._volume = this.output = new Volume_Volume({\n            context: this.context,\n            volume: options.volume,\n        });\n        this.volume = this._volume.volume;\n        readOnly(this, "volume");\n        this._buffers = new ToneAudioBuffers_ToneAudioBuffers({\n            urls: options.urls,\n            onload: options.onload,\n            baseUrl: options.baseUrl,\n            onerror: options.onerror\n        });\n        // mute initially\n        this.mute = options.mute;\n        this._fadeIn = options.fadeIn;\n        this._fadeOut = options.fadeOut;\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            baseUrl: "",\n            fadeIn: 0,\n            fadeOut: 0,\n            mute: false,\n            onload: noOp,\n            onerror: noOp,\n            urls: {},\n            volume: 0,\n        });\n    }\n    /**\n     * Mute the output.\n     */\n    get mute() {\n        return this._volume.mute;\n    }\n    set mute(mute) {\n        this._volume.mute = mute;\n    }\n    /**\n     * The fadeIn time of the envelope applied to the source.\n     */\n    get fadeIn() {\n        return this._fadeIn;\n    }\n    set fadeIn(fadeIn) {\n        this._fadeIn = fadeIn;\n        this._players.forEach(player => {\n            player.fadeIn = fadeIn;\n        });\n    }\n    /**\n     * The fadeOut time of the each of the sources.\n     */\n    get fadeOut() {\n        return this._fadeOut;\n    }\n    set fadeOut(fadeOut) {\n        this._fadeOut = fadeOut;\n        this._players.forEach(player => {\n            player.fadeOut = fadeOut;\n        });\n    }\n    /**\n     * The state of the players object. Returns "started" if any of the players are playing.\n     */\n    get state() {\n        const playing = Array.from(this._players).some(([_, player]) => player.state === "started");\n        return playing ? "started" : "stopped";\n    }\n    /**\n     * True if the buffers object has a buffer by that name.\n     * @param name  The key or index of the buffer.\n     */\n    has(name) {\n        return this._buffers.has(name);\n    }\n    /**\n     * Get a player by name.\n     * @param  name  The players name as defined in the constructor object or `add` method.\n     */\n    player(name) {\n        assert(this.has(name), `No Player with the name ${name} exists on this object`);\n        if (!this._players.has(name)) {\n            const player = new Player_Player({\n                context: this.context,\n                fadeIn: this._fadeIn,\n                fadeOut: this._fadeOut,\n                url: this._buffers.get(name),\n            }).connect(this.output);\n            this._players.set(name, player);\n        }\n        return this._players.get(name);\n    }\n    /**\n     * If all the buffers are loaded or not\n     */\n    get loaded() {\n        return this._buffers.loaded;\n    }\n    /**\n     * Add a player by name and url to the Players\n     * @param  name A unique name to give the player\n     * @param  url  Either the url of the bufer or a buffer which will be added with the given name.\n     * @param callback  The callback to invoke when the url is loaded.\n     */\n    add(name, url, callback) {\n        assert(!this._buffers.has(name), "A buffer with that name already exists on this object");\n        this._buffers.add(name, url, callback);\n        return this;\n    }\n    /**\n     * Stop all of the players at the given time\n     * @param time The time to stop all of the players.\n     */\n    stopAll(time) {\n        this._players.forEach(player => player.stop(time));\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._volume.dispose();\n        this.volume.dispose();\n        this._players.forEach(player => player.dispose());\n        this._buffers.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Players.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/buffer/GrainPlayer.js\n\n\n\n\n\n\n\n\n/**\n * GrainPlayer implements [granular synthesis](https://en.wikipedia.org/wiki/Granular_synthesis).\n * Granular Synthesis enables you to adjust pitch and playback rate independently. The grainSize is the\n * amount of time each small chunk of audio is played for and the overlap is the\n * amount of crossfading transition time between successive grains.\n * @category Source\n */\nclass GrainPlayer_GrainPlayer extends Source_Source {\n    constructor() {\n        super(optionsFromArguments(GrainPlayer_GrainPlayer.getDefaults(), arguments, ["url", "onload"]));\n        this.name = "GrainPlayer";\n        /**\n         * Internal loopStart value\n         */\n        this._loopStart = 0;\n        /**\n         * Internal loopStart value\n         */\n        this._loopEnd = 0;\n        /**\n         * All of the currently playing BufferSources\n         */\n        this._activeSources = [];\n        const options = optionsFromArguments(GrainPlayer_GrainPlayer.getDefaults(), arguments, ["url", "onload"]);\n        this.buffer = new ToneAudioBuffer_ToneAudioBuffer({\n            onload: options.onload,\n            onerror: options.onerror,\n            reverse: options.reverse,\n            url: options.url,\n        });\n        this._clock = new Clock_Clock({\n            context: this.context,\n            callback: this._tick.bind(this),\n            frequency: 1 / options.grainSize\n        });\n        this._playbackRate = options.playbackRate;\n        this._grainSize = options.grainSize;\n        this._overlap = options.overlap;\n        this.detune = options.detune;\n        // setup\n        this.overlap = options.overlap;\n        this.loop = options.loop;\n        this.playbackRate = options.playbackRate;\n        this.grainSize = options.grainSize;\n        this.loopStart = options.loopStart;\n        this.loopEnd = options.loopEnd;\n        this.reverse = options.reverse;\n        this._clock.on("stop", this._onstop.bind(this));\n    }\n    static getDefaults() {\n        return Object.assign(Source_Source.getDefaults(), {\n            onload: noOp,\n            onerror: noOp,\n            overlap: 0.1,\n            grainSize: 0.2,\n            playbackRate: 1,\n            detune: 0,\n            loop: false,\n            loopStart: 0,\n            loopEnd: 0,\n            reverse: false\n        });\n    }\n    /**\n     * Internal start method\n     */\n    _start(time, offset, duration) {\n        offset = defaultArg(offset, 0);\n        offset = this.toSeconds(offset);\n        time = this.toSeconds(time);\n        const grainSize = 1 / this._clock.frequency.getValueAtTime(time);\n        this._clock.start(time, offset / grainSize);\n        if (duration) {\n            this.stop(time + this.toSeconds(duration));\n        }\n    }\n    /**\n     * Stop and then restart the player from the beginning (or offset)\n     * @param  time When the player should start.\n     * @param  offset The offset from the beginning of the sample to start at.\n     * @param  duration How long the sample should play. If no duration is given,\n     * \t\t\t\t\tit will default to the full length of the sample (minus any offset)\n     */\n    restart(time, offset, duration) {\n        super.restart(time, offset, duration);\n        return this;\n    }\n    _restart(time, offset, duration) {\n        this._stop(time);\n        this._start(time, offset, duration);\n    }\n    /**\n     * Internal stop method\n     */\n    _stop(time) {\n        this._clock.stop(time);\n    }\n    /**\n     * Invoked when the clock is stopped\n     */\n    _onstop(time) {\n        // stop the players\n        this._activeSources.forEach((source) => {\n            source.fadeOut = 0;\n            source.stop(time);\n        });\n        this.onstop(this);\n    }\n    /**\n     * Invoked on each clock tick. scheduled a new grain at this time.\n     */\n    _tick(time) {\n        // check if it should stop looping\n        const ticks = this._clock.getTicksAtTime(time);\n        const offset = ticks * this._grainSize;\n        this.log("offset", offset);\n        if (!this.loop && offset > this.buffer.duration) {\n            this.stop(time);\n            return;\n        }\n        // at the beginning of the file, the fade in should be 0\n        const fadeIn = offset < this._overlap ? 0 : this._overlap;\n        // create a buffer source\n        const source = new ToneBufferSource_ToneBufferSource({\n            context: this.context,\n            url: this.buffer,\n            fadeIn: fadeIn,\n            fadeOut: this._overlap,\n            loop: this.loop,\n            loopStart: this._loopStart,\n            loopEnd: this._loopEnd,\n            // compute the playbackRate based on the detune\n            playbackRate: intervalToFrequencyRatio(this.detune / 100)\n        }).connect(this.output);\n        source.start(time, this._grainSize * ticks);\n        source.stop(time + this._grainSize / this.playbackRate);\n        // add it to the active sources\n        this._activeSources.push(source);\n        // remove it when it\'s done\n        source.onended = () => {\n            const index = this._activeSources.indexOf(source);\n            if (index !== -1) {\n                this._activeSources.splice(index, 1);\n            }\n        };\n    }\n    /**\n     * The playback rate of the sample\n     */\n    get playbackRate() {\n        return this._playbackRate;\n    }\n    set playbackRate(rate) {\n        assertRange(rate, 0.001);\n        this._playbackRate = rate;\n        this.grainSize = this._grainSize;\n    }\n    /**\n     * The loop start time.\n     */\n    get loopStart() {\n        return this._loopStart;\n    }\n    set loopStart(time) {\n        if (this.buffer.loaded) {\n            assertRange(this.toSeconds(time), 0, this.buffer.duration);\n        }\n        this._loopStart = this.toSeconds(time);\n    }\n    /**\n     * The loop end time.\n     */\n    get loopEnd() {\n        return this._loopEnd;\n    }\n    set loopEnd(time) {\n        if (this.buffer.loaded) {\n            assertRange(this.toSeconds(time), 0, this.buffer.duration);\n        }\n        this._loopEnd = this.toSeconds(time);\n    }\n    /**\n     * The direction the buffer should play in\n     */\n    get reverse() {\n        return this.buffer.reverse;\n    }\n    set reverse(rev) {\n        this.buffer.reverse = rev;\n    }\n    /**\n     * The size of each chunk of audio that the\n     * buffer is chopped into and played back at.\n     */\n    get grainSize() {\n        return this._grainSize;\n    }\n    set grainSize(size) {\n        this._grainSize = this.toSeconds(size);\n        this._clock.frequency.setValueAtTime(this._playbackRate / this._grainSize, this.now());\n    }\n    /**\n     * The duration of the cross-fade between successive grains.\n     */\n    get overlap() {\n        return this._overlap;\n    }\n    set overlap(time) {\n        const computedTime = this.toSeconds(time);\n        assertRange(computedTime, 0);\n        this._overlap = computedTime;\n    }\n    /**\n     * If all the buffer is loaded\n     */\n    get loaded() {\n        return this.buffer.loaded;\n    }\n    dispose() {\n        super.dispose();\n        this.buffer.dispose();\n        this._clock.dispose();\n        this._activeSources.forEach((source) => source.dispose());\n        return this;\n    }\n}\n//# sourceMappingURL=GrainPlayer.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/source/index.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Abs.js\n\n\n/**\n * Return the absolute value of an incoming signal.\n *\n * @example\n * return Tone.Offline(() => {\n * \tconst abs = new Tone.Abs().toDestination();\n * \tconst signal = new Tone.Signal(1);\n * \tsignal.rampTo(-1, 0.5);\n * \tsignal.connect(abs);\n * }, 0.5, 1);\n * @category Signal\n */\nclass Abs_Abs extends SignalOperator_SignalOperator {\n    constructor() {\n        super(...arguments);\n        this.name = "Abs";\n        /**\n         * The node which converts the audio ranges\n         */\n        this._abs = new WaveShaper_WaveShaper({\n            context: this.context,\n            mapping: val => {\n                if (Math.abs(val) < 0.001) {\n                    return 0;\n                }\n                else {\n                    return Math.abs(val);\n                }\n            },\n        });\n        /**\n         * The AudioRange input [-1, 1]\n         */\n        this.input = this._abs;\n        /**\n         * The output range [0, 1]\n         */\n        this.output = this._abs;\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        this._abs.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Abs.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/GainToAudio.js\n\n\n/**\n * GainToAudio converts an input in NormalRange [0,1] to AudioRange [-1,1].\n * See [[AudioToGain]].\n * @category Signal\n */\nclass GainToAudio_GainToAudio extends SignalOperator_SignalOperator {\n    constructor() {\n        super(...arguments);\n        this.name = "GainToAudio";\n        /**\n         * The node which converts the audio ranges\n         */\n        this._norm = new WaveShaper_WaveShaper({\n            context: this.context,\n            mapping: x => Math.abs(x) * 2 - 1,\n        });\n        /**\n         * The NormalRange input [0, 1]\n         */\n        this.input = this._norm;\n        /**\n         * The AudioRange output [-1, 1]\n         */\n        this.output = this._norm;\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        this._norm.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=GainToAudio.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Negate.js\n\n\n/**\n * Negate the incoming signal. i.e. an input signal of 10 will output -10\n *\n * @example\n * const neg = new Tone.Negate();\n * const sig = new Tone.Signal(-2).connect(neg);\n * // output of neg is positive 2.\n * @category Signal\n */\nclass Negate_Negate extends SignalOperator_SignalOperator {\n    constructor() {\n        super(...arguments);\n        this.name = "Negate";\n        /**\n         * negation is done by multiplying by -1\n         */\n        this._multiply = new Multiply_Multiply({\n            context: this.context,\n            value: -1,\n        });\n        /**\n         * The input and output are equal to the multiply node\n         */\n        this.input = this._multiply;\n        this.output = this._multiply;\n    }\n    /**\n     * clean up\n     * @returns {Negate} this\n     */\n    dispose() {\n        super.dispose();\n        this._multiply.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Negate.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Subtract.js\n\n\n\n\n\n/**\n * Subtract the signal connected to the input is subtracted from the signal connected\n * The subtrahend.\n *\n * @example\n * // subtract a scalar from a signal\n * const sub = new Tone.Subtract(1);\n * const sig = new Tone.Signal(4).connect(sub);\n * // the output of sub is 3.\n * @example\n * // subtract two signals\n * const sub = new Tone.Subtract();\n * const sigA = new Tone.Signal(10);\n * const sigB = new Tone.Signal(2.5);\n * sigA.connect(sub);\n * sigB.connect(sub.subtrahend);\n * // output of sub is 7.5\n * @category Signal\n */\nclass Subtract_Subtract extends Signal_Signal {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Subtract_Subtract.getDefaults(), arguments, ["value"])));\n        this.override = false;\n        this.name = "Subtract";\n        /**\n         * the summing node\n         */\n        this._sum = new Gain_Gain({ context: this.context });\n        this.input = this._sum;\n        this.output = this._sum;\n        /**\n         * Negate the input of the second input before connecting it to the summing node.\n         */\n        this._neg = new Negate_Negate({ context: this.context });\n        /**\n         * The value which is subtracted from the main signal\n         */\n        this.subtrahend = this._param;\n        connectSeries(this._constantSource, this._neg, this._sum);\n    }\n    static getDefaults() {\n        return Object.assign(Signal_Signal.getDefaults(), {\n            value: 0,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._neg.dispose();\n        this._sum.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Subtract.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/GreaterThanZero.js\n\n\n\n\n/**\n * GreaterThanZero outputs 1 when the input is strictly greater than zero\n * @example\n * return Tone.Offline(() => {\n * \tconst gt0 = new Tone.GreaterThanZero().toDestination();\n * \tconst sig = new Tone.Signal(0.5).connect(gt0);\n * \tsig.setValueAtTime(-1, 0.05);\n * }, 0.1, 1);\n * @category Signal\n */\nclass GreaterThanZero_GreaterThanZero extends SignalOperator_SignalOperator {\n    constructor() {\n        super(Object.assign(optionsFromArguments(GreaterThanZero_GreaterThanZero.getDefaults(), arguments)));\n        this.name = "GreaterThanZero";\n        this._thresh = this.output = new WaveShaper_WaveShaper({\n            context: this.context,\n            length: 127,\n            mapping: (val) => {\n                if (val <= 0) {\n                    return 0;\n                }\n                else {\n                    return 1;\n                }\n            },\n        });\n        this._scale = this.input = new Multiply_Multiply({\n            context: this.context,\n            value: 10000\n        });\n        // connections\n        this._scale.connect(this._thresh);\n    }\n    dispose() {\n        super.dispose();\n        this._scale.dispose();\n        this._thresh.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=GreaterThanZero.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/GreaterThan.js\n\n\n\n\n\n/**\n * Output 1 if the signal is greater than the value, otherwise outputs 0.\n * can compare two signals or a signal and a number.\n *\n * @example\n * return Tone.Offline(() => {\n * \tconst gt = new Tone.GreaterThan(2).toDestination();\n * \tconst sig = new Tone.Signal(4).connect(gt);\n * }, 0.1, 1);\n * @category Signal\n */\nclass GreaterThan_GreaterThan extends Signal_Signal {\n    constructor() {\n        super(Object.assign(optionsFromArguments(GreaterThan_GreaterThan.getDefaults(), arguments, ["value"])));\n        this.name = "GreaterThan";\n        this.override = false;\n        const options = optionsFromArguments(GreaterThan_GreaterThan.getDefaults(), arguments, ["value"]);\n        this._subtract = this.input = new Subtract_Subtract({\n            context: this.context,\n            value: options.value\n        });\n        this._gtz = this.output = new GreaterThanZero_GreaterThanZero({ context: this.context });\n        this.comparator = this._param = this._subtract.subtrahend;\n        readOnly(this, "comparator");\n        // connect\n        this._subtract.connect(this._gtz);\n    }\n    static getDefaults() {\n        return Object.assign(Signal_Signal.getDefaults(), {\n            value: 0,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._gtz.dispose();\n        this._subtract.dispose();\n        this.comparator.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=GreaterThan.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/Pow.js\n\n\n\n/**\n * Pow applies an exponent to the incoming signal. The incoming signal must be AudioRange [-1, 1]\n *\n * @example\n * const pow = new Tone.Pow(2);\n * const sig = new Tone.Signal(0.5).connect(pow);\n * // output of pow is 0.25.\n * @category Signal\n */\nclass Pow_Pow extends SignalOperator_SignalOperator {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Pow_Pow.getDefaults(), arguments, ["value"])));\n        this.name = "Pow";\n        const options = optionsFromArguments(Pow_Pow.getDefaults(), arguments, ["value"]);\n        this._exponentScaler = this.input = this.output = new WaveShaper_WaveShaper({\n            context: this.context,\n            mapping: this._expFunc(options.value),\n            length: 8192,\n        });\n        this._exponent = options.value;\n    }\n    static getDefaults() {\n        return Object.assign(SignalOperator_SignalOperator.getDefaults(), {\n            value: 1,\n        });\n    }\n    /**\n     * the function which maps the waveshaper\n     * @param exponent exponent value\n     */\n    _expFunc(exponent) {\n        return (val) => {\n            return Math.pow(Math.abs(val), exponent);\n        };\n    }\n    /**\n     * The value of the exponent.\n     */\n    get value() {\n        return this._exponent;\n    }\n    set value(exponent) {\n        this._exponent = exponent;\n        this._exponentScaler.setMap(this._expFunc(this._exponent));\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._exponentScaler.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Pow.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/ScaleExp.js\n\n\n\n/**\n * Performs an exponential scaling on an input signal.\n * Scales a NormalRange value [0,1] exponentially\n * to the output range of outputMin to outputMax.\n * @example\n * const scaleExp = new Tone.ScaleExp(0, 100, 2);\n * const signal = new Tone.Signal(0.5).connect(scaleExp);\n * @category Signal\n */\nclass ScaleExp_ScaleExp extends Scale_Scale {\n    constructor() {\n        super(Object.assign(optionsFromArguments(ScaleExp_ScaleExp.getDefaults(), arguments, ["min", "max", "exponent"])));\n        this.name = "ScaleExp";\n        const options = optionsFromArguments(ScaleExp_ScaleExp.getDefaults(), arguments, ["min", "max", "exponent"]);\n        this.input = this._exp = new Pow_Pow({\n            context: this.context,\n            value: options.exponent,\n        });\n        this._exp.connect(this._mult);\n    }\n    static getDefaults() {\n        return Object.assign(Scale_Scale.getDefaults(), {\n            exponent: 1,\n        });\n    }\n    /**\n     * Instead of interpolating linearly between the [[min]] and\n     * [[max]] values, setting the exponent will interpolate between\n     * the two values with an exponential curve.\n     */\n    get exponent() {\n        return this._exp.value;\n    }\n    set exponent(exp) {\n        this._exp.value = exp;\n    }\n    dispose() {\n        super.dispose();\n        this._exp.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=ScaleExp.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/SyncedSignal.js\n\n\n\n\n/**\n * Adds the ability to synchronize the signal to the [[Transport]]\n */\nclass SyncedSignal_SyncedSignal extends Signal_Signal {\n    constructor() {\n        super(optionsFromArguments(Signal_Signal.getDefaults(), arguments, ["value", "units"]));\n        this.name = "SyncedSignal";\n        /**\n         * Don\'t override when something is connected to the input\n         */\n        this.override = false;\n        const options = optionsFromArguments(Signal_Signal.getDefaults(), arguments, ["value", "units"]);\n        this._lastVal = options.value;\n        this._synced = this.context.transport.scheduleRepeat(this._onTick.bind(this), "1i");\n        this._syncedCallback = this._anchorValue.bind(this);\n        this.context.transport.on("start", this._syncedCallback);\n        this.context.transport.on("pause", this._syncedCallback);\n        this.context.transport.on("stop", this._syncedCallback);\n        // disconnect the constant source from the output and replace it with another one\n        this._constantSource.disconnect();\n        this._constantSource.stop(0);\n        // create a new one\n        this._constantSource = this.output = new ToneConstantSource_ToneConstantSource({\n            context: this.context,\n            offset: options.value,\n            units: options.units,\n        }).start(0);\n        this.setValueAtTime(options.value, 0);\n    }\n    /**\n     * Callback which is invoked every tick.\n     */\n    _onTick(time) {\n        const val = super.getValueAtTime(this.context.transport.seconds);\n        // approximate ramp curves with linear ramps\n        if (this._lastVal !== val) {\n            this._lastVal = val;\n            this._constantSource.offset.setValueAtTime(val, time);\n        }\n    }\n    /**\n     * Anchor the value at the start and stop of the Transport\n     */\n    _anchorValue(time) {\n        const val = super.getValueAtTime(this.context.transport.seconds);\n        this._lastVal = val;\n        this._constantSource.offset.cancelAndHoldAtTime(time);\n        this._constantSource.offset.setValueAtTime(val, time);\n    }\n    getValueAtTime(time) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, time).toSeconds();\n        return super.getValueAtTime(computedTime);\n    }\n    setValueAtTime(value, time) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, time).toSeconds();\n        super.setValueAtTime(value, computedTime);\n        return this;\n    }\n    linearRampToValueAtTime(value, time) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, time).toSeconds();\n        super.linearRampToValueAtTime(value, computedTime);\n        return this;\n    }\n    exponentialRampToValueAtTime(value, time) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, time).toSeconds();\n        super.exponentialRampToValueAtTime(value, computedTime);\n        return this;\n    }\n    setTargetAtTime(value, startTime, timeConstant) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, startTime).toSeconds();\n        super.setTargetAtTime(value, computedTime, timeConstant);\n        return this;\n    }\n    cancelScheduledValues(startTime) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, startTime).toSeconds();\n        super.cancelScheduledValues(computedTime);\n        return this;\n    }\n    setValueCurveAtTime(values, startTime, duration, scaling) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, startTime).toSeconds();\n        duration = this.toSeconds(duration);\n        super.setValueCurveAtTime(values, computedTime, duration, scaling);\n        return this;\n    }\n    cancelAndHoldAtTime(time) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, time).toSeconds();\n        super.cancelAndHoldAtTime(computedTime);\n        return this;\n    }\n    setRampPoint(time) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, time).toSeconds();\n        super.setRampPoint(computedTime);\n        return this;\n    }\n    exponentialRampTo(value, rampTime, startTime) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, startTime).toSeconds();\n        super.exponentialRampTo(value, rampTime, computedTime);\n        return this;\n    }\n    linearRampTo(value, rampTime, startTime) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, startTime).toSeconds();\n        super.linearRampTo(value, rampTime, computedTime);\n        return this;\n    }\n    targetRampTo(value, rampTime, startTime) {\n        const computedTime = new TransportTime_TransportTimeClass(this.context, startTime).toSeconds();\n        super.targetRampTo(value, rampTime, computedTime);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this.context.transport.clear(this._synced);\n        this.context.transport.off("start", this._syncedCallback);\n        this.context.transport.off("pause", this._syncedCallback);\n        this.context.transport.off("stop", this._syncedCallback);\n        this._constantSource.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=SyncedSignal.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/signal/index.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/envelope/Envelope.js\n\n\n\n\n\n\n\n\n/**\n * Envelope is an [ADSR](https://en.wikipedia.org/wiki/Synthesizer#ADSR_envelope)\n * envelope generator. Envelope outputs a signal which\n * can be connected to an AudioParam or Tone.Signal.\n * ```\n *           /\\\n *          /  \\\n *         /    \\\n *        /      \\\n *       /        \\___________\n *      /                     \\\n *     /                       \\\n *    /                         \\\n *   /                           \\\n * ```\n * @example\n * return Tone.Offline(() => {\n * \tconst env = new Tone.Envelope({\n * \t\tattack: 0.1,\n * \t\tdecay: 0.2,\n * \t\tsustain: 0.5,\n * \t\trelease: 0.8,\n * \t}).toDestination();\n * \tenv.triggerAttackRelease(0.5);\n * }, 1.5, 1);\n * @category Component\n */\nclass Envelope_Envelope extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Envelope_Envelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]));\n        this.name = "Envelope";\n        /**\n         * the signal which is output.\n         */\n        this._sig = new Signal_Signal({\n            context: this.context,\n            value: 0,\n        });\n        /**\n         * The output signal of the envelope\n         */\n        this.output = this._sig;\n        /**\n         * Envelope has no input\n         */\n        this.input = undefined;\n        const options = optionsFromArguments(Envelope_Envelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]);\n        this.attack = options.attack;\n        this.decay = options.decay;\n        this.sustain = options.sustain;\n        this.release = options.release;\n        this.attackCurve = options.attackCurve;\n        this.releaseCurve = options.releaseCurve;\n        this.decayCurve = options.decayCurve;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            attack: 0.01,\n            attackCurve: "linear",\n            decay: 0.1,\n            decayCurve: "exponential",\n            release: 1,\n            releaseCurve: "exponential",\n            sustain: 0.5,\n        });\n    }\n    /**\n     * Read the current value of the envelope. Useful for\n     * synchronizing visual output to the envelope.\n     */\n    get value() {\n        return this.getValueAtTime(this.now());\n    }\n    /**\n     * Get the curve\n     * @param  curve\n     * @param  direction  In/Out\n     * @return The curve name\n     */\n    _getCurve(curve, direction) {\n        if (isString(curve)) {\n            return curve;\n        }\n        else {\n            // look up the name in the curves array\n            let curveName;\n            for (curveName in EnvelopeCurves) {\n                if (EnvelopeCurves[curveName][direction] === curve) {\n                    return curveName;\n                }\n            }\n            // return the custom curve\n            return curve;\n        }\n    }\n    /**\n     * Assign a the curve to the given name using the direction\n     * @param  name\n     * @param  direction In/Out\n     * @param  curve\n     */\n    _setCurve(name, direction, curve) {\n        // check if it\'s a valid type\n        if (isString(curve) && Reflect.has(EnvelopeCurves, curve)) {\n            const curveDef = EnvelopeCurves[curve];\n            if (isObject(curveDef)) {\n                if (name !== "_decayCurve") {\n                    this[name] = curveDef[direction];\n                }\n            }\n            else {\n                this[name] = curveDef;\n            }\n        }\n        else if (isArray(curve) && name !== "_decayCurve") {\n            this[name] = curve;\n        }\n        else {\n            throw new Error("Envelope: invalid curve: " + curve);\n        }\n    }\n    /**\n     * The shape of the attack.\n     * Can be any of these strings:\n     * * "linear"\n     * * "exponential"\n     * * "sine"\n     * * "cosine"\n     * * "bounce"\n     * * "ripple"\n     * * "step"\n     *\n     * Can also be an array which describes the curve. Values\n     * in the array are evenly subdivided and linearly\n     * interpolated over the duration of the attack.\n     * @example\n     * return Tone.Offline(() => {\n     * \tconst env = new Tone.Envelope(0.4).toDestination();\n     * \tenv.attackCurve = "linear";\n     * \tenv.triggerAttack();\n     * }, 1, 1);\n     */\n    get attackCurve() {\n        return this._getCurve(this._attackCurve, "In");\n    }\n    set attackCurve(curve) {\n        this._setCurve("_attackCurve", "In", curve);\n    }\n    /**\n     * The shape of the release. See the attack curve types.\n     * @example\n     * return Tone.Offline(() => {\n     * \tconst env = new Tone.Envelope({\n     * \t\trelease: 0.8\n     * \t}).toDestination();\n     * \tenv.triggerAttack();\n     * \t// release curve could also be defined by an array\n     * \tenv.releaseCurve = [1, 0.3, 0.4, 0.2, 0.7, 0];\n     * \tenv.triggerRelease(0.2);\n     * }, 1, 1);\n     */\n    get releaseCurve() {\n        return this._getCurve(this._releaseCurve, "Out");\n    }\n    set releaseCurve(curve) {\n        this._setCurve("_releaseCurve", "Out", curve);\n    }\n    /**\n     * The shape of the decay either "linear" or "exponential"\n     * @example\n     * return Tone.Offline(() => {\n     * \tconst env = new Tone.Envelope({\n     * \t\tsustain: 0.1,\n     * \t\tdecay: 0.5\n     * \t}).toDestination();\n     * \tenv.decayCurve = "linear";\n     * \tenv.triggerAttack();\n     * }, 1, 1);\n     */\n    get decayCurve() {\n        return this._decayCurve;\n    }\n    set decayCurve(curve) {\n        assert(["linear", "exponential"].some(c => c === curve), `Invalid envelope curve: ${curve}`);\n        this._decayCurve = curve;\n    }\n    /**\n     * Trigger the attack/decay portion of the ADSR envelope.\n     * @param  time When the attack should start.\n     * @param velocity The velocity of the envelope scales the vales.\n     *                             number between 0-1\n     * @example\n     * const env = new Tone.AmplitudeEnvelope().toDestination();\n     * const osc = new Tone.Oscillator().connect(env).start();\n     * // trigger the attack 0.5 seconds from now with a velocity of 0.2\n     * env.triggerAttack("+0.5", 0.2);\n     */\n    triggerAttack(time, velocity = 1) {\n        this.log("triggerAttack", time, velocity);\n        time = this.toSeconds(time);\n        const originalAttack = this.toSeconds(this.attack);\n        let attack = originalAttack;\n        const decay = this.toSeconds(this.decay);\n        // check if it\'s not a complete attack\n        const currentValue = this.getValueAtTime(time);\n        if (currentValue > 0) {\n            // subtract the current value from the attack time\n            const attackRate = 1 / attack;\n            const remainingDistance = 1 - currentValue;\n            // the attack is now the remaining time\n            attack = remainingDistance / attackRate;\n        }\n        // attack\n        if (attack < this.sampleTime) {\n            this._sig.cancelScheduledValues(time);\n            // case where the attack time is 0 should set instantly\n            this._sig.setValueAtTime(velocity, time);\n        }\n        else if (this._attackCurve === "linear") {\n            this._sig.linearRampTo(velocity, attack, time);\n        }\n        else if (this._attackCurve === "exponential") {\n            this._sig.targetRampTo(velocity, attack, time);\n        }\n        else {\n            this._sig.cancelAndHoldAtTime(time);\n            let curve = this._attackCurve;\n            // find the starting position in the curve\n            for (let i = 1; i < curve.length; i++) {\n                // the starting index is between the two values\n                if (curve[i - 1] <= currentValue && currentValue <= curve[i]) {\n                    curve = this._attackCurve.slice(i);\n                    // the first index is the current value\n                    curve[0] = currentValue;\n                    break;\n                }\n            }\n            this._sig.setValueCurveAtTime(curve, time, attack, velocity);\n        }\n        // decay\n        if (decay && this.sustain < 1) {\n            const decayValue = velocity * this.sustain;\n            const decayStart = time + attack;\n            this.log("decay", decayStart);\n            if (this._decayCurve === "linear") {\n                this._sig.linearRampToValueAtTime(decayValue, decay + decayStart);\n            }\n            else {\n                this._sig.exponentialApproachValueAtTime(decayValue, decayStart, decay);\n            }\n        }\n        return this;\n    }\n    /**\n     * Triggers the release of the envelope.\n     * @param  time When the release portion of the envelope should start.\n     * @example\n     * const env = new Tone.AmplitudeEnvelope().toDestination();\n     * const osc = new Tone.Oscillator({\n     * \ttype: "sawtooth"\n     * }).connect(env).start();\n     * env.triggerAttack();\n     * // trigger the release half a second after the attack\n     * env.triggerRelease("+0.5");\n     */\n    triggerRelease(time) {\n        this.log("triggerRelease", time);\n        time = this.toSeconds(time);\n        const currentValue = this.getValueAtTime(time);\n        if (currentValue > 0) {\n            const release = this.toSeconds(this.release);\n            if (release < this.sampleTime) {\n                this._sig.setValueAtTime(0, time);\n            }\n            else if (this._releaseCurve === "linear") {\n                this._sig.linearRampTo(0, release, time);\n            }\n            else if (this._releaseCurve === "exponential") {\n                this._sig.targetRampTo(0, release, time);\n            }\n            else {\n                assert(isArray(this._releaseCurve), "releaseCurve must be either \'linear\', \'exponential\' or an array");\n                this._sig.cancelAndHoldAtTime(time);\n                this._sig.setValueCurveAtTime(this._releaseCurve, time, release, currentValue);\n            }\n        }\n        return this;\n    }\n    /**\n     * Get the scheduled value at the given time. This will\n     * return the unconverted (raw) value.\n     * @example\n     * const env = new Tone.Envelope(0.5, 1, 0.4, 2);\n     * env.triggerAttackRelease(2);\n     * setInterval(() => console.log(env.getValueAtTime(Tone.now())), 100);\n     */\n    getValueAtTime(time) {\n        return this._sig.getValueAtTime(time);\n    }\n    /**\n     * triggerAttackRelease is shorthand for triggerAttack, then waiting\n     * some duration, then triggerRelease.\n     * @param duration The duration of the sustain.\n     * @param time When the attack should be triggered.\n     * @param velocity The velocity of the envelope.\n     * @example\n     * const env = new Tone.AmplitudeEnvelope().toDestination();\n     * const osc = new Tone.Oscillator().connect(env).start();\n     * // trigger the release 0.5 seconds after the attack\n     * env.triggerAttackRelease(0.5);\n     */\n    triggerAttackRelease(duration, time, velocity = 1) {\n        time = this.toSeconds(time);\n        this.triggerAttack(time, velocity);\n        this.triggerRelease(time + this.toSeconds(duration));\n        return this;\n    }\n    /**\n     * Cancels all scheduled envelope changes after the given time.\n     */\n    cancel(after) {\n        this._sig.cancelScheduledValues(this.toSeconds(after));\n        return this;\n    }\n    /**\n     * Connect the envelope to a destination node.\n     */\n    connect(destination, outputNumber = 0, inputNumber = 0) {\n        connectSignal(this, destination, outputNumber, inputNumber);\n        return this;\n    }\n    /**\n     * Render the envelope curve to an array of the given length.\n     * Good for visualizing the envelope curve. Rescales the duration of the\n     * envelope to fit the length.\n     */\n    asArray(length = 1024) {\n        return __awaiter(this, void 0, void 0, function* () {\n            const duration = length / this.context.sampleRate;\n            const context = new OfflineContext_OfflineContext(1, duration, this.context.sampleRate);\n            // normalize the ADSR for the given duration with 20% sustain time\n            const attackPortion = this.toSeconds(this.attack) + this.toSeconds(this.decay);\n            const envelopeDuration = attackPortion + this.toSeconds(this.release);\n            const sustainTime = envelopeDuration * 0.1;\n            const totalDuration = envelopeDuration + sustainTime;\n            // @ts-ignore\n            const clone = new this.constructor(Object.assign(this.get(), {\n                attack: duration * this.toSeconds(this.attack) / totalDuration,\n                decay: duration * this.toSeconds(this.decay) / totalDuration,\n                release: duration * this.toSeconds(this.release) / totalDuration,\n                context\n            }));\n            clone._sig.toDestination();\n            clone.triggerAttackRelease(duration * (attackPortion + sustainTime) / totalDuration, 0);\n            const buffer = yield context.render();\n            return buffer.getChannelData(0);\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._sig.dispose();\n        return this;\n    }\n}\n__decorate([\n    timeRange(0)\n], Envelope_Envelope.prototype, "attack", void 0);\n__decorate([\n    timeRange(0)\n], Envelope_Envelope.prototype, "decay", void 0);\n__decorate([\n    Decorator_range(0, 1)\n], Envelope_Envelope.prototype, "sustain", void 0);\n__decorate([\n    timeRange(0)\n], Envelope_Envelope.prototype, "release", void 0);\n/**\n * Generate some complex envelope curves.\n */\nconst EnvelopeCurves = (() => {\n    const curveLen = 128;\n    let i;\n    let k;\n    // cosine curve\n    const cosineCurve = [];\n    for (i = 0; i < curveLen; i++) {\n        cosineCurve[i] = Math.sin((i / (curveLen - 1)) * (Math.PI / 2));\n    }\n    // ripple curve\n    const rippleCurve = [];\n    const rippleCurveFreq = 6.4;\n    for (i = 0; i < curveLen - 1; i++) {\n        k = (i / (curveLen - 1));\n        const sineWave = Math.sin(k * (Math.PI * 2) * rippleCurveFreq - Math.PI / 2) + 1;\n        rippleCurve[i] = sineWave / 10 + k * 0.83;\n    }\n    rippleCurve[curveLen - 1] = 1;\n    // stairs curve\n    const stairsCurve = [];\n    const steps = 5;\n    for (i = 0; i < curveLen; i++) {\n        stairsCurve[i] = Math.ceil((i / (curveLen - 1)) * steps) / steps;\n    }\n    // in-out easing curve\n    const sineCurve = [];\n    for (i = 0; i < curveLen; i++) {\n        k = i / (curveLen - 1);\n        sineCurve[i] = 0.5 * (1 - Math.cos(Math.PI * k));\n    }\n    // a bounce curve\n    const bounceCurve = [];\n    for (i = 0; i < curveLen; i++) {\n        k = i / (curveLen - 1);\n        const freq = Math.pow(k, 3) * 4 + 0.2;\n        const val = Math.cos(freq * Math.PI * 2 * k);\n        bounceCurve[i] = Math.abs(val * (1 - k));\n    }\n    /**\n     * Invert a value curve to make it work for the release\n     */\n    function invertCurve(curve) {\n        const out = new Array(curve.length);\n        for (let j = 0; j < curve.length; j++) {\n            out[j] = 1 - curve[j];\n        }\n        return out;\n    }\n    /**\n     * reverse the curve\n     */\n    function reverseCurve(curve) {\n        return curve.slice(0).reverse();\n    }\n    /**\n     * attack and release curve arrays\n     */\n    return {\n        bounce: {\n            In: invertCurve(bounceCurve),\n            Out: bounceCurve,\n        },\n        cosine: {\n            In: cosineCurve,\n            Out: reverseCurve(cosineCurve),\n        },\n        exponential: "exponential",\n        linear: "linear",\n        ripple: {\n            In: rippleCurve,\n            Out: invertCurve(rippleCurve),\n        },\n        sine: {\n            In: sineCurve,\n            Out: invertCurve(sineCurve),\n        },\n        step: {\n            In: stairsCurve,\n            Out: invertCurve(stairsCurve),\n        },\n    };\n})();\n//# sourceMappingURL=Envelope.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/Instrument.js\n\n\n\n\n/**\n * Base-class for all instruments\n */\nclass Instrument_Instrument extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Instrument_Instrument.getDefaults(), arguments));\n        /**\n         * Keep track of all events scheduled to the transport\n         * when the instrument is \'synced\'\n         */\n        this._scheduledEvents = [];\n        /**\n         * If the instrument is currently synced\n         */\n        this._synced = false;\n        this._original_triggerAttack = this.triggerAttack;\n        this._original_triggerRelease = this.triggerRelease;\n        const options = optionsFromArguments(Instrument_Instrument.getDefaults(), arguments);\n        this._volume = this.output = new Volume_Volume({\n            context: this.context,\n            volume: options.volume,\n        });\n        this.volume = this._volume.volume;\n        readOnly(this, "volume");\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            volume: 0,\n        });\n    }\n    /**\n     * Sync the instrument to the Transport. All subsequent calls of\n     * [[triggerAttack]] and [[triggerRelease]] will be scheduled along the transport.\n     * @example\n     * const fmSynth = new Tone.FMSynth().toDestination();\n     * fmSynth.volume.value = -6;\n     * fmSynth.sync();\n     * // schedule 3 notes when the transport first starts\n     * fmSynth.triggerAttackRelease("C4", "8n", 0);\n     * fmSynth.triggerAttackRelease("E4", "8n", "8n");\n     * fmSynth.triggerAttackRelease("G4", "8n", "4n");\n     * // start the transport to hear the notes\n     * Tone.Transport.start();\n     */\n    sync() {\n        if (this._syncState()) {\n            this._syncMethod("triggerAttack", 1);\n            this._syncMethod("triggerRelease", 0);\n        }\n        return this;\n    }\n    /**\n     * set _sync\n     */\n    _syncState() {\n        let changed = false;\n        if (!this._synced) {\n            this._synced = true;\n            changed = true;\n        }\n        return changed;\n    }\n    /**\n     * Wrap the given method so that it can be synchronized\n     * @param method Which method to wrap and sync\n     * @param  timePosition What position the time argument appears in\n     */\n    _syncMethod(method, timePosition) {\n        const originalMethod = this["_original_" + method] = this[method];\n        this[method] = (...args) => {\n            const time = args[timePosition];\n            const id = this.context.transport.schedule((t) => {\n                args[timePosition] = t;\n                originalMethod.apply(this, args);\n            }, time);\n            this._scheduledEvents.push(id);\n        };\n    }\n    /**\n     * Unsync the instrument from the Transport\n     */\n    unsync() {\n        this._scheduledEvents.forEach(id => this.context.transport.clear(id));\n        this._scheduledEvents = [];\n        if (this._synced) {\n            this._synced = false;\n            this.triggerAttack = this._original_triggerAttack;\n            this.triggerRelease = this._original_triggerRelease;\n        }\n        return this;\n    }\n    /**\n     * Trigger the attack and then the release after the duration.\n     * @param  note     The note to trigger.\n     * @param  duration How long the note should be held for before\n     *                         triggering the release. This value must be greater than 0.\n     * @param time  When the note should be triggered.\n     * @param  velocity The velocity the note should be triggered at.\n     * @example\n     * const synth = new Tone.Synth().toDestination();\n     * // trigger "C4" for the duration of an 8th note\n     * synth.triggerAttackRelease("C4", "8n");\n     */\n    triggerAttackRelease(note, duration, time, velocity) {\n        const computedTime = this.toSeconds(time);\n        const computedDuration = this.toSeconds(duration);\n        this.triggerAttack(note, computedTime, velocity);\n        this.triggerRelease(computedTime + computedDuration);\n        return this;\n    }\n    /**\n     * clean up\n     * @returns {Instrument} this\n     */\n    dispose() {\n        super.dispose();\n        this._volume.dispose();\n        this.unsync();\n        this._scheduledEvents = [];\n        return this;\n    }\n}\n//# sourceMappingURL=Instrument.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/Monophonic.js\n\n\n\n\n\n\n/**\n * Abstract base class for other monophonic instruments to extend.\n */\nclass Monophonic_Monophonic extends Instrument_Instrument {\n    constructor() {\n        super(optionsFromArguments(Monophonic_Monophonic.getDefaults(), arguments));\n        const options = optionsFromArguments(Monophonic_Monophonic.getDefaults(), arguments);\n        this.portamento = options.portamento;\n        this.onsilence = options.onsilence;\n    }\n    static getDefaults() {\n        return Object.assign(Instrument_Instrument.getDefaults(), {\n            detune: 0,\n            onsilence: noOp,\n            portamento: 0,\n        });\n    }\n    /**\n     * Trigger the attack of the note optionally with a given velocity.\n     * @param  note The note to trigger.\n     * @param  time When the note should start.\n     * @param  velocity The velocity scaler determines how "loud" the note will be triggered.\n     * @example\n     * const synth = new Tone.Synth().toDestination();\n     * // trigger the note a half second from now at half velocity\n     * synth.triggerAttack("C4", "+0.5", 0.5);\n     */\n    triggerAttack(note, time, velocity = 1) {\n        this.log("triggerAttack", note, time, velocity);\n        const seconds = this.toSeconds(time);\n        this._triggerEnvelopeAttack(seconds, velocity);\n        this.setNote(note, seconds);\n        return this;\n    }\n    /**\n     * Trigger the release portion of the envelope\n     * @param  time If no time is given, the release happens immediatly\n     * @example\n     * const synth = new Tone.Synth().toDestination();\n     * synth.triggerAttack("C4");\n     * // trigger the release a second from now\n     * synth.triggerRelease("+1");\n     */\n    triggerRelease(time) {\n        this.log("triggerRelease", time);\n        const seconds = this.toSeconds(time);\n        this._triggerEnvelopeRelease(seconds);\n        return this;\n    }\n    /**\n     * Set the note at the given time. If no time is given, the note\n     * will set immediately.\n     * @param note The note to change to.\n     * @param  time The time when the note should be set.\n     * @example\n     * const synth = new Tone.Synth().toDestination();\n     * synth.triggerAttack("C4");\n     * // change to F#6 in one quarter note from now.\n     * synth.setNote("F#6", "+4n");\n     */\n    setNote(note, time) {\n        const computedTime = this.toSeconds(time);\n        const computedFrequency = note instanceof Frequency_FrequencyClass ? note.toFrequency() : note;\n        if (this.portamento > 0 && this.getLevelAtTime(computedTime) > 0.05) {\n            const portTime = this.toSeconds(this.portamento);\n            this.frequency.exponentialRampTo(computedFrequency, portTime, computedTime);\n        }\n        else {\n            this.frequency.setValueAtTime(computedFrequency, computedTime);\n        }\n        return this;\n    }\n}\n__decorate([\n    timeRange(0)\n], Monophonic_Monophonic.prototype, "portamento", void 0);\n//# sourceMappingURL=Monophonic.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/envelope/AmplitudeEnvelope.js\n\n\n\n/**\n * AmplitudeEnvelope is a Tone.Envelope connected to a gain node.\n * Unlike Tone.Envelope, which outputs the envelope\'s value, AmplitudeEnvelope accepts\n * an audio signal as the input and will apply the envelope to the amplitude\n * of the signal.\n * Read more about ADSR Envelopes on [Wikipedia](https://en.wikipedia.org/wiki/Synthesizer#ADSR_envelope).\n *\n * @example\n * return Tone.Offline(() => {\n * \tconst ampEnv = new Tone.AmplitudeEnvelope({\n * \t\tattack: 0.1,\n * \t\tdecay: 0.2,\n * \t\tsustain: 1.0,\n * \t\trelease: 0.8\n * \t}).toDestination();\n * \t// create an oscillator and connect it\n * \tconst osc = new Tone.Oscillator().connect(ampEnv).start();\n * \t// trigger the envelopes attack and release "8t" apart\n * \tampEnv.triggerAttackRelease("8t");\n * }, 1.5, 1);\n * @category Component\n */\nclass AmplitudeEnvelope_AmplitudeEnvelope extends Envelope_Envelope {\n    constructor() {\n        super(optionsFromArguments(AmplitudeEnvelope_AmplitudeEnvelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]));\n        this.name = "AmplitudeEnvelope";\n        this._gainNode = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        this.output = this._gainNode;\n        this.input = this._gainNode;\n        this._sig.connect(this._gainNode.gain);\n        this.output = this._gainNode;\n        this.input = this._gainNode;\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this._gainNode.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AmplitudeEnvelope.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/Synth.js\n\n\n\n\n\n\n\n\n/**\n * Synth is composed simply of a [[OmniOscillator]] routed through an [[AmplitudeEnvelope]].\n * ```\n * +----------------+   +-------------------+\n * | OmniOscillator +>--\x3e AmplitudeEnvelope +>--\x3e Output\n * +----------------+   +-------------------+\n * ```\n * @example\n * const synth = new Tone.Synth().toDestination();\n * synth.triggerAttackRelease("C4", "8n");\n * @category Instrument\n */\nclass Synth_Synth extends Monophonic_Monophonic {\n    constructor() {\n        super(optionsFromArguments(Synth_Synth.getDefaults(), arguments));\n        this.name = "Synth";\n        const options = optionsFromArguments(Synth_Synth.getDefaults(), arguments);\n        this.oscillator = new OmniOscillator_OmniOscillator(Object.assign({\n            context: this.context,\n            detune: options.detune,\n            onstop: () => this.onsilence(this),\n        }, options.oscillator));\n        this.frequency = this.oscillator.frequency;\n        this.detune = this.oscillator.detune;\n        this.envelope = new AmplitudeEnvelope_AmplitudeEnvelope(Object.assign({\n            context: this.context,\n        }, options.envelope));\n        // connect the oscillators to the output\n        this.oscillator.chain(this.envelope, this.output);\n        readOnly(this, ["oscillator", "frequency", "detune", "envelope"]);\n    }\n    static getDefaults() {\n        return Object.assign(Monophonic_Monophonic.getDefaults(), {\n            envelope: Object.assign(omitFromObject(Envelope_Envelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                attack: 0.005,\n                decay: 0.1,\n                release: 1,\n                sustain: 0.3,\n            }),\n            oscillator: Object.assign(omitFromObject(OmniOscillator_OmniOscillator.getDefaults(), [...Object.keys(Source_Source.getDefaults()), "frequency", "detune"]), {\n                type: "triangle",\n            }),\n        });\n    }\n    /**\n     * start the attack portion of the envelope\n     * @param time the time the attack should start\n     * @param velocity the velocity of the note (0-1)\n     */\n    _triggerEnvelopeAttack(time, velocity) {\n        // the envelopes\n        this.envelope.triggerAttack(time, velocity);\n        this.oscillator.start(time);\n        // if there is no release portion, stop the oscillator\n        if (this.envelope.sustain === 0) {\n            const computedAttack = this.toSeconds(this.envelope.attack);\n            const computedDecay = this.toSeconds(this.envelope.decay);\n            this.oscillator.stop(time + computedAttack + computedDecay);\n        }\n    }\n    /**\n     * start the release portion of the envelope\n     * @param time the time the release should start\n     */\n    _triggerEnvelopeRelease(time) {\n        this.envelope.triggerRelease(time);\n        this.oscillator.stop(time + this.toSeconds(this.envelope.release));\n    }\n    getLevelAtTime(time) {\n        time = this.toSeconds(time);\n        return this.envelope.getValueAtTime(time);\n    }\n    /**\n     * clean up\n     */\n    dispose() {\n        super.dispose();\n        this.oscillator.dispose();\n        this.envelope.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Synth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/ModulationSynth.js\n\n\n\n\n\n\n\n\n\n\n\n/**\n * Base class for both AM and FM synths\n */\nclass ModulationSynth_ModulationSynth extends Monophonic_Monophonic {\n    constructor() {\n        super(optionsFromArguments(ModulationSynth_ModulationSynth.getDefaults(), arguments));\n        this.name = "ModulationSynth";\n        const options = optionsFromArguments(ModulationSynth_ModulationSynth.getDefaults(), arguments);\n        this._carrier = new Synth_Synth({\n            context: this.context,\n            oscillator: options.oscillator,\n            envelope: options.envelope,\n            onsilence: () => this.onsilence(this),\n            volume: -10,\n        });\n        this._modulator = new Synth_Synth({\n            context: this.context,\n            oscillator: options.modulation,\n            envelope: options.modulationEnvelope,\n            volume: -10,\n        });\n        this.oscillator = this._carrier.oscillator;\n        this.envelope = this._carrier.envelope;\n        this.modulation = this._modulator.oscillator;\n        this.modulationEnvelope = this._modulator.envelope;\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n        });\n        this.detune = new Signal_Signal({\n            context: this.context,\n            value: options.detune,\n            units: "cents"\n        });\n        this.harmonicity = new Multiply_Multiply({\n            context: this.context,\n            value: options.harmonicity,\n            minValue: 0,\n        });\n        this._modulationNode = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        readOnly(this, ["frequency", "harmonicity", "oscillator", "envelope", "modulation", "modulationEnvelope", "detune"]);\n    }\n    static getDefaults() {\n        return Object.assign(Monophonic_Monophonic.getDefaults(), {\n            harmonicity: 3,\n            oscillator: Object.assign(omitFromObject(OmniOscillator_OmniOscillator.getDefaults(), [\n                ...Object.keys(Source_Source.getDefaults()),\n                "frequency",\n                "detune"\n            ]), {\n                type: "sine"\n            }),\n            envelope: Object.assign(omitFromObject(Envelope_Envelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                attack: 0.01,\n                decay: 0.01,\n                sustain: 1,\n                release: 0.5\n            }),\n            modulation: Object.assign(omitFromObject(OmniOscillator_OmniOscillator.getDefaults(), [\n                ...Object.keys(Source_Source.getDefaults()),\n                "frequency",\n                "detune"\n            ]), {\n                type: "square"\n            }),\n            modulationEnvelope: Object.assign(omitFromObject(Envelope_Envelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                attack: 0.5,\n                decay: 0.0,\n                sustain: 1,\n                release: 0.5\n            })\n        });\n    }\n    /**\n     * Trigger the attack portion of the note\n     */\n    _triggerEnvelopeAttack(time, velocity) {\n        // @ts-ignore\n        this._carrier._triggerEnvelopeAttack(time, velocity);\n        // @ts-ignore\n        this._modulator._triggerEnvelopeAttack(time, velocity);\n    }\n    /**\n     * Trigger the release portion of the note\n     */\n    _triggerEnvelopeRelease(time) {\n        // @ts-ignore\n        this._carrier._triggerEnvelopeRelease(time);\n        // @ts-ignore\n        this._modulator._triggerEnvelopeRelease(time);\n        return this;\n    }\n    getLevelAtTime(time) {\n        time = this.toSeconds(time);\n        return this.envelope.getValueAtTime(time);\n    }\n    dispose() {\n        super.dispose();\n        this._carrier.dispose();\n        this._modulator.dispose();\n        this.frequency.dispose();\n        this.detune.dispose();\n        this.harmonicity.dispose();\n        this._modulationNode.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=ModulationSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/AMSynth.js\n\n\n\n/**\n * AMSynth uses the output of one Tone.Synth to modulate the\n * amplitude of another Tone.Synth. The harmonicity (the ratio between\n * the two signals) affects the timbre of the output signal greatly.\n * Read more about Amplitude Modulation Synthesis on\n * [SoundOnSound](https://web.archive.org/web/20160404103653/http://www.soundonsound.com:80/sos/mar00/articles/synthsecrets.htm).\n *\n * @example\n * const synth = new Tone.AMSynth().toDestination();\n * synth.triggerAttackRelease("C4", "4n");\n *\n * @category Instrument\n */\nclass AMSynth_AMSynth extends ModulationSynth_ModulationSynth {\n    constructor() {\n        super(optionsFromArguments(AMSynth_AMSynth.getDefaults(), arguments));\n        this.name = "AMSynth";\n        this._modulationScale = new AudioToGain_AudioToGain({\n            context: this.context,\n        });\n        // control the two voices frequency\n        this.frequency.connect(this._carrier.frequency);\n        this.frequency.chain(this.harmonicity, this._modulator.frequency);\n        this.detune.fan(this._carrier.detune, this._modulator.detune);\n        this._modulator.chain(this._modulationScale, this._modulationNode.gain);\n        this._carrier.chain(this._modulationNode, this.output);\n    }\n    dispose() {\n        super.dispose();\n        this._modulationScale.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AMSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/BiquadFilter.js\n\n\n\n\n/**\n * Thin wrapper around the native Web Audio [BiquadFilterNode](https://webaudio.github.io/web-audio-api/#biquadfilternode).\n * BiquadFilter is similar to [[Filter]] but doesn\'t have the option to set the "rolloff" value.\n * @category Component\n */\nclass BiquadFilter_BiquadFilter extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(BiquadFilter_BiquadFilter.getDefaults(), arguments, ["frequency", "type"]));\n        this.name = "BiquadFilter";\n        const options = optionsFromArguments(BiquadFilter_BiquadFilter.getDefaults(), arguments, ["frequency", "type"]);\n        this._filter = this.context.createBiquadFilter();\n        this.input = this.output = this._filter;\n        this.Q = new Param_Param({\n            context: this.context,\n            units: "number",\n            value: options.Q,\n            param: this._filter.Q,\n        });\n        this.frequency = new Param_Param({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n            param: this._filter.frequency,\n        });\n        this.detune = new Param_Param({\n            context: this.context,\n            units: "cents",\n            value: options.detune,\n            param: this._filter.detune,\n        });\n        this.gain = new Param_Param({\n            context: this.context,\n            units: "decibels",\n            convert: false,\n            value: options.gain,\n            param: this._filter.gain,\n        });\n        this.type = options.type;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            Q: 1,\n            type: "lowpass",\n            frequency: 350,\n            detune: 0,\n            gain: 0,\n        });\n    }\n    /**\n     * The type of this BiquadFilterNode. For a complete list of types and their attributes, see the\n     * [Web Audio API](https://webaudio.github.io/web-audio-api/#dom-biquadfiltertype-lowpass)\n     */\n    get type() {\n        return this._filter.type;\n    }\n    set type(type) {\n        const types = ["lowpass", "highpass", "bandpass",\n            "lowshelf", "highshelf", "notch", "allpass", "peaking"];\n        assert(types.indexOf(type) !== -1, `Invalid filter type: ${type}`);\n        this._filter.type = type;\n    }\n    /**\n     * Get the frequency response curve. This curve represents how the filter\n     * responses to frequencies between 20hz-20khz.\n     * @param  len The number of values to return\n     * @return The frequency response curve between 20-20kHz\n     */\n    getFrequencyResponse(len = 128) {\n        // start with all 1s\n        const freqValues = new Float32Array(len);\n        for (let i = 0; i < len; i++) {\n            const norm = Math.pow(i / len, 2);\n            const freq = norm * (20000 - 20) + 20;\n            freqValues[i] = freq;\n        }\n        const magValues = new Float32Array(len);\n        const phaseValues = new Float32Array(len);\n        // clone the filter to remove any connections which may be changing the value\n        const filterClone = this.context.createBiquadFilter();\n        filterClone.type = this.type;\n        filterClone.Q.value = this.Q.value;\n        filterClone.frequency.value = this.frequency.value;\n        filterClone.gain.value = this.gain.value;\n        filterClone.getFrequencyResponse(freqValues, magValues, phaseValues);\n        return magValues;\n    }\n    dispose() {\n        super.dispose();\n        this._filter.disconnect();\n        this.Q.dispose();\n        this.frequency.dispose();\n        this.gain.dispose();\n        this.detune.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=BiquadFilter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/Filter.js\n\n\n\n\n\n\n\n\n/**\n * Tone.Filter is a filter which allows for all of the same native methods\n * as the [BiquadFilterNode](http://webaudio.github.io/web-audio-api/#the-biquadfilternode-interface).\n * Tone.Filter has the added ability to set the filter rolloff at -12\n * (default), -24 and -48.\n * @example\n * const filter = new Tone.Filter(1500, "highpass").toDestination();\n * filter.frequency.rampTo(20000, 10);\n * const noise = new Tone.Noise().connect(filter).start();\n * @category Component\n */\nclass Filter_Filter extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Filter_Filter.getDefaults(), arguments, ["frequency", "type", "rolloff"]));\n        this.name = "Filter";\n        this.input = new Gain_Gain({ context: this.context });\n        this.output = new Gain_Gain({ context: this.context });\n        this._filters = [];\n        const options = optionsFromArguments(Filter_Filter.getDefaults(), arguments, ["frequency", "type", "rolloff"]);\n        this._filters = [];\n        this.Q = new Signal_Signal({\n            context: this.context,\n            units: "positive",\n            value: options.Q,\n        });\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n        });\n        this.detune = new Signal_Signal({\n            context: this.context,\n            units: "cents",\n            value: options.detune,\n        });\n        this.gain = new Signal_Signal({\n            context: this.context,\n            units: "decibels",\n            convert: false,\n            value: options.gain,\n        });\n        this._type = options.type;\n        this.rolloff = options.rolloff;\n        readOnly(this, ["detune", "frequency", "gain", "Q"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            Q: 1,\n            detune: 0,\n            frequency: 350,\n            gain: 0,\n            rolloff: -12,\n            type: "lowpass",\n        });\n    }\n    /**\n     * The type of the filter. Types: "lowpass", "highpass",\n     * "bandpass", "lowshelf", "highshelf", "notch", "allpass", or "peaking".\n     */\n    get type() {\n        return this._type;\n    }\n    set type(type) {\n        const types = ["lowpass", "highpass", "bandpass",\n            "lowshelf", "highshelf", "notch", "allpass", "peaking"];\n        assert(types.indexOf(type) !== -1, `Invalid filter type: ${type}`);\n        this._type = type;\n        this._filters.forEach(filter => filter.type = type);\n    }\n    /**\n     * The rolloff of the filter which is the drop in db\n     * per octave. Implemented internally by cascading filters.\n     * Only accepts the values -12, -24, -48 and -96.\n     */\n    get rolloff() {\n        return this._rolloff;\n    }\n    set rolloff(rolloff) {\n        const rolloffNum = isNumber(rolloff) ? rolloff : parseInt(rolloff, 10);\n        const possibilities = [-12, -24, -48, -96];\n        let cascadingCount = possibilities.indexOf(rolloffNum);\n        // check the rolloff is valid\n        assert(cascadingCount !== -1, `rolloff can only be ${possibilities.join(", ")}`);\n        cascadingCount += 1;\n        this._rolloff = rolloffNum;\n        this.input.disconnect();\n        this._filters.forEach(filter => filter.disconnect());\n        this._filters = new Array(cascadingCount);\n        for (let count = 0; count < cascadingCount; count++) {\n            const filter = new BiquadFilter_BiquadFilter({\n                context: this.context,\n            });\n            filter.type = this._type;\n            this.frequency.connect(filter.frequency);\n            this.detune.connect(filter.detune);\n            this.Q.connect(filter.Q);\n            this.gain.connect(filter.gain);\n            this._filters[count] = filter;\n        }\n        this._internalChannels = this._filters;\n        connectSeries(this.input, ...this._internalChannels, this.output);\n    }\n    /**\n     * Get the frequency response curve. This curve represents how the filter\n     * responses to frequencies between 20hz-20khz.\n     * @param  len The number of values to return\n     * @return The frequency response curve between 20-20kHz\n     */\n    getFrequencyResponse(len = 128) {\n        const filterClone = new BiquadFilter_BiquadFilter({\n            frequency: this.frequency.value,\n            gain: this.gain.value,\n            Q: this.Q.value,\n            type: this._type,\n            detune: this.detune.value,\n        });\n        // start with all 1s\n        const totalResponse = new Float32Array(len).map(() => 1);\n        this._filters.forEach(() => {\n            const response = filterClone.getFrequencyResponse(len);\n            response.forEach((val, i) => totalResponse[i] *= val);\n        });\n        filterClone.dispose();\n        return totalResponse;\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._filters.forEach(filter => {\n            filter.dispose();\n        });\n        writable(this, ["detune", "frequency", "gain", "Q"]);\n        this.frequency.dispose();\n        this.Q.dispose();\n        this.detune.dispose();\n        this.gain.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Filter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/envelope/FrequencyEnvelope.js\n\n\n\n\n\n/**\n * FrequencyEnvelope is an [[Envelope]] which ramps between [[baseFrequency]]\n * and [[octaves]]. It can also have an optional [[exponent]] to adjust the curve\n * which it ramps.\n * @example\n * const oscillator = new Tone.Oscillator().toDestination().start();\n * const freqEnv = new Tone.FrequencyEnvelope({\n * \tattack: 0.2,\n * \tbaseFrequency: "C2",\n * \toctaves: 4\n * });\n * freqEnv.connect(oscillator.frequency);\n * freqEnv.triggerAttack();\n * @category Component\n */\nclass FrequencyEnvelope_FrequencyEnvelope extends Envelope_Envelope {\n    constructor() {\n        super(optionsFromArguments(FrequencyEnvelope_FrequencyEnvelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]));\n        this.name = "FrequencyEnvelope";\n        const options = optionsFromArguments(FrequencyEnvelope_FrequencyEnvelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]);\n        this._octaves = options.octaves;\n        this._baseFrequency = this.toFrequency(options.baseFrequency);\n        this._exponent = this.input = new Pow_Pow({\n            context: this.context,\n            value: options.exponent\n        });\n        this._scale = this.output = new Scale_Scale({\n            context: this.context,\n            min: this._baseFrequency,\n            max: this._baseFrequency * Math.pow(2, this._octaves),\n        });\n        this._sig.chain(this._exponent, this._scale);\n    }\n    static getDefaults() {\n        return Object.assign(Envelope_Envelope.getDefaults(), {\n            baseFrequency: 200,\n            exponent: 1,\n            octaves: 4,\n        });\n    }\n    /**\n     * The envelope\'s minimum output value. This is the value which it\n     * starts at.\n     */\n    get baseFrequency() {\n        return this._baseFrequency;\n    }\n    set baseFrequency(min) {\n        const freq = this.toFrequency(min);\n        assertRange(freq, 0);\n        this._baseFrequency = freq;\n        this._scale.min = this._baseFrequency;\n        // update the max value when the min changes\n        this.octaves = this._octaves;\n    }\n    /**\n     * The number of octaves above the baseFrequency that the\n     * envelope will scale to.\n     */\n    get octaves() {\n        return this._octaves;\n    }\n    set octaves(octaves) {\n        this._octaves = octaves;\n        this._scale.max = this._baseFrequency * Math.pow(2, octaves);\n    }\n    /**\n     * The envelope\'s exponent value.\n     */\n    get exponent() {\n        return this._exponent.value;\n    }\n    set exponent(exponent) {\n        this._exponent.value = exponent;\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this._exponent.dispose();\n        this._scale.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FrequencyEnvelope.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/MonoSynth.js\n\n\n\n\n\n\n\n\n\n\n/**\n * MonoSynth is composed of one `oscillator`, one `filter`, and two `envelopes`.\n * The amplitude of the Oscillator and the cutoff frequency of the\n * Filter are controlled by Envelopes.\n * <img src="https://docs.google.com/drawings/d/1gaY1DF9_Hzkodqf8JI1Cg2VZfwSElpFQfI94IQwad38/pub?w=924&h=240">\n * @example\n * const synth = new Tone.MonoSynth({\n * \toscillator: {\n * \t\ttype: "square"\n * \t},\n * \tenvelope: {\n * \t\tattack: 0.1\n * \t}\n * }).toDestination();\n * synth.triggerAttackRelease("C4", "8n");\n * @category Instrument\n */\nclass MonoSynth_MonoSynth extends Monophonic_Monophonic {\n    constructor() {\n        super(optionsFromArguments(MonoSynth_MonoSynth.getDefaults(), arguments));\n        this.name = "MonoSynth";\n        const options = optionsFromArguments(MonoSynth_MonoSynth.getDefaults(), arguments);\n        this.oscillator = new OmniOscillator_OmniOscillator(Object.assign(options.oscillator, {\n            context: this.context,\n            detune: options.detune,\n            onstop: () => this.onsilence(this),\n        }));\n        this.frequency = this.oscillator.frequency;\n        this.detune = this.oscillator.detune;\n        this.filter = new Filter_Filter(Object.assign(options.filter, { context: this.context }));\n        this.filterEnvelope = new FrequencyEnvelope_FrequencyEnvelope(Object.assign(options.filterEnvelope, { context: this.context }));\n        this.envelope = new AmplitudeEnvelope_AmplitudeEnvelope(Object.assign(options.envelope, { context: this.context }));\n        // connect the oscillators to the output\n        this.oscillator.chain(this.filter, this.envelope, this.output);\n        // connect the filter envelope\n        this.filterEnvelope.connect(this.filter.frequency);\n        readOnly(this, ["oscillator", "frequency", "detune", "filter", "filterEnvelope", "envelope"]);\n    }\n    static getDefaults() {\n        return Object.assign(Monophonic_Monophonic.getDefaults(), {\n            envelope: Object.assign(omitFromObject(Envelope_Envelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                attack: 0.005,\n                decay: 0.1,\n                release: 1,\n                sustain: 0.9,\n            }),\n            filter: Object.assign(omitFromObject(Filter_Filter.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                Q: 1,\n                rolloff: -12,\n                type: "lowpass",\n            }),\n            filterEnvelope: Object.assign(omitFromObject(FrequencyEnvelope_FrequencyEnvelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                attack: 0.6,\n                baseFrequency: 200,\n                decay: 0.2,\n                exponent: 2,\n                octaves: 3,\n                release: 2,\n                sustain: 0.5,\n            }),\n            oscillator: Object.assign(omitFromObject(OmniOscillator_OmniOscillator.getDefaults(), Object.keys(Source_Source.getDefaults())), {\n                type: "sawtooth",\n            }),\n        });\n    }\n    /**\n     * start the attack portion of the envelope\n     * @param time the time the attack should start\n     * @param velocity the velocity of the note (0-1)\n     */\n    _triggerEnvelopeAttack(time, velocity = 1) {\n        this.envelope.triggerAttack(time, velocity);\n        this.filterEnvelope.triggerAttack(time);\n        this.oscillator.start(time);\n        if (this.envelope.sustain === 0) {\n            const computedAttack = this.toSeconds(this.envelope.attack);\n            const computedDecay = this.toSeconds(this.envelope.decay);\n            this.oscillator.stop(time + computedAttack + computedDecay);\n        }\n    }\n    /**\n     * start the release portion of the envelope\n     * @param time the time the release should start\n     */\n    _triggerEnvelopeRelease(time) {\n        this.envelope.triggerRelease(time);\n        this.filterEnvelope.triggerRelease(time);\n        this.oscillator.stop(time + this.toSeconds(this.envelope.release));\n    }\n    getLevelAtTime(time) {\n        time = this.toSeconds(time);\n        return this.envelope.getValueAtTime(time);\n    }\n    dispose() {\n        super.dispose();\n        this.oscillator.dispose();\n        this.envelope.dispose();\n        this.filterEnvelope.dispose();\n        this.filter.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MonoSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/DuoSynth.js\n\n\n\n\n\n\n\n\n/**\n * DuoSynth is a monophonic synth composed of two [[MonoSynths]] run in parallel with control over the\n * frequency ratio between the two voices and vibrato effect.\n * @example\n * const duoSynth = new Tone.DuoSynth().toDestination();\n * duoSynth.triggerAttackRelease("C4", "2n");\n * @category Instrument\n */\nclass DuoSynth_DuoSynth extends Monophonic_Monophonic {\n    constructor() {\n        super(optionsFromArguments(DuoSynth_DuoSynth.getDefaults(), arguments));\n        this.name = "DuoSynth";\n        const options = optionsFromArguments(DuoSynth_DuoSynth.getDefaults(), arguments);\n        this.voice0 = new MonoSynth_MonoSynth(Object.assign(options.voice0, {\n            context: this.context,\n            onsilence: () => this.onsilence(this)\n        }));\n        this.voice1 = new MonoSynth_MonoSynth(Object.assign(options.voice1, {\n            context: this.context,\n        }));\n        this.harmonicity = new Multiply_Multiply({\n            context: this.context,\n            units: "positive",\n            value: options.harmonicity,\n        });\n        this._vibrato = new LFO_LFO({\n            frequency: options.vibratoRate,\n            context: this.context,\n            min: -50,\n            max: 50\n        });\n        // start the vibrato immediately\n        this._vibrato.start();\n        this.vibratoRate = this._vibrato.frequency;\n        this._vibratoGain = new Gain_Gain({\n            context: this.context,\n            units: "normalRange",\n            gain: options.vibratoAmount\n        });\n        this.vibratoAmount = this._vibratoGain.gain;\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: 440\n        });\n        this.detune = new Signal_Signal({\n            context: this.context,\n            units: "cents",\n            value: options.detune\n        });\n        // control the two voices frequency\n        this.frequency.connect(this.voice0.frequency);\n        this.frequency.chain(this.harmonicity, this.voice1.frequency);\n        this._vibrato.connect(this._vibratoGain);\n        this._vibratoGain.fan(this.voice0.detune, this.voice1.detune);\n        this.detune.fan(this.voice0.detune, this.voice1.detune);\n        this.voice0.connect(this.output);\n        this.voice1.connect(this.output);\n        readOnly(this, ["voice0", "voice1", "frequency", "vibratoAmount", "vibratoRate"]);\n    }\n    getLevelAtTime(time) {\n        time = this.toSeconds(time);\n        return this.voice0.envelope.getValueAtTime(time) + this.voice1.envelope.getValueAtTime(time);\n    }\n    static getDefaults() {\n        return deepMerge(Monophonic_Monophonic.getDefaults(), {\n            vibratoAmount: 0.5,\n            vibratoRate: 5,\n            harmonicity: 1.5,\n            voice0: deepMerge(omitFromObject(MonoSynth_MonoSynth.getDefaults(), Object.keys(Monophonic_Monophonic.getDefaults())), {\n                filterEnvelope: {\n                    attack: 0.01,\n                    decay: 0.0,\n                    sustain: 1,\n                    release: 0.5\n                },\n                envelope: {\n                    attack: 0.01,\n                    decay: 0.0,\n                    sustain: 1,\n                    release: 0.5\n                }\n            }),\n            voice1: deepMerge(omitFromObject(MonoSynth_MonoSynth.getDefaults(), Object.keys(Monophonic_Monophonic.getDefaults())), {\n                filterEnvelope: {\n                    attack: 0.01,\n                    decay: 0.0,\n                    sustain: 1,\n                    release: 0.5\n                },\n                envelope: {\n                    attack: 0.01,\n                    decay: 0.0,\n                    sustain: 1,\n                    release: 0.5\n                }\n            }),\n        });\n    }\n    /**\n     * Trigger the attack portion of the note\n     */\n    _triggerEnvelopeAttack(time, velocity) {\n        // @ts-ignore\n        this.voice0._triggerEnvelopeAttack(time, velocity);\n        // @ts-ignore\n        this.voice1._triggerEnvelopeAttack(time, velocity);\n    }\n    /**\n     * Trigger the release portion of the note\n     */\n    _triggerEnvelopeRelease(time) {\n        // @ts-ignore\n        this.voice0._triggerEnvelopeRelease(time);\n        // @ts-ignore\n        this.voice1._triggerEnvelopeRelease(time);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this.voice0.dispose();\n        this.voice1.dispose();\n        this.frequency.dispose();\n        this.detune.dispose();\n        this._vibrato.dispose();\n        this.vibratoRate.dispose();\n        this._vibratoGain.dispose();\n        this.harmonicity.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=DuoSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/FMSynth.js\n\n\n\n/**\n * FMSynth is composed of two Tone.Synths where one Tone.Synth modulates\n * the frequency of a second Tone.Synth. A lot of spectral content\n * can be explored using the modulationIndex parameter. Read more about\n * frequency modulation synthesis on Sound On Sound: [Part 1](https://web.archive.org/web/20160403123704/http://www.soundonsound.com/sos/apr00/articles/synthsecrets.htm), [Part 2](https://web.archive.org/web/20160403115835/http://www.soundonsound.com/sos/may00/articles/synth.htm).\n *\n * @example\n * const fmSynth = new Tone.FMSynth().toDestination();\n * fmSynth.triggerAttackRelease("C5", "4n");\n *\n * @category Instrument\n */\nclass FMSynth_FMSynth extends ModulationSynth_ModulationSynth {\n    constructor() {\n        super(optionsFromArguments(FMSynth_FMSynth.getDefaults(), arguments));\n        this.name = "FMSynth";\n        const options = optionsFromArguments(FMSynth_FMSynth.getDefaults(), arguments);\n        this.modulationIndex = new Multiply_Multiply({\n            context: this.context,\n            value: options.modulationIndex,\n        });\n        // control the two voices frequency\n        this.frequency.connect(this._carrier.frequency);\n        this.frequency.chain(this.harmonicity, this._modulator.frequency);\n        this.frequency.chain(this.modulationIndex, this._modulationNode);\n        this.detune.fan(this._carrier.detune, this._modulator.detune);\n        this._modulator.connect(this._modulationNode.gain);\n        this._modulationNode.connect(this._carrier.frequency);\n        this._carrier.connect(this.output);\n    }\n    static getDefaults() {\n        return Object.assign(ModulationSynth_ModulationSynth.getDefaults(), {\n            modulationIndex: 10,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.modulationIndex.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FMSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/MetalSynth.js\n\n\n\n\n\n\n\n\n\n\n\n/**\n * Inharmonic ratio of frequencies based on the Roland TR-808\n * Taken from https://ccrma.stanford.edu/papers/tr-808-cymbal-physically-informed-circuit-bendable-digital-model\n */\nconst inharmRatios = [1.0, 1.483, 1.932, 2.546, 2.630, 3.897];\n/**\n * A highly inharmonic and spectrally complex source with a highpass filter\n * and amplitude envelope which is good for making metallophone sounds.\n * Based on CymbalSynth by [@polyrhythmatic](https://github.com/polyrhythmatic).\n * Inspiration from [Sound on Sound](https://shorturl.at/rSZ12).\n * @category Instrument\n */\nclass MetalSynth_MetalSynth extends Monophonic_Monophonic {\n    constructor() {\n        super(optionsFromArguments(MetalSynth_MetalSynth.getDefaults(), arguments));\n        this.name = "MetalSynth";\n        /**\n         * The array of FMOscillators\n         */\n        this._oscillators = [];\n        /**\n         * The frequency multipliers\n         */\n        this._freqMultipliers = [];\n        const options = optionsFromArguments(MetalSynth_MetalSynth.getDefaults(), arguments);\n        this.detune = new Signal_Signal({\n            context: this.context,\n            units: "cents",\n            value: options.detune,\n        });\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n        });\n        this._amplitude = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        }).connect(this.output);\n        this._highpass = new Filter_Filter({\n            // Q: -3.0102999566398125,\n            Q: 0,\n            context: this.context,\n            type: "highpass",\n        }).connect(this._amplitude);\n        for (let i = 0; i < inharmRatios.length; i++) {\n            const osc = new FMOscillator_FMOscillator({\n                context: this.context,\n                harmonicity: options.harmonicity,\n                modulationIndex: options.modulationIndex,\n                modulationType: "square",\n                onstop: i === 0 ? () => this.onsilence(this) : noOp,\n                type: "square",\n            });\n            osc.connect(this._highpass);\n            this._oscillators[i] = osc;\n            const mult = new Multiply_Multiply({\n                context: this.context,\n                value: inharmRatios[i],\n            });\n            this._freqMultipliers[i] = mult;\n            this.frequency.chain(mult, osc.frequency);\n            this.detune.connect(osc.detune);\n        }\n        this._filterFreqScaler = new Scale_Scale({\n            context: this.context,\n            max: 7000,\n            min: this.toFrequency(options.resonance),\n        });\n        this.envelope = new Envelope_Envelope({\n            attack: options.envelope.attack,\n            attackCurve: "linear",\n            context: this.context,\n            decay: options.envelope.decay,\n            release: options.envelope.release,\n            sustain: 0,\n        });\n        this.envelope.chain(this._filterFreqScaler, this._highpass.frequency);\n        this.envelope.connect(this._amplitude.gain);\n        // set the octaves\n        this._octaves = options.octaves;\n        this.octaves = options.octaves;\n    }\n    static getDefaults() {\n        return deepMerge(Monophonic_Monophonic.getDefaults(), {\n            envelope: Object.assign(omitFromObject(Envelope_Envelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                attack: 0.001,\n                decay: 1.4,\n                release: 0.2,\n            }),\n            harmonicity: 5.1,\n            modulationIndex: 32,\n            octaves: 1.5,\n            resonance: 4000,\n        });\n    }\n    /**\n     * Trigger the attack.\n     * @param time When the attack should be triggered.\n     * @param velocity The velocity that the envelope should be triggered at.\n     */\n    _triggerEnvelopeAttack(time, velocity = 1) {\n        this.envelope.triggerAttack(time, velocity);\n        this._oscillators.forEach(osc => osc.start(time));\n        if (this.envelope.sustain === 0) {\n            this._oscillators.forEach(osc => {\n                osc.stop(time + this.toSeconds(this.envelope.attack) + this.toSeconds(this.envelope.decay));\n            });\n        }\n        return this;\n    }\n    /**\n     * Trigger the release of the envelope.\n     * @param time When the release should be triggered.\n     */\n    _triggerEnvelopeRelease(time) {\n        this.envelope.triggerRelease(time);\n        this._oscillators.forEach(osc => osc.stop(time + this.toSeconds(this.envelope.release)));\n        return this;\n    }\n    getLevelAtTime(time) {\n        time = this.toSeconds(time);\n        return this.envelope.getValueAtTime(time);\n    }\n    /**\n     * The modulationIndex of the oscillators which make up the source.\n     * see [[FMOscillator.modulationIndex]]\n     * @min 1\n     * @max 100\n     */\n    get modulationIndex() {\n        return this._oscillators[0].modulationIndex.value;\n    }\n    set modulationIndex(val) {\n        this._oscillators.forEach(osc => (osc.modulationIndex.value = val));\n    }\n    /**\n     * The harmonicity of the oscillators which make up the source.\n     * see Tone.FMOscillator.harmonicity\n     * @min 0.1\n     * @max 10\n     */\n    get harmonicity() {\n        return this._oscillators[0].harmonicity.value;\n    }\n    set harmonicity(val) {\n        this._oscillators.forEach(osc => (osc.harmonicity.value = val));\n    }\n    /**\n     * The lower level of the highpass filter which is attached to the envelope.\n     * This value should be between [0, 7000]\n     * @min 0\n     * @max 7000\n     */\n    get resonance() {\n        return this._filterFreqScaler.min;\n    }\n    set resonance(val) {\n        this._filterFreqScaler.min = this.toFrequency(val);\n        this.octaves = this._octaves;\n    }\n    /**\n     * The number of octaves above the "resonance" frequency\n     * that the filter ramps during the attack/decay envelope\n     * @min 0\n     * @max 8\n     */\n    get octaves() {\n        return this._octaves;\n    }\n    set octaves(val) {\n        this._octaves = val;\n        this._filterFreqScaler.max = this._filterFreqScaler.min * Math.pow(2, val);\n    }\n    dispose() {\n        super.dispose();\n        this._oscillators.forEach(osc => osc.dispose());\n        this._freqMultipliers.forEach(freqMult => freqMult.dispose());\n        this.frequency.dispose();\n        this.detune.dispose();\n        this._filterFreqScaler.dispose();\n        this._amplitude.dispose();\n        this.envelope.dispose();\n        this._highpass.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MetalSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/MembraneSynth.js\n\n\n\n\n\n\n\n/**\n * MembraneSynth makes kick and tom sounds using a single oscillator\n * with an amplitude envelope and frequency ramp. A Tone.OmniOscillator\n * is routed through a Tone.AmplitudeEnvelope to the output. The drum\n * quality of the sound comes from the frequency envelope applied\n * during MembraneSynth.triggerAttack(note). The frequency envelope\n * starts at <code>note * .octaves</code> and ramps to <code>note</code>\n * over the duration of <code>.pitchDecay</code>.\n * @example\n * const synth = new Tone.MembraneSynth().toDestination();\n * synth.triggerAttackRelease("C2", "8n");\n * @category Instrument\n */\nclass MembraneSynth_MembraneSynth extends Synth_Synth {\n    constructor() {\n        super(optionsFromArguments(MembraneSynth_MembraneSynth.getDefaults(), arguments));\n        this.name = "MembraneSynth";\n        /**\n         * Portamento is ignored in this synth. use pitch decay instead.\n         */\n        this.portamento = 0;\n        const options = optionsFromArguments(MembraneSynth_MembraneSynth.getDefaults(), arguments);\n        this.pitchDecay = options.pitchDecay;\n        this.octaves = options.octaves;\n        readOnly(this, ["oscillator", "envelope"]);\n    }\n    static getDefaults() {\n        return deepMerge(Monophonic_Monophonic.getDefaults(), Synth_Synth.getDefaults(), {\n            envelope: {\n                attack: 0.001,\n                attackCurve: "exponential",\n                decay: 0.4,\n                release: 1.4,\n                sustain: 0.01,\n            },\n            octaves: 10,\n            oscillator: {\n                type: "sine",\n            },\n            pitchDecay: 0.05,\n        });\n    }\n    setNote(note, time) {\n        const seconds = this.toSeconds(time);\n        const hertz = this.toFrequency(note instanceof Frequency_FrequencyClass ? note.toFrequency() : note);\n        const maxNote = hertz * this.octaves;\n        this.oscillator.frequency.setValueAtTime(maxNote, seconds);\n        this.oscillator.frequency.exponentialRampToValueAtTime(hertz, seconds + this.toSeconds(this.pitchDecay));\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        return this;\n    }\n}\n__decorate([\n    Decorator_range(0)\n], MembraneSynth_MembraneSynth.prototype, "octaves", void 0);\n__decorate([\n    timeRange(0)\n], MembraneSynth_MembraneSynth.prototype, "pitchDecay", void 0);\n//# sourceMappingURL=MembraneSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/NoiseSynth.js\n\n\n\n\n\n\n\n/**\n * Tone.NoiseSynth is composed of [[Noise]] through an [[AmplitudeEnvelope]].\n * ```\n * +-------+   +-------------------+\n * | Noise +>--\x3e AmplitudeEnvelope +>--\x3e Output\n * +-------+   +-------------------+\n * ```\n * @example\n * const noiseSynth = new Tone.NoiseSynth().toDestination();\n * noiseSynth.triggerAttackRelease("8n", 0.05);\n * @category Instrument\n */\nclass NoiseSynth_NoiseSynth extends Instrument_Instrument {\n    constructor() {\n        super(optionsFromArguments(NoiseSynth_NoiseSynth.getDefaults(), arguments));\n        this.name = "NoiseSynth";\n        const options = optionsFromArguments(NoiseSynth_NoiseSynth.getDefaults(), arguments);\n        this.noise = new Noise_Noise(Object.assign({\n            context: this.context,\n        }, options.noise));\n        this.envelope = new AmplitudeEnvelope_AmplitudeEnvelope(Object.assign({\n            context: this.context,\n        }, options.envelope));\n        // connect the noise to the output\n        this.noise.chain(this.envelope, this.output);\n    }\n    static getDefaults() {\n        return Object.assign(Instrument_Instrument.getDefaults(), {\n            envelope: Object.assign(omitFromObject(Envelope_Envelope.getDefaults(), Object.keys(ToneAudioNode_ToneAudioNode.getDefaults())), {\n                decay: 0.1,\n                sustain: 0.0,\n            }),\n            noise: Object.assign(omitFromObject(Noise_Noise.getDefaults(), Object.keys(Source_Source.getDefaults())), {\n                type: "white",\n            }),\n        });\n    }\n    /**\n     * Start the attack portion of the envelopes. Unlike other\n     * instruments, Tone.NoiseSynth doesn\'t have a note.\n     * @example\n     * const noiseSynth = new Tone.NoiseSynth().toDestination();\n     * noiseSynth.triggerAttack();\n     */\n    triggerAttack(time, velocity = 1) {\n        time = this.toSeconds(time);\n        // the envelopes\n        this.envelope.triggerAttack(time, velocity);\n        // start the noise\n        this.noise.start(time);\n        if (this.envelope.sustain === 0) {\n            this.noise.stop(time + this.toSeconds(this.envelope.attack) + this.toSeconds(this.envelope.decay));\n        }\n        return this;\n    }\n    /**\n     * Start the release portion of the envelopes.\n     */\n    triggerRelease(time) {\n        time = this.toSeconds(time);\n        this.envelope.triggerRelease(time);\n        this.noise.stop(time + this.toSeconds(this.envelope.release));\n        return this;\n    }\n    sync() {\n        if (this._syncState()) {\n            this._syncMethod("triggerAttack", 0);\n            this._syncMethod("triggerRelease", 0);\n        }\n        return this;\n    }\n    triggerAttackRelease(duration, time, velocity = 1) {\n        time = this.toSeconds(time);\n        duration = this.toSeconds(duration);\n        this.triggerAttack(time, velocity);\n        this.triggerRelease(time + duration);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this.noise.dispose();\n        this.envelope.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=NoiseSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/worklet/WorkletGlobalScope.js\n/**\n * All of the classes or functions which are loaded into the AudioWorkletGlobalScope\n */\nconst workletContext = new Set();\n/**\n * Add a class to the AudioWorkletGlobalScope\n */\nfunction addToWorklet(classOrFunction) {\n    workletContext.add(classOrFunction);\n}\n/**\n * Register a processor in the AudioWorkletGlobalScope with the given name\n */\nfunction registerProcessor(name, classDesc) {\n    const processor = /* javascript */ `registerProcessor("${name}", ${classDesc})`;\n    workletContext.add(processor);\n}\n/**\n * Get all of the modules which have been registered to the AudioWorkletGlobalScope\n */\nfunction getWorkletGlobalScope() {\n    return Array.from(workletContext).join("\\n");\n}\n//# sourceMappingURL=WorkletGlobalScope.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/worklet/ToneAudioWorklet.js\n\n\n\nclass ToneAudioWorklet_ToneAudioWorklet extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        this.name = "ToneAudioWorklet";\n        /**\n         * The constructor options for the node\n         */\n        this.workletOptions = {};\n        /**\n         * Callback which is invoked when there is an error in the processing\n         */\n        this.onprocessorerror = noOp;\n        const blobUrl = URL.createObjectURL(new Blob([getWorkletGlobalScope()], { type: "text/javascript" }));\n        const name = this._audioWorkletName();\n        this._dummyGain = this.context.createGain();\n        this._dummyParam = this._dummyGain.gain;\n        // Register the processor\n        this.context.addAudioWorkletModule(blobUrl, name).then(() => {\n            // create the worklet when it\'s read\n            if (!this.disposed) {\n                this._worklet = this.context.createAudioWorkletNode(name, this.workletOptions);\n                this._worklet.onprocessorerror = this.onprocessorerror.bind(this);\n                this.onReady(this._worklet);\n            }\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._dummyGain.disconnect();\n        if (this._worklet) {\n            this._worklet.port.postMessage("dispose");\n            this._worklet.disconnect();\n        }\n        return this;\n    }\n}\n//# sourceMappingURL=ToneAudioWorklet.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/worklet/ToneAudioWorkletProcessor.worklet.js\n\nconst toneAudioWorkletProcessor = /* javascript */ `\n\t/**\n\t * The base AudioWorkletProcessor for use in Tone.js. Works with the [[ToneAudioWorklet]]. \n\t */\n\tclass ToneAudioWorkletProcessor extends AudioWorkletProcessor {\n\n\t\tconstructor(options) {\n\t\t\t\n\t\t\tsuper(options);\n\t\t\t/**\n\t\t\t * If the processor was disposed or not. Keep alive until it\'s disposed.\n\t\t\t */\n\t\t\tthis.disposed = false;\n\t\t   \t/** \n\t\t\t * The number of samples in the processing block\n\t\t\t */\n\t\t\tthis.blockSize = 128;\n\t\t\t/**\n\t\t\t * the sample rate\n\t\t\t */\n\t\t\tthis.sampleRate = sampleRate;\n\n\t\t\tthis.port.onmessage = (event) => {\n\t\t\t\t// when it receives a dispose \n\t\t\t\tif (event.data === "dispose") {\n\t\t\t\t\tthis.disposed = true;\n\t\t\t\t}\n\t\t\t};\n\t\t}\n\t}\n`;\naddToWorklet(toneAudioWorkletProcessor);\n//# sourceMappingURL=ToneAudioWorkletProcessor.worklet.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/worklet/SingleIOProcessor.worklet.js\n\n\nconst singleIOProcess = /* javascript */ `\n\t/**\n\t * Abstract class for a single input/output processor. \n\t * has a \'generate\' function which processes one sample at a time\n\t */\n\tclass SingleIOProcessor extends ToneAudioWorkletProcessor {\n\n\t\tconstructor(options) {\n\t\t\tsuper(Object.assign(options, {\n\t\t\t\tnumberOfInputs: 1,\n\t\t\t\tnumberOfOutputs: 1\n\t\t\t}));\n\t\t\t/**\n\t\t\t * Holds the name of the parameter and a single value of that\n\t\t\t * parameter at the current sample\n\t\t\t * @type { [name: string]: number }\n\t\t\t */\n\t\t\tthis.params = {}\n\t\t}\n\n\t\t/**\n\t\t * Generate an output sample from the input sample and parameters\n\t\t * @abstract\n\t\t * @param input number\n\t\t * @param channel number\n\t\t * @param parameters { [name: string]: number }\n\t\t * @returns number\n\t\t */\n\t\tgenerate(){}\n\n\t\t/**\n\t\t * Update the private params object with the \n\t\t * values of the parameters at the given index\n\t\t * @param parameters { [name: string]: Float32Array },\n\t\t * @param index number\n\t\t */\n\t\tupdateParams(parameters, index) {\n\t\t\tfor (const paramName in parameters) {\n\t\t\t\tconst param = parameters[paramName];\n\t\t\t\tif (param.length > 1) {\n\t\t\t\t\tthis.params[paramName] = parameters[paramName][index];\n\t\t\t\t} else {\n\t\t\t\t\tthis.params[paramName] = parameters[paramName][0];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t/**\n\t\t * Process a single frame of the audio\n\t\t * @param inputs Float32Array[][]\n\t\t * @param outputs Float32Array[][]\n\t\t */\n\t\tprocess(inputs, outputs, parameters) {\n\t\t\tconst input = inputs[0];\n\t\t\tconst output = outputs[0];\n\t\t\t// get the parameter values\n\t\t\tconst channelCount = Math.max(input && input.length || 0, output.length);\n\t\t\tfor (let sample = 0; sample < this.blockSize; sample++) {\n\t\t\t\tthis.updateParams(parameters, sample);\n\t\t\t\tfor (let channel = 0; channel < channelCount; channel++) {\n\t\t\t\t\tconst inputSample = input && input.length ? input[channel][sample] : 0;\n\t\t\t\t\toutput[channel][sample] = this.generate(inputSample, channel, this.params);\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn !this.disposed;\n\t\t}\n\t};\n`;\naddToWorklet(singleIOProcess);\n//# sourceMappingURL=SingleIOProcessor.worklet.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/worklet/DelayLine.worklet.js\n\nconst delayLine = /* javascript */ `\n\t/**\n\t * A multichannel buffer for use within an AudioWorkletProcessor as a delay line\n\t */\n\tclass DelayLine {\n\t\t\n\t\tconstructor(size, channels) {\n\t\t\tthis.buffer = [];\n\t\t\tthis.writeHead = []\n\t\t\tthis.size = size;\n\n\t\t\t// create the empty channels\n\t\t\tfor (let i = 0; i < channels; i++) {\n\t\t\t\tthis.buffer[i] = new Float32Array(this.size);\n\t\t\t\tthis.writeHead[i] = 0;\n\t\t\t}\n\t\t}\n\n\t\t/**\n\t\t * Push a value onto the end\n\t\t * @param channel number\n\t\t * @param value number\n\t\t */\n\t\tpush(channel, value) {\n\t\t\tthis.writeHead[channel] += 1;\n\t\t\tif (this.writeHead[channel] > this.size) {\n\t\t\t\tthis.writeHead[channel] = 0;\n\t\t\t}\n\t\t\tthis.buffer[channel][this.writeHead[channel]] = value;\n\t\t}\n\n\t\t/**\n\t\t * Get the recorded value of the channel given the delay\n\t\t * @param channel number\n\t\t * @param delay number delay samples\n\t\t */\n\t\tget(channel, delay) {\n\t\t\tlet readHead = this.writeHead[channel] - Math.floor(delay);\n\t\t\tif (readHead < 0) {\n\t\t\t\treadHead += this.size;\n\t\t\t}\n\t\t\treturn this.buffer[channel][readHead];\n\t\t}\n\t}\n`;\naddToWorklet(delayLine);\n//# sourceMappingURL=DelayLine.worklet.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/FeedbackCombFilter.worklet.js\n\n\n\nconst workletName = "feedback-comb-filter";\nconst feedbackCombFilter = /* javascript */ `\n\tclass FeedbackCombFilterWorklet extends SingleIOProcessor {\n\n\t\tconstructor(options) {\n\t\t\tsuper(options);\n\t\t\tthis.delayLine = new DelayLine(this.sampleRate, options.channelCount || 2);\n\t\t}\n\n\t\tstatic get parameterDescriptors() {\n\t\t\treturn [{\n\t\t\t\tname: "delayTime",\n\t\t\t\tdefaultValue: 0.1,\n\t\t\t\tminValue: 0,\n\t\t\t\tmaxValue: 1,\n\t\t\t\tautomationRate: "k-rate"\n\t\t\t}, {\n\t\t\t\tname: "feedback",\n\t\t\t\tdefaultValue: 0.5,\n\t\t\t\tminValue: 0,\n\t\t\t\tmaxValue: 0.9999,\n\t\t\t\tautomationRate: "k-rate"\n\t\t\t}];\n\t\t}\n\n\t\tgenerate(input, channel, parameters) {\n\t\t\tconst delayedSample = this.delayLine.get(channel, parameters.delayTime * this.sampleRate);\n\t\t\tthis.delayLine.push(channel, input + delayedSample * parameters.feedback);\n\t\t\treturn delayedSample;\n\t\t}\n\t}\n`;\nregisterProcessor(workletName, feedbackCombFilter);\n//# sourceMappingURL=FeedbackCombFilter.worklet.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/FeedbackCombFilter.js\n\n\n\n\n\n\n\n/**\n * Comb filters are basic building blocks for physical modeling. Read more\n * about comb filters on [CCRMA\'s website](https://ccrma.stanford.edu/~jos/pasp/Feedback_Comb_Filters.html).\n *\n * This comb filter is implemented with the AudioWorkletNode which allows it to have feedback delays less than the\n * Web Audio processing block of 128 samples. There is a polyfill for browsers that don\'t yet support the\n * AudioWorkletNode, but it will add some latency and have slower performance than the AudioWorkletNode.\n * @category Component\n */\nclass FeedbackCombFilter_FeedbackCombFilter extends ToneAudioWorklet_ToneAudioWorklet {\n    constructor() {\n        super(optionsFromArguments(FeedbackCombFilter_FeedbackCombFilter.getDefaults(), arguments, ["delayTime", "resonance"]));\n        this.name = "FeedbackCombFilter";\n        const options = optionsFromArguments(FeedbackCombFilter_FeedbackCombFilter.getDefaults(), arguments, ["delayTime", "resonance"]);\n        this.input = new Gain_Gain({ context: this.context });\n        this.output = new Gain_Gain({ context: this.context });\n        this.delayTime = new Param_Param({\n            context: this.context,\n            value: options.delayTime,\n            units: "time",\n            minValue: 0,\n            maxValue: 1,\n            param: this._dummyParam,\n            swappable: true,\n        });\n        this.resonance = new Param_Param({\n            context: this.context,\n            value: options.resonance,\n            units: "normalRange",\n            param: this._dummyParam,\n            swappable: true,\n        });\n        readOnly(this, ["resonance", "delayTime"]);\n    }\n    _audioWorkletName() {\n        return workletName;\n    }\n    /**\n     * The default parameters\n     */\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            delayTime: 0.1,\n            resonance: 0.5,\n        });\n    }\n    onReady(node) {\n        connectSeries(this.input, node, this.output);\n        const delayTime = node.parameters.get("delayTime");\n        ;\n        this.delayTime.setParam(delayTime);\n        const feedback = node.parameters.get("feedback");\n        ;\n        this.resonance.setParam(feedback);\n    }\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this.output.dispose();\n        this.delayTime.dispose();\n        this.resonance.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FeedbackCombFilter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/OnePoleFilter.js\n\n\n\n/**\n * A one pole filter with 6db-per-octave rolloff. Either "highpass" or "lowpass".\n * Note that changing the type or frequency may result in a discontinuity which\n * can sound like a click or pop.\n * References:\n * * http://www.earlevel.com/main/2012/12/15/a-one-pole-filter/\n * * http://www.dspguide.com/ch19/2.htm\n * * https://github.com/vitaliy-bobrov/js-rocks/blob/master/src/app/audio/effects/one-pole-filters.ts\n * @category Component\n */\nclass OnePoleFilter_OnePoleFilter extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(OnePoleFilter_OnePoleFilter.getDefaults(), arguments, ["frequency", "type"]));\n        this.name = "OnePoleFilter";\n        const options = optionsFromArguments(OnePoleFilter_OnePoleFilter.getDefaults(), arguments, ["frequency", "type"]);\n        this._frequency = options.frequency;\n        this._type = options.type;\n        this.input = new Gain_Gain({ context: this.context });\n        this.output = new Gain_Gain({ context: this.context });\n        this._createFilter();\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            frequency: 880,\n            type: "lowpass"\n        });\n    }\n    /**\n     * Create a filter and dispose the old one\n     */\n    _createFilter() {\n        const oldFilter = this._filter;\n        const freq = this.toFrequency(this._frequency);\n        const t = 1 / (2 * Math.PI * freq);\n        if (this._type === "lowpass") {\n            const a0 = 1 / (t * this.context.sampleRate);\n            const b1 = a0 - 1;\n            this._filter = this.context.createIIRFilter([a0, 0], [1, b1]);\n        }\n        else {\n            const b1 = 1 / (t * this.context.sampleRate) - 1;\n            this._filter = this.context.createIIRFilter([1, -1], [1, b1]);\n        }\n        this.input.chain(this._filter, this.output);\n        if (oldFilter) {\n            // dispose it on the next block\n            this.context.setTimeout(() => {\n                if (!this.disposed) {\n                    this.input.disconnect(oldFilter);\n                    oldFilter.disconnect();\n                }\n            }, this.blockTime);\n        }\n    }\n    /**\n     * The frequency value.\n     */\n    get frequency() {\n        return this._frequency;\n    }\n    set frequency(fq) {\n        this._frequency = fq;\n        this._createFilter();\n    }\n    /**\n     * The OnePole Filter type, either "highpass" or "lowpass"\n     */\n    get type() {\n        return this._type;\n    }\n    set type(t) {\n        this._type = t;\n        this._createFilter();\n    }\n    /**\n     * Get the frequency response curve. This curve represents how the filter\n     * responses to frequencies between 20hz-20khz.\n     * @param  len The number of values to return\n     * @return The frequency response curve between 20-20kHz\n     */\n    getFrequencyResponse(len = 128) {\n        const freqValues = new Float32Array(len);\n        for (let i = 0; i < len; i++) {\n            const norm = Math.pow(i / len, 2);\n            const freq = norm * (20000 - 20) + 20;\n            freqValues[i] = freq;\n        }\n        const magValues = new Float32Array(len);\n        const phaseValues = new Float32Array(len);\n        this._filter.getFrequencyResponse(freqValues, magValues, phaseValues);\n        return magValues;\n    }\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this.output.dispose();\n        this._filter.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=OnePoleFilter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/LowpassCombFilter.js\n\n\n\n\n/**\n * A lowpass feedback comb filter. It is similar to\n * [[FeedbackCombFilter]], but includes a lowpass filter.\n * @category Component\n */\nclass LowpassCombFilter_LowpassCombFilter extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(LowpassCombFilter_LowpassCombFilter.getDefaults(), arguments, ["delayTime", "resonance", "dampening"]));\n        this.name = "LowpassCombFilter";\n        const options = optionsFromArguments(LowpassCombFilter_LowpassCombFilter.getDefaults(), arguments, ["delayTime", "resonance", "dampening"]);\n        this._combFilter = this.output = new FeedbackCombFilter_FeedbackCombFilter({\n            context: this.context,\n            delayTime: options.delayTime,\n            resonance: options.resonance,\n        });\n        this.delayTime = this._combFilter.delayTime;\n        this.resonance = this._combFilter.resonance;\n        this._lowpass = this.input = new OnePoleFilter_OnePoleFilter({\n            context: this.context,\n            frequency: options.dampening,\n            type: "lowpass",\n        });\n        // connections\n        this._lowpass.connect(this._combFilter);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            dampening: 3000,\n            delayTime: 0.1,\n            resonance: 0.5,\n        });\n    }\n    /**\n     * The dampening control of the feedback\n     */\n    get dampening() {\n        return this._lowpass.frequency;\n    }\n    set dampening(fq) {\n        this._lowpass.frequency = fq;\n    }\n    dispose() {\n        super.dispose();\n        this._combFilter.dispose();\n        this._lowpass.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=LowpassCombFilter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/PluckSynth.js\n\n\n\n\n\n/**\n * Karplus-String string synthesis.\n * @example\n * const plucky = new Tone.PluckSynth().toDestination();\n * plucky.triggerAttack("C4", "+0.5");\n * plucky.triggerAttack("C3", "+1");\n * plucky.triggerAttack("C2", "+1.5");\n * plucky.triggerAttack("C1", "+2");\n * @category Instrument\n */\nclass PluckSynth_PluckSynth extends Instrument_Instrument {\n    constructor() {\n        super(optionsFromArguments(PluckSynth_PluckSynth.getDefaults(), arguments));\n        this.name = "PluckSynth";\n        const options = optionsFromArguments(PluckSynth_PluckSynth.getDefaults(), arguments);\n        this._noise = new Noise_Noise({\n            context: this.context,\n            type: "pink"\n        });\n        this.attackNoise = options.attackNoise;\n        this._lfcf = new LowpassCombFilter_LowpassCombFilter({\n            context: this.context,\n            dampening: options.dampening,\n            resonance: options.resonance,\n        });\n        this.resonance = options.resonance;\n        this.release = options.release;\n        this._noise.connect(this._lfcf);\n        this._lfcf.connect(this.output);\n    }\n    static getDefaults() {\n        return deepMerge(Instrument_Instrument.getDefaults(), {\n            attackNoise: 1,\n            dampening: 4000,\n            resonance: 0.7,\n            release: 1,\n        });\n    }\n    /**\n     * The dampening control. i.e. the lowpass filter frequency of the comb filter\n     * @min 0\n     * @max 7000\n     */\n    get dampening() {\n        return this._lfcf.dampening;\n    }\n    set dampening(fq) {\n        this._lfcf.dampening = fq;\n    }\n    triggerAttack(note, time) {\n        const freq = this.toFrequency(note);\n        time = this.toSeconds(time);\n        const delayAmount = 1 / freq;\n        this._lfcf.delayTime.setValueAtTime(delayAmount, time);\n        this._noise.start(time);\n        this._noise.stop(time + delayAmount * this.attackNoise);\n        this._lfcf.resonance.cancelScheduledValues(time);\n        this._lfcf.resonance.setValueAtTime(this.resonance, time);\n        return this;\n    }\n    /**\n     * Ramp down the [[resonance]] to 0 over the duration of the release time.\n     */\n    triggerRelease(time) {\n        this._lfcf.resonance.linearRampTo(0, this.release, time);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._noise.dispose();\n        this._lfcf.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=PluckSynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/PolySynth.js\n\n\n\n\n\n\n/**\n * PolySynth handles voice creation and allocation for any\n * instruments passed in as the second paramter. PolySynth is\n * not a synthesizer by itself, it merely manages voices of\n * one of the other types of synths, allowing any of the\n * monophonic synthesizers to be polyphonic.\n *\n * @example\n * const synth = new Tone.PolySynth().toDestination();\n * // set the attributes across all the voices using \'set\'\n * synth.set({ detune: -1200 });\n * // play a chord\n * synth.triggerAttackRelease(["C4", "E4", "A4"], 1);\n * @category Instrument\n */\nclass PolySynth_PolySynth extends Instrument_Instrument {\n    constructor() {\n        super(optionsFromArguments(PolySynth_PolySynth.getDefaults(), arguments, ["voice", "options"]));\n        this.name = "PolySynth";\n        /**\n         * The voices which are not currently in use\n         */\n        this._availableVoices = [];\n        /**\n         * The currently active voices\n         */\n        this._activeVoices = [];\n        /**\n         * All of the allocated voices for this synth.\n         */\n        this._voices = [];\n        /**\n         * The GC timeout. Held so that it could be cancelled when the node is disposed.\n         */\n        this._gcTimeout = -1;\n        /**\n         * A moving average of the number of active voices\n         */\n        this._averageActiveVoices = 0;\n        const options = optionsFromArguments(PolySynth_PolySynth.getDefaults(), arguments, ["voice", "options"]);\n        // check against the old API (pre 14.3.0)\n        assert(!isNumber(options.voice), "DEPRECATED: The polyphony count is no longer the first argument.");\n        const defaults = options.voice.getDefaults();\n        this.options = Object.assign(defaults, options.options);\n        this.voice = options.voice;\n        this.maxPolyphony = options.maxPolyphony;\n        // create the first voice\n        this._dummyVoice = this._getNextAvailableVoice();\n        // remove it from the voices list\n        const index = this._voices.indexOf(this._dummyVoice);\n        this._voices.splice(index, 1);\n        // kick off the GC interval\n        this._gcTimeout = this.context.setInterval(this._collectGarbage.bind(this), 1);\n    }\n    static getDefaults() {\n        return Object.assign(Instrument_Instrument.getDefaults(), {\n            maxPolyphony: 32,\n            options: {},\n            voice: Synth_Synth,\n        });\n    }\n    /**\n     * The number of active voices.\n     */\n    get activeVoices() {\n        return this._activeVoices.length;\n    }\n    /**\n     * Invoked when the source is done making sound, so that it can be\n     * readded to the pool of available voices\n     */\n    _makeVoiceAvailable(voice) {\n        this._availableVoices.push(voice);\n        // remove the midi note from \'active voices\'\n        const activeVoiceIndex = this._activeVoices.findIndex((e) => e.voice === voice);\n        this._activeVoices.splice(activeVoiceIndex, 1);\n    }\n    /**\n     * Get an available voice from the pool of available voices.\n     * If one is not available and the maxPolyphony limit is reached,\n     * steal a voice, otherwise return null.\n     */\n    _getNextAvailableVoice() {\n        // if there are available voices, return the first one\n        if (this._availableVoices.length) {\n            return this._availableVoices.shift();\n        }\n        else if (this._voices.length < this.maxPolyphony) {\n            // otherwise if there is still more maxPolyphony, make a new voice\n            const voice = new this.voice(Object.assign(this.options, {\n                context: this.context,\n                onsilence: this._makeVoiceAvailable.bind(this),\n            }));\n            voice.connect(this.output);\n            this._voices.push(voice);\n            return voice;\n        }\n        else {\n            warn("Max polyphony exceeded. Note dropped.");\n        }\n    }\n    /**\n     * Occasionally check if there are any allocated voices which can be cleaned up.\n     */\n    _collectGarbage() {\n        this._averageActiveVoices = Math.max(this._averageActiveVoices * 0.95, this.activeVoices);\n        if (this._availableVoices.length && this._voices.length > Math.ceil(this._averageActiveVoices + 1)) {\n            // take off an available note\n            const firstAvail = this._availableVoices.shift();\n            const index = this._voices.indexOf(firstAvail);\n            this._voices.splice(index, 1);\n            if (!this.context.isOffline) {\n                firstAvail.dispose();\n            }\n        }\n    }\n    /**\n     * Internal method which triggers the attack\n     */\n    _triggerAttack(notes, time, velocity) {\n        notes.forEach(note => {\n            const midiNote = new Midi_MidiClass(this.context, note).toMidi();\n            const voice = this._getNextAvailableVoice();\n            if (voice) {\n                voice.triggerAttack(note, time, velocity);\n                this._activeVoices.push({\n                    midi: midiNote, voice, released: false,\n                });\n                this.log("triggerAttack", note, time);\n            }\n        });\n    }\n    /**\n     * Internal method which triggers the release\n     */\n    _triggerRelease(notes, time) {\n        notes.forEach(note => {\n            const midiNote = new Midi_MidiClass(this.context, note).toMidi();\n            const event = this._activeVoices.find(({ midi, released }) => midi === midiNote && !released);\n            if (event) {\n                // trigger release on that note\n                event.voice.triggerRelease(time);\n                // mark it as released\n                event.released = true;\n                this.log("triggerRelease", note, time);\n            }\n        });\n    }\n    /**\n     * Schedule the attack/release events. If the time is in the future, then it should set a timeout\n     * to wait for just-in-time scheduling\n     */\n    _scheduleEvent(type, notes, time, velocity) {\n        assert(!this.disposed, "Synth was already disposed");\n        // if the notes are greater than this amount of time in the future, they should be scheduled with setTimeout\n        if (time <= this.now()) {\n            // do it immediately\n            if (type === "attack") {\n                this._triggerAttack(notes, time, velocity);\n            }\n            else {\n                this._triggerRelease(notes, time);\n            }\n        }\n        else {\n            // schedule it to start in the future\n            this.context.setTimeout(() => {\n                this._scheduleEvent(type, notes, time, velocity);\n            }, time - this.now());\n        }\n    }\n    /**\n     * Trigger the attack portion of the note\n     * @param  notes The notes to play. Accepts a single Frequency or an array of frequencies.\n     * @param  time  The start time of the note.\n     * @param velocity The velocity of the note.\n     * @example\n     * const synth = new Tone.PolySynth(Tone.FMSynth).toDestination();\n     * // trigger a chord immediately with a velocity of 0.2\n     * synth.triggerAttack(["Ab3", "C4", "F5"], Tone.now(), 0.2);\n     */\n    triggerAttack(notes, time, velocity) {\n        if (!Array.isArray(notes)) {\n            notes = [notes];\n        }\n        const computedTime = this.toSeconds(time);\n        this._scheduleEvent("attack", notes, computedTime, velocity);\n        return this;\n    }\n    /**\n     * Trigger the release of the note. Unlike monophonic instruments,\n     * a note (or array of notes) needs to be passed in as the first argument.\n     * @param  notes The notes to play. Accepts a single Frequency or an array of frequencies.\n     * @param  time  When the release will be triggered.\n     * @example\n     * @example\n     * const poly = new Tone.PolySynth(Tone.AMSynth).toDestination();\n     * poly.triggerAttack(["Ab3", "C4", "F5"]);\n     * // trigger the release of the given notes.\n     * poly.triggerRelease(["Ab3", "C4"], "+1");\n     * poly.triggerRelease("F5", "+3");\n     */\n    triggerRelease(notes, time) {\n        if (!Array.isArray(notes)) {\n            notes = [notes];\n        }\n        const computedTime = this.toSeconds(time);\n        this._scheduleEvent("release", notes, computedTime);\n        return this;\n    }\n    /**\n     * Trigger the attack and release after the specified duration\n     * @param  notes The notes to play. Accepts a single  Frequency or an array of frequencies.\n     * @param  duration the duration of the note\n     * @param  time  if no time is given, defaults to now\n     * @param  velocity the velocity of the attack (0-1)\n     * @example\n     * const poly = new Tone.PolySynth(Tone.AMSynth).toDestination();\n     * // can pass in an array of durations as well\n     * poly.triggerAttackRelease(["Eb3", "G4", "Bb4", "D5"], [4, 3, 2, 1]);\n     */\n    triggerAttackRelease(notes, duration, time, velocity) {\n        const computedTime = this.toSeconds(time);\n        this.triggerAttack(notes, computedTime, velocity);\n        if (isArray(duration)) {\n            assert(isArray(notes), "If the duration is an array, the notes must also be an array");\n            notes = notes;\n            for (let i = 0; i < notes.length; i++) {\n                const d = duration[Math.min(i, duration.length - 1)];\n                const durationSeconds = this.toSeconds(d);\n                assert(durationSeconds > 0, "The duration must be greater than 0");\n                this.triggerRelease(notes[i], computedTime + durationSeconds);\n            }\n        }\n        else {\n            const durationSeconds = this.toSeconds(duration);\n            assert(durationSeconds > 0, "The duration must be greater than 0");\n            this.triggerRelease(notes, computedTime + durationSeconds);\n        }\n        return this;\n    }\n    sync() {\n        if (this._syncState()) {\n            this._syncMethod("triggerAttack", 1);\n            this._syncMethod("triggerRelease", 1);\n        }\n        return this;\n    }\n    /**\n     * Set a member/attribute of the voices\n     * @example\n     * const poly = new Tone.PolySynth().toDestination();\n     * // set all of the voices using an options object for the synth type\n     * poly.set({\n     * \tenvelope: {\n     * \t\tattack: 0.25\n     * \t}\n     * });\n     * poly.triggerAttackRelease("Bb3", 0.2);\n     */\n    set(options) {\n        // remove options which are controlled by the PolySynth\n        const sanitizedOptions = omitFromObject(options, ["onsilence", "context"]);\n        // store all of the options\n        this.options = deepMerge(this.options, sanitizedOptions);\n        this._voices.forEach(voice => voice.set(sanitizedOptions));\n        this._dummyVoice.set(sanitizedOptions);\n        return this;\n    }\n    get() {\n        return this._dummyVoice.get();\n    }\n    /**\n     * Trigger the release portion of all the currently active voices immediately.\n     * Useful for silencing the synth.\n     */\n    releaseAll(time) {\n        const computedTime = this.toSeconds(time);\n        this._activeVoices.forEach(({ voice }) => {\n            voice.triggerRelease(computedTime);\n        });\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._dummyVoice.dispose();\n        this._voices.forEach(v => v.dispose());\n        this._activeVoices = [];\n        this._availableVoices = [];\n        this.context.clearInterval(this._gcTimeout);\n        return this;\n    }\n}\n//# sourceMappingURL=PolySynth.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/Sampler.js\n\n\n\n\n\n\n\n\n\n\n\n/**\n * Pass in an object which maps the note\'s pitch or midi value to the url,\n * then you can trigger the attack and release of that note like other instruments.\n * By automatically repitching the samples, it is possible to play pitches which\n * were not explicitly included which can save loading time.\n *\n * For sample or buffer playback where repitching is not necessary,\n * use [[Player]].\n * @example\n * const sampler = new Tone.Sampler({\n * \turls: {\n * \t\tA1: "A1.mp3",\n * \t\tA2: "A2.mp3",\n * \t},\n * \tbaseUrl: "https://tonejs.github.io/audio/casio/",\n * \tonload: () => {\n * \t\tsampler.triggerAttackRelease(["C1", "E1", "G1", "B1"], 0.5);\n * \t}\n * }).toDestination();\n * @category Instrument\n */\nclass Sampler_Sampler extends Instrument_Instrument {\n    constructor() {\n        super(optionsFromArguments(Sampler_Sampler.getDefaults(), arguments, ["urls", "onload", "baseUrl"], "urls"));\n        this.name = "Sampler";\n        /**\n         * The object of all currently playing BufferSources\n         */\n        this._activeSources = new Map();\n        const options = optionsFromArguments(Sampler_Sampler.getDefaults(), arguments, ["urls", "onload", "baseUrl"], "urls");\n        const urlMap = {};\n        Object.keys(options.urls).forEach((note) => {\n            const noteNumber = parseInt(note, 10);\n            assert(isNote(note)\n                || (isNumber(noteNumber) && isFinite(noteNumber)), `url key is neither a note or midi pitch: ${note}`);\n            if (isNote(note)) {\n                // convert the note name to MIDI\n                const mid = new Frequency_FrequencyClass(this.context, note).toMidi();\n                urlMap[mid] = options.urls[note];\n            }\n            else if (isNumber(noteNumber) && isFinite(noteNumber)) {\n                // otherwise if it\'s numbers assume it\'s midi\n                urlMap[noteNumber] = options.urls[noteNumber];\n            }\n        });\n        this._buffers = new ToneAudioBuffers_ToneAudioBuffers({\n            urls: urlMap,\n            onload: options.onload,\n            baseUrl: options.baseUrl,\n            onerror: options.onerror,\n        });\n        this.attack = options.attack;\n        this.release = options.release;\n        this.curve = options.curve;\n        // invoke the callback if it\'s already loaded\n        if (this._buffers.loaded) {\n            // invoke onload deferred\n            Promise.resolve().then(options.onload);\n        }\n    }\n    static getDefaults() {\n        return Object.assign(Instrument_Instrument.getDefaults(), {\n            attack: 0,\n            baseUrl: "",\n            curve: "exponential",\n            onload: noOp,\n            onerror: noOp,\n            release: 0.1,\n            urls: {},\n        });\n    }\n    /**\n     * Returns the difference in steps between the given midi note at the closets sample.\n     */\n    _findClosest(midi) {\n        // searches within 8 octaves of the given midi note\n        const MAX_INTERVAL = 96;\n        let interval = 0;\n        while (interval < MAX_INTERVAL) {\n            // check above and below\n            if (this._buffers.has(midi + interval)) {\n                return -interval;\n            }\n            else if (this._buffers.has(midi - interval)) {\n                return interval;\n            }\n            interval++;\n        }\n        throw new Error(`No available buffers for note: ${midi}`);\n    }\n    /**\n     * @param  notes\tThe note to play, or an array of notes.\n     * @param  time     When to play the note\n     * @param  velocity The velocity to play the sample back.\n     */\n    triggerAttack(notes, time, velocity = 1) {\n        this.log("triggerAttack", notes, time, velocity);\n        if (!Array.isArray(notes)) {\n            notes = [notes];\n        }\n        notes.forEach(note => {\n            const midiFloat = ftomf(new Frequency_FrequencyClass(this.context, note).toFrequency());\n            const midi = Math.round(midiFloat);\n            const remainder = midiFloat - midi;\n            // find the closest note pitch\n            const difference = this._findClosest(midi);\n            const closestNote = midi - difference;\n            const buffer = this._buffers.get(closestNote);\n            const playbackRate = intervalToFrequencyRatio(difference + remainder);\n            // play that note\n            const source = new ToneBufferSource_ToneBufferSource({\n                url: buffer,\n                context: this.context,\n                curve: this.curve,\n                fadeIn: this.attack,\n                fadeOut: this.release,\n                playbackRate,\n            }).connect(this.output);\n            source.start(time, 0, buffer.duration / playbackRate, velocity);\n            // add it to the active sources\n            if (!isArray(this._activeSources.get(midi))) {\n                this._activeSources.set(midi, []);\n            }\n            this._activeSources.get(midi).push(source);\n            // remove it when it\'s done\n            source.onended = () => {\n                if (this._activeSources && this._activeSources.has(midi)) {\n                    const sources = this._activeSources.get(midi);\n                    const index = sources.indexOf(source);\n                    if (index !== -1) {\n                        sources.splice(index, 1);\n                    }\n                }\n            };\n        });\n        return this;\n    }\n    /**\n     * @param  notes\tThe note to release, or an array of notes.\n     * @param  time     \tWhen to release the note.\n     */\n    triggerRelease(notes, time) {\n        this.log("triggerRelease", notes, time);\n        if (!Array.isArray(notes)) {\n            notes = [notes];\n        }\n        notes.forEach(note => {\n            const midi = new Frequency_FrequencyClass(this.context, note).toMidi();\n            // find the note\n            if (this._activeSources.has(midi) && this._activeSources.get(midi).length) {\n                const sources = this._activeSources.get(midi);\n                time = this.toSeconds(time);\n                sources.forEach(source => {\n                    source.stop(time);\n                });\n                this._activeSources.set(midi, []);\n            }\n        });\n        return this;\n    }\n    /**\n     * Release all currently active notes.\n     * @param  time     \tWhen to release the notes.\n     */\n    releaseAll(time) {\n        const computedTime = this.toSeconds(time);\n        this._activeSources.forEach(sources => {\n            while (sources.length) {\n                const source = sources.shift();\n                source.stop(computedTime);\n            }\n        });\n        return this;\n    }\n    sync() {\n        if (this._syncState()) {\n            this._syncMethod("triggerAttack", 1);\n            this._syncMethod("triggerRelease", 1);\n        }\n        return this;\n    }\n    /**\n     * Invoke the attack phase, then after the duration, invoke the release.\n     * @param  notes\tThe note to play and release, or an array of notes.\n     * @param  duration The time the note should be held\n     * @param  time     When to start the attack\n     * @param  velocity The velocity of the attack\n     */\n    triggerAttackRelease(notes, duration, time, velocity = 1) {\n        const computedTime = this.toSeconds(time);\n        this.triggerAttack(notes, computedTime, velocity);\n        if (isArray(duration)) {\n            assert(isArray(notes), "notes must be an array when duration is array");\n            notes.forEach((note, index) => {\n                const d = duration[Math.min(index, duration.length - 1)];\n                this.triggerRelease(note, computedTime + this.toSeconds(d));\n            });\n        }\n        else {\n            this.triggerRelease(notes, computedTime + this.toSeconds(duration));\n        }\n        return this;\n    }\n    /**\n     * Add a note to the sampler.\n     * @param  note      The buffer\'s pitch.\n     * @param  url  Either the url of the buffer, or a buffer which will be added with the given name.\n     * @param  callback  The callback to invoke when the url is loaded.\n     */\n    add(note, url, callback) {\n        assert(isNote(note) || isFinite(note), `note must be a pitch or midi: ${note}`);\n        if (isNote(note)) {\n            // convert the note name to MIDI\n            const mid = new Frequency_FrequencyClass(this.context, note).toMidi();\n            this._buffers.add(mid, url, callback);\n        }\n        else {\n            // otherwise if it\'s numbers assume it\'s midi\n            this._buffers.add(note, url, callback);\n        }\n        return this;\n    }\n    /**\n     * If the buffers are loaded or not\n     */\n    get loaded() {\n        return this._buffers.loaded;\n    }\n    /**\n     * Clean up\n     */\n    dispose() {\n        super.dispose();\n        this._buffers.dispose();\n        this._activeSources.forEach(sources => {\n            sources.forEach(source => source.dispose());\n        });\n        this._activeSources.clear();\n        return this;\n    }\n}\n__decorate([\n    timeRange(0)\n], Sampler_Sampler.prototype, "attack", void 0);\n__decorate([\n    timeRange(0)\n], Sampler_Sampler.prototype, "release", void 0);\n//# sourceMappingURL=Sampler.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/instrument/index.js\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/ToneEvent.js\n\n\n\n\n\n\n\n/**\n * ToneEvent abstracts away this.context.transport.schedule and provides a schedulable\n * callback for a single or repeatable events along the timeline.\n *\n * @example\n * const synth = new Tone.PolySynth().toDestination();\n * const chordEvent = new Tone.ToneEvent(((time, chord) => {\n * \t// the chord as well as the exact time of the event\n * \t// are passed in as arguments to the callback function\n * \tsynth.triggerAttackRelease(chord, 0.5, time);\n * }), ["D4", "E4", "F4"]);\n * // start the chord at the beginning of the transport timeline\n * chordEvent.start();\n * // loop it every measure for 8 measures\n * chordEvent.loop = 8;\n * chordEvent.loopEnd = "1m";\n * @category Event\n */\nclass ToneEvent_ToneEvent extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(optionsFromArguments(ToneEvent_ToneEvent.getDefaults(), arguments, ["callback", "value"]));\n        this.name = "ToneEvent";\n        /**\n         * Tracks the scheduled events\n         */\n        this._state = new StateTimeline_StateTimeline("stopped");\n        /**\n         * A delay time from when the event is scheduled to start\n         */\n        this._startOffset = 0;\n        const options = optionsFromArguments(ToneEvent_ToneEvent.getDefaults(), arguments, ["callback", "value"]);\n        this._loop = options.loop;\n        this.callback = options.callback;\n        this.value = options.value;\n        this._loopStart = this.toTicks(options.loopStart);\n        this._loopEnd = this.toTicks(options.loopEnd);\n        this._playbackRate = options.playbackRate;\n        this._probability = options.probability;\n        this._humanize = options.humanize;\n        this.mute = options.mute;\n        this._playbackRate = options.playbackRate;\n        this._state.increasing = true;\n        // schedule the events for the first time\n        this._rescheduleEvents();\n    }\n    static getDefaults() {\n        return Object.assign(ToneWithContext_ToneWithContext.getDefaults(), {\n            callback: noOp,\n            humanize: false,\n            loop: false,\n            loopEnd: "1m",\n            loopStart: 0,\n            mute: false,\n            playbackRate: 1,\n            probability: 1,\n            value: null,\n        });\n    }\n    /**\n     * Reschedule all of the events along the timeline\n     * with the updated values.\n     * @param after Only reschedules events after the given time.\n     */\n    _rescheduleEvents(after = -1) {\n        // if no argument is given, schedules all of the events\n        this._state.forEachFrom(after, event => {\n            let duration;\n            if (event.state === "started") {\n                if (event.id !== -1) {\n                    this.context.transport.clear(event.id);\n                }\n                const startTick = event.time + Math.round(this.startOffset / this._playbackRate);\n                if (this._loop === true || isNumber(this._loop) && this._loop > 1) {\n                    duration = Infinity;\n                    if (isNumber(this._loop)) {\n                        duration = (this._loop) * this._getLoopDuration();\n                    }\n                    const nextEvent = this._state.getAfter(startTick);\n                    if (nextEvent !== null) {\n                        duration = Math.min(duration, nextEvent.time - startTick);\n                    }\n                    if (duration !== Infinity) {\n                        // schedule a stop since it\'s finite duration\n                        this._state.setStateAtTime("stopped", startTick + duration + 1, { id: -1 });\n                        duration = new Ticks_TicksClass(this.context, duration);\n                    }\n                    const interval = new Ticks_TicksClass(this.context, this._getLoopDuration());\n                    event.id = this.context.transport.scheduleRepeat(this._tick.bind(this), interval, new Ticks_TicksClass(this.context, startTick), duration);\n                }\n                else {\n                    event.id = this.context.transport.schedule(this._tick.bind(this), new Ticks_TicksClass(this.context, startTick));\n                }\n            }\n        });\n    }\n    /**\n     * Returns the playback state of the note, either "started" or "stopped".\n     */\n    get state() {\n        return this._state.getValueAtTime(this.context.transport.ticks);\n    }\n    /**\n     * The start from the scheduled start time.\n     */\n    get startOffset() {\n        return this._startOffset;\n    }\n    set startOffset(offset) {\n        this._startOffset = offset;\n    }\n    /**\n     * The probability of the notes being triggered.\n     */\n    get probability() {\n        return this._probability;\n    }\n    set probability(prob) {\n        this._probability = prob;\n    }\n    /**\n     * If set to true, will apply small random variation\n     * to the callback time. If the value is given as a time, it will randomize\n     * by that amount.\n     * @example\n     * const event = new Tone.ToneEvent();\n     * event.humanize = true;\n     */\n    get humanize() {\n        return this._humanize;\n    }\n    set humanize(variation) {\n        this._humanize = variation;\n    }\n    /**\n     * Start the note at the given time.\n     * @param  time  When the event should start.\n     */\n    start(time) {\n        const ticks = this.toTicks(time);\n        if (this._state.getValueAtTime(ticks) === "stopped") {\n            this._state.add({\n                id: -1,\n                state: "started",\n                time: ticks,\n            });\n            this._rescheduleEvents(ticks);\n        }\n        return this;\n    }\n    /**\n     * Stop the Event at the given time.\n     * @param  time  When the event should stop.\n     */\n    stop(time) {\n        this.cancel(time);\n        const ticks = this.toTicks(time);\n        if (this._state.getValueAtTime(ticks) === "started") {\n            this._state.setStateAtTime("stopped", ticks, { id: -1 });\n            const previousEvent = this._state.getBefore(ticks);\n            let reschedulTime = ticks;\n            if (previousEvent !== null) {\n                reschedulTime = previousEvent.time;\n            }\n            this._rescheduleEvents(reschedulTime);\n        }\n        return this;\n    }\n    /**\n     * Cancel all scheduled events greater than or equal to the given time\n     * @param  time  The time after which events will be cancel.\n     */\n    cancel(time) {\n        time = defaultArg(time, -Infinity);\n        const ticks = this.toTicks(time);\n        this._state.forEachFrom(ticks, event => {\n            this.context.transport.clear(event.id);\n        });\n        this._state.cancel(ticks);\n        return this;\n    }\n    /**\n     * The callback function invoker. Also\n     * checks if the Event is done playing\n     * @param  time  The time of the event in seconds\n     */\n    _tick(time) {\n        const ticks = this.context.transport.getTicksAtTime(time);\n        if (!this.mute && this._state.getValueAtTime(ticks) === "started") {\n            if (this.probability < 1 && Math.random() > this.probability) {\n                return;\n            }\n            if (this.humanize) {\n                let variation = 0.02;\n                if (!isBoolean(this.humanize)) {\n                    variation = this.toSeconds(this.humanize);\n                }\n                time += (Math.random() * 2 - 1) * variation;\n            }\n            this.callback(time, this.value);\n        }\n    }\n    /**\n     * Get the duration of the loop.\n     */\n    _getLoopDuration() {\n        return Math.round((this._loopEnd - this._loopStart) / this._playbackRate);\n    }\n    /**\n     * If the note should loop or not\n     * between ToneEvent.loopStart and\n     * ToneEvent.loopEnd. If set to true,\n     * the event will loop indefinitely,\n     * if set to a number greater than 1\n     * it will play a specific number of\n     * times, if set to false, 0 or 1, the\n     * part will only play once.\n     */\n    get loop() {\n        return this._loop;\n    }\n    set loop(loop) {\n        this._loop = loop;\n        this._rescheduleEvents();\n    }\n    /**\n     * The playback rate of the note. Defaults to 1.\n     * @example\n     * const note = new Tone.ToneEvent();\n     * note.loop = true;\n     * // repeat the note twice as fast\n     * note.playbackRate = 2;\n     */\n    get playbackRate() {\n        return this._playbackRate;\n    }\n    set playbackRate(rate) {\n        this._playbackRate = rate;\n        this._rescheduleEvents();\n    }\n    /**\n     * The loopEnd point is the time the event will loop\n     * if ToneEvent.loop is true.\n     */\n    get loopEnd() {\n        return new Ticks_TicksClass(this.context, this._loopEnd).toSeconds();\n    }\n    set loopEnd(loopEnd) {\n        this._loopEnd = this.toTicks(loopEnd);\n        if (this._loop) {\n            this._rescheduleEvents();\n        }\n    }\n    /**\n     * The time when the loop should start.\n     */\n    get loopStart() {\n        return new Ticks_TicksClass(this.context, this._loopStart).toSeconds();\n    }\n    set loopStart(loopStart) {\n        this._loopStart = this.toTicks(loopStart);\n        if (this._loop) {\n            this._rescheduleEvents();\n        }\n    }\n    /**\n     * The current progress of the loop interval.\n     * Returns 0 if the event is not started yet or\n     * it is not set to loop.\n     */\n    get progress() {\n        if (this._loop) {\n            const ticks = this.context.transport.ticks;\n            const lastEvent = this._state.get(ticks);\n            if (lastEvent !== null && lastEvent.state === "started") {\n                const loopDuration = this._getLoopDuration();\n                const progress = (ticks - lastEvent.time) % loopDuration;\n                return progress / loopDuration;\n            }\n            else {\n                return 0;\n            }\n        }\n        else {\n            return 0;\n        }\n    }\n    dispose() {\n        super.dispose();\n        this.cancel();\n        this._state.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=ToneEvent.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/Loop.js\n\n\n\n\n/**\n * Loop creates a looped callback at the\n * specified interval. The callback can be\n * started, stopped and scheduled along\n * the Transport\'s timeline.\n * @example\n * const loop = new Tone.Loop((time) => {\n * \t// triggered every eighth note.\n * \tconsole.log(time);\n * }, "8n").start(0);\n * Tone.Transport.start();\n * @category Event\n */\nclass Loop_Loop extends ToneWithContext_ToneWithContext {\n    constructor() {\n        super(optionsFromArguments(Loop_Loop.getDefaults(), arguments, ["callback", "interval"]));\n        this.name = "Loop";\n        const options = optionsFromArguments(Loop_Loop.getDefaults(), arguments, ["callback", "interval"]);\n        this._event = new ToneEvent_ToneEvent({\n            context: this.context,\n            callback: this._tick.bind(this),\n            loop: true,\n            loopEnd: options.interval,\n            playbackRate: options.playbackRate,\n            probability: options.probability\n        });\n        this.callback = options.callback;\n        // set the iterations\n        this.iterations = options.iterations;\n    }\n    static getDefaults() {\n        return Object.assign(ToneWithContext_ToneWithContext.getDefaults(), {\n            interval: "4n",\n            callback: noOp,\n            playbackRate: 1,\n            iterations: Infinity,\n            probability: 1,\n            mute: false,\n            humanize: false\n        });\n    }\n    /**\n     * Start the loop at the specified time along the Transport\'s timeline.\n     * @param  time  When to start the Loop.\n     */\n    start(time) {\n        this._event.start(time);\n        return this;\n    }\n    /**\n     * Stop the loop at the given time.\n     * @param  time  When to stop the Loop.\n     */\n    stop(time) {\n        this._event.stop(time);\n        return this;\n    }\n    /**\n     * Cancel all scheduled events greater than or equal to the given time\n     * @param  time  The time after which events will be cancel.\n     */\n    cancel(time) {\n        this._event.cancel(time);\n        return this;\n    }\n    /**\n     * Internal function called when the notes should be called\n     * @param time  The time the event occurs\n     */\n    _tick(time) {\n        this.callback(time);\n    }\n    /**\n     * The state of the Loop, either started or stopped.\n     */\n    get state() {\n        return this._event.state;\n    }\n    /**\n     * The progress of the loop as a value between 0-1. 0, when the loop is stopped or done iterating.\n     */\n    get progress() {\n        return this._event.progress;\n    }\n    /**\n     * The time between successive callbacks.\n     * @example\n     * const loop = new Tone.Loop();\n     * loop.interval = "8n"; // loop every 8n\n     */\n    get interval() {\n        return this._event.loopEnd;\n    }\n    set interval(interval) {\n        this._event.loopEnd = interval;\n    }\n    /**\n     * The playback rate of the loop. The normal playback rate is 1 (no change).\n     * A `playbackRate` of 2 would be twice as fast.\n     */\n    get playbackRate() {\n        return this._event.playbackRate;\n    }\n    set playbackRate(rate) {\n        this._event.playbackRate = rate;\n    }\n    /**\n     * Random variation +/-0.01s to the scheduled time.\n     * Or give it a time value which it will randomize by.\n     */\n    get humanize() {\n        return this._event.humanize;\n    }\n    set humanize(variation) {\n        this._event.humanize = variation;\n    }\n    /**\n     * The probably of the callback being invoked.\n     */\n    get probability() {\n        return this._event.probability;\n    }\n    set probability(prob) {\n        this._event.probability = prob;\n    }\n    /**\n     * Muting the Loop means that no callbacks are invoked.\n     */\n    get mute() {\n        return this._event.mute;\n    }\n    set mute(mute) {\n        this._event.mute = mute;\n    }\n    /**\n     * The number of iterations of the loop. The default value is `Infinity` (loop forever).\n     */\n    get iterations() {\n        if (this._event.loop === true) {\n            return Infinity;\n        }\n        else {\n            return this._event.loop;\n        }\n    }\n    set iterations(iters) {\n        if (iters === Infinity) {\n            this._event.loop = true;\n        }\n        else {\n            this._event.loop = iters;\n        }\n    }\n    dispose() {\n        super.dispose();\n        this._event.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Loop.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/Part.js\n\n\n\n\n\n\n/**\n * Part is a collection ToneEvents which can be started/stopped and looped as a single unit.\n *\n * @example\n * const synth = new Tone.Synth().toDestination();\n * const part = new Tone.Part(((time, note) => {\n * \t// the notes given as the second element in the array\n * \t// will be passed in as the second argument\n * \tsynth.triggerAttackRelease(note, "8n", time);\n * }), [[0, "C2"], ["0:2", "C3"], ["0:3:2", "G2"]]);\n * Tone.Transport.start();\n * @example\n * const synth = new Tone.Synth().toDestination();\n * // use an array of objects as long as the object has a "time" attribute\n * const part = new Tone.Part(((time, value) => {\n * \t// the value is an object which contains both the note and the velocity\n * \tsynth.triggerAttackRelease(value.note, "8n", time, value.velocity);\n * }), [{ time: 0, note: "C3", velocity: 0.9 },\n * \t{ time: "0:2", note: "C4", velocity: 0.5 }\n * ]).start(0);\n * Tone.Transport.start();\n * @category Event\n */\nclass Part_Part extends ToneEvent_ToneEvent {\n    constructor() {\n        super(optionsFromArguments(Part_Part.getDefaults(), arguments, ["callback", "events"]));\n        this.name = "Part";\n        /**\n         * Tracks the scheduled events\n         */\n        this._state = new StateTimeline_StateTimeline("stopped");\n        /**\n         * The events that belong to this part\n         */\n        this._events = new Set();\n        const options = optionsFromArguments(Part_Part.getDefaults(), arguments, ["callback", "events"]);\n        // make sure things are assigned in the right order\n        this._state.increasing = true;\n        // add the events\n        options.events.forEach(event => {\n            if (isArray(event)) {\n                this.add(event[0], event[1]);\n            }\n            else {\n                this.add(event);\n            }\n        });\n    }\n    static getDefaults() {\n        return Object.assign(ToneEvent_ToneEvent.getDefaults(), {\n            events: [],\n        });\n    }\n    /**\n     * Start the part at the given time.\n     * @param  time    When to start the part.\n     * @param  offset  The offset from the start of the part to begin playing at.\n     */\n    start(time, offset) {\n        const ticks = this.toTicks(time);\n        if (this._state.getValueAtTime(ticks) !== "started") {\n            offset = defaultArg(offset, this._loop ? this._loopStart : 0);\n            if (this._loop) {\n                offset = defaultArg(offset, this._loopStart);\n            }\n            else {\n                offset = defaultArg(offset, 0);\n            }\n            const computedOffset = this.toTicks(offset);\n            this._state.add({\n                id: -1,\n                offset: computedOffset,\n                state: "started",\n                time: ticks,\n            });\n            this._forEach(event => {\n                this._startNote(event, ticks, computedOffset);\n            });\n        }\n        return this;\n    }\n    /**\n     * Start the event in the given event at the correct time given\n     * the ticks and offset and looping.\n     * @param  event\n     * @param  ticks\n     * @param  offset\n     */\n    _startNote(event, ticks, offset) {\n        ticks -= offset;\n        if (this._loop) {\n            if (event.startOffset >= this._loopStart && event.startOffset < this._loopEnd) {\n                if (event.startOffset < offset) {\n                    // start it on the next loop\n                    ticks += this._getLoopDuration();\n                }\n                event.start(new Ticks_TicksClass(this.context, ticks));\n            }\n            else if (event.startOffset < this._loopStart && event.startOffset >= offset) {\n                event.loop = false;\n                event.start(new Ticks_TicksClass(this.context, ticks));\n            }\n        }\n        else if (event.startOffset >= offset) {\n            event.start(new Ticks_TicksClass(this.context, ticks));\n        }\n    }\n    get startOffset() {\n        return this._startOffset;\n    }\n    set startOffset(offset) {\n        this._startOffset = offset;\n        this._forEach(event => {\n            event.startOffset += this._startOffset;\n        });\n    }\n    /**\n     * Stop the part at the given time.\n     * @param  time  When to stop the part.\n     */\n    stop(time) {\n        const ticks = this.toTicks(time);\n        this._state.cancel(ticks);\n        this._state.setStateAtTime("stopped", ticks);\n        this._forEach(event => {\n            event.stop(time);\n        });\n        return this;\n    }\n    /**\n     * Get/Set an Event\'s value at the given time.\n     * If a value is passed in and no event exists at\n     * the given time, one will be created with that value.\n     * If two events are at the same time, the first one will\n     * be returned.\n     * @example\n     * const part = new Tone.Part();\n     * part.at("1m"); // returns the part at the first measure\n     * part.at("2m", "C2"); // set the value at "2m" to C2.\n     * // if an event didn\'t exist at that time, it will be created.\n     * @param time The time of the event to get or set.\n     * @param value If a value is passed in, the value of the event at the given time will be set to it.\n     */\n    at(time, value) {\n        const timeInTicks = new TransportTime_TransportTimeClass(this.context, time).toTicks();\n        const tickTime = new Ticks_TicksClass(this.context, 1).toSeconds();\n        const iterator = this._events.values();\n        let result = iterator.next();\n        while (!result.done) {\n            const event = result.value;\n            if (Math.abs(timeInTicks - event.startOffset) < tickTime) {\n                if (isDefined(value)) {\n                    event.value = value;\n                }\n                return event;\n            }\n            result = iterator.next();\n        }\n        // if there was no event at that time, create one\n        if (isDefined(value)) {\n            this.add(time, value);\n            // return the new event\n            return this.at(time);\n        }\n        else {\n            return null;\n        }\n    }\n    add(time, value) {\n        // extract the parameters\n        if (time instanceof Object && Reflect.has(time, "time")) {\n            value = time;\n            time = value.time;\n        }\n        const ticks = this.toTicks(time);\n        let event;\n        if (value instanceof ToneEvent_ToneEvent) {\n            event = value;\n            event.callback = this._tick.bind(this);\n        }\n        else {\n            event = new ToneEvent_ToneEvent({\n                callback: this._tick.bind(this),\n                context: this.context,\n                value,\n            });\n        }\n        // the start offset\n        event.startOffset = ticks;\n        // initialize the values\n        event.set({\n            humanize: this.humanize,\n            loop: this.loop,\n            loopEnd: this.loopEnd,\n            loopStart: this.loopStart,\n            playbackRate: this.playbackRate,\n            probability: this.probability,\n        });\n        this._events.add(event);\n        // start the note if it should be played right now\n        this._restartEvent(event);\n        return this;\n    }\n    /**\n     * Restart the given event\n     */\n    _restartEvent(event) {\n        this._state.forEach((stateEvent) => {\n            if (stateEvent.state === "started") {\n                this._startNote(event, stateEvent.time, stateEvent.offset);\n            }\n            else {\n                // stop the note\n                event.stop(new Ticks_TicksClass(this.context, stateEvent.time));\n            }\n        });\n    }\n    remove(time, value) {\n        // extract the parameters\n        if (isObject(time) && time.hasOwnProperty("time")) {\n            value = time;\n            time = value.time;\n        }\n        time = this.toTicks(time);\n        this._events.forEach(event => {\n            if (event.startOffset === time) {\n                if (isUndef(value) || (isDefined(value) && event.value === value)) {\n                    this._events.delete(event);\n                    event.dispose();\n                }\n            }\n        });\n        return this;\n    }\n    /**\n     * Remove all of the notes from the group.\n     */\n    clear() {\n        this._forEach(event => event.dispose());\n        this._events.clear();\n        return this;\n    }\n    /**\n     * Cancel scheduled state change events: i.e. "start" and "stop".\n     * @param after The time after which to cancel the scheduled events.\n     */\n    cancel(after) {\n        this._forEach(event => event.cancel(after));\n        this._state.cancel(this.toTicks(after));\n        return this;\n    }\n    /**\n     * Iterate over all of the events\n     */\n    _forEach(callback) {\n        if (this._events) {\n            this._events.forEach(event => {\n                if (event instanceof Part_Part) {\n                    event._forEach(callback);\n                }\n                else {\n                    callback(event);\n                }\n            });\n        }\n        return this;\n    }\n    /**\n     * Set the attribute of all of the events\n     * @param  attr  the attribute to set\n     * @param  value      The value to set it to\n     */\n    _setAll(attr, value) {\n        this._forEach(event => {\n            event[attr] = value;\n        });\n    }\n    /**\n     * Internal tick method\n     * @param  time  The time of the event in seconds\n     */\n    _tick(time, value) {\n        if (!this.mute) {\n            this.callback(time, value);\n        }\n    }\n    /**\n     * Determine if the event should be currently looping\n     * given the loop boundries of this Part.\n     * @param  event  The event to test\n     */\n    _testLoopBoundries(event) {\n        if (this._loop && (event.startOffset < this._loopStart || event.startOffset >= this._loopEnd)) {\n            event.cancel(0);\n        }\n        else if (event.state === "stopped") {\n            // reschedule it if it\'s stopped\n            this._restartEvent(event);\n        }\n    }\n    get probability() {\n        return this._probability;\n    }\n    set probability(prob) {\n        this._probability = prob;\n        this._setAll("probability", prob);\n    }\n    get humanize() {\n        return this._humanize;\n    }\n    set humanize(variation) {\n        this._humanize = variation;\n        this._setAll("humanize", variation);\n    }\n    /**\n     * If the part should loop or not\n     * between Part.loopStart and\n     * Part.loopEnd. If set to true,\n     * the part will loop indefinitely,\n     * if set to a number greater than 1\n     * it will play a specific number of\n     * times, if set to false, 0 or 1, the\n     * part will only play once.\n     * @example\n     * const part = new Tone.Part();\n     * // loop the part 8 times\n     * part.loop = 8;\n     */\n    get loop() {\n        return this._loop;\n    }\n    set loop(loop) {\n        this._loop = loop;\n        this._forEach(event => {\n            event.loopStart = this.loopStart;\n            event.loopEnd = this.loopEnd;\n            event.loop = loop;\n            this._testLoopBoundries(event);\n        });\n    }\n    /**\n     * The loopEnd point determines when it will\n     * loop if Part.loop is true.\n     */\n    get loopEnd() {\n        return new Ticks_TicksClass(this.context, this._loopEnd).toSeconds();\n    }\n    set loopEnd(loopEnd) {\n        this._loopEnd = this.toTicks(loopEnd);\n        if (this._loop) {\n            this._forEach(event => {\n                event.loopEnd = loopEnd;\n                this._testLoopBoundries(event);\n            });\n        }\n    }\n    /**\n     * The loopStart point determines when it will\n     * loop if Part.loop is true.\n     */\n    get loopStart() {\n        return new Ticks_TicksClass(this.context, this._loopStart).toSeconds();\n    }\n    set loopStart(loopStart) {\n        this._loopStart = this.toTicks(loopStart);\n        if (this._loop) {\n            this._forEach(event => {\n                event.loopStart = this.loopStart;\n                this._testLoopBoundries(event);\n            });\n        }\n    }\n    /**\n     * The playback rate of the part\n     */\n    get playbackRate() {\n        return this._playbackRate;\n    }\n    set playbackRate(rate) {\n        this._playbackRate = rate;\n        this._setAll("playbackRate", rate);\n    }\n    /**\n     * The number of scheduled notes in the part.\n     */\n    get length() {\n        return this._events.size;\n    }\n    dispose() {\n        super.dispose();\n        this.clear();\n        return this;\n    }\n}\n//# sourceMappingURL=Part.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/PatternGenerator.js\n\n\n/**\n * Start at the first value and go up to the last\n */\nfunction* upPatternGen(values) {\n    let index = 0;\n    while (index < values.length) {\n        index = clampToArraySize(index, values);\n        yield values[index];\n        index++;\n    }\n}\n/**\n * Start at the last value and go down to 0\n */\nfunction* downPatternGen(values) {\n    let index = values.length - 1;\n    while (index >= 0) {\n        index = clampToArraySize(index, values);\n        yield values[index];\n        index--;\n    }\n}\n/**\n * Infinitely yield the generator\n */\nfunction* infiniteGen(values, gen) {\n    while (true) {\n        yield* gen(values);\n    }\n}\n/**\n * Make sure that the index is in the given range\n */\nfunction clampToArraySize(index, values) {\n    return clamp(index, 0, values.length - 1);\n}\n/**\n * Alternate between two generators\n */\nfunction* alternatingGenerator(values, directionUp) {\n    let index = directionUp ? 0 : values.length - 1;\n    while (true) {\n        index = clampToArraySize(index, values);\n        yield values[index];\n        if (directionUp) {\n            index++;\n            if (index >= values.length - 1) {\n                directionUp = false;\n            }\n        }\n        else {\n            index--;\n            if (index <= 0) {\n                directionUp = true;\n            }\n        }\n    }\n}\n/**\n * Starting from the bottom move up 2, down 1\n */\nfunction* jumpUp(values) {\n    let index = 0;\n    let stepIndex = 0;\n    while (index < values.length) {\n        index = clampToArraySize(index, values);\n        yield values[index];\n        stepIndex++;\n        index += (stepIndex % 2 ? 2 : -1);\n    }\n}\n/**\n * Starting from the top move down 2, up 1\n */\nfunction* jumpDown(values) {\n    let index = values.length - 1;\n    let stepIndex = 0;\n    while (index >= 0) {\n        index = clampToArraySize(index, values);\n        yield values[index];\n        stepIndex++;\n        index += (stepIndex % 2 ? -2 : 1);\n    }\n}\n/**\n * Choose a random index each time\n */\nfunction* randomGen(values) {\n    while (true) {\n        const randomIndex = Math.floor(Math.random() * values.length);\n        yield values[randomIndex];\n    }\n}\n/**\n * Randomly go through all of the values once before choosing a new random order\n */\nfunction* randomOnce(values) {\n    // create an array of indices\n    const copy = [];\n    for (let i = 0; i < values.length; i++) {\n        copy.push(i);\n    }\n    while (copy.length > 0) {\n        // random choose an index, and then remove it so it\'s not chosen again\n        const randVal = copy.splice(Math.floor(copy.length * Math.random()), 1);\n        const index = clampToArraySize(randVal[0], values);\n        yield values[index];\n    }\n}\n/**\n * Randomly choose to walk up or down 1 index in the values array\n */\nfunction* randomWalk(values) {\n    // randomly choose a starting index in the values array\n    let index = Math.floor(Math.random() * values.length);\n    while (true) {\n        if (index === 0) {\n            index++; // at bottom of array, so force upward step\n        }\n        else if (index === values.length - 1) {\n            index--; // at top of array, so force downward step\n        }\n        else if (Math.random() < 0.5) { // else choose random downward or upward step\n            index--;\n        }\n        else {\n            index++;\n        }\n        yield values[index];\n    }\n}\n/**\n * PatternGenerator returns a generator which will iterate over the given array\n * of values and yield the items according to the passed in pattern\n * @param values An array of values to iterate over\n * @param pattern The name of the pattern use when iterating over\n * @param index Where to start in the offset of the values array\n */\nfunction* PatternGenerator(values, pattern = "up", index = 0) {\n    // safeguards\n    assert(values.length > 0, "The array must have more than one value in it");\n    switch (pattern) {\n        case "up":\n            yield* infiniteGen(values, upPatternGen);\n        case "down":\n            yield* infiniteGen(values, downPatternGen);\n        case "upDown":\n            yield* alternatingGenerator(values, true);\n        case "downUp":\n            yield* alternatingGenerator(values, false);\n        case "alternateUp":\n            yield* infiniteGen(values, jumpUp);\n        case "alternateDown":\n            yield* infiniteGen(values, jumpDown);\n        case "random":\n            yield* randomGen(values);\n        case "randomOnce":\n            yield* infiniteGen(values, randomOnce);\n        case "randomWalk":\n            yield* randomWalk(values);\n    }\n}\n//# sourceMappingURL=PatternGenerator.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/Pattern.js\n\n\n\n\n/**\n * Pattern arpeggiates between the given notes\n * in a number of patterns.\n * @example\n * const pattern = new Tone.Pattern((time, note) => {\n * \t// the order of the notes passed in depends on the pattern\n * }, ["C2", "D4", "E5", "A6"], "upDown");\n * @category Event\n */\nclass Pattern_Pattern extends Loop_Loop {\n    constructor() {\n        super(optionsFromArguments(Pattern_Pattern.getDefaults(), arguments, ["callback", "values", "pattern"]));\n        this.name = "Pattern";\n        const options = optionsFromArguments(Pattern_Pattern.getDefaults(), arguments, ["callback", "values", "pattern"]);\n        this.callback = options.callback;\n        this._values = options.values;\n        this._pattern = PatternGenerator(options.values, options.pattern);\n        this._type = options.pattern;\n    }\n    static getDefaults() {\n        return Object.assign(Loop_Loop.getDefaults(), {\n            pattern: "up",\n            values: [],\n            callback: noOp,\n        });\n    }\n    /**\n     * Internal function called when the notes should be called\n     */\n    _tick(time) {\n        const value = this._pattern.next();\n        this._value = value.value;\n        this.callback(time, this._value);\n    }\n    /**\n     * The array of events.\n     */\n    get values() {\n        return this._values;\n    }\n    set values(val) {\n        this._values = val;\n        // reset the pattern\n        this.pattern = this._type;\n    }\n    /**\n     * The current value of the pattern.\n     */\n    get value() {\n        return this._value;\n    }\n    /**\n     * The pattern type. See Tone.CtrlPattern for the full list of patterns.\n     */\n    get pattern() {\n        return this._type;\n    }\n    set pattern(pattern) {\n        this._type = pattern;\n        this._pattern = PatternGenerator(this._values, this._type);\n    }\n}\n//# sourceMappingURL=Pattern.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/Sequence.js\n\n\n\n\n\n/**\n * A sequence is an alternate notation of a part. Instead\n * of passing in an array of [time, event] pairs, pass\n * in an array of events which will be spaced at the\n * given subdivision. Sub-arrays will subdivide that beat\n * by the number of items are in the array.\n * Sequence notation inspiration from [Tidal](http://yaxu.org/tidal/)\n * @example\n * const synth = new Tone.Synth().toDestination();\n * const seq = new Tone.Sequence((time, note) => {\n * \tsynth.triggerAttackRelease(note, 0.1, time);\n * \t// subdivisions are given as subarrays\n * }, ["C4", ["E4", "D4", "E4"], "G4", ["A4", "G4"]]).start(0);\n * Tone.Transport.start();\n * @category Event\n */\nclass Sequence_Sequence extends ToneEvent_ToneEvent {\n    constructor() {\n        super(optionsFromArguments(Sequence_Sequence.getDefaults(), arguments, ["callback", "events", "subdivision"]));\n        this.name = "Sequence";\n        /**\n         * The object responsible for scheduling all of the events\n         */\n        this._part = new Part_Part({\n            callback: this._seqCallback.bind(this),\n            context: this.context,\n        });\n        /**\n         * private reference to all of the sequence proxies\n         */\n        this._events = [];\n        /**\n         * The proxied array\n         */\n        this._eventsArray = [];\n        const options = optionsFromArguments(Sequence_Sequence.getDefaults(), arguments, ["callback", "events", "subdivision"]);\n        this._subdivision = this.toTicks(options.subdivision);\n        this.events = options.events;\n        // set all of the values\n        this.loop = options.loop;\n        this.loopStart = options.loopStart;\n        this.loopEnd = options.loopEnd;\n        this.playbackRate = options.playbackRate;\n        this.probability = options.probability;\n        this.humanize = options.humanize;\n        this.mute = options.mute;\n        this.playbackRate = options.playbackRate;\n    }\n    static getDefaults() {\n        return Object.assign(omitFromObject(ToneEvent_ToneEvent.getDefaults(), ["value"]), {\n            events: [],\n            loop: true,\n            loopEnd: 0,\n            loopStart: 0,\n            subdivision: "8n",\n        });\n    }\n    /**\n     * The internal callback for when an event is invoked\n     */\n    _seqCallback(time, value) {\n        if (value !== null) {\n            this.callback(time, value);\n        }\n    }\n    /**\n     * The sequence\n     */\n    get events() {\n        return this._events;\n    }\n    set events(s) {\n        this.clear();\n        this._eventsArray = s;\n        this._events = this._createSequence(this._eventsArray);\n        this._eventsUpdated();\n    }\n    /**\n     * Start the part at the given time.\n     * @param  time    When to start the part.\n     * @param  offset  The offset index to start at\n     */\n    start(time, offset) {\n        this._part.start(time, offset ? this._indexTime(offset) : offset);\n        return this;\n    }\n    /**\n     * Stop the part at the given time.\n     * @param  time  When to stop the part.\n     */\n    stop(time) {\n        this._part.stop(time);\n        return this;\n    }\n    /**\n     * The subdivision of the sequence. This can only be\n     * set in the constructor. The subdivision is the\n     * interval between successive steps.\n     */\n    get subdivision() {\n        return new Ticks_TicksClass(this.context, this._subdivision).toSeconds();\n    }\n    /**\n     * Create a sequence proxy which can be monitored to create subsequences\n     */\n    _createSequence(array) {\n        return new Proxy(array, {\n            get: (target, property) => {\n                // property is index in this case\n                return target[property];\n            },\n            set: (target, property, value) => {\n                if (isString(property) && isFinite(parseInt(property, 10))) {\n                    if (isArray(value)) {\n                        target[property] = this._createSequence(value);\n                    }\n                    else {\n                        target[property] = value;\n                    }\n                }\n                else {\n                    target[property] = value;\n                }\n                this._eventsUpdated();\n                // return true to accept the changes\n                return true;\n            },\n        });\n    }\n    /**\n     * When the sequence has changed, all of the events need to be recreated\n     */\n    _eventsUpdated() {\n        this._part.clear();\n        this._rescheduleSequence(this._eventsArray, this._subdivision, this.startOffset);\n        // update the loopEnd\n        this.loopEnd = this.loopEnd;\n    }\n    /**\n     * reschedule all of the events that need to be rescheduled\n     */\n    _rescheduleSequence(sequence, subdivision, startOffset) {\n        sequence.forEach((value, index) => {\n            const eventOffset = index * (subdivision) + startOffset;\n            if (isArray(value)) {\n                this._rescheduleSequence(value, subdivision / value.length, eventOffset);\n            }\n            else {\n                const startTime = new Ticks_TicksClass(this.context, eventOffset, "i").toSeconds();\n                this._part.add(startTime, value);\n            }\n        });\n    }\n    /**\n     * Get the time of the index given the Sequence\'s subdivision\n     * @param  index\n     * @return The time of that index\n     */\n    _indexTime(index) {\n        return new Ticks_TicksClass(this.context, index * (this._subdivision) + this.startOffset).toSeconds();\n    }\n    /**\n     * Clear all of the events\n     */\n    clear() {\n        this._part.clear();\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._part.dispose();\n        return this;\n    }\n    //-------------------------------------\n    // PROXY CALLS\n    //-------------------------------------\n    get loop() {\n        return this._part.loop;\n    }\n    set loop(l) {\n        this._part.loop = l;\n    }\n    /**\n     * The index at which the sequence should start looping\n     */\n    get loopStart() {\n        return this._loopStart;\n    }\n    set loopStart(index) {\n        this._loopStart = index;\n        this._part.loopStart = this._indexTime(index);\n    }\n    /**\n     * The index at which the sequence should end looping\n     */\n    get loopEnd() {\n        return this._loopEnd;\n    }\n    set loopEnd(index) {\n        this._loopEnd = index;\n        if (index === 0) {\n            this._part.loopEnd = this._indexTime(this._eventsArray.length);\n        }\n        else {\n            this._part.loopEnd = this._indexTime(index);\n        }\n    }\n    get startOffset() {\n        return this._part.startOffset;\n    }\n    set startOffset(start) {\n        this._part.startOffset = start;\n    }\n    get playbackRate() {\n        return this._part.playbackRate;\n    }\n    set playbackRate(rate) {\n        this._part.playbackRate = rate;\n    }\n    get probability() {\n        return this._part.probability;\n    }\n    set probability(prob) {\n        this._part.probability = prob;\n    }\n    get progress() {\n        return this._part.progress;\n    }\n    get humanize() {\n        return this._part.humanize;\n    }\n    set humanize(variation) {\n        this._part.humanize = variation;\n    }\n    /**\n     * The number of scheduled events\n     */\n    get length() {\n        return this._part.length;\n    }\n}\n//# sourceMappingURL=Sequence.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/event/index.js\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/CrossFade.js\n\n\n\n\n\n\n/**\n * Tone.Crossfade provides equal power fading between two inputs.\n * More on crossfading technique [here](https://en.wikipedia.org/wiki/Fade_(audio_engineering)#Crossfading).\n * ```\n *                                             +---------+\n *                                            +> input a +>--+\n * +-----------+   +---------------------+     |         |   |\n * | 1s signal +>--\x3e stereoPannerNode  L +>----\x3e gain    |   |\n * +-----------+   |                     |     +---------+   |\n *               +-> pan               R +>-+                |   +--------+\n *               | +---------------------+  |                +---\x3e output +>\n *  +------+     |                          |  +---------+   |   +--------+\n *  | fade +>----+                          | +> input b +>--+\n *  +------+                                |  |         |\n *                                          +--\x3e gain    |\n *                                             +---------+\n * ```\n * @example\n * const crossFade = new Tone.CrossFade().toDestination();\n * // connect two inputs Tone.to a/b\n * const inputA = new Tone.Oscillator(440, "square").connect(crossFade.a).start();\n * const inputB = new Tone.Oscillator(440, "sine").connect(crossFade.b).start();\n * // use the fade to control the mix between the two\n * crossFade.fade.value = 0.5;\n * @category Component\n */\nclass CrossFade_CrossFade extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(CrossFade_CrossFade.getDefaults(), arguments, ["fade"])));\n        this.name = "CrossFade";\n        /**\n         * The crossfading is done by a StereoPannerNode\n         */\n        this._panner = this.context.createStereoPanner();\n        /**\n         * Split the output of the panner node into two values used to control the gains.\n         */\n        this._split = this.context.createChannelSplitter(2);\n        /**\n         * Convert the fade value into an audio range value so it can be connected\n         * to the panner.pan AudioParam\n         */\n        this._g2a = new GainToAudio_GainToAudio({ context: this.context });\n        /**\n         * The input which is at full level when fade = 0\n         */\n        this.a = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        /**\n         * The input which is at full level when fade = 1\n         */\n        this.b = new Gain_Gain({\n            context: this.context,\n            gain: 0,\n        });\n        /**\n         * The output is a mix between `a` and `b` at the ratio of `fade`\n         */\n        this.output = new Gain_Gain({ context: this.context });\n        this._internalChannels = [this.a, this.b];\n        const options = optionsFromArguments(CrossFade_CrossFade.getDefaults(), arguments, ["fade"]);\n        this.fade = new Signal_Signal({\n            context: this.context,\n            units: "normalRange",\n            value: options.fade,\n        });\n        readOnly(this, "fade");\n        this.context.getConstant(1).connect(this._panner);\n        this._panner.connect(this._split);\n        // this is necessary for standardized-audio-context\n        // doesn\'t make any difference for the native AudioContext\n        // https://github.com/chrisguttandin/standardized-audio-context/issues/647\n        this._panner.channelCount = 1;\n        this._panner.channelCountMode = "explicit";\n        ToneAudioNode_connect(this._split, this.a.gain, 0);\n        ToneAudioNode_connect(this._split, this.b.gain, 1);\n        this.fade.chain(this._g2a, this._panner.pan);\n        this.a.connect(this.output);\n        this.b.connect(this.output);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            fade: 0.5,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.a.dispose();\n        this.b.dispose();\n        this.output.dispose();\n        this.fade.dispose();\n        this._g2a.dispose();\n        this._panner.disconnect();\n        this._split.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=CrossFade.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Effect.js\n\n\n\n\n/**\n * Effect is the base class for effects. Connect the effect between\n * the effectSend and effectReturn GainNodes, then control the amount of\n * effect which goes to the output using the wet control.\n */\nclass Effect_Effect extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        this.name = "Effect";\n        /**\n         * the drywet knob to control the amount of effect\n         */\n        this._dryWet = new CrossFade_CrossFade({ context: this.context });\n        /**\n         * The wet control is how much of the effected\n         * will pass through to the output. 1 = 100% effected\n         * signal, 0 = 100% dry signal.\n         */\n        this.wet = this._dryWet.fade;\n        /**\n         * connect the effectSend to the input of hte effect\n         */\n        this.effectSend = new Gain_Gain({ context: this.context });\n        /**\n         * connect the output of the effect to the effectReturn\n         */\n        this.effectReturn = new Gain_Gain({ context: this.context });\n        /**\n         * The effect input node\n         */\n        this.input = new Gain_Gain({ context: this.context });\n        /**\n         * The effect output\n         */\n        this.output = this._dryWet;\n        // connections\n        this.input.fan(this._dryWet.a, this.effectSend);\n        this.effectReturn.connect(this._dryWet.b);\n        this.wet.setValueAtTime(options.wet, 0);\n        this._internalChannels = [this.effectReturn, this.effectSend];\n        readOnly(this, "wet");\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            wet: 1,\n        });\n    }\n    /**\n     * chains the effect in between the effectSend and effectReturn\n     */\n    connectEffect(effect) {\n        // add it to the internal channels\n        this._internalChannels.push(effect);\n        this.effectSend.chain(effect, this.effectReturn);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._dryWet.dispose();\n        this.effectSend.dispose();\n        this.effectReturn.dispose();\n        this.wet.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Effect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/LFOEffect.js\n\n\n\n/**\n * Base class for LFO-based effects.\n */\nclass LFOEffect_LFOEffect extends Effect_Effect {\n    constructor(options) {\n        super(options);\n        this.name = "LFOEffect";\n        this._lfo = new LFO_LFO({\n            context: this.context,\n            frequency: options.frequency,\n            amplitude: options.depth,\n        });\n        this.depth = this._lfo.amplitude;\n        this.frequency = this._lfo.frequency;\n        this.type = options.type;\n        readOnly(this, ["frequency", "depth"]);\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            frequency: 1,\n            type: "sine",\n            depth: 1,\n        });\n    }\n    /**\n     * Start the effect.\n     */\n    start(time) {\n        this._lfo.start(time);\n        return this;\n    }\n    /**\n     * Stop the lfo\n     */\n    stop(time) {\n        this._lfo.stop(time);\n        return this;\n    }\n    /**\n     * Sync the filter to the transport. See [[LFO.sync]]\n     */\n    sync() {\n        this._lfo.sync();\n        return this;\n    }\n    /**\n     * Unsync the filter from the transport.\n     */\n    unsync() {\n        this._lfo.unsync();\n        return this;\n    }\n    /**\n     * The type of the LFO\'s oscillator: See [[Oscillator.type]]\n     * @example\n     * const autoFilter = new Tone.AutoFilter().start().toDestination();\n     * const noise = new Tone.Noise().start().connect(autoFilter);\n     * autoFilter.type = "square";\n     */\n    get type() {\n        return this._lfo.type;\n    }\n    set type(type) {\n        this._lfo.type = type;\n    }\n    dispose() {\n        super.dispose();\n        this._lfo.dispose();\n        this.frequency.dispose();\n        this.depth.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=LFOEffect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/AutoFilter.js\n\n\n\n/**\n * AutoFilter is a Tone.Filter with a Tone.LFO connected to the filter cutoff frequency.\n * Setting the LFO rate and depth allows for control over the filter modulation rate\n * and depth.\n *\n * @example\n * // create an autofilter and start it\'s LFO\n * const autoFilter = new Tone.AutoFilter("4n").toDestination().start();\n * // route an oscillator through the filter and start it\n * const oscillator = new Tone.Oscillator().connect(autoFilter).start();\n * @category Effect\n */\nclass AutoFilter_AutoFilter extends LFOEffect_LFOEffect {\n    constructor() {\n        super(optionsFromArguments(AutoFilter_AutoFilter.getDefaults(), arguments, ["frequency", "baseFrequency", "octaves"]));\n        this.name = "AutoFilter";\n        const options = optionsFromArguments(AutoFilter_AutoFilter.getDefaults(), arguments, ["frequency", "baseFrequency", "octaves"]);\n        this.filter = new Filter_Filter(Object.assign(options.filter, {\n            context: this.context,\n        }));\n        // connections\n        this.connectEffect(this.filter);\n        this._lfo.connect(this.filter.frequency);\n        this.octaves = options.octaves;\n        this.baseFrequency = options.baseFrequency;\n    }\n    static getDefaults() {\n        return Object.assign(LFOEffect_LFOEffect.getDefaults(), {\n            baseFrequency: 200,\n            octaves: 2.6,\n            filter: {\n                type: "lowpass",\n                rolloff: -12,\n                Q: 1,\n            }\n        });\n    }\n    /**\n     * The minimum value of the filter\'s cutoff frequency.\n     */\n    get baseFrequency() {\n        return this._lfo.min;\n    }\n    set baseFrequency(freq) {\n        this._lfo.min = this.toFrequency(freq);\n        // and set the max\n        this.octaves = this._octaves;\n    }\n    /**\n     * The maximum value of the filter\'s cutoff frequency.\n     */\n    get octaves() {\n        return this._octaves;\n    }\n    set octaves(oct) {\n        this._octaves = oct;\n        this._lfo.max = this._lfo.min * Math.pow(2, oct);\n    }\n    dispose() {\n        super.dispose();\n        this.filter.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AutoFilter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Panner.js\n\n\n\n\n/**\n * Panner is an equal power Left/Right Panner. It is a wrapper around the StereoPannerNode.\n * @example\n * return Tone.Offline(() => {\n * // move the input signal from right to left\n * \tconst panner = new Tone.Panner(1).toDestination();\n * \tpanner.pan.rampTo(-1, 0.5);\n * \tconst osc = new Tone.Oscillator(100).connect(panner).start();\n * }, 0.5, 2);\n * @category Component\n */\nclass Panner_Panner extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Panner_Panner.getDefaults(), arguments, ["pan"])));\n        this.name = "Panner";\n        /**\n         * the panner node\n         */\n        this._panner = this.context.createStereoPanner();\n        this.input = this._panner;\n        this.output = this._panner;\n        const options = optionsFromArguments(Panner_Panner.getDefaults(), arguments, ["pan"]);\n        this.pan = new Param_Param({\n            context: this.context,\n            param: this._panner.pan,\n            value: options.pan,\n            minValue: -1,\n            maxValue: 1,\n        });\n        // this is necessary for standardized-audio-context\n        // doesn\'t make any difference for the native AudioContext\n        // https://github.com/chrisguttandin/standardized-audio-context/issues/647\n        this._panner.channelCount = options.channelCount;\n        this._panner.channelCountMode = "explicit";\n        // initial value\n        readOnly(this, "pan");\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            pan: 0,\n            channelCount: 1,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._panner.disconnect();\n        this.pan.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Panner.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/AutoPanner.js\n\n\n\n/**\n * AutoPanner is a [[Panner]] with an [[LFO]] connected to the pan amount.\n * [Related Reading](https://www.ableton.com/en/blog/autopan-chopper-effect-and-more-liveschool/).\n *\n * @example\n * // create an autopanner and start it\n * const autoPanner = new Tone.AutoPanner("4n").toDestination().start();\n * // route an oscillator through the panner and start it\n * const oscillator = new Tone.Oscillator().connect(autoPanner).start();\n * @category Effect\n */\nclass AutoPanner_AutoPanner extends LFOEffect_LFOEffect {\n    constructor() {\n        super(optionsFromArguments(AutoPanner_AutoPanner.getDefaults(), arguments, ["frequency"]));\n        this.name = "AutoPanner";\n        const options = optionsFromArguments(AutoPanner_AutoPanner.getDefaults(), arguments, ["frequency"]);\n        this._panner = new Panner_Panner({\n            context: this.context,\n            channelCount: options.channelCount\n        });\n        // connections\n        this.connectEffect(this._panner);\n        this._lfo.connect(this._panner.pan);\n        this._lfo.min = -1;\n        this._lfo.max = 1;\n    }\n    static getDefaults() {\n        return Object.assign(LFOEffect_LFOEffect.getDefaults(), {\n            channelCount: 1\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._panner.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AutoPanner.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/Follower.js\n\n\n\n\n/**\n * Follower is a simple envelope follower.\n * It\'s implemented by applying a lowpass filter to the absolute value of the incoming signal.\n * ```\n *          +-----+    +---------------+\n * Input +--\x3e Abs +----\x3e OnePoleFilter +--\x3e Output\n *          +-----+    +---------------+\n * ```\n * @category Component\n */\nclass Follower_Follower extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Follower_Follower.getDefaults(), arguments, ["smoothing"]));\n        this.name = "Follower";\n        const options = optionsFromArguments(Follower_Follower.getDefaults(), arguments, ["smoothing"]);\n        this._abs = this.input = new Abs_Abs({ context: this.context });\n        this._lowpass = this.output = new OnePoleFilter_OnePoleFilter({\n            context: this.context,\n            frequency: 1 / this.toSeconds(options.smoothing),\n            type: "lowpass"\n        });\n        this._abs.connect(this._lowpass);\n        this._smoothing = options.smoothing;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            smoothing: 0.05\n        });\n    }\n    /**\n     * The amount of time it takes a value change to arrive at the updated value.\n     */\n    get smoothing() {\n        return this._smoothing;\n    }\n    set smoothing(smoothing) {\n        this._smoothing = smoothing;\n        this._lowpass.frequency = 1 / this.toSeconds(this.smoothing);\n    }\n    dispose() {\n        super.dispose();\n        this._abs.dispose();\n        this._lowpass.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Follower.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/AutoWah.js\n\n\n\n\n\n\n\n\n/**\n * AutoWah connects a [[Follower]] to a [[Filter]].\n * The frequency of the filter, follows the input amplitude curve.\n * Inspiration from [Tuna.js](https://github.com/Dinahmoe/tuna).\n *\n * @example\n * const autoWah = new Tone.AutoWah(50, 6, -30).toDestination();\n * // initialize the synth and connect to autowah\n * const synth = new Tone.Synth().connect(autoWah);\n * // Q value influences the effect of the wah - default is 2\n * autoWah.Q.value = 6;\n * // more audible on higher notes\n * synth.triggerAttackRelease("C4", "8n");\n * @category Effect\n */\nclass AutoWah_AutoWah extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(AutoWah_AutoWah.getDefaults(), arguments, ["baseFrequency", "octaves", "sensitivity"]));\n        this.name = "AutoWah";\n        const options = optionsFromArguments(AutoWah_AutoWah.getDefaults(), arguments, ["baseFrequency", "octaves", "sensitivity"]);\n        this._follower = new Follower_Follower({\n            context: this.context,\n            smoothing: options.follower,\n        });\n        this._sweepRange = new ScaleExp_ScaleExp({\n            context: this.context,\n            min: 0,\n            max: 1,\n            exponent: 0.5,\n        });\n        this._baseFrequency = this.toFrequency(options.baseFrequency);\n        this._octaves = options.octaves;\n        this._inputBoost = new Gain_Gain({ context: this.context });\n        this._bandpass = new Filter_Filter({\n            context: this.context,\n            rolloff: -48,\n            frequency: 0,\n            Q: options.Q,\n        });\n        this._peaking = new Filter_Filter({\n            context: this.context,\n            type: "peaking"\n        });\n        this._peaking.gain.value = options.gain;\n        this.gain = this._peaking.gain;\n        this.Q = this._bandpass.Q;\n        // the control signal path\n        this.effectSend.chain(this._inputBoost, this._follower, this._sweepRange);\n        this._sweepRange.connect(this._bandpass.frequency);\n        this._sweepRange.connect(this._peaking.frequency);\n        // the filtered path\n        this.effectSend.chain(this._bandpass, this._peaking, this.effectReturn);\n        // set the initial value\n        this._setSweepRange();\n        this.sensitivity = options.sensitivity;\n        readOnly(this, ["gain", "Q"]);\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            baseFrequency: 100,\n            octaves: 6,\n            sensitivity: 0,\n            Q: 2,\n            gain: 2,\n            follower: 0.2,\n        });\n    }\n    /**\n     * The number of octaves that the filter will sweep above the baseFrequency.\n     */\n    get octaves() {\n        return this._octaves;\n    }\n    set octaves(octaves) {\n        this._octaves = octaves;\n        this._setSweepRange();\n    }\n    /**\n     * The follower\'s smoothing time\n     */\n    get follower() {\n        return this._follower.smoothing;\n    }\n    set follower(follower) {\n        this._follower.smoothing = follower;\n    }\n    /**\n     * The base frequency from which the sweep will start from.\n     */\n    get baseFrequency() {\n        return this._baseFrequency;\n    }\n    set baseFrequency(baseFreq) {\n        this._baseFrequency = this.toFrequency(baseFreq);\n        this._setSweepRange();\n    }\n    /**\n     * The sensitivity to control how responsive to the input signal the filter is.\n     */\n    get sensitivity() {\n        return gainToDb(1 / this._inputBoost.gain.value);\n    }\n    set sensitivity(sensitivity) {\n        this._inputBoost.gain.value = 1 / dbToGain(sensitivity);\n    }\n    /**\n     * sets the sweep range of the scaler\n     */\n    _setSweepRange() {\n        this._sweepRange.min = this._baseFrequency;\n        this._sweepRange.max = Math.min(this._baseFrequency * Math.pow(2, this._octaves), this.context.sampleRate / 2);\n    }\n    dispose() {\n        super.dispose();\n        this._follower.dispose();\n        this._sweepRange.dispose();\n        this._bandpass.dispose();\n        this._peaking.dispose();\n        this._inputBoost.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=AutoWah.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/BitCrusher.worklet.js\n\n\nconst BitCrusher_worklet_workletName = "bit-crusher";\nconst bitCrusherWorklet = /* javascript */ `\n\tclass BitCrusherWorklet extends SingleIOProcessor {\n\n\t\tstatic get parameterDescriptors() {\n\t\t\treturn [{\n\t\t\t\tname: "bits",\n\t\t\t\tdefaultValue: 12,\n\t\t\t\tminValue: 1,\n\t\t\t\tmaxValue: 16,\n\t\t\t\tautomationRate: \'k-rate\'\n\t\t\t}];\n\t\t}\n\n\t\tgenerate(input, _channel, parameters) {\n\t\t\tconst step = Math.pow(0.5, parameters.bits - 1);\n\t\t\tconst val = step * Math.floor(input / step + 0.5);\n\t\t\treturn val;\n\t\t}\n\t}\n`;\nregisterProcessor(BitCrusher_worklet_workletName, bitCrusherWorklet);\n//# sourceMappingURL=BitCrusher.worklet.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/BitCrusher.js\n\n\n\n\n\n\n\n/**\n * BitCrusher down-samples the incoming signal to a different bit depth.\n * Lowering the bit depth of the signal creates distortion. Read more about BitCrushing\n * on [Wikipedia](https://en.wikipedia.org/wiki/Bitcrusher).\n * @example\n * // initialize crusher and route a synth through it\n * const crusher = new Tone.BitCrusher(4).toDestination();\n * const synth = new Tone.Synth().connect(crusher);\n * synth.triggerAttackRelease("C2", 2);\n *\n * @category Effect\n */\nclass BitCrusher_BitCrusher extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(BitCrusher_BitCrusher.getDefaults(), arguments, ["bits"]));\n        this.name = "BitCrusher";\n        const options = optionsFromArguments(BitCrusher_BitCrusher.getDefaults(), arguments, ["bits"]);\n        this._bitCrusherWorklet = new BitCrusher_BitCrusherWorklet({\n            context: this.context,\n            bits: options.bits,\n        });\n        // connect it up\n        this.connectEffect(this._bitCrusherWorklet);\n        this.bits = this._bitCrusherWorklet.bits;\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            bits: 4,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._bitCrusherWorklet.dispose();\n        return this;\n    }\n}\n/**\n * Internal class which creates an AudioWorklet to do the bit crushing\n */\nclass BitCrusher_BitCrusherWorklet extends ToneAudioWorklet_ToneAudioWorklet {\n    constructor() {\n        super(optionsFromArguments(BitCrusher_BitCrusherWorklet.getDefaults(), arguments));\n        this.name = "BitCrusherWorklet";\n        const options = optionsFromArguments(BitCrusher_BitCrusherWorklet.getDefaults(), arguments);\n        this.input = new Gain_Gain({ context: this.context });\n        this.output = new Gain_Gain({ context: this.context });\n        this.bits = new Param_Param({\n            context: this.context,\n            value: options.bits,\n            units: "positive",\n            minValue: 1,\n            maxValue: 16,\n            param: this._dummyParam,\n            swappable: true,\n        });\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioWorklet_ToneAudioWorklet.getDefaults(), {\n            bits: 12,\n        });\n    }\n    _audioWorkletName() {\n        return BitCrusher_worklet_workletName;\n    }\n    onReady(node) {\n        connectSeries(this.input, node, this.output);\n        const bits = node.parameters.get("bits");\n        this.bits.setParam(bits);\n    }\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this.output.dispose();\n        this.bits.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=BitCrusher.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Chebyshev.js\n\n\n\n/**\n * Chebyshev is a waveshaper which is good\n * for making different types of distortion sounds.\n * Note that odd orders sound very different from even ones,\n * and order = 1 is no change.\n * Read more at [music.columbia.edu](http://music.columbia.edu/cmc/musicandcomputers/chapter4/04_06.php).\n * @example\n * // create a new cheby\n * const cheby = new Tone.Chebyshev(50).toDestination();\n * // create a monosynth connected to our cheby\n * const synth = new Tone.MonoSynth().connect(cheby);\n * synth.triggerAttackRelease("C2", 0.4);\n * @category Effect\n */\nclass Chebyshev_Chebyshev extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(Chebyshev_Chebyshev.getDefaults(), arguments, ["order"]));\n        this.name = "Chebyshev";\n        const options = optionsFromArguments(Chebyshev_Chebyshev.getDefaults(), arguments, ["order"]);\n        this._shaper = new WaveShaper_WaveShaper({\n            context: this.context,\n            length: 4096\n        });\n        this._order = options.order;\n        this.connectEffect(this._shaper);\n        this.order = options.order;\n        this.oversample = options.oversample;\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            order: 1,\n            oversample: "none"\n        });\n    }\n    /**\n     * get the coefficient for that degree\n     * @param  x the x value\n     * @param  degree\n     * @param  memo memoize the computed value. this speeds up computation greatly.\n     */\n    _getCoefficient(x, degree, memo) {\n        if (memo.has(degree)) {\n            return memo.get(degree);\n        }\n        else if (degree === 0) {\n            memo.set(degree, 0);\n        }\n        else if (degree === 1) {\n            memo.set(degree, x);\n        }\n        else {\n            memo.set(degree, 2 * x * this._getCoefficient(x, degree - 1, memo) - this._getCoefficient(x, degree - 2, memo));\n        }\n        return memo.get(degree);\n    }\n    /**\n     * The order of the Chebyshev polynomial which creates the equation which is applied to the incoming\n     * signal through a Tone.WaveShaper. The equations are in the form:\n     * ```\n     * order 2: 2x^2 + 1\n     * order 3: 4x^3 + 3x\n     * ```\n     * @min 1\n     * @max 100\n     */\n    get order() {\n        return this._order;\n    }\n    set order(order) {\n        this._order = order;\n        this._shaper.setMap((x => {\n            return this._getCoefficient(x, order, new Map());\n        }));\n    }\n    /**\n     * The oversampling of the effect. Can either be "none", "2x" or "4x".\n     */\n    get oversample() {\n        return this._shaper.oversample;\n    }\n    set oversample(oversampling) {\n        this._shaper.oversample = oversampling;\n    }\n    dispose() {\n        super.dispose();\n        this._shaper.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Chebyshev.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Split.js\n\n\n/**\n * Split splits an incoming signal into the number of given channels.\n *\n * @example\n * const split = new Tone.Split();\n * // stereoSignal.connect(split);\n * @category Component\n */\nclass Split_Split extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Split_Split.getDefaults(), arguments, ["channels"]));\n        this.name = "Split";\n        const options = optionsFromArguments(Split_Split.getDefaults(), arguments, ["channels"]);\n        this._splitter = this.input = this.output = this.context.createChannelSplitter(options.channels);\n        this._internalChannels = [this._splitter];\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            channels: 2,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._splitter.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=Split.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Merge.js\n\n\n/**\n * Merge brings multiple mono input channels into a single multichannel output channel.\n *\n * @example\n * const merge = new Tone.Merge().toDestination();\n * // routing a sine tone in the left channel\n * const osc = new Tone.Oscillator().connect(merge, 0, 0).start();\n * // and noise in the right channel\n * const noise = new Tone.Noise().connect(merge, 0, 1).start();;\n * @category Component\n */\nclass Merge_Merge extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Merge_Merge.getDefaults(), arguments, ["channels"]));\n        this.name = "Merge";\n        const options = optionsFromArguments(Merge_Merge.getDefaults(), arguments, ["channels"]);\n        this._merger = this.output = this.input = this.context.createChannelMerger(options.channels);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            channels: 2,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._merger.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=Merge.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/StereoEffect.js\n\n\n\n\n\n\n/**\n * Base class for Stereo effects.\n */\nclass StereoEffect_StereoEffect extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        this.name = "StereoEffect";\n        this.input = new Gain_Gain({ context: this.context });\n        // force mono sources to be stereo\n        this.input.channelCount = 2;\n        this.input.channelCountMode = "explicit";\n        this._dryWet = this.output = new CrossFade_CrossFade({\n            context: this.context,\n            fade: options.wet\n        });\n        this.wet = this._dryWet.fade;\n        this._split = new Split_Split({ context: this.context, channels: 2 });\n        this._merge = new Merge_Merge({ context: this.context, channels: 2 });\n        // connections\n        this.input.connect(this._split);\n        // dry wet connections\n        this.input.connect(this._dryWet.a);\n        this._merge.connect(this._dryWet.b);\n        readOnly(this, ["wet"]);\n    }\n    /**\n     * Connect the left part of the effect\n     */\n    connectEffectLeft(...nodes) {\n        this._split.connect(nodes[0], 0, 0);\n        connectSeries(...nodes);\n        ToneAudioNode_connect(nodes[nodes.length - 1], this._merge, 0, 0);\n    }\n    /**\n     * Connect the right part of the effect\n     */\n    connectEffectRight(...nodes) {\n        this._split.connect(nodes[0], 1, 0);\n        connectSeries(...nodes);\n        ToneAudioNode_connect(nodes[nodes.length - 1], this._merge, 0, 1);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            wet: 1,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._dryWet.dispose();\n        this._split.dispose();\n        this._merge.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=StereoEffect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/StereoFeedbackEffect.js\n\n\n\n\n\n\n/**\n * Base class for stereo feedback effects where the effectReturn is fed back into the same channel.\n */\nclass StereoFeedbackEffect_StereoFeedbackEffect extends StereoEffect_StereoEffect {\n    constructor(options) {\n        super(options);\n        this.feedback = new Signal_Signal({\n            context: this.context,\n            value: options.feedback,\n            units: "normalRange"\n        });\n        this._feedbackL = new Gain_Gain({ context: this.context });\n        this._feedbackR = new Gain_Gain({ context: this.context });\n        this._feedbackSplit = new Split_Split({ context: this.context, channels: 2 });\n        this._feedbackMerge = new Merge_Merge({ context: this.context, channels: 2 });\n        this._merge.connect(this._feedbackSplit);\n        this._feedbackMerge.connect(this._split);\n        // the left output connected to the left input\n        this._feedbackSplit.connect(this._feedbackL, 0, 0);\n        this._feedbackL.connect(this._feedbackMerge, 0, 0);\n        // the right output connected to the right input\n        this._feedbackSplit.connect(this._feedbackR, 1, 0);\n        this._feedbackR.connect(this._feedbackMerge, 0, 1);\n        // the feedback control\n        this.feedback.fan(this._feedbackL.gain, this._feedbackR.gain);\n        readOnly(this, ["feedback"]);\n    }\n    static getDefaults() {\n        return Object.assign(StereoEffect_StereoEffect.getDefaults(), {\n            feedback: 0.5,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.feedback.dispose();\n        this._feedbackL.dispose();\n        this._feedbackR.dispose();\n        this._feedbackSplit.dispose();\n        this._feedbackMerge.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=StereoFeedbackEffect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Chorus.js\n\n\n\n\n\n/**\n * Chorus is a stereo chorus effect composed of a left and right delay with an [[LFO]] applied to the delayTime of each channel.\n * When [[feedback]] is set to a value larger than 0, you also get Flanger-type effects.\n * Inspiration from [Tuna.js](https://github.com/Dinahmoe/tuna/blob/master/tuna.js).\n * Read more on the chorus effect on [SoundOnSound](http://www.soundonsound.com/sos/jun04/articles/synthsecrets.htm).\n *\n * @example\n * const chorus = new Tone.Chorus(4, 2.5, 0.5).toDestination().start();\n * const synth = new Tone.PolySynth().connect(chorus);\n * synth.triggerAttackRelease(["C3", "E3", "G3"], "8n");\n *\n * @category Effect\n */\nclass Chorus_Chorus extends StereoFeedbackEffect_StereoFeedbackEffect {\n    constructor() {\n        super(optionsFromArguments(Chorus_Chorus.getDefaults(), arguments, ["frequency", "delayTime", "depth"]));\n        this.name = "Chorus";\n        const options = optionsFromArguments(Chorus_Chorus.getDefaults(), arguments, ["frequency", "delayTime", "depth"]);\n        this._depth = options.depth;\n        this._delayTime = options.delayTime / 1000;\n        this._lfoL = new LFO_LFO({\n            context: this.context,\n            frequency: options.frequency,\n            min: 0,\n            max: 1,\n        });\n        this._lfoR = new LFO_LFO({\n            context: this.context,\n            frequency: options.frequency,\n            min: 0,\n            max: 1,\n            phase: 180\n        });\n        this._delayNodeL = new Delay_Delay({ context: this.context });\n        this._delayNodeR = new Delay_Delay({ context: this.context });\n        this.frequency = this._lfoL.frequency;\n        readOnly(this, ["frequency"]);\n        // have one LFO frequency control the other\n        this._lfoL.frequency.connect(this._lfoR.frequency);\n        // connections\n        this.connectEffectLeft(this._delayNodeL);\n        this.connectEffectRight(this._delayNodeR);\n        // lfo setup\n        this._lfoL.connect(this._delayNodeL.delayTime);\n        this._lfoR.connect(this._delayNodeR.delayTime);\n        // set the initial values\n        this.depth = this._depth;\n        this.type = options.type;\n        this.spread = options.spread;\n    }\n    static getDefaults() {\n        return Object.assign(StereoFeedbackEffect_StereoFeedbackEffect.getDefaults(), {\n            frequency: 1.5,\n            delayTime: 3.5,\n            depth: 0.7,\n            type: "sine",\n            spread: 180,\n            feedback: 0,\n            wet: 0.5,\n        });\n    }\n    /**\n     * The depth of the effect. A depth of 1 makes the delayTime\n     * modulate between 0 and 2*delayTime (centered around the delayTime).\n     */\n    get depth() {\n        return this._depth;\n    }\n    set depth(depth) {\n        this._depth = depth;\n        const deviation = this._delayTime * depth;\n        this._lfoL.min = Math.max(this._delayTime - deviation, 0);\n        this._lfoL.max = this._delayTime + deviation;\n        this._lfoR.min = Math.max(this._delayTime - deviation, 0);\n        this._lfoR.max = this._delayTime + deviation;\n    }\n    /**\n     * The delayTime in milliseconds of the chorus. A larger delayTime\n     * will give a more pronounced effect. Nominal range a delayTime\n     * is between 2 and 20ms.\n     */\n    get delayTime() {\n        return this._delayTime * 1000;\n    }\n    set delayTime(delayTime) {\n        this._delayTime = delayTime / 1000;\n        this.depth = this._depth;\n    }\n    /**\n     * The oscillator type of the LFO.\n     */\n    get type() {\n        return this._lfoL.type;\n    }\n    set type(type) {\n        this._lfoL.type = type;\n        this._lfoR.type = type;\n    }\n    /**\n     * Amount of stereo spread. When set to 0, both LFO\'s will be panned centrally.\n     * When set to 180, LFO\'s will be panned hard left and right respectively.\n     */\n    get spread() {\n        return this._lfoR.phase - this._lfoL.phase;\n    }\n    set spread(spread) {\n        this._lfoL.phase = 90 - (spread / 2);\n        this._lfoR.phase = (spread / 2) + 90;\n    }\n    /**\n     * Start the effect.\n     */\n    start(time) {\n        this._lfoL.start(time);\n        this._lfoR.start(time);\n        return this;\n    }\n    /**\n     * Stop the lfo\n     */\n    stop(time) {\n        this._lfoL.stop(time);\n        this._lfoR.stop(time);\n        return this;\n    }\n    /**\n     * Sync the filter to the transport. See [[LFO.sync]]\n     */\n    sync() {\n        this._lfoL.sync();\n        this._lfoR.sync();\n        return this;\n    }\n    /**\n     * Unsync the filter from the transport.\n     */\n    unsync() {\n        this._lfoL.unsync();\n        this._lfoR.unsync();\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._lfoL.dispose();\n        this._lfoR.dispose();\n        this._delayNodeL.dispose();\n        this._delayNodeR.dispose();\n        this.frequency.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Chorus.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Distortion.js\n\n\n\n/**\n * A simple distortion effect using Tone.WaveShaper.\n * Algorithm from [this stackoverflow answer](http://stackoverflow.com/a/22313408).\n *\n * @example\n * const dist = new Tone.Distortion(0.8).toDestination();\n * const fm = new Tone.FMSynth().connect(dist);\n * fm.triggerAttackRelease("A1", "8n");\n * @category Effect\n */\nclass Distortion_Distortion extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(Distortion_Distortion.getDefaults(), arguments, ["distortion"]));\n        this.name = "Distortion";\n        const options = optionsFromArguments(Distortion_Distortion.getDefaults(), arguments, ["distortion"]);\n        this._shaper = new WaveShaper_WaveShaper({\n            context: this.context,\n            length: 4096,\n        });\n        this._distortion = options.distortion;\n        this.connectEffect(this._shaper);\n        this.distortion = options.distortion;\n        this.oversample = options.oversample;\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            distortion: 0.4,\n            oversample: "none",\n        });\n    }\n    /**\n     * The amount of distortion. Nominal range is between 0 and 1.\n     */\n    get distortion() {\n        return this._distortion;\n    }\n    set distortion(amount) {\n        this._distortion = amount;\n        const k = amount * 100;\n        const deg = Math.PI / 180;\n        this._shaper.setMap((x) => {\n            if (Math.abs(x) < 0.001) {\n                // should output 0 when input is 0\n                return 0;\n            }\n            else {\n                return (3 + k) * x * 20 * deg / (Math.PI + k * Math.abs(x));\n            }\n        });\n    }\n    /**\n     * The oversampling of the effect. Can either be "none", "2x" or "4x".\n     */\n    get oversample() {\n        return this._shaper.oversample;\n    }\n    set oversample(oversampling) {\n        this._shaper.oversample = oversampling;\n    }\n    dispose() {\n        super.dispose();\n        this._shaper.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Distortion.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/FeedbackEffect.js\n\n\n\n/**\n * FeedbackEffect provides a loop between an audio source and its own output.\n * This is a base-class for feedback effects.\n */\nclass FeedbackEffect_FeedbackEffect extends Effect_Effect {\n    constructor(options) {\n        super(options);\n        this.name = "FeedbackEffect";\n        this._feedbackGain = new Gain_Gain({\n            context: this.context,\n            gain: options.feedback,\n            units: "normalRange",\n        });\n        this.feedback = this._feedbackGain.gain;\n        readOnly(this, "feedback");\n        // the feedback loop\n        this.effectReturn.chain(this._feedbackGain, this.effectSend);\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            feedback: 0.125,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._feedbackGain.dispose();\n        this.feedback.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FeedbackEffect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/FeedbackDelay.js\n\n\n\n\n/**\n * FeedbackDelay is a DelayNode in which part of output signal is fed back into the delay.\n *\n * @param delayTime The delay applied to the incoming signal.\n * @param feedback The amount of the effected signal which is fed back through the delay.\n * @example\n * const feedbackDelay = new Tone.FeedbackDelay("8n", 0.5).toDestination();\n * const tom = new Tone.MembraneSynth({\n * \toctaves: 4,\n * \tpitchDecay: 0.1\n * }).connect(feedbackDelay);\n * tom.triggerAttackRelease("A2", "32n");\n * @category Effect\n */\nclass FeedbackDelay_FeedbackDelay extends FeedbackEffect_FeedbackEffect {\n    constructor() {\n        super(optionsFromArguments(FeedbackDelay_FeedbackDelay.getDefaults(), arguments, ["delayTime", "feedback"]));\n        this.name = "FeedbackDelay";\n        const options = optionsFromArguments(FeedbackDelay_FeedbackDelay.getDefaults(), arguments, ["delayTime", "feedback"]);\n        this._delayNode = new Delay_Delay({\n            context: this.context,\n            delayTime: options.delayTime,\n            maxDelay: options.maxDelay,\n        });\n        this.delayTime = this._delayNode.delayTime;\n        // connect it up\n        this.connectEffect(this._delayNode);\n        readOnly(this, "delayTime");\n    }\n    static getDefaults() {\n        return Object.assign(FeedbackEffect_FeedbackEffect.getDefaults(), {\n            delayTime: 0.25,\n            maxDelay: 1,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._delayNode.dispose();\n        this.delayTime.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FeedbackDelay.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/PhaseShiftAllpass.js\n\n\n/**\n * PhaseShiftAllpass is an very efficient implementation of a Hilbert Transform\n * using two Allpass filter banks whose outputs have a phase difference of 90°.\n * Here the `offset90` phase is offset by +90° in relation to `output`.\n * Coefficients and structure was developed by Olli Niemitalo.\n * For more details see: http://yehar.com/blog/?p=368\n * @category Component\n */\nclass PhaseShiftAllpass_PhaseShiftAllpass extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        this.name = "PhaseShiftAllpass";\n        this.input = new Gain_Gain({ context: this.context });\n        /**\n         * The phase shifted output\n         */\n        this.output = new Gain_Gain({ context: this.context });\n        /**\n         * The PhaseShifted allpass output\n         */\n        this.offset90 = new Gain_Gain({ context: this.context });\n        const allpassBank1Values = [0.6923878, 0.9360654322959, 0.9882295226860, 0.9987488452737];\n        const allpassBank2Values = [0.4021921162426, 0.8561710882420, 0.9722909545651, 0.9952884791278];\n        this._bank0 = this._createAllPassFilterBank(allpassBank1Values);\n        this._bank1 = this._createAllPassFilterBank(allpassBank2Values);\n        this._oneSampleDelay = this.context.createIIRFilter([0.0, 1.0], [1.0, 0.0]);\n        // connect Allpass filter banks\n        connectSeries(this.input, ...this._bank0, this._oneSampleDelay, this.output);\n        connectSeries(this.input, ...this._bank1, this.offset90);\n    }\n    /**\n     * Create all of the IIR filters from an array of values using the coefficient calculation.\n     */\n    _createAllPassFilterBank(bankValues) {\n        const nodes = bankValues.map(value => {\n            const coefficients = [[value * value, 0, -1], [1, 0, -(value * value)]];\n            return this.context.createIIRFilter(coefficients[0], coefficients[1]);\n        });\n        return nodes;\n    }\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this.output.dispose();\n        this.offset90.dispose();\n        this._bank0.forEach(f => f.disconnect());\n        this._bank1.forEach(f => f.disconnect());\n        this._oneSampleDelay.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=PhaseShiftAllpass.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/FrequencyShifter.js\n\n\n\n\n\n\n\n\n\n/**\n * FrequencyShifter can be used to shift all frequencies of a signal by a fixed amount.\n * The amount can be changed at audio rate and the effect is applied in real time.\n * The frequency shifting is implemented with a technique called single side band modulation using a ring modulator.\n * Note: Contrary to pitch shifting, all frequencies are shifted by the same amount,\n * destroying the harmonic relationship between them. This leads to the classic ring modulator timbre distortion.\n * The algorithm will produces some aliasing towards the high end, especially if your source material\n * contains a lot of high frequencies. Unfortunatelly the webaudio API does not support resampling\n * buffers in real time, so it is not possible to fix it properly. Depending on the use case it might\n * be an option to low pass filter your input before frequency shifting it to get ride of the aliasing.\n * You can find a very detailed description of the algorithm here: https://larzeitlin.github.io/RMFS/\n *\n * @example\n * const input = new Tone.Oscillator(230, "sawtooth").start();\n * const shift = new Tone.FrequencyShifter(42).toDestination();\n * input.connect(shift);\n * @category Effect\n */\nclass FrequencyShifter_FrequencyShifter extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(FrequencyShifter_FrequencyShifter.getDefaults(), arguments, ["frequency"]));\n        this.name = "FrequencyShifter";\n        const options = optionsFromArguments(FrequencyShifter_FrequencyShifter.getDefaults(), arguments, ["frequency"]);\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.frequency,\n            minValue: -this.context.sampleRate / 2,\n            maxValue: this.context.sampleRate / 2,\n        });\n        this._sine = new ToneOscillatorNode_ToneOscillatorNode({\n            context: this.context,\n            type: "sine",\n        });\n        this._cosine = new Oscillator_Oscillator({\n            context: this.context,\n            phase: -90,\n            type: "sine",\n        });\n        this._sineMultiply = new Multiply_Multiply({ context: this.context });\n        this._cosineMultiply = new Multiply_Multiply({ context: this.context });\n        this._negate = new Negate_Negate({ context: this.context });\n        this._add = new Add_Add({ context: this.context });\n        this._phaseShifter = new PhaseShiftAllpass_PhaseShiftAllpass({ context: this.context });\n        this.effectSend.connect(this._phaseShifter);\n        // connect the carrier frequency signal to the two oscillators\n        this.frequency.fan(this._sine.frequency, this._cosine.frequency);\n        this._phaseShifter.offset90.connect(this._cosineMultiply);\n        this._cosine.connect(this._cosineMultiply.factor);\n        this._phaseShifter.connect(this._sineMultiply);\n        this._sine.connect(this._sineMultiply.factor);\n        this._sineMultiply.connect(this._negate);\n        this._cosineMultiply.connect(this._add);\n        this._negate.connect(this._add.addend);\n        this._add.connect(this.effectReturn);\n        // start the oscillators at the same time\n        const now = this.immediate();\n        this._sine.start(now);\n        this._cosine.start(now);\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            frequency: 0,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.frequency.dispose();\n        this._add.dispose();\n        this._cosine.dispose();\n        this._cosineMultiply.dispose();\n        this._negate.dispose();\n        this._phaseShifter.dispose();\n        this._sine.dispose();\n        this._sineMultiply.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=FrequencyShifter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Freeverb.js\n\n\n\n\n\n/**\n * An array of comb filter delay values from Freeverb implementation\n */\nconst combFilterTunings = [1557 / 44100, 1617 / 44100, 1491 / 44100, 1422 / 44100, 1277 / 44100, 1356 / 44100, 1188 / 44100, 1116 / 44100];\n/**\n * An array of allpass filter frequency values from Freeverb implementation\n */\nconst allpassFilterFrequencies = [225, 556, 441, 341];\n/**\n * Freeverb is a reverb based on [Freeverb](https://ccrma.stanford.edu/~jos/pasp/Freeverb.html).\n * Read more on reverb on [Sound On Sound](https://web.archive.org/web/20160404083902/http://www.soundonsound.com:80/sos/feb01/articles/synthsecrets.asp).\n * Freeverb is now implemented with an AudioWorkletNode which may result on performance degradation on some platforms. Consider using [[Reverb]].\n * @example\n * const freeverb = new Tone.Freeverb().toDestination();\n * freeverb.dampening = 1000;\n * // routing synth through the reverb\n * const synth = new Tone.NoiseSynth().connect(freeverb);\n * synth.triggerAttackRelease(0.05);\n * @category Effect\n */\nclass Freeverb_Freeverb extends StereoEffect_StereoEffect {\n    constructor() {\n        super(optionsFromArguments(Freeverb_Freeverb.getDefaults(), arguments, ["roomSize", "dampening"]));\n        this.name = "Freeverb";\n        /**\n         * the comb filters\n         */\n        this._combFilters = [];\n        /**\n         * the allpass filters on the left\n         */\n        this._allpassFiltersL = [];\n        /**\n         * the allpass filters on the right\n         */\n        this._allpassFiltersR = [];\n        const options = optionsFromArguments(Freeverb_Freeverb.getDefaults(), arguments, ["roomSize", "dampening"]);\n        this.roomSize = new Signal_Signal({\n            context: this.context,\n            value: options.roomSize,\n            units: "normalRange",\n        });\n        // make the allpass filters on the right\n        this._allpassFiltersL = allpassFilterFrequencies.map(freq => {\n            const allpassL = this.context.createBiquadFilter();\n            allpassL.type = "allpass";\n            allpassL.frequency.value = freq;\n            return allpassL;\n        });\n        // make the allpass filters on the left\n        this._allpassFiltersR = allpassFilterFrequencies.map(freq => {\n            const allpassR = this.context.createBiquadFilter();\n            allpassR.type = "allpass";\n            allpassR.frequency.value = freq;\n            return allpassR;\n        });\n        // make the comb filters\n        this._combFilters = combFilterTunings.map((delayTime, index) => {\n            const lfpf = new LowpassCombFilter_LowpassCombFilter({\n                context: this.context,\n                dampening: options.dampening,\n                delayTime,\n            });\n            if (index < combFilterTunings.length / 2) {\n                this.connectEffectLeft(lfpf, ...this._allpassFiltersL);\n            }\n            else {\n                this.connectEffectRight(lfpf, ...this._allpassFiltersR);\n            }\n            this.roomSize.connect(lfpf.resonance);\n            return lfpf;\n        });\n        readOnly(this, ["roomSize"]);\n    }\n    static getDefaults() {\n        return Object.assign(StereoEffect_StereoEffect.getDefaults(), {\n            roomSize: 0.7,\n            dampening: 3000\n        });\n    }\n    /**\n     * The amount of dampening of the reverberant signal.\n     */\n    get dampening() {\n        return this._combFilters[0].dampening;\n    }\n    set dampening(d) {\n        this._combFilters.forEach(c => c.dampening = d);\n    }\n    dispose() {\n        super.dispose();\n        this._allpassFiltersL.forEach(al => al.disconnect());\n        this._allpassFiltersR.forEach(ar => ar.disconnect());\n        this._combFilters.forEach(cf => cf.dispose());\n        this.roomSize.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Freeverb.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/JCReverb.js\n\n\n\n\n\n\n/**\n * an array of the comb filter delay time values\n */\nconst combFilterDelayTimes = [1687 / 25000, 1601 / 25000, 2053 / 25000, 2251 / 25000];\n/**\n * the resonances of each of the comb filters\n */\nconst combFilterResonances = [0.773, 0.802, 0.753, 0.733];\n/**\n * the allpass filter frequencies\n */\nconst allpassFilterFreqs = [347, 113, 37];\n/**\n * JCReverb is a simple [Schroeder Reverberator](https://ccrma.stanford.edu/~jos/pasp/Schroeder_Reverberators.html)\n * tuned by John Chowning in 1970.\n * It is made up of three allpass filters and four [[FeedbackCombFilter]].\n * JCReverb is now implemented with an AudioWorkletNode which may result on performance degradation on some platforms. Consider using [[Reverb]].\n * @example\n * const reverb = new Tone.JCReverb(0.4).toDestination();\n * const delay = new Tone.FeedbackDelay(0.5);\n * // connecting the synth to reverb through delay\n * const synth = new Tone.DuoSynth().chain(delay, reverb);\n * synth.triggerAttackRelease("A4", "8n");\n *\n * @category Effect\n */\nclass JCReverb_JCReverb extends StereoEffect_StereoEffect {\n    constructor() {\n        super(optionsFromArguments(JCReverb_JCReverb.getDefaults(), arguments, ["roomSize"]));\n        this.name = "JCReverb";\n        /**\n         * a series of allpass filters\n         */\n        this._allpassFilters = [];\n        /**\n         * parallel feedback comb filters\n         */\n        this._feedbackCombFilters = [];\n        const options = optionsFromArguments(JCReverb_JCReverb.getDefaults(), arguments, ["roomSize"]);\n        this.roomSize = new Signal_Signal({\n            context: this.context,\n            value: options.roomSize,\n            units: "normalRange",\n        });\n        this._scaleRoomSize = new Scale_Scale({\n            context: this.context,\n            min: -0.733,\n            max: 0.197,\n        });\n        // make the allpass filters\n        this._allpassFilters = allpassFilterFreqs.map(freq => {\n            const allpass = this.context.createBiquadFilter();\n            allpass.type = "allpass";\n            allpass.frequency.value = freq;\n            return allpass;\n        });\n        // and the comb filters\n        this._feedbackCombFilters = combFilterDelayTimes.map((delayTime, index) => {\n            const fbcf = new FeedbackCombFilter_FeedbackCombFilter({\n                context: this.context,\n                delayTime,\n            });\n            this._scaleRoomSize.connect(fbcf.resonance);\n            fbcf.resonance.value = combFilterResonances[index];\n            if (index < combFilterDelayTimes.length / 2) {\n                this.connectEffectLeft(...this._allpassFilters, fbcf);\n            }\n            else {\n                this.connectEffectRight(...this._allpassFilters, fbcf);\n            }\n            return fbcf;\n        });\n        // chain the allpass filters together\n        this.roomSize.connect(this._scaleRoomSize);\n        readOnly(this, ["roomSize"]);\n    }\n    static getDefaults() {\n        return Object.assign(StereoEffect_StereoEffect.getDefaults(), {\n            roomSize: 0.5,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._allpassFilters.forEach(apf => apf.disconnect());\n        this._feedbackCombFilters.forEach(fbcf => fbcf.dispose());\n        this.roomSize.dispose();\n        this._scaleRoomSize.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=JCReverb.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/StereoXFeedbackEffect.js\n\n\n/**\n * Just like a [[StereoFeedbackEffect]], but the feedback is routed from left to right\n * and right to left instead of on the same channel.\n * ```\n * +--------------------------------+ feedbackL <-----------------------------------+\n * |                                                                                |\n * +--\x3e                          +-----\x3e        +----\x3e                          +-----+\n *      feedbackMerge +--\x3e split        (EFFECT)       merge +--\x3e feedbackSplit     | |\n * +--\x3e                          +-----\x3e        +----\x3e                          +---+ |\n * |                                                                                  |\n * +--------------------------------+ feedbackR <-------------------------------------+\n * ```\n */\nclass StereoXFeedbackEffect_StereoXFeedbackEffect extends StereoFeedbackEffect_StereoFeedbackEffect {\n    constructor(options) {\n        super(options);\n        // the left output connected to the right input\n        this._feedbackL.disconnect();\n        this._feedbackL.connect(this._feedbackMerge, 0, 1);\n        // the left output connected to the right input\n        this._feedbackR.disconnect();\n        this._feedbackR.connect(this._feedbackMerge, 0, 0);\n        readOnly(this, ["feedback"]);\n    }\n}\n//# sourceMappingURL=StereoXFeedbackEffect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/PingPongDelay.js\n\n\n\n\n\n/**\n * PingPongDelay is a feedback delay effect where the echo is heard\n * first in one channel and next in the opposite channel. In a stereo\n * system these are the right and left channels.\n * PingPongDelay in more simplified terms is two Tone.FeedbackDelays\n * with independent delay values. Each delay is routed to one channel\n * (left or right), and the channel triggered second will always\n * trigger at the same interval after the first.\n * @example\n * const pingPong = new Tone.PingPongDelay("4n", 0.2).toDestination();\n * const drum = new Tone.MembraneSynth().connect(pingPong);\n * drum.triggerAttackRelease("C4", "32n");\n * @category Effect\n */\nclass PingPongDelay_PingPongDelay extends StereoXFeedbackEffect_StereoXFeedbackEffect {\n    constructor() {\n        super(optionsFromArguments(PingPongDelay_PingPongDelay.getDefaults(), arguments, ["delayTime", "feedback"]));\n        this.name = "PingPongDelay";\n        const options = optionsFromArguments(PingPongDelay_PingPongDelay.getDefaults(), arguments, ["delayTime", "feedback"]);\n        this._leftDelay = new Delay_Delay({\n            context: this.context,\n            maxDelay: options.maxDelay,\n        });\n        this._rightDelay = new Delay_Delay({\n            context: this.context,\n            maxDelay: options.maxDelay\n        });\n        this._rightPreDelay = new Delay_Delay({\n            context: this.context,\n            maxDelay: options.maxDelay\n        });\n        this.delayTime = new Signal_Signal({\n            context: this.context,\n            units: "time",\n            value: options.delayTime,\n        });\n        // connect it up\n        this.connectEffectLeft(this._leftDelay);\n        this.connectEffectRight(this._rightPreDelay, this._rightDelay);\n        this.delayTime.fan(this._leftDelay.delayTime, this._rightDelay.delayTime, this._rightPreDelay.delayTime);\n        // rearranged the feedback to be after the rightPreDelay\n        this._feedbackL.disconnect();\n        this._feedbackL.connect(this._rightDelay);\n        readOnly(this, ["delayTime"]);\n    }\n    static getDefaults() {\n        return Object.assign(StereoXFeedbackEffect_StereoXFeedbackEffect.getDefaults(), {\n            delayTime: 0.25,\n            maxDelay: 1\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._leftDelay.dispose();\n        this._rightDelay.dispose();\n        this._rightPreDelay.dispose();\n        this.delayTime.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=PingPongDelay.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/PitchShift.js\n\n\n\n\n\n\n\n\n/**\n * PitchShift does near-realtime pitch shifting to the incoming signal.\n * The effect is achieved by speeding up or slowing down the delayTime\n * of a DelayNode using a sawtooth wave.\n * Algorithm found in [this pdf](http://dsp-book.narod.ru/soundproc.pdf).\n * Additional reference by [Miller Pucket](http://msp.ucsd.edu/techniques/v0.11/book-html/node115.html).\n * @category Effect\n */\nclass PitchShift_PitchShift extends FeedbackEffect_FeedbackEffect {\n    constructor() {\n        super(optionsFromArguments(PitchShift_PitchShift.getDefaults(), arguments, ["pitch"]));\n        this.name = "PitchShift";\n        const options = optionsFromArguments(PitchShift_PitchShift.getDefaults(), arguments, ["pitch"]);\n        this._frequency = new Signal_Signal({ context: this.context });\n        this._delayA = new Delay_Delay({\n            maxDelay: 1,\n            context: this.context\n        });\n        this._lfoA = new LFO_LFO({\n            context: this.context,\n            min: 0,\n            max: 0.1,\n            type: "sawtooth"\n        }).connect(this._delayA.delayTime);\n        this._delayB = new Delay_Delay({\n            maxDelay: 1,\n            context: this.context\n        });\n        this._lfoB = new LFO_LFO({\n            context: this.context,\n            min: 0,\n            max: 0.1,\n            type: "sawtooth",\n            phase: 180\n        }).connect(this._delayB.delayTime);\n        this._crossFade = new CrossFade_CrossFade({ context: this.context });\n        this._crossFadeLFO = new LFO_LFO({\n            context: this.context,\n            min: 0,\n            max: 1,\n            type: "triangle",\n            phase: 90\n        }).connect(this._crossFade.fade);\n        this._feedbackDelay = new Delay_Delay({\n            delayTime: options.delayTime,\n            context: this.context,\n        });\n        this.delayTime = this._feedbackDelay.delayTime;\n        readOnly(this, "delayTime");\n        this._pitch = options.pitch;\n        this._windowSize = options.windowSize;\n        // connect the two delay lines up\n        this._delayA.connect(this._crossFade.a);\n        this._delayB.connect(this._crossFade.b);\n        // connect the frequency\n        this._frequency.fan(this._lfoA.frequency, this._lfoB.frequency, this._crossFadeLFO.frequency);\n        // route the input\n        this.effectSend.fan(this._delayA, this._delayB);\n        this._crossFade.chain(this._feedbackDelay, this.effectReturn);\n        // start the LFOs at the same time\n        const now = this.now();\n        this._lfoA.start(now);\n        this._lfoB.start(now);\n        this._crossFadeLFO.start(now);\n        // set the initial value\n        this.windowSize = this._windowSize;\n    }\n    static getDefaults() {\n        return Object.assign(FeedbackEffect_FeedbackEffect.getDefaults(), {\n            pitch: 0,\n            windowSize: 0.1,\n            delayTime: 0,\n            feedback: 0\n        });\n    }\n    /**\n     * Repitch the incoming signal by some interval (measured in semi-tones).\n     * @example\n     * const pitchShift = new Tone.PitchShift().toDestination();\n     * const osc = new Tone.Oscillator().connect(pitchShift).start().toDestination();\n     * pitchShift.pitch = -12; // down one octave\n     * pitchShift.pitch = 7; // up a fifth\n     */\n    get pitch() {\n        return this._pitch;\n    }\n    set pitch(interval) {\n        this._pitch = interval;\n        let factor = 0;\n        if (interval < 0) {\n            this._lfoA.min = 0;\n            this._lfoA.max = this._windowSize;\n            this._lfoB.min = 0;\n            this._lfoB.max = this._windowSize;\n            factor = intervalToFrequencyRatio(interval - 1) + 1;\n        }\n        else {\n            this._lfoA.min = this._windowSize;\n            this._lfoA.max = 0;\n            this._lfoB.min = this._windowSize;\n            this._lfoB.max = 0;\n            factor = intervalToFrequencyRatio(interval) - 1;\n        }\n        this._frequency.value = factor * (1.2 / this._windowSize);\n    }\n    /**\n     * The window size corresponds roughly to the sample length in a looping sampler.\n     * Smaller values are desirable for a less noticeable delay time of the pitch shifted\n     * signal, but larger values will result in smoother pitch shifting for larger intervals.\n     * A nominal range of 0.03 to 0.1 is recommended.\n     */\n    get windowSize() {\n        return this._windowSize;\n    }\n    set windowSize(size) {\n        this._windowSize = this.toSeconds(size);\n        this.pitch = this._pitch;\n    }\n    dispose() {\n        super.dispose();\n        this._frequency.dispose();\n        this._delayA.dispose();\n        this._delayB.dispose();\n        this._lfoA.dispose();\n        this._lfoB.dispose();\n        this._crossFade.dispose();\n        this._crossFadeLFO.dispose();\n        this._feedbackDelay.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=PitchShift.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Phaser.js\n\n\n\n\n\n/**\n * Phaser is a phaser effect. Phasers work by changing the phase\n * of different frequency components of an incoming signal. Read more on\n * [Wikipedia](https://en.wikipedia.org/wiki/Phaser_(effect)).\n * Inspiration for this phaser comes from [Tuna.js](https://github.com/Dinahmoe/tuna/).\n * @example\n * const phaser = new Tone.Phaser({\n * \tfrequency: 15,\n * \toctaves: 5,\n * \tbaseFrequency: 1000\n * }).toDestination();\n * const synth = new Tone.FMSynth().connect(phaser);\n * synth.triggerAttackRelease("E3", "2n");\n * @category Effect\n */\nclass Phaser_Phaser extends StereoEffect_StereoEffect {\n    constructor() {\n        super(optionsFromArguments(Phaser_Phaser.getDefaults(), arguments, ["frequency", "octaves", "baseFrequency"]));\n        this.name = "Phaser";\n        const options = optionsFromArguments(Phaser_Phaser.getDefaults(), arguments, ["frequency", "octaves", "baseFrequency"]);\n        this._lfoL = new LFO_LFO({\n            context: this.context,\n            frequency: options.frequency,\n            min: 0,\n            max: 1\n        });\n        this._lfoR = new LFO_LFO({\n            context: this.context,\n            frequency: options.frequency,\n            min: 0,\n            max: 1,\n            phase: 180,\n        });\n        this._baseFrequency = this.toFrequency(options.baseFrequency);\n        this._octaves = options.octaves;\n        this.Q = new Signal_Signal({\n            context: this.context,\n            value: options.Q,\n            units: "positive",\n        });\n        this._filtersL = this._makeFilters(options.stages, this._lfoL);\n        this._filtersR = this._makeFilters(options.stages, this._lfoR);\n        this.frequency = this._lfoL.frequency;\n        this.frequency.value = options.frequency;\n        // connect them up\n        this.connectEffectLeft(...this._filtersL);\n        this.connectEffectRight(...this._filtersR);\n        // control the frequency with one LFO\n        this._lfoL.frequency.connect(this._lfoR.frequency);\n        // set the options\n        this.baseFrequency = options.baseFrequency;\n        this.octaves = options.octaves;\n        // start the lfo\n        this._lfoL.start();\n        this._lfoR.start();\n        readOnly(this, ["frequency", "Q"]);\n    }\n    static getDefaults() {\n        return Object.assign(StereoEffect_StereoEffect.getDefaults(), {\n            frequency: 0.5,\n            octaves: 3,\n            stages: 10,\n            Q: 10,\n            baseFrequency: 350,\n        });\n    }\n    _makeFilters(stages, connectToFreq) {\n        const filters = [];\n        // make all the filters\n        for (let i = 0; i < stages; i++) {\n            const filter = this.context.createBiquadFilter();\n            filter.type = "allpass";\n            this.Q.connect(filter.Q);\n            connectToFreq.connect(filter.frequency);\n            filters.push(filter);\n        }\n        return filters;\n    }\n    /**\n     * The number of octaves the phase goes above the baseFrequency\n     */\n    get octaves() {\n        return this._octaves;\n    }\n    set octaves(octaves) {\n        this._octaves = octaves;\n        const max = this._baseFrequency * Math.pow(2, octaves);\n        this._lfoL.max = max;\n        this._lfoR.max = max;\n    }\n    /**\n     * The the base frequency of the filters.\n     */\n    get baseFrequency() {\n        return this._baseFrequency;\n    }\n    set baseFrequency(freq) {\n        this._baseFrequency = this.toFrequency(freq);\n        this._lfoL.min = this._baseFrequency;\n        this._lfoR.min = this._baseFrequency;\n        this.octaves = this._octaves;\n    }\n    dispose() {\n        super.dispose();\n        this.Q.dispose();\n        this._lfoL.dispose();\n        this._lfoR.dispose();\n        this._filtersL.forEach(f => f.disconnect());\n        this._filtersR.forEach(f => f.disconnect());\n        this.frequency.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Phaser.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Reverb.js\n\n\n\n\n\n\n\n\n\n/**\n * Simple convolution created with decaying noise.\n * Generates an Impulse Response Buffer\n * with Tone.Offline then feeds the IR into ConvolverNode.\n * The impulse response generation is async, so you have\n * to wait until [[ready]] resolves before it will make a sound.\n *\n * Inspiration from [ReverbGen](https://github.com/adelespinasse/reverbGen).\n * Copyright (c) 2014 Alan deLespinasse Apache 2.0 License.\n *\n * @category Effect\n */\nclass Reverb_Reverb extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(Reverb_Reverb.getDefaults(), arguments, ["decay"]));\n        this.name = "Reverb";\n        /**\n         * Convolver node\n         */\n        this._convolver = this.context.createConvolver();\n        /**\n         * Resolves when the reverb buffer is generated. Whenever either [[decay]]\n         * or [[preDelay]] are set, you have to wait until [[ready]] resolves\n         * before the IR is generated with the latest values.\n         */\n        this.ready = Promise.resolve();\n        const options = optionsFromArguments(Reverb_Reverb.getDefaults(), arguments, ["decay"]);\n        this._decay = options.decay;\n        this._preDelay = options.preDelay;\n        this.generate();\n        this.connectEffect(this._convolver);\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            decay: 1.5,\n            preDelay: 0.01,\n        });\n    }\n    /**\n     * The duration of the reverb.\n     */\n    get decay() {\n        return this._decay;\n    }\n    set decay(time) {\n        time = this.toSeconds(time);\n        assertRange(time, 0.001);\n        this._decay = time;\n        this.generate();\n    }\n    /**\n     * The amount of time before the reverb is fully ramped in.\n     */\n    get preDelay() {\n        return this._preDelay;\n    }\n    set preDelay(time) {\n        time = this.toSeconds(time);\n        assertRange(time, 0);\n        this._preDelay = time;\n        this.generate();\n    }\n    /**\n     * Generate the Impulse Response. Returns a promise while the IR is being generated.\n     * @return Promise which returns this object.\n     */\n    generate() {\n        return __awaiter(this, void 0, void 0, function* () {\n            const previousReady = this.ready;\n            // create a noise burst which decays over the duration in each channel\n            const context = new OfflineContext_OfflineContext(2, this._decay + this._preDelay, this.context.sampleRate);\n            const noiseL = new Noise_Noise({ context });\n            const noiseR = new Noise_Noise({ context });\n            const merge = new Merge_Merge({ context });\n            noiseL.connect(merge, 0, 0);\n            noiseR.connect(merge, 0, 1);\n            const gainNode = new Gain_Gain({ context }).toDestination();\n            merge.connect(gainNode);\n            noiseL.start(0);\n            noiseR.start(0);\n            // predelay\n            gainNode.gain.setValueAtTime(0, 0);\n            gainNode.gain.setValueAtTime(1, this._preDelay);\n            // decay\n            gainNode.gain.exponentialApproachValueAtTime(0, this._preDelay, this.decay);\n            // render the buffer\n            const renderPromise = context.render();\n            this.ready = renderPromise.then(noOp);\n            // wait for the previous `ready` to resolve\n            yield previousReady;\n            // set the buffer\n            this._convolver.buffer = (yield renderPromise).get();\n            return this;\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._convolver.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=Reverb.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/MidSideSplit.js\n\n\n\n\n\n\n/**\n * Mid/Side processing separates the the \'mid\' signal (which comes out of both the left and the right channel)\n * and the \'side\' (which only comes out of the the side channels).\n * ```\n * Mid = (Left+Right)/sqrt(2);   // obtain mid-signal from left and right\n * Side = (Left-Right)/sqrt(2);   // obtain side-signal from left and right\n * ```\n * @category Component\n */\nclass MidSideSplit_MidSideSplit extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(MidSideSplit_MidSideSplit.getDefaults(), arguments));\n        this.name = "MidSideSplit";\n        this._split = this.input = new Split_Split({\n            channels: 2,\n            context: this.context\n        });\n        this._midAdd = new Add_Add({ context: this.context });\n        this.mid = new Multiply_Multiply({\n            context: this.context,\n            value: Math.SQRT1_2,\n        });\n        this._sideSubtract = new Subtract_Subtract({ context: this.context });\n        this.side = new Multiply_Multiply({\n            context: this.context,\n            value: Math.SQRT1_2,\n        });\n        this._split.connect(this._midAdd, 0);\n        this._split.connect(this._midAdd.addend, 1);\n        this._split.connect(this._sideSubtract, 0);\n        this._split.connect(this._sideSubtract.subtrahend, 1);\n        this._midAdd.connect(this.mid);\n        this._sideSubtract.connect(this.side);\n    }\n    dispose() {\n        super.dispose();\n        this.mid.dispose();\n        this.side.dispose();\n        this._midAdd.dispose();\n        this._sideSubtract.dispose();\n        this._split.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MidSideSplit.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/MidSideMerge.js\n\n\n\n\n\n\n\n/**\n * MidSideMerge merges the mid and side signal after they\'ve been separated by [[MidSideSplit]]\n * ```\n * Mid = (Left+Right)/sqrt(2);   // obtain mid-signal from left and right\n * Side = (Left-Right)/sqrt(2);   // obtain side-signal from left and right\n * ```\n * @category Component\n */\nclass MidSideMerge_MidSideMerge extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(MidSideMerge_MidSideMerge.getDefaults(), arguments));\n        this.name = "MidSideMerge";\n        this.mid = new Gain_Gain({ context: this.context });\n        this.side = new Gain_Gain({ context: this.context });\n        this._left = new Add_Add({ context: this.context });\n        this._leftMult = new Multiply_Multiply({\n            context: this.context,\n            value: Math.SQRT1_2\n        });\n        this._right = new Subtract_Subtract({ context: this.context });\n        this._rightMult = new Multiply_Multiply({\n            context: this.context,\n            value: Math.SQRT1_2\n        });\n        this._merge = this.output = new Merge_Merge({ context: this.context });\n        this.mid.fan(this._left);\n        this.side.connect(this._left.addend);\n        this.mid.connect(this._right);\n        this.side.connect(this._right.subtrahend);\n        this._left.connect(this._leftMult);\n        this._right.connect(this._rightMult);\n        this._leftMult.connect(this._merge, 0, 0);\n        this._rightMult.connect(this._merge, 0, 1);\n    }\n    dispose() {\n        super.dispose();\n        this.mid.dispose();\n        this.side.dispose();\n        this._leftMult.dispose();\n        this._rightMult.dispose();\n        this._left.dispose();\n        this._right.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MidSideMerge.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/MidSideEffect.js\n\n\n\n/**\n * Mid/Side processing separates the the \'mid\' signal\n * (which comes out of both the left and the right channel)\n * and the \'side\' (which only comes out of the the side channels)\n * and effects them separately before being recombined.\n * Applies a Mid/Side seperation and recombination.\n * Algorithm found in [kvraudio forums](http://www.kvraudio.com/forum/viewtopic.php?t=212587).\n * This is a base-class for Mid/Side Effects.\n * @category Effect\n */\nclass MidSideEffect_MidSideEffect extends Effect_Effect {\n    constructor(options) {\n        super(options);\n        this.name = "MidSideEffect";\n        this._midSideMerge = new MidSideMerge_MidSideMerge({ context: this.context });\n        this._midSideSplit = new MidSideSplit_MidSideSplit({ context: this.context });\n        this._midSend = this._midSideSplit.mid;\n        this._sideSend = this._midSideSplit.side;\n        this._midReturn = this._midSideMerge.mid;\n        this._sideReturn = this._midSideMerge.side;\n        // the connections\n        this.effectSend.connect(this._midSideSplit);\n        this._midSideMerge.connect(this.effectReturn);\n    }\n    /**\n     * Connect the mid chain of the effect\n     */\n    connectEffectMid(...nodes) {\n        this._midSend.chain(...nodes, this._midReturn);\n    }\n    /**\n     * Connect the side chain of the effect\n     */\n    connectEffectSide(...nodes) {\n        this._sideSend.chain(...nodes, this._sideReturn);\n    }\n    dispose() {\n        super.dispose();\n        this._midSideSplit.dispose();\n        this._midSideMerge.dispose();\n        this._midSend.dispose();\n        this._sideSend.dispose();\n        this._midReturn.dispose();\n        this._sideReturn.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MidSideEffect.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/StereoWidener.js\n\n\n\n\n\n\n\n/**\n * Applies a width factor to the mid/side seperation.\n * 0 is all mid and 1 is all side.\n * Algorithm found in [kvraudio forums](http://www.kvraudio.com/forum/viewtopic.php?t=212587).\n * ```\n * Mid *= 2*(1-width)<br>\n * Side *= 2*width\n * ```\n * @category Effect\n */\nclass StereoWidener_StereoWidener extends MidSideEffect_MidSideEffect {\n    constructor() {\n        super(optionsFromArguments(StereoWidener_StereoWidener.getDefaults(), arguments, ["width"]));\n        this.name = "StereoWidener";\n        const options = optionsFromArguments(StereoWidener_StereoWidener.getDefaults(), arguments, ["width"]);\n        this.width = new Signal_Signal({\n            context: this.context,\n            value: options.width,\n            units: "normalRange",\n        });\n        readOnly(this, ["width"]);\n        this._twoTimesWidthMid = new Multiply_Multiply({\n            context: this.context,\n            value: 2,\n        });\n        this._twoTimesWidthSide = new Multiply_Multiply({\n            context: this.context,\n            value: 2,\n        });\n        this._midMult = new Multiply_Multiply({ context: this.context });\n        this._twoTimesWidthMid.connect(this._midMult.factor);\n        this.connectEffectMid(this._midMult);\n        this._oneMinusWidth = new Subtract_Subtract({ context: this.context });\n        this._oneMinusWidth.connect(this._twoTimesWidthMid);\n        ToneAudioNode_connect(this.context.getConstant(1), this._oneMinusWidth);\n        this.width.connect(this._oneMinusWidth.subtrahend);\n        this._sideMult = new Multiply_Multiply({ context: this.context });\n        this.width.connect(this._twoTimesWidthSide);\n        this._twoTimesWidthSide.connect(this._sideMult.factor);\n        this.connectEffectSide(this._sideMult);\n    }\n    static getDefaults() {\n        return Object.assign(MidSideEffect_MidSideEffect.getDefaults(), {\n            width: 0.5,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.width.dispose();\n        this._midMult.dispose();\n        this._sideMult.dispose();\n        this._twoTimesWidthMid.dispose();\n        this._twoTimesWidthSide.dispose();\n        this._oneMinusWidth.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=StereoWidener.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Tremolo.js\n\n\n\n\n\n\n/**\n * Tremolo modulates the amplitude of an incoming signal using an [[LFO]].\n * The effect is a stereo effect where the modulation phase is inverted in each channel.\n *\n * @example\n * // create a tremolo and start it\'s LFO\n * const tremolo = new Tone.Tremolo(9, 0.75).toDestination().start();\n * // route an oscillator through the tremolo and start it\n * const oscillator = new Tone.Oscillator().connect(tremolo).start();\n *\n * @category Effect\n */\nclass Tremolo_Tremolo extends StereoEffect_StereoEffect {\n    constructor() {\n        super(optionsFromArguments(Tremolo_Tremolo.getDefaults(), arguments, ["frequency", "depth"]));\n        this.name = "Tremolo";\n        const options = optionsFromArguments(Tremolo_Tremolo.getDefaults(), arguments, ["frequency", "depth"]);\n        this._lfoL = new LFO_LFO({\n            context: this.context,\n            type: options.type,\n            min: 1,\n            max: 0,\n        });\n        this._lfoR = new LFO_LFO({\n            context: this.context,\n            type: options.type,\n            min: 1,\n            max: 0,\n        });\n        this._amplitudeL = new Gain_Gain({ context: this.context });\n        this._amplitudeR = new Gain_Gain({ context: this.context });\n        this.frequency = new Signal_Signal({\n            context: this.context,\n            value: options.frequency,\n            units: "frequency",\n        });\n        this.depth = new Signal_Signal({\n            context: this.context,\n            value: options.depth,\n            units: "normalRange",\n        });\n        readOnly(this, ["frequency", "depth"]);\n        this.connectEffectLeft(this._amplitudeL);\n        this.connectEffectRight(this._amplitudeR);\n        this._lfoL.connect(this._amplitudeL.gain);\n        this._lfoR.connect(this._amplitudeR.gain);\n        this.frequency.fan(this._lfoL.frequency, this._lfoR.frequency);\n        this.depth.fan(this._lfoR.amplitude, this._lfoL.amplitude);\n        this.spread = options.spread;\n    }\n    static getDefaults() {\n        return Object.assign(StereoEffect_StereoEffect.getDefaults(), {\n            frequency: 10,\n            type: "sine",\n            depth: 0.5,\n            spread: 180,\n        });\n    }\n    /**\n     * Start the tremolo.\n     */\n    start(time) {\n        this._lfoL.start(time);\n        this._lfoR.start(time);\n        return this;\n    }\n    /**\n     * Stop the tremolo.\n     */\n    stop(time) {\n        this._lfoL.stop(time);\n        this._lfoR.stop(time);\n        return this;\n    }\n    /**\n     * Sync the effect to the transport.\n     */\n    sync() {\n        this._lfoL.sync();\n        this._lfoR.sync();\n        this.context.transport.syncSignal(this.frequency);\n        return this;\n    }\n    /**\n     * Unsync the filter from the transport\n     */\n    unsync() {\n        this._lfoL.unsync();\n        this._lfoR.unsync();\n        this.context.transport.unsyncSignal(this.frequency);\n        return this;\n    }\n    /**\n     * The oscillator type.\n     */\n    get type() {\n        return this._lfoL.type;\n    }\n    set type(type) {\n        this._lfoL.type = type;\n        this._lfoR.type = type;\n    }\n    /**\n     * Amount of stereo spread. When set to 0, both LFO\'s will be panned centrally.\n     * When set to 180, LFO\'s will be panned hard left and right respectively.\n     */\n    get spread() {\n        return this._lfoR.phase - this._lfoL.phase; // 180\n    }\n    set spread(spread) {\n        this._lfoL.phase = 90 - (spread / 2);\n        this._lfoR.phase = (spread / 2) + 90;\n    }\n    dispose() {\n        super.dispose();\n        this._lfoL.dispose();\n        this._lfoR.dispose();\n        this._amplitudeL.dispose();\n        this._amplitudeR.dispose();\n        this.frequency.dispose();\n        this.depth.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Tremolo.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/Vibrato.js\n\n\n\n\n\n/**\n * A Vibrato effect composed of a Tone.Delay and a Tone.LFO. The LFO\n * modulates the delayTime of the delay, causing the pitch to rise and fall.\n * @category Effect\n */\nclass Vibrato_Vibrato extends Effect_Effect {\n    constructor() {\n        super(optionsFromArguments(Vibrato_Vibrato.getDefaults(), arguments, ["frequency", "depth"]));\n        this.name = "Vibrato";\n        const options = optionsFromArguments(Vibrato_Vibrato.getDefaults(), arguments, ["frequency", "depth"]);\n        this._delayNode = new Delay_Delay({\n            context: this.context,\n            delayTime: 0,\n            maxDelay: options.maxDelay,\n        });\n        this._lfo = new LFO_LFO({\n            context: this.context,\n            type: options.type,\n            min: 0,\n            max: options.maxDelay,\n            frequency: options.frequency,\n            phase: -90 // offse the phase so the resting position is in the center\n        }).start().connect(this._delayNode.delayTime);\n        this.frequency = this._lfo.frequency;\n        this.depth = this._lfo.amplitude;\n        this.depth.value = options.depth;\n        readOnly(this, ["frequency", "depth"]);\n        this.effectSend.chain(this._delayNode, this.effectReturn);\n    }\n    static getDefaults() {\n        return Object.assign(Effect_Effect.getDefaults(), {\n            maxDelay: 0.005,\n            frequency: 5,\n            depth: 0.1,\n            type: "sine"\n        });\n    }\n    /**\n     * Type of oscillator attached to the Vibrato.\n     */\n    get type() {\n        return this._lfo.type;\n    }\n    set type(type) {\n        this._lfo.type = type;\n    }\n    dispose() {\n        super.dispose();\n        this._delayNode.dispose();\n        this._lfo.dispose();\n        this.frequency.dispose();\n        this.depth.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Vibrato.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/effect/index.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/Analyser.js\n\n\n\n\n\n/**\n * Wrapper around the native Web Audio\'s [AnalyserNode](http://webaudio.github.io/web-audio-api/#idl-def-AnalyserNode).\n * Extracts FFT or Waveform data from the incoming signal.\n * @category Component\n */\nclass Analyser_Analyser extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Analyser_Analyser.getDefaults(), arguments, ["type", "size"]));\n        this.name = "Analyser";\n        /**\n         * The analyser node.\n         */\n        this._analysers = [];\n        /**\n         * The buffer that the FFT data is written to\n         */\n        this._buffers = [];\n        const options = optionsFromArguments(Analyser_Analyser.getDefaults(), arguments, ["type", "size"]);\n        this.input = this.output = this._gain = new Gain_Gain({ context: this.context });\n        this._split = new Split_Split({\n            context: this.context,\n            channels: options.channels,\n        });\n        this.input.connect(this._split);\n        assertRange(options.channels, 1);\n        // create the analysers\n        for (let channel = 0; channel < options.channels; channel++) {\n            this._analysers[channel] = this.context.createAnalyser();\n            this._split.connect(this._analysers[channel], channel, 0);\n        }\n        // set the values initially\n        this.size = options.size;\n        this.type = options.type;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            size: 1024,\n            smoothing: 0.8,\n            type: "fft",\n            channels: 1,\n        });\n    }\n    /**\n     * Run the analysis given the current settings. If [[channels]] = 1,\n     * it will return a Float32Array. If [[channels]] > 1, it will\n     * return an array of Float32Arrays where each index in the array\n     * represents the analysis done on a channel.\n     */\n    getValue() {\n        this._analysers.forEach((analyser, index) => {\n            const buffer = this._buffers[index];\n            if (this._type === "fft") {\n                analyser.getFloatFrequencyData(buffer);\n            }\n            else if (this._type === "waveform") {\n                analyser.getFloatTimeDomainData(buffer);\n            }\n        });\n        if (this.channels === 1) {\n            return this._buffers[0];\n        }\n        else {\n            return this._buffers;\n        }\n    }\n    /**\n     * The size of analysis. This must be a power of two in the range 16 to 16384.\n     */\n    get size() {\n        return this._analysers[0].frequencyBinCount;\n    }\n    set size(size) {\n        this._analysers.forEach((analyser, index) => {\n            analyser.fftSize = size * 2;\n            this._buffers[index] = new Float32Array(size);\n        });\n    }\n    /**\n     * The number of channels the analyser does the analysis on. Channel\n     * separation is done using [[Split]]\n     */\n    get channels() {\n        return this._analysers.length;\n    }\n    /**\n     * The analysis function returned by analyser.getValue(), either "fft" or "waveform".\n     */\n    get type() {\n        return this._type;\n    }\n    set type(type) {\n        assert(type === "waveform" || type === "fft", `Analyser: invalid type: ${type}`);\n        this._type = type;\n    }\n    /**\n     * 0 represents no time averaging with the last analysis frame.\n     */\n    get smoothing() {\n        return this._analysers[0].smoothingTimeConstant;\n    }\n    set smoothing(val) {\n        this._analysers.forEach(a => a.smoothingTimeConstant = val);\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        this._analysers.forEach(a => a.disconnect());\n        this._split.dispose();\n        this._gain.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Analyser.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/MeterBase.js\n\n\n\n/**\n * The base class for Metering classes.\n */\nclass MeterBase_MeterBase extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(MeterBase_MeterBase.getDefaults(), arguments));\n        this.name = "MeterBase";\n        this.input = this.output = this._analyser = new Analyser_Analyser({\n            context: this.context,\n            size: 256,\n            type: "waveform",\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._analyser.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MeterBase.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/Meter.js\n\n\n\n\n\n/**\n * Meter gets the [RMS](https://en.wikipedia.org/wiki/Root_mean_square)\n * of an input signal. It can also get the raw value of the input signal.\n *\n * @example\n * const meter = new Tone.Meter();\n * const mic = new Tone.UserMedia();\n * mic.open();\n * // connect mic to the meter\n * mic.connect(meter);\n * // the current level of the mic\n * setInterval(() => console.log(meter.getValue()), 100);\n * @category Component\n */\nclass Meter_Meter extends MeterBase_MeterBase {\n    constructor() {\n        super(optionsFromArguments(Meter_Meter.getDefaults(), arguments, ["smoothing"]));\n        this.name = "Meter";\n        /**\n         * The previous frame\'s value\n         */\n        this._rms = 0;\n        const options = optionsFromArguments(Meter_Meter.getDefaults(), arguments, ["smoothing"]);\n        this.input = this.output = this._analyser = new Analyser_Analyser({\n            context: this.context,\n            size: 256,\n            type: "waveform",\n            channels: options.channels,\n        });\n        this.smoothing = options.smoothing,\n            this.normalRange = options.normalRange;\n    }\n    static getDefaults() {\n        return Object.assign(MeterBase_MeterBase.getDefaults(), {\n            smoothing: 0.8,\n            normalRange: false,\n            channels: 1,\n        });\n    }\n    /**\n     * Use [[getValue]] instead. For the previous getValue behavior, use DCMeter.\n     * @deprecated\n     */\n    getLevel() {\n        warn("\'getLevel\' has been changed to \'getValue\'");\n        return this.getValue();\n    }\n    /**\n     * Get the current value of the incoming signal.\n     * Output is in decibels when [[normalRange]] is `false`.\n     * If [[channels]] = 1, then the output is a single number\n     * representing the value of the input signal. When [[channels]] > 1,\n     * then each channel is returned as a value in a number array.\n     */\n    getValue() {\n        const aValues = this._analyser.getValue();\n        const channelValues = this.channels === 1 ? [aValues] : aValues;\n        const vals = channelValues.map(values => {\n            const totalSquared = values.reduce((total, current) => total + current * current, 0);\n            const rms = Math.sqrt(totalSquared / values.length);\n            // the rms can only fall at the rate of the smoothing\n            // but can jump up instantly\n            this._rms = Math.max(rms, this._rms * this.smoothing);\n            return this.normalRange ? this._rms : gainToDb(this._rms);\n        });\n        if (this.channels === 1) {\n            return vals[0];\n        }\n        else {\n            return vals;\n        }\n    }\n    /**\n     * The number of channels of analysis.\n     */\n    get channels() {\n        return this._analyser.channels;\n    }\n    dispose() {\n        super.dispose();\n        this._analyser.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Meter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/FFT.js\n\n\n\n\n\n/**\n * Get the current frequency data of the connected audio source using a fast Fourier transform.\n * @category Component\n */\nclass FFT_FFT extends MeterBase_MeterBase {\n    constructor() {\n        super(optionsFromArguments(FFT_FFT.getDefaults(), arguments, ["size"]));\n        this.name = "FFT";\n        const options = optionsFromArguments(FFT_FFT.getDefaults(), arguments, ["size"]);\n        this.normalRange = options.normalRange;\n        this._analyser.type = "fft";\n        this.size = options.size;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            normalRange: false,\n            size: 1024,\n            smoothing: 0.8,\n        });\n    }\n    /**\n     * Gets the current frequency data from the connected audio source.\n     * Returns the frequency data of length [[size]] as a Float32Array of decibel values.\n     */\n    getValue() {\n        const values = this._analyser.getValue();\n        return values.map(v => this.normalRange ? dbToGain(v) : v);\n    }\n    /**\n     * The size of analysis. This must be a power of two in the range 16 to 16384.\n     * Determines the size of the array returned by [[getValue]] (i.e. the number of\n     * frequency bins). Large FFT sizes may be costly to compute.\n     */\n    get size() {\n        return this._analyser.size;\n    }\n    set size(size) {\n        this._analyser.size = size;\n    }\n    /**\n     * 0 represents no time averaging with the last analysis frame.\n     */\n    get smoothing() {\n        return this._analyser.smoothing;\n    }\n    set smoothing(val) {\n        this._analyser.smoothing = val;\n    }\n    /**\n     * Returns the frequency value in hertz of each of the indices of the FFT\'s [[getValue]] response.\n     * @example\n     * const fft = new Tone.FFT(32);\n     * console.log([0, 1, 2, 3, 4].map(index => fft.getFrequencyOfIndex(index)));\n     */\n    getFrequencyOfIndex(index) {\n        assert(0 <= index && index < this.size, `index must be greater than or equal to 0 and less than ${this.size}`);\n        return index * this.context.sampleRate / (this.size * 2);\n    }\n}\n//# sourceMappingURL=FFT.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/DCMeter.js\n\n\n/**\n * DCMeter gets the raw value of the input signal at the current time.\n *\n * @example\n * const meter = new Tone.DCMeter();\n * const mic = new Tone.UserMedia();\n * mic.open();\n * // connect mic to the meter\n * mic.connect(meter);\n * // the current level of the mic\n * const level = meter.getValue();\n * @category Component\n */\nclass DCMeter_DCMeter extends MeterBase_MeterBase {\n    constructor() {\n        super(optionsFromArguments(DCMeter_DCMeter.getDefaults(), arguments));\n        this.name = "DCMeter";\n        this._analyser.type = "waveform";\n        this._analyser.size = 256;\n    }\n    /**\n     * Get the signal value of the incoming signal\n     */\n    getValue() {\n        const value = this._analyser.getValue();\n        return value[0];\n    }\n}\n//# sourceMappingURL=DCMeter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/analysis/Waveform.js\n\n\n/**\n * Get the current waveform data of the connected audio source.\n * @category Component\n */\nclass Waveform_Waveform extends MeterBase_MeterBase {\n    constructor() {\n        super(optionsFromArguments(Waveform_Waveform.getDefaults(), arguments, ["size"]));\n        this.name = "Waveform";\n        const options = optionsFromArguments(Waveform_Waveform.getDefaults(), arguments, ["size"]);\n        this._analyser.type = "waveform";\n        this.size = options.size;\n    }\n    static getDefaults() {\n        return Object.assign(MeterBase_MeterBase.getDefaults(), {\n            size: 1024,\n        });\n    }\n    /**\n     * Return the waveform for the current time as a Float32Array where each value in the array\n     * represents a sample in the waveform.\n     */\n    getValue() {\n        return this._analyser.getValue();\n    }\n    /**\n     * The size of analysis. This must be a power of two in the range 16 to 16384.\n     * Determines the size of the array returned by [[getValue]].\n     */\n    get size() {\n        return this._analyser.size;\n    }\n    set size(size) {\n        this._analyser.size = size;\n    }\n}\n//# sourceMappingURL=Waveform.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Solo.js\n\n\n\n/**\n * Solo lets you isolate a specific audio stream. When an instance is set to `solo=true`,\n * it will mute all other instances of Solo.\n * @example\n * const soloA = new Tone.Solo().toDestination();\n * const oscA = new Tone.Oscillator("C4", "sawtooth").connect(soloA);\n * const soloB = new Tone.Solo().toDestination();\n * const oscB = new Tone.Oscillator("E4", "square").connect(soloB);\n * soloA.solo = true;\n * // no audio will pass through soloB\n * @category Component\n */\nclass Solo_Solo extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Solo_Solo.getDefaults(), arguments, ["solo"]));\n        this.name = "Solo";\n        const options = optionsFromArguments(Solo_Solo.getDefaults(), arguments, ["solo"]);\n        this.input = this.output = new Gain_Gain({\n            context: this.context,\n        });\n        if (!Solo_Solo._allSolos.has(this.context)) {\n            Solo_Solo._allSolos.set(this.context, new Set());\n        }\n        Solo_Solo._allSolos.get(this.context).add(this);\n        // set initially\n        this.solo = options.solo;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            solo: false,\n        });\n    }\n    /**\n     * Isolates this instance and mutes all other instances of Solo.\n     * Only one instance can be soloed at a time. A soloed\n     * instance will report `solo=false` when another instance is soloed.\n     */\n    get solo() {\n        return this._isSoloed();\n    }\n    set solo(solo) {\n        if (solo) {\n            this._addSolo();\n        }\n        else {\n            this._removeSolo();\n        }\n        Solo_Solo._allSolos.get(this.context).forEach(instance => instance._updateSolo());\n    }\n    /**\n     * If the current instance is muted, i.e. another instance is soloed\n     */\n    get muted() {\n        return this.input.gain.value === 0;\n    }\n    /**\n     * Add this to the soloed array\n     */\n    _addSolo() {\n        if (!Solo_Solo._soloed.has(this.context)) {\n            Solo_Solo._soloed.set(this.context, new Set());\n        }\n        Solo_Solo._soloed.get(this.context).add(this);\n    }\n    /**\n     * Remove this from the soloed array\n     */\n    _removeSolo() {\n        if (Solo_Solo._soloed.has(this.context)) {\n            Solo_Solo._soloed.get(this.context).delete(this);\n        }\n    }\n    /**\n     * Is this on the soloed array\n     */\n    _isSoloed() {\n        return Solo_Solo._soloed.has(this.context) && Solo_Solo._soloed.get(this.context).has(this);\n    }\n    /**\n     * Returns true if no one is soloed\n     */\n    _noSolos() {\n        // either does not have any soloed added\n        return !Solo_Solo._soloed.has(this.context) ||\n            // or has a solo set but doesn\'t include any items\n            (Solo_Solo._soloed.has(this.context) && Solo_Solo._soloed.get(this.context).size === 0);\n    }\n    /**\n     * Solo the current instance and unsolo all other instances.\n     */\n    _updateSolo() {\n        if (this._isSoloed()) {\n            this.input.gain.value = 1;\n        }\n        else if (this._noSolos()) {\n            // no one is soloed\n            this.input.gain.value = 1;\n        }\n        else {\n            this.input.gain.value = 0;\n        }\n    }\n    dispose() {\n        super.dispose();\n        Solo_Solo._allSolos.get(this.context).delete(this);\n        this._removeSolo();\n        return this;\n    }\n}\n/**\n * Hold all of the solo\'ed tracks belonging to a specific context\n */\nSolo_Solo._allSolos = new Map();\n/**\n * Hold the currently solo\'ed instance(s)\n */\nSolo_Solo._soloed = new Map();\n//# sourceMappingURL=Solo.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/PanVol.js\n\n\n\n\n\n/**\n * PanVol is a Tone.Panner and Tone.Volume in one.\n * @example\n * // pan the incoming signal left and drop the volume\n * const panVol = new Tone.PanVol(-0.25, -12).toDestination();\n * const osc = new Tone.Oscillator().connect(panVol).start();\n * @category Component\n */\nclass PanVol_PanVol extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(PanVol_PanVol.getDefaults(), arguments, ["pan", "volume"]));\n        this.name = "PanVol";\n        const options = optionsFromArguments(PanVol_PanVol.getDefaults(), arguments, ["pan", "volume"]);\n        this._panner = this.input = new Panner_Panner({\n            context: this.context,\n            pan: options.pan,\n            channelCount: options.channelCount,\n        });\n        this.pan = this._panner.pan;\n        this._volume = this.output = new Volume_Volume({\n            context: this.context,\n            volume: options.volume,\n        });\n        this.volume = this._volume.volume;\n        // connections\n        this._panner.connect(this._volume);\n        this.mute = options.mute;\n        readOnly(this, ["pan", "volume"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            mute: false,\n            pan: 0,\n            volume: 0,\n            channelCount: 1,\n        });\n    }\n    /**\n     * Mute/unmute the volume\n     */\n    get mute() {\n        return this._volume.mute;\n    }\n    set mute(mute) {\n        this._volume.mute = mute;\n    }\n    dispose() {\n        super.dispose();\n        this._panner.dispose();\n        this.pan.dispose();\n        this._volume.dispose();\n        this.volume.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=PanVol.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Channel.js\n\n\n\n\n\n\n/**\n * Channel provides a channel strip interface with volume, pan, solo and mute controls.\n * See [[PanVol]] and [[Solo]]\n * @example\n * // pan the incoming signal left and drop the volume 12db\n * const channel = new Tone.Channel(-0.25, -12);\n * @category Component\n */\nclass Channel_Channel extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Channel_Channel.getDefaults(), arguments, ["volume", "pan"]));\n        this.name = "Channel";\n        const options = optionsFromArguments(Channel_Channel.getDefaults(), arguments, ["volume", "pan"]);\n        this._solo = this.input = new Solo_Solo({\n            solo: options.solo,\n            context: this.context,\n        });\n        this._panVol = this.output = new PanVol_PanVol({\n            context: this.context,\n            pan: options.pan,\n            volume: options.volume,\n            mute: options.mute,\n            channelCount: options.channelCount\n        });\n        this.pan = this._panVol.pan;\n        this.volume = this._panVol.volume;\n        this._solo.connect(this._panVol);\n        readOnly(this, ["pan", "volume"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            pan: 0,\n            volume: 0,\n            mute: false,\n            solo: false,\n            channelCount: 1,\n        });\n    }\n    /**\n     * Solo/unsolo the channel. Soloing is only relative to other [[Channels]] and [[Solo]] instances\n     */\n    get solo() {\n        return this._solo.solo;\n    }\n    set solo(solo) {\n        this._solo.solo = solo;\n    }\n    /**\n     * If the current instance is muted, i.e. another instance is soloed,\n     * or the channel is muted\n     */\n    get muted() {\n        return this._solo.muted || this.mute;\n    }\n    /**\n     * Mute/unmute the volume\n     */\n    get mute() {\n        return this._panVol.mute;\n    }\n    set mute(mute) {\n        this._panVol.mute = mute;\n    }\n    /**\n     * Get the gain node belonging to the bus name. Create it if\n     * it doesn\'t exist\n     * @param name The bus name\n     */\n    _getBus(name) {\n        if (!Channel_Channel.buses.has(name)) {\n            Channel_Channel.buses.set(name, new Gain_Gain({ context: this.context }));\n        }\n        return Channel_Channel.buses.get(name);\n    }\n    /**\n     * Send audio to another channel using a string. `send` is a lot like\n     * [[connect]], except it uses a string instead of an object. This can\n     * be useful in large applications to decouple sections since [[send]]\n     * and [[receive]] can be invoked separately in order to connect an object\n     * @param name The channel name to send the audio\n     * @param volume The amount of the signal to send.\n     * \tDefaults to 0db, i.e. send the entire signal\n     * @returns Returns the gain node of this connection.\n     */\n    send(name, volume = 0) {\n        const bus = this._getBus(name);\n        const sendKnob = new Gain_Gain({\n            context: this.context,\n            units: "decibels",\n            gain: volume,\n        });\n        this.connect(sendKnob);\n        sendKnob.connect(bus);\n        return sendKnob;\n    }\n    /**\n     * Receive audio from a channel which was connected with [[send]].\n     * @param name The channel name to receive audio from.\n     */\n    receive(name) {\n        const bus = this._getBus(name);\n        bus.connect(this);\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this._panVol.dispose();\n        this.pan.dispose();\n        this.volume.dispose();\n        this._solo.dispose();\n        return this;\n    }\n}\n/**\n * Store the send/receive channels by name.\n */\nChannel_Channel.buses = new Map();\n//# sourceMappingURL=Channel.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Mono.js\n\n\n\n\n/**\n * Mono coerces the incoming mono or stereo signal into a mono signal\n * where both left and right channels have the same value. This can be useful\n * for [stereo imaging](https://en.wikipedia.org/wiki/Stereo_imaging).\n * @category Component\n */\nclass Mono_Mono extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Mono_Mono.getDefaults(), arguments));\n        this.name = "Mono";\n        this.input = new Gain_Gain({ context: this.context });\n        this._merge = this.output = new Merge_Merge({\n            channels: 2,\n            context: this.context,\n        });\n        this.input.connect(this._merge, 0, 0);\n        this.input.connect(this._merge, 0, 1);\n    }\n    dispose() {\n        super.dispose();\n        this._merge.dispose();\n        this.input.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Mono.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/MultibandSplit.js\n\n\n\n\n\n\n/**\n * Split the incoming signal into three bands (low, mid, high)\n * with two crossover frequency controls.\n * ```\n *            +----------------------+\n *          +-> input < lowFrequency +------------------\x3e low\n *          | +----------------------+\n *          |\n *          | +--------------------------------------+\n * input ---+-> lowFrequency < input < highFrequency +--\x3e mid\n *          | +--------------------------------------+\n *          |\n *          | +-----------------------+\n *          +-> highFrequency < input +-----------------\x3e high\n *            +-----------------------+\n * ```\n * @category Component\n */\nclass MultibandSplit_MultibandSplit extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(MultibandSplit_MultibandSplit.getDefaults(), arguments, ["lowFrequency", "highFrequency"]));\n        this.name = "MultibandSplit";\n        /**\n         * the input\n         */\n        this.input = new Gain_Gain({ context: this.context });\n        /**\n         * no output node, use either low, mid or high outputs\n         */\n        this.output = undefined;\n        /**\n         * The low band.\n         */\n        this.low = new Filter_Filter({\n            context: this.context,\n            frequency: 0,\n            type: "lowpass",\n        });\n        /**\n         * the lower filter of the mid band\n         */\n        this._lowMidFilter = new Filter_Filter({\n            context: this.context,\n            frequency: 0,\n            type: "highpass",\n        });\n        /**\n         * The mid band output.\n         */\n        this.mid = new Filter_Filter({\n            context: this.context,\n            frequency: 0,\n            type: "lowpass",\n        });\n        /**\n         * The high band output.\n         */\n        this.high = new Filter_Filter({\n            context: this.context,\n            frequency: 0,\n            type: "highpass",\n        });\n        this._internalChannels = [this.low, this.mid, this.high];\n        const options = optionsFromArguments(MultibandSplit_MultibandSplit.getDefaults(), arguments, ["lowFrequency", "highFrequency"]);\n        this.lowFrequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.lowFrequency,\n        });\n        this.highFrequency = new Signal_Signal({\n            context: this.context,\n            units: "frequency",\n            value: options.highFrequency,\n        });\n        this.Q = new Signal_Signal({\n            context: this.context,\n            units: "positive",\n            value: options.Q,\n        });\n        this.input.fan(this.low, this.high);\n        this.input.chain(this._lowMidFilter, this.mid);\n        // the frequency control signal\n        this.lowFrequency.fan(this.low.frequency, this._lowMidFilter.frequency);\n        this.highFrequency.fan(this.mid.frequency, this.high.frequency);\n        // the Q value\n        this.Q.connect(this.low.Q);\n        this.Q.connect(this._lowMidFilter.Q);\n        this.Q.connect(this.mid.Q);\n        this.Q.connect(this.high.Q);\n        readOnly(this, ["high", "mid", "low", "highFrequency", "lowFrequency"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            Q: 1,\n            highFrequency: 2500,\n            lowFrequency: 400,\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        writable(this, ["high", "mid", "low", "highFrequency", "lowFrequency"]);\n        this.low.dispose();\n        this._lowMidFilter.dispose();\n        this.mid.dispose();\n        this.high.dispose();\n        this.lowFrequency.dispose();\n        this.highFrequency.dispose();\n        this.Q.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MultibandSplit.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/core/context/Listener.js\n\n\n\n/**\n * Tone.Listener is a thin wrapper around the AudioListener. Listener combined\n * with [[Panner3D]] makes up the Web Audio API\'s 3D panning system. Panner3D allows you\n * to place sounds in 3D and Listener allows you to navigate the 3D sound environment from\n * a first-person perspective. There is only one listener per audio context.\n */\nclass Listener_Listener extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(...arguments);\n        this.name = "Listener";\n        this.positionX = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.positionX,\n        });\n        this.positionY = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.positionY,\n        });\n        this.positionZ = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.positionZ,\n        });\n        this.forwardX = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.forwardX,\n        });\n        this.forwardY = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.forwardY,\n        });\n        this.forwardZ = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.forwardZ,\n        });\n        this.upX = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.upX,\n        });\n        this.upY = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.upY,\n        });\n        this.upZ = new Param_Param({\n            context: this.context,\n            param: this.context.rawContext.listener.upZ,\n        });\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            positionX: 0,\n            positionY: 0,\n            positionZ: 0,\n            forwardX: 0,\n            forwardY: 0,\n            forwardZ: -1,\n            upX: 0,\n            upY: 1,\n            upZ: 0,\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.positionX.dispose();\n        this.positionY.dispose();\n        this.positionZ.dispose();\n        this.forwardX.dispose();\n        this.forwardY.dispose();\n        this.forwardZ.dispose();\n        this.upX.dispose();\n        this.upY.dispose();\n        this.upZ.dispose();\n        return this;\n    }\n}\n//-------------------------------------\n// \tINITIALIZATION\n//-------------------------------------\nonContextInit(context => {\n    context.listener = new Listener_Listener({ context });\n});\nonContextClose(context => {\n    context.listener.dispose();\n});\n//# sourceMappingURL=Listener.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Panner3D.js\n\n\n\n\n/**\n * A spatialized panner node which supports equalpower or HRTF panning.\n * @category Component\n */\nclass Panner3D_Panner3D extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Panner3D_Panner3D.getDefaults(), arguments, ["positionX", "positionY", "positionZ"]));\n        this.name = "Panner3D";\n        const options = optionsFromArguments(Panner3D_Panner3D.getDefaults(), arguments, ["positionX", "positionY", "positionZ"]);\n        this._panner = this.input = this.output = this.context.createPanner();\n        // set some values\n        this.panningModel = options.panningModel;\n        this.maxDistance = options.maxDistance;\n        this.distanceModel = options.distanceModel;\n        this.coneOuterGain = options.coneOuterGain;\n        this.coneOuterAngle = options.coneOuterAngle;\n        this.coneInnerAngle = options.coneInnerAngle;\n        this.refDistance = options.refDistance;\n        this.rolloffFactor = options.rolloffFactor;\n        this.positionX = new Param_Param({\n            context: this.context,\n            param: this._panner.positionX,\n            value: options.positionX,\n        });\n        this.positionY = new Param_Param({\n            context: this.context,\n            param: this._panner.positionY,\n            value: options.positionY,\n        });\n        this.positionZ = new Param_Param({\n            context: this.context,\n            param: this._panner.positionZ,\n            value: options.positionZ,\n        });\n        this.orientationX = new Param_Param({\n            context: this.context,\n            param: this._panner.orientationX,\n            value: options.orientationX,\n        });\n        this.orientationY = new Param_Param({\n            context: this.context,\n            param: this._panner.orientationY,\n            value: options.orientationY,\n        });\n        this.orientationZ = new Param_Param({\n            context: this.context,\n            param: this._panner.orientationZ,\n            value: options.orientationZ,\n        });\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            coneInnerAngle: 360,\n            coneOuterAngle: 360,\n            coneOuterGain: 0,\n            distanceModel: "inverse",\n            maxDistance: 10000,\n            orientationX: 0,\n            orientationY: 0,\n            orientationZ: 0,\n            panningModel: "equalpower",\n            positionX: 0,\n            positionY: 0,\n            positionZ: 0,\n            refDistance: 1,\n            rolloffFactor: 1,\n        });\n    }\n    /**\n     * Sets the position of the source in 3d space.\n     */\n    setPosition(x, y, z) {\n        this.positionX.value = x;\n        this.positionY.value = y;\n        this.positionZ.value = z;\n        return this;\n    }\n    /**\n     * Sets the orientation of the source in 3d space.\n     */\n    setOrientation(x, y, z) {\n        this.orientationX.value = x;\n        this.orientationY.value = y;\n        this.orientationZ.value = z;\n        return this;\n    }\n    /**\n     * The panning model. Either "equalpower" or "HRTF".\n     */\n    get panningModel() {\n        return this._panner.panningModel;\n    }\n    set panningModel(val) {\n        this._panner.panningModel = val;\n    }\n    /**\n     * A reference distance for reducing volume as source move further from the listener\n     */\n    get refDistance() {\n        return this._panner.refDistance;\n    }\n    set refDistance(val) {\n        this._panner.refDistance = val;\n    }\n    /**\n     * Describes how quickly the volume is reduced as source moves away from listener.\n     */\n    get rolloffFactor() {\n        return this._panner.rolloffFactor;\n    }\n    set rolloffFactor(val) {\n        this._panner.rolloffFactor = val;\n    }\n    /**\n     * The distance model used by,  "linear", "inverse", or "exponential".\n     */\n    get distanceModel() {\n        return this._panner.distanceModel;\n    }\n    set distanceModel(val) {\n        this._panner.distanceModel = val;\n    }\n    /**\n     * The angle, in degrees, inside of which there will be no volume reduction\n     */\n    get coneInnerAngle() {\n        return this._panner.coneInnerAngle;\n    }\n    set coneInnerAngle(val) {\n        this._panner.coneInnerAngle = val;\n    }\n    /**\n     * The angle, in degrees, outside of which the volume will be reduced\n     * to a constant value of coneOuterGain\n     */\n    get coneOuterAngle() {\n        return this._panner.coneOuterAngle;\n    }\n    set coneOuterAngle(val) {\n        this._panner.coneOuterAngle = val;\n    }\n    /**\n     * The gain outside of the coneOuterAngle\n     */\n    get coneOuterGain() {\n        return this._panner.coneOuterGain;\n    }\n    set coneOuterGain(val) {\n        this._panner.coneOuterGain = val;\n    }\n    /**\n     * The maximum distance between source and listener,\n     * after which the volume will not be reduced any further.\n     */\n    get maxDistance() {\n        return this._panner.maxDistance;\n    }\n    set maxDistance(val) {\n        this._panner.maxDistance = val;\n    }\n    dispose() {\n        super.dispose();\n        this._panner.disconnect();\n        this.orientationX.dispose();\n        this.orientationY.dispose();\n        this.orientationZ.dispose();\n        this.positionX.dispose();\n        this.positionY.dispose();\n        this.positionZ.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Panner3D.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/channel/Recorder.js\n\n\n\n\n\n\n/**\n * A wrapper around the MediaRecorder API. Unlike the rest of Tone.js, this module does not offer\n * any sample-accurate scheduling because it is not a feature of the MediaRecorder API.\n * This is only natively supported in Chrome and Firefox.\n * For a cross-browser shim, install (audio-recorder-polyfill)[https://www.npmjs.com/package/audio-recorder-polyfill].\n * @example\n * const recorder = new Tone.Recorder();\n * const synth = new Tone.Synth().connect(recorder);\n * // start recording\n * recorder.start();\n * // generate a few notes\n * synth.triggerAttackRelease("C3", 0.5);\n * synth.triggerAttackRelease("C4", 0.5, "+1");\n * synth.triggerAttackRelease("C5", 0.5, "+2");\n * // wait for the notes to end and stop the recording\n * setTimeout(async () => {\n * \t// the recorded audio is returned as a blob\n * \tconst recording = await recorder.stop();\n * \t// download the recording by creating an anchor element and blob url\n * \tconst url = URL.createObjectURL(recording);\n * \tconst anchor = document.createElement("a");\n * \tanchor.download = "recording.webm";\n * \tanchor.href = url;\n * \tanchor.click();\n * }, 4000);\n * @category Component\n */\nclass Recorder_Recorder extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Recorder_Recorder.getDefaults(), arguments));\n        this.name = "Recorder";\n        const options = optionsFromArguments(Recorder_Recorder.getDefaults(), arguments);\n        this.input = new Gain_Gain({\n            context: this.context\n        });\n        assert(Recorder_Recorder.supported, "Media Recorder API is not available");\n        this._stream = this.context.createMediaStreamDestination();\n        this.input.connect(this._stream);\n        this._recorder = new MediaRecorder(this._stream.stream, {\n            mimeType: options.mimeType\n        });\n    }\n    static getDefaults() {\n        return ToneAudioNode_ToneAudioNode.getDefaults();\n    }\n    /**\n     * The mime type is the format that the audio is encoded in. For Chrome\n     * that is typically webm encoded as "vorbis".\n     */\n    get mimeType() {\n        return this._recorder.mimeType;\n    }\n    /**\n     * Test if your platform supports the Media Recorder API. If it\'s not available,\n     * try installing this (polyfill)[https://www.npmjs.com/package/audio-recorder-polyfill].\n     */\n    static get supported() {\n        return theWindow !== null && Reflect.has(theWindow, "MediaRecorder");\n    }\n    /**\n     * Get the playback state of the Recorder, either "started", "stopped" or "paused"\n     */\n    get state() {\n        if (this._recorder.state === "inactive") {\n            return "stopped";\n        }\n        else if (this._recorder.state === "paused") {\n            return "paused";\n        }\n        else {\n            return "started";\n        }\n    }\n    /**\n     * Start the Recorder. Returns a promise which resolves\n     * when the recorder has started.\n     */\n    start() {\n        return __awaiter(this, void 0, void 0, function* () {\n            assert(this.state !== "started", "Recorder is already started");\n            const startPromise = new Promise(done => {\n                const handleStart = () => {\n                    this._recorder.removeEventListener("start", handleStart, false);\n                    done();\n                };\n                this._recorder.addEventListener("start", handleStart, false);\n            });\n            this._recorder.start();\n            return yield startPromise;\n        });\n    }\n    /**\n     * Stop the recorder. Returns a promise with the recorded content until this point\n     * encoded as [[mimeType]]\n     */\n    stop() {\n        return __awaiter(this, void 0, void 0, function* () {\n            assert(this.state !== "stopped", "Recorder is not started");\n            const dataPromise = new Promise(done => {\n                const handleData = (e) => {\n                    this._recorder.removeEventListener("dataavailable", handleData, false);\n                    done(e.data);\n                };\n                this._recorder.addEventListener("dataavailable", handleData, false);\n            });\n            this._recorder.stop();\n            return yield dataPromise;\n        });\n    }\n    /**\n     * Pause the recorder\n     */\n    pause() {\n        assert(this.state === "started", "Recorder must be started");\n        this._recorder.pause();\n        return this;\n    }\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this._stream.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=Recorder.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/dynamics/Compressor.js\n\n\n\n\n/**\n * Compressor is a thin wrapper around the Web Audio\n * [DynamicsCompressorNode](http://webaudio.github.io/web-audio-api/#the-dynamicscompressornode-interface).\n * Compression reduces the volume of loud sounds or amplifies quiet sounds\n * by narrowing or "compressing" an audio signal\'s dynamic range.\n * Read more on [Wikipedia](https://en.wikipedia.org/wiki/Dynamic_range_compression).\n * @example\n * const comp = new Tone.Compressor(-30, 3);\n * @category Component\n */\nclass Compressor_Compressor extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Compressor_Compressor.getDefaults(), arguments, ["threshold", "ratio"]));\n        this.name = "Compressor";\n        /**\n         * the compressor node\n         */\n        this._compressor = this.context.createDynamicsCompressor();\n        this.input = this._compressor;\n        this.output = this._compressor;\n        const options = optionsFromArguments(Compressor_Compressor.getDefaults(), arguments, ["threshold", "ratio"]);\n        this.threshold = new Param_Param({\n            minValue: this._compressor.threshold.minValue,\n            maxValue: this._compressor.threshold.maxValue,\n            context: this.context,\n            convert: false,\n            param: this._compressor.threshold,\n            units: "decibels",\n            value: options.threshold,\n        });\n        this.attack = new Param_Param({\n            minValue: this._compressor.attack.minValue,\n            maxValue: this._compressor.attack.maxValue,\n            context: this.context,\n            param: this._compressor.attack,\n            units: "time",\n            value: options.attack,\n        });\n        this.release = new Param_Param({\n            minValue: this._compressor.release.minValue,\n            maxValue: this._compressor.release.maxValue,\n            context: this.context,\n            param: this._compressor.release,\n            units: "time",\n            value: options.release,\n        });\n        this.knee = new Param_Param({\n            minValue: this._compressor.knee.minValue,\n            maxValue: this._compressor.knee.maxValue,\n            context: this.context,\n            convert: false,\n            param: this._compressor.knee,\n            units: "decibels",\n            value: options.knee,\n        });\n        this.ratio = new Param_Param({\n            minValue: this._compressor.ratio.minValue,\n            maxValue: this._compressor.ratio.maxValue,\n            context: this.context,\n            convert: false,\n            param: this._compressor.ratio,\n            units: "positive",\n            value: options.ratio,\n        });\n        // set the defaults\n        readOnly(this, ["knee", "release", "attack", "ratio", "threshold"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            attack: 0.003,\n            knee: 30,\n            ratio: 12,\n            release: 0.25,\n            threshold: -24,\n        });\n    }\n    /**\n     * A read-only decibel value for metering purposes, representing the current amount of gain\n     * reduction that the compressor is applying to the signal. If fed no signal the value will be 0 (no gain reduction).\n     */\n    get reduction() {\n        return this._compressor.reduction;\n    }\n    dispose() {\n        super.dispose();\n        this._compressor.disconnect();\n        this.attack.dispose();\n        this.release.dispose();\n        this.threshold.dispose();\n        this.ratio.dispose();\n        this.knee.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Compressor.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/dynamics/Gate.js\n\n\n\n\n\n\n/**\n * Gate only passes a signal through when the incoming\n * signal exceeds a specified threshold. It uses [[Follower]] to follow the ampltiude\n * of the incoming signal and compares it to the [[threshold]] value using [[GreaterThan]].\n *\n * @example\n * const gate = new Tone.Gate(-30, 0.2).toDestination();\n * const mic = new Tone.UserMedia().connect(gate);\n * // the gate will only pass through the incoming\n * // signal when it\'s louder than -30db\n * @category Component\n */\nclass Gate_Gate extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Gate_Gate.getDefaults(), arguments, ["threshold", "smoothing"])));\n        this.name = "Gate";\n        const options = optionsFromArguments(Gate_Gate.getDefaults(), arguments, ["threshold", "smoothing"]);\n        this._follower = new Follower_Follower({\n            context: this.context,\n            smoothing: options.smoothing,\n        });\n        this._gt = new GreaterThan_GreaterThan({\n            context: this.context,\n            value: dbToGain(options.threshold),\n        });\n        this.input = new Gain_Gain({ context: this.context });\n        this._gate = this.output = new Gain_Gain({ context: this.context });\n        // connections\n        this.input.connect(this._gate);\n        // the control signal\n        this.input.chain(this._follower, this._gt, this._gate.gain);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            smoothing: 0.1,\n            threshold: -40\n        });\n    }\n    /**\n     * The threshold of the gate in decibels\n     */\n    get threshold() {\n        return gainToDb(this._gt.value);\n    }\n    set threshold(thresh) {\n        this._gt.value = dbToGain(thresh);\n    }\n    /**\n     * The attack/decay speed of the gate. See [[Follower.smoothing]]\n     */\n    get smoothing() {\n        return this._follower.smoothing;\n    }\n    set smoothing(smoothingTime) {\n        this._follower.smoothing = smoothingTime;\n    }\n    dispose() {\n        super.dispose();\n        this.input.dispose();\n        this._follower.dispose();\n        this._gt.dispose();\n        this._gate.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Gate.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/dynamics/Limiter.js\n\n\n\n\n;\n/**\n * Limiter will limit the loudness of an incoming signal.\n * Under the hood it\'s composed of a [[Compressor]] with a fast attack\n * and release and max compression ratio.\n *\n * @example\n * const limiter = new Tone.Limiter(-20).toDestination();\n * const oscillator = new Tone.Oscillator().connect(limiter);\n * oscillator.start();\n * @category Component\n */\nclass Limiter_Limiter extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(Limiter_Limiter.getDefaults(), arguments, ["threshold"])));\n        this.name = "Limiter";\n        const options = optionsFromArguments(Limiter_Limiter.getDefaults(), arguments, ["threshold"]);\n        this._compressor = this.input = this.output = new Compressor_Compressor({\n            context: this.context,\n            ratio: 20,\n            attack: 0.003,\n            release: 0.01,\n            threshold: options.threshold\n        });\n        this.threshold = this._compressor.threshold;\n        readOnly(this, "threshold");\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            threshold: -12\n        });\n    }\n    /**\n     * A read-only decibel value for metering purposes, representing the current amount of gain\n     * reduction that the compressor is applying to the signal.\n     */\n    get reduction() {\n        return this._compressor.reduction;\n    }\n    dispose() {\n        super.dispose();\n        this._compressor.dispose();\n        this.threshold.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=Limiter.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/dynamics/MidSideCompressor.js\n\n\n\n\n\n\n/**\n * MidSideCompressor applies two different compressors to the [[mid]]\n * and [[side]] signal components of the input. See [[MidSideSplit]] and [[MidSideMerge]].\n * @category Component\n */\nclass MidSideCompressor_MidSideCompressor extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(MidSideCompressor_MidSideCompressor.getDefaults(), arguments)));\n        this.name = "MidSideCompressor";\n        const options = optionsFromArguments(MidSideCompressor_MidSideCompressor.getDefaults(), arguments);\n        this._midSideSplit = this.input = new MidSideSplit_MidSideSplit({ context: this.context });\n        this._midSideMerge = this.output = new MidSideMerge_MidSideMerge({ context: this.context });\n        this.mid = new Compressor_Compressor(Object.assign(options.mid, { context: this.context }));\n        this.side = new Compressor_Compressor(Object.assign(options.side, { context: this.context }));\n        this._midSideSplit.mid.chain(this.mid, this._midSideMerge.mid);\n        this._midSideSplit.side.chain(this.side, this._midSideMerge.side);\n        readOnly(this, ["mid", "side"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            mid: {\n                ratio: 3,\n                threshold: -24,\n                release: 0.03,\n                attack: 0.02,\n                knee: 16\n            },\n            side: {\n                ratio: 6,\n                threshold: -30,\n                release: 0.25,\n                attack: 0.03,\n                knee: 10\n            }\n        });\n    }\n    dispose() {\n        super.dispose();\n        this.mid.dispose();\n        this.side.dispose();\n        this._midSideSplit.dispose();\n        this._midSideMerge.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MidSideCompressor.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/dynamics/MultibandCompressor.js\n\n\n\n\n\n\n/**\n * A compressor with separate controls over low/mid/high dynamics. See [[Compressor]] and [[MultibandSplit]]\n *\n * @example\n * const multiband = new Tone.MultibandCompressor({\n * \tlowFrequency: 200,\n * \thighFrequency: 1300,\n * \tlow: {\n * \t\tthreshold: -12\n * \t}\n * });\n * @category Component\n */\nclass MultibandCompressor_MultibandCompressor extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(Object.assign(optionsFromArguments(MultibandCompressor_MultibandCompressor.getDefaults(), arguments)));\n        this.name = "MultibandCompressor";\n        const options = optionsFromArguments(MultibandCompressor_MultibandCompressor.getDefaults(), arguments);\n        this._splitter = this.input = new MultibandSplit_MultibandSplit({\n            context: this.context,\n            lowFrequency: options.lowFrequency,\n            highFrequency: options.highFrequency\n        });\n        this.lowFrequency = this._splitter.lowFrequency;\n        this.highFrequency = this._splitter.highFrequency;\n        this.output = new Gain_Gain({ context: this.context });\n        this.low = new Compressor_Compressor(Object.assign(options.low, { context: this.context }));\n        this.mid = new Compressor_Compressor(Object.assign(options.mid, { context: this.context }));\n        this.high = new Compressor_Compressor(Object.assign(options.high, { context: this.context }));\n        // connect the compressor\n        this._splitter.low.chain(this.low, this.output);\n        this._splitter.mid.chain(this.mid, this.output);\n        this._splitter.high.chain(this.high, this.output);\n        readOnly(this, ["high", "mid", "low", "highFrequency", "lowFrequency"]);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            lowFrequency: 250,\n            highFrequency: 2000,\n            low: {\n                ratio: 6,\n                threshold: -30,\n                release: 0.25,\n                attack: 0.03,\n                knee: 10\n            },\n            mid: {\n                ratio: 3,\n                threshold: -24,\n                release: 0.03,\n                attack: 0.02,\n                knee: 16\n            },\n            high: {\n                ratio: 3,\n                threshold: -24,\n                release: 0.03,\n                attack: 0.02,\n                knee: 16\n            },\n        });\n    }\n    dispose() {\n        super.dispose();\n        this._splitter.dispose();\n        this.low.dispose();\n        this.mid.dispose();\n        this.high.dispose();\n        this.output.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=MultibandCompressor.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/EQ3.js\n\n\n\n\n\n/**\n * EQ3 provides 3 equalizer bins: Low/Mid/High.\n * @category Component\n */\nclass EQ3_EQ3 extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(EQ3_EQ3.getDefaults(), arguments, ["low", "mid", "high"]));\n        this.name = "EQ3";\n        /**\n         * the output\n         */\n        this.output = new Gain_Gain({ context: this.context });\n        this._internalChannels = [];\n        const options = optionsFromArguments(EQ3_EQ3.getDefaults(), arguments, ["low", "mid", "high"]);\n        this.input = this._multibandSplit = new MultibandSplit_MultibandSplit({\n            context: this.context,\n            highFrequency: options.highFrequency,\n            lowFrequency: options.lowFrequency,\n        });\n        this._lowGain = new Gain_Gain({\n            context: this.context,\n            gain: options.low,\n            units: "decibels",\n        });\n        this._midGain = new Gain_Gain({\n            context: this.context,\n            gain: options.mid,\n            units: "decibels",\n        });\n        this._highGain = new Gain_Gain({\n            context: this.context,\n            gain: options.high,\n            units: "decibels",\n        });\n        this.low = this._lowGain.gain;\n        this.mid = this._midGain.gain;\n        this.high = this._highGain.gain;\n        this.Q = this._multibandSplit.Q;\n        this.lowFrequency = this._multibandSplit.lowFrequency;\n        this.highFrequency = this._multibandSplit.highFrequency;\n        // the frequency bands\n        this._multibandSplit.low.chain(this._lowGain, this.output);\n        this._multibandSplit.mid.chain(this._midGain, this.output);\n        this._multibandSplit.high.chain(this._highGain, this.output);\n        readOnly(this, ["low", "mid", "high", "lowFrequency", "highFrequency"]);\n        this._internalChannels = [this._multibandSplit];\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            high: 0,\n            highFrequency: 2500,\n            low: 0,\n            lowFrequency: 400,\n            mid: 0,\n        });\n    }\n    /**\n     * Clean up.\n     */\n    dispose() {\n        super.dispose();\n        writable(this, ["low", "mid", "high", "lowFrequency", "highFrequency"]);\n        this._multibandSplit.dispose();\n        this.lowFrequency.dispose();\n        this.highFrequency.dispose();\n        this._lowGain.dispose();\n        this._midGain.dispose();\n        this._highGain.dispose();\n        this.low.dispose();\n        this.mid.dispose();\n        this.high.dispose();\n        this.Q.dispose();\n        return this;\n    }\n}\n//# sourceMappingURL=EQ3.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/filter/Convolver.js\n\n\n\n\n\n\n/**\n * Convolver is a wrapper around the Native Web Audio\n * [ConvolverNode](http://webaudio.github.io/web-audio-api/#the-convolvernode-interface).\n * Convolution is useful for reverb and filter emulation. Read more about convolution reverb on\n * [Wikipedia](https://en.wikipedia.org/wiki/Convolution_reverb).\n *\n * @example\n * // initializing the convolver with an impulse response\n * const convolver = new Tone.Convolver("./path/to/ir.wav").toDestination();\n * @category Component\n */\nclass Convolver_Convolver extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Convolver_Convolver.getDefaults(), arguments, ["url", "onload"]));\n        this.name = "Convolver";\n        /**\n         * The native ConvolverNode\n         */\n        this._convolver = this.context.createConvolver();\n        const options = optionsFromArguments(Convolver_Convolver.getDefaults(), arguments, ["url", "onload"]);\n        this._buffer = new ToneAudioBuffer_ToneAudioBuffer(options.url, buffer => {\n            this.buffer = buffer;\n            options.onload();\n        });\n        this.input = new Gain_Gain({ context: this.context });\n        this.output = new Gain_Gain({ context: this.context });\n        // set if it\'s already loaded, set it immediately\n        if (this._buffer.loaded) {\n            this.buffer = this._buffer;\n        }\n        // initially set normalization\n        this.normalize = options.normalize;\n        // connect it up\n        this.input.chain(this._convolver, this.output);\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            normalize: true,\n            onload: noOp,\n        });\n    }\n    /**\n     * Load an impulse response url as an audio buffer.\n     * Decodes the audio asynchronously and invokes\n     * the callback once the audio buffer loads.\n     * @param url The url of the buffer to load. filetype support depends on the browser.\n     */\n    load(url) {\n        return __awaiter(this, void 0, void 0, function* () {\n            this.buffer = yield this._buffer.load(url);\n        });\n    }\n    /**\n     * The convolver\'s buffer\n     */\n    get buffer() {\n        if (this._buffer.length) {\n            return this._buffer;\n        }\n        else {\n            return null;\n        }\n    }\n    set buffer(buffer) {\n        if (buffer) {\n            this._buffer.set(buffer);\n        }\n        // if it\'s already got a buffer, create a new one\n        if (this._convolver.buffer) {\n            // disconnect the old one\n            this.input.disconnect();\n            this._convolver.disconnect();\n            // create and connect a new one\n            this._convolver = this.context.createConvolver();\n            this.input.chain(this._convolver, this.output);\n        }\n        const buff = this._buffer.get();\n        this._convolver.buffer = buff ? buff : null;\n    }\n    /**\n     * The normalize property of the ConvolverNode interface is a boolean that\n     * controls whether the impulse response from the buffer will be scaled by\n     * an equal-power normalization when the buffer attribute is set, or not.\n     */\n    get normalize() {\n        return this._convolver.normalize;\n    }\n    set normalize(norm) {\n        this._convolver.normalize = norm;\n    }\n    dispose() {\n        super.dispose();\n        this._buffer.dispose();\n        this._convolver.disconnect();\n        return this;\n    }\n}\n//# sourceMappingURL=Convolver.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/component/index.js\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/classes.js\n\n\n\n\n\n\n\n//# sourceMappingURL=classes.js.map\n// CONCATENATED MODULE: ./node_modules/tone/build/esm/index.js\n\n\n\n\n\n\n\n/**\n * The current audio context time of the global [[Context]].\n * See [[Context.now]]\n * @category Core\n */\nfunction esm_now() {\n    return getContext().now();\n}\n/**\n * The current audio context time of the global [[Context]] without the [[Context.lookAhead]]\n * See [[Context.immediate]]\n * @category Core\n */\nfunction immediate() {\n    return getContext().immediate();\n}\n/**\n * The Transport object belonging to the global Tone.js Context.\n * See [[Transport]]\n * @category Core\n */\nconst esm_Transport = getContext().transport;\n/**\n * The Transport object belonging to the global Tone.js Context.\n * See [[Transport]]\n * @category Core\n */\nfunction getTransport() {\n    return getContext().transport;\n}\n/**\n * The Destination (output) belonging to the global Tone.js Context.\n * See [[Destination]]\n * @category Core\n */\nconst esm_Destination = getContext().destination;\n/**\n * @deprecated Use [[Destination]]\n */\nconst Master = getContext().destination;\n/**\n * The Destination (output) belonging to the global Tone.js Context.\n * See [[Destination]]\n * @category Core\n */\nfunction getDestination() {\n    return getContext().destination;\n}\n/**\n * The [[Listener]] belonging to the global Tone.js Context.\n * @category Core\n */\nconst esm_Listener = getContext().listener;\n/**\n * The [[Listener]] belonging to the global Tone.js Context.\n * @category Core\n */\nfunction getListener() {\n    return getContext().listener;\n}\n/**\n * Draw is used to synchronize the draw frame with the Transport\'s callbacks.\n * See [[Draw]]\n * @category Core\n */\nconst esm_Draw = getContext().draw;\n/**\n * Get the singleton attached to the global context.\n * Draw is used to synchronize the draw frame with the Transport\'s callbacks.\n * See [[Draw]]\n * @category Core\n */\nfunction getDraw() {\n    return getContext().draw;\n}\n/**\n * A reference to the global context\n * See [[Context]]\n */\nconst esm_context = getContext();\n/**\n * Promise which resolves when all of the loading promises are resolved.\n * Alias for static [[ToneAudioBuffer.loaded]] method.\n * @category Core\n */\nfunction loaded() {\n    return ToneAudioBuffer_ToneAudioBuffer.loaded();\n}\n// this fills in name changes from 13.x to 14.x\n\n\nconst Buffer = ToneAudioBuffer_ToneAudioBuffer;\nconst Buffers = ToneAudioBuffers_ToneAudioBuffers;\nconst BufferSource = ToneBufferSource_ToneBufferSource;\n//# sourceMappingURL=index.js.map\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Component.js\nvar Component_awaiter = (undefined && undefined.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\n\n/**\n * Base class for the other components\n */\nclass Component_PianoComponent extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        this.name = \'PianoComponent\';\n        this.input = undefined;\n        this.output = new Volume_Volume({ context: this.context });\n        /**\n         * If the component is enabled or not\n         */\n        this._enabled = false;\n        /**\n         * The volume output of the component\n         */\n        this.volume = this.output.volume;\n        /**\n         * Boolean indication of if the component is loaded or not\n         */\n        this._loaded = false;\n        this.volume.value = options.volume;\n        this._enabled = options.enabled;\n        this.samples = options.samples;\n    }\n    /**\n     * If the samples are loaded or not\n     */\n    get loaded() {\n        return this._loaded;\n    }\n    /**\n     * Load the samples\n     */\n    load() {\n        return Component_awaiter(this, void 0, void 0, function* () {\n            if (this._enabled) {\n                yield this._internalLoad();\n                this._loaded = true;\n            }\n            else {\n                return Promise.resolve();\n            }\n        });\n    }\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Util.js\n// import * as Tone from \'../node_modules/tone/Tone\'\n\nfunction noteToMidi(note) {\n    return Frequency(note).toMidi();\n}\nfunction midiToNote(midi) {\n    const frequency = Frequency(midi, \'midi\');\n    const ret = frequency.toNote();\n    return ret;\n}\nfunction midiToFrequencyRatio(midi) {\n    const mod = midi % 3;\n    if (mod === 1) {\n        return [midi - 1, intervalToFrequencyRatio(1)];\n    }\n    else if (mod === 2) {\n        // @ts-ignore\n        return [midi + 1, intervalToFrequencyRatio(-1)];\n    }\n    else {\n        return [midi, 1];\n    }\n}\nfunction createSource(buffer) {\n    return new ToneBufferSource_ToneBufferSource(buffer);\n}\nfunction randomBetween(low, high) {\n    return Math.random() * (high - low) + low;\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Salamander.js\n\nconst githubURL = \'https://tambien.github.io/Piano/Salamander/\';\nfunction getReleasesUrl(midi) {\n    return `rel${midi - 20}.[mp3|ogg]`;\n}\nfunction getHarmonicsUrl(midi) {\n    return `harmS${midiToNote(midi).replace(\'#\', \'s\')}.[mp3|ogg]`;\n}\nfunction getNotesUrl(midi, vel) {\n    return `${midiToNote(midi).replace(\'#\', \'s\')}v${vel}.[mp3|ogg]`;\n}\n/**\n * Maps velocity depths to Salamander velocities\n */\nconst velocitiesMap = {\n    1: [8],\n    2: [6, 12],\n    3: [1, 7, 15],\n    4: [1, 5, 10, 15],\n    5: [1, 4, 8, 12, 16],\n    6: [1, 3, 7, 10, 13, 16],\n    7: [1, 3, 6, 9, 11, 13, 16],\n    8: [1, 3, 5, 7, 9, 11, 13, 16],\n    9: [1, 3, 5, 7, 9, 11, 13, 15, 16],\n    10: [1, 2, 3, 5, 7, 9, 11, 13, 15, 16],\n    11: [1, 2, 3, 5, 7, 9, 11, 13, 14, 15, 16],\n    12: [1, 2, 3, 4, 5, 7, 9, 11, 13, 14, 15, 16],\n    13: [1, 2, 3, 4, 5, 7, 9, 11, 12, 13, 14, 15, 16],\n    14: [1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14, 15, 16],\n    15: [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16],\n    16: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],\n};\n/**\n * All the notes of audio samples\n */\nconst allNotes = [\n    21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54,\n    57, 60, 63, 66, 69, 72, 75, 78, 81, 84,\n    87, 90, 93, 96, 99, 102, 105, 108\n];\nfunction getNotesInRange(min, max) {\n    return allNotes.filter(note => min <= note && note <= max);\n}\n/**\n * All the notes of audio samples\n */\nconst harmonics = [21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87];\nfunction getHarmonicsInRange(min, max) {\n    return harmonics.filter(note => min <= note && note <= max);\n}\nfunction inHarmonicsRange(note) {\n    return harmonics[0] <= note && note <= harmonics[harmonics.length - 1];\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Harmonics.js\n\n\n\n\nclass Harmonics_Harmonics extends Component_PianoComponent {\n    constructor(options) {\n        super(options);\n        this._urls = {};\n        const notes = getHarmonicsInRange(options.minNote, options.maxNote);\n        for (const n of notes) {\n            this._urls[n] = getHarmonicsUrl(n);\n        }\n    }\n    triggerAttack(note, time, velocity) {\n        if (this._enabled && inHarmonicsRange(note)) {\n            this._sampler.triggerAttack(Midi(note).toNote(), time, velocity * randomBetween(0.5, 1));\n        }\n    }\n    _internalLoad() {\n        return new Promise(onload => {\n            this._sampler = new Sampler_Sampler({\n                baseUrl: this.samples,\n                onload,\n                urls: this._urls,\n            }).connect(this.output);\n        });\n    }\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Keybed.js\n\n\n\n\nclass Keybed_Keybed extends Component_PianoComponent {\n    constructor(options) {\n        super(options);\n        /**\n         * The urls to load\n         */\n        this._urls = {};\n        for (let i = options.minNote; i <= options.maxNote; i++) {\n            this._urls[i] = getReleasesUrl(i);\n        }\n    }\n    _internalLoad() {\n        return new Promise(success => {\n            this._buffers = new ToneAudioBuffers_ToneAudioBuffers(this._urls, success, this.samples);\n        });\n    }\n    start(note, time, velocity) {\n        if (this._enabled && this._buffers.has(note)) {\n            const source = new ToneBufferSource_ToneBufferSource({\n                url: this._buffers.get(note),\n                context: this.context,\n            }).connect(this.output);\n            // randomize the velocity slightly\n            source.start(time, 0, undefined, 0.015 * velocity * randomBetween(0.5, 1));\n        }\n    }\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Pedal.js\n\n\n\nclass Pedal_Pedal extends Component_PianoComponent {\n    constructor(options) {\n        super(options);\n        this._downTime = Infinity;\n        this._currentSound = null;\n        this._downTime = Infinity;\n    }\n    _internalLoad() {\n        return new Promise((success) => {\n            this._buffers = new ToneAudioBuffers_ToneAudioBuffers({\n                down1: \'pedalD1.mp3\',\n                down2: \'pedalD2.mp3\',\n                up1: \'pedalU1.mp3\',\n                up2: \'pedalU2.mp3\',\n            }, success, this.samples);\n        });\n    }\n    /**\n     *  Squash the current playing sound\n     */\n    _squash(time) {\n        if (this._currentSound && this._currentSound.state !== \'stopped\') {\n            this._currentSound.stop(time);\n        }\n        this._currentSound = null;\n    }\n    _playSample(time, dir) {\n        if (this._enabled) {\n            this._currentSound = new ToneBufferSource_ToneBufferSource({\n                url: this._buffers.get(`${dir}${Math.random() > 0.5 ? 1 : 2}`),\n                context: this.context,\n                curve: \'exponential\',\n                fadeIn: 0.05,\n                fadeOut: 0.1,\n            }).connect(this.output);\n            this._currentSound.start(time, randomBetween(0, 0.01), undefined, 0.1 * randomBetween(0.5, 1));\n        }\n    }\n    /**\n     * Put the pedal down\n     */\n    down(time) {\n        this._squash(time);\n        this._downTime = time;\n        this._playSample(time, \'down\');\n    }\n    /**\n     * Put the pedal up\n     */\n    up(time) {\n        this._squash(time);\n        this._downTime = Infinity;\n        this._playSample(time, \'up\');\n    }\n    /**\n     * Indicates if the pedal is down at the given time\n     */\n    isDown(time) {\n        return time > this._downTime;\n    }\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/String.js\n\n\n/**\n * A single velocity of strings\n */\nclass String_PianoString extends ToneAudioNode_ToneAudioNode {\n    constructor(options) {\n        super(options);\n        this.name = \'PianoString\';\n        this._urls = {};\n        // create the urls\n        options.notes.forEach(note => this._urls[note] = getNotesUrl(note, options.velocity));\n        this.samples = options.samples;\n    }\n    load() {\n        return new Promise(onload => {\n            this._sampler = this.output = new Sampler_Sampler({\n                attack: 0,\n                baseUrl: this.samples,\n                curve: \'exponential\',\n                onload,\n                release: 0.4,\n                urls: this._urls,\n                volume: 3,\n            });\n        });\n    }\n    triggerAttack(note, time, velocity) {\n        this._sampler.triggerAttack(note, time, velocity);\n    }\n    triggerRelease(note, time) {\n        this._sampler.triggerRelease(note, time);\n    }\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Strings.js\nvar Strings_awaiter = (undefined && undefined.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\n\n\n\n\n/**\n *  Manages all of the hammered string sounds\n */\nclass Strings_PianoStrings extends Component_PianoComponent {\n    constructor(options) {\n        super(options);\n        const notes = getNotesInRange(options.minNote, options.maxNote);\n        const velocities = velocitiesMap[options.velocities].slice();\n        this._strings = velocities.map(velocity => {\n            const string = new String_PianoString(Object.assign(options, {\n                notes, velocity,\n            }));\n            return string;\n        });\n        this._activeNotes = new Map();\n    }\n    /**\n     * Scale a value between a given range\n     */\n    scale(val, inMin, inMax, outMin, outMax) {\n        return ((val - inMin) / (inMax - inMin)) * (outMax - outMin) + outMin;\n    }\n    triggerAttack(note, time, velocity) {\n        const scaledVel = this.scale(velocity, 0, 1, -0.5, this._strings.length - 0.51);\n        const stringIndex = Math.max(Math.round(scaledVel), 0);\n        let gain = 1 + scaledVel - stringIndex;\n        if (this._strings.length === 1) {\n            gain = velocity;\n        }\n        const sampler = this._strings[stringIndex];\n        if (this._activeNotes.has(note)) {\n            this.triggerRelease(note, time);\n        }\n        this._activeNotes.set(note, sampler);\n        sampler.triggerAttack(Midi(note).toNote(), time, gain);\n    }\n    triggerRelease(note, time) {\n        // trigger the release of all of the notes at that velociy\n        if (this._activeNotes.has(note)) {\n            this._activeNotes.get(note).triggerRelease(Midi(note).toNote(), time);\n            this._activeNotes.delete(note);\n        }\n    }\n    _internalLoad() {\n        return Strings_awaiter(this, void 0, void 0, function* () {\n            yield Promise.all(this._strings.map((s) => Strings_awaiter(this, void 0, void 0, function* () {\n                yield s.load();\n                s.connect(this.output);\n            })));\n        });\n    }\n}\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/piano/Piano.js\nvar Piano_awaiter = (undefined && undefined.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\n\n\n\n\n\n/**\n *  The Piano\n */\nclass Piano_Piano extends ToneAudioNode_ToneAudioNode {\n    constructor() {\n        super(optionsFromArguments(Piano_Piano.getDefaults(), arguments));\n        this.name = \'Piano\';\n        this.input = undefined;\n        this.output = new Gain_Gain({ context: this.context });\n        /**\n         * The currently held notes\n         */\n        this._heldNotes = new Map();\n        /**\n         * If it\'s loaded or not\n         */\n        this._loaded = false;\n        const options = optionsFromArguments(Piano_Piano.getDefaults(), arguments);\n        // make sure it ends with a /\n        if (!options.url.endsWith(\'/\')) {\n            options.url += \'/\';\n        }\n        this.maxPolyphony = options.maxPolyphony;\n        this._heldNotes = new Map();\n        this._sustainedNotes = new Map();\n        this._strings = new Strings_PianoStrings(Object.assign({}, options, {\n            enabled: true,\n            samples: options.url,\n            volume: options.volume.strings,\n        })).connect(this.output);\n        this.strings = this._strings.volume;\n        this._pedal = new Pedal_Pedal(Object.assign({}, options, {\n            enabled: options.pedal,\n            samples: options.url,\n            volume: options.volume.pedal,\n        })).connect(this.output);\n        this.pedal = this._pedal.volume;\n        this._keybed = new Keybed_Keybed(Object.assign({}, options, {\n            enabled: options.release,\n            samples: options.url,\n            volume: options.volume.keybed,\n        })).connect(this.output);\n        this.keybed = this._keybed.volume;\n        this._harmonics = new Harmonics_Harmonics(Object.assign({}, options, {\n            enabled: options.release,\n            samples: options.url,\n            volume: options.volume.harmonics,\n        })).connect(this.output);\n        this.harmonics = this._harmonics.volume;\n    }\n    static getDefaults() {\n        return Object.assign(ToneAudioNode_ToneAudioNode.getDefaults(), {\n            maxNote: 108,\n            minNote: 21,\n            pedal: true,\n            release: false,\n            url: \'https://tambien.github.io/Piano/audio/\',\n            velocities: 1,\n            maxPolyphony: 32,\n            volume: {\n                harmonics: 0,\n                keybed: 0,\n                pedal: 0,\n                strings: 0,\n            },\n        });\n    }\n    /**\n     *  Load all the samples\n     */\n    load() {\n        return Piano_awaiter(this, void 0, void 0, function* () {\n            yield Promise.all([\n                this._strings.load(),\n                this._pedal.load(),\n                this._keybed.load(),\n                this._harmonics.load(),\n            ]);\n            this._loaded = true;\n        });\n    }\n    /**\n     * If all the samples are loaded or not\n     */\n    get loaded() {\n        return this._loaded;\n    }\n    /**\n     *  Put the pedal down at the given time. Causes subsequent\n     *  notes and currently held notes to sustain.\n     */\n    pedalDown({ time = this.immediate() } = {}) {\n        if (this.loaded) {\n            time = this.toSeconds(time);\n            if (!this._pedal.isDown(time)) {\n                this._pedal.down(time);\n            }\n        }\n        return this;\n    }\n    /**\n     *  Put the pedal up. Dampens sustained notes\n     */\n    pedalUp({ time = this.immediate() } = {}) {\n        if (this.loaded) {\n            const seconds = this.toSeconds(time);\n            if (this._pedal.isDown(seconds)) {\n                this._pedal.up(seconds);\n                // dampen each of the notes\n                this._sustainedNotes.forEach((t, note) => {\n                    if (!this._heldNotes.has(note)) {\n                        this._strings.triggerRelease(note, seconds);\n                    }\n                });\n                this._sustainedNotes.clear();\n            }\n        }\n        return this;\n    }\n    /**\n     *  Play a note.\n     *  @param note\t  The note to play. If it is a number, it is assumed to be MIDI\n     *  @param velocity  The velocity to play the note\n     *  @param time\t  The time of the event\n     */\n    keyDown({ note, midi, time = this.immediate(), velocity = 0.8 }) {\n        if (this.loaded && this.maxPolyphony > this._heldNotes.size + this._sustainedNotes.size) {\n            time = this.toSeconds(time);\n            if (isString(note)) {\n                midi = Math.round(Midi(note).toMidi());\n            }\n            if (!this._heldNotes.has(midi)) {\n                // record the start time and velocity\n                this._heldNotes.set(midi, { time, velocity });\n                this._strings.triggerAttack(midi, time, velocity);\n            }\n        }\n        else {\n            console.warn(\'samples not loaded\');\n        }\n        return this;\n    }\n    /**\n     *  Release a held note.\n     */\n    keyUp({ note, midi, time = this.immediate(), velocity = 0.8 }) {\n        if (this.loaded) {\n            time = this.toSeconds(time);\n            if (isString(note)) {\n                midi = Math.round(Midi(note).toMidi());\n            }\n            if (this._heldNotes.has(midi)) {\n                const prevNote = this._heldNotes.get(midi);\n                this._heldNotes.delete(midi);\n                // compute the release velocity\n                const holdTime = Math.pow(Math.max(time - prevNote.time, 0.1), 0.7);\n                const prevVel = prevNote.velocity;\n                let dampenGain = (3 / holdTime) * prevVel * velocity;\n                dampenGain = Math.max(dampenGain, 0.4);\n                dampenGain = Math.min(dampenGain, 4);\n                if (this._pedal.isDown(time)) {\n                    if (!this._sustainedNotes.has(midi)) {\n                        this._sustainedNotes.set(midi, time);\n                    }\n                }\n                else {\n                    // release the string sound\n                    this._strings.triggerRelease(midi, time);\n                    // trigger the harmonics sound\n                    this._harmonics.triggerAttack(midi, time, dampenGain);\n                }\n                // trigger the keybed release sound\n                this._keybed.start(midi, time, velocity);\n            }\n        }\n        return this;\n    }\n    stopAll() {\n        this.pedalUp();\n        this._heldNotes.forEach((_, midi) => {\n            this.keyUp({ midi });\n        });\n        return this;\n    }\n}\n\n// EXTERNAL MODULE: ./node_modules/events/events.js\nvar events_events = __webpack_require__(5);\n\n// EXTERNAL MODULE: ./node_modules/webmidi/webmidi.min.js\nvar webmidi_min = __webpack_require__(1);\nvar webmidi_min_default = /*#__PURE__*/__webpack_require__.n(webmidi_min);\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/midi/MidiInput.js\nvar MidiInput_awaiter = (undefined && undefined.__awaiter) || function (thisArg, _arguments, P, generator) {\n    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }\n    return new (P || (P = Promise))(function (resolve, reject) {\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\n        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }\n        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\n    });\n};\n\n\nclass MidiInput_MidiInput extends events_events["EventEmitter"] {\n    constructor(deviceId = \'all\') {\n        super();\n        this.deviceId = deviceId;\n        /**\n         * Automatically attaches the event listeners when a device is connect\n         * and removes listeners when a device is disconnected\n         */\n        MidiInput_MidiInput.enabled().then(() => {\n            webmidi_min_default.a.addListener(\'connected\', (event) => {\n                if (event.port.type === \'input\') {\n                    this._addListeners(event.port);\n                }\n            });\n            webmidi_min_default.a.addListener(\'disconnected\', (event) => {\n                this._removeListeners(event.port);\n            });\n            // add all of the existing inputs\n            webmidi_min_default.a.inputs.forEach(input => this._addListeners(input));\n        });\n    }\n    /**\n     * Attach listeners to the device when it\'s connected\n     */\n    _addListeners(device) {\n        if (!MidiInput_MidiInput.connectedDevices.has(device.id)) {\n            MidiInput_MidiInput.connectedDevices.set(device.id, device);\n            this.emit(\'connect\', this._inputToDevice(device));\n            device.addListener(\'noteon\', \'all\', (event) => {\n                if (this.deviceId === \'all\' || this.deviceId === device.id) {\n                    this.emit(\'keyDown\', {\n                        note: `${event.note.name}${event.note.octave}`,\n                        midi: event.note.number,\n                        velocity: event.velocity,\n                        device: this._inputToDevice(device)\n                    });\n                }\n            });\n            device.addListener(\'noteoff\', \'all\', (event) => {\n                if (this.deviceId === \'all\' || this.deviceId === device.id) {\n                    this.emit(\'keyUp\', {\n                        note: `${event.note.name}${event.note.octave}`,\n                        midi: event.note.number,\n                        velocity: event.velocity,\n                        device: this._inputToDevice(device)\n                    });\n                }\n            });\n            device.addListener(\'controlchange\', \'all\', (event) => {\n                if (this.deviceId === \'all\' || this.deviceId === device.id) {\n                    if (event.controller.name === \'holdpedal\') {\n                        this.emit(event.value ? \'pedalDown\' : \'pedalUp\', {\n                            device: this._inputToDevice(device)\n                        });\n                    }\n                }\n            });\n        }\n    }\n    _inputToDevice(input) {\n        return {\n            name: input.name,\n            id: input.id,\n            manufacturer: input.manufacturer\n        };\n    }\n    /**\n     * Internal call to remove all event listeners associated with the device\n     */\n    _removeListeners(event) {\n        if (MidiInput_MidiInput.connectedDevices.has(event.id)) {\n            const device = MidiInput_MidiInput.connectedDevices.get(event.id);\n            this.emit(\'disconnect\', this._inputToDevice(device));\n            MidiInput_MidiInput.connectedDevices.delete(event.id);\n            device.removeListener(\'noteon\');\n            device.removeListener(\'noteoff\');\n            device.removeListener(\'controlchange\');\n        }\n    }\n    // EVENT FUNCTIONS\n    emit(event, data) {\n        return super.emit(event, data);\n    }\n    on(event, listener) {\n        super.on(event, listener);\n        return this;\n    }\n    once(event, listener) {\n        super.once(event, listener);\n        return this;\n    }\n    off(event, listener) {\n        super.off(event, listener);\n        return this;\n    }\n    /**\n     * Resolves when the MIDI Input is enabled and ready to use\n     */\n    static enabled() {\n        return MidiInput_awaiter(this, void 0, void 0, function* () {\n            if (!MidiInput_MidiInput._isEnabled) {\n                yield new Promise((done, error) => {\n                    webmidi_min_default.a.enable((e) => {\n                        if (e) {\n                            error(e);\n                        }\n                        else {\n                            MidiInput_MidiInput._isEnabled = true;\n                            done();\n                        }\n                    });\n                });\n            }\n        });\n    }\n    /**\n     * Get a list of devices that are currently connected\n     */\n    static getDevices() {\n        return MidiInput_awaiter(this, void 0, void 0, function* () {\n            yield MidiInput_MidiInput.enabled();\n            return webmidi_min_default.a.inputs;\n        });\n    }\n}\n// STATIC\nMidiInput_MidiInput.connectedDevices = new Map();\nMidiInput_MidiInput._isEnabled = false;\n\n// CONCATENATED MODULE: ./node_modules/@tonejs/piano/build/index.js\n\n\n\n// CONCATENATED MODULE: ./src/engine.js\n\r\n\r\n\r\nclass engine_Engine {\r\n    constructor() {\r\n        this.notes = [];\r\n        this.toneParts = [];\r\n    }\r\n\r\n    init() {\r\n        this.piano = new Piano_Piano({\r\n            url: \'audio\',\r\n            release: true,\r\n            pedal: true,\r\n            volume: {\r\n                strings: 3,\r\n                harmonics: 3,\r\n                keybed: 3,\r\n                pedal: 3\r\n            }\r\n        }).toDestination();\r\n\r\n        return this.piano.load();\r\n    }\r\n\r\n    queueNotes(notes, onNoteDown, onNoteUp) {\r\n\r\n        console.log(\'queuing notes...\');\r\n\r\n        let nextNoteIndex = this.notes.length;\r\n\r\n        const noteOnNotes = notes.map(note => ({\r\n            ...note,\r\n            index: nextNoteIndex++\r\n        }));\r\n\r\n        this.notes.push(...noteOnNotes);\r\n\r\n        const noteOffNotes = noteOnNotes.map(note => ({\r\n            ...note,\r\n            time: note.time + note.duration\r\n        }));\r\n\r\n        this.toneParts.push(\r\n            new Part_Part((time, event) => {\r\n\r\n                const {note, velocity} = event;\r\n                this.piano.keyDown({note, time, velocity});\r\n                if (onNoteDown) {\r\n                    onNoteDown(event);\r\n                }\r\n\r\n            }, noteOnNotes).start(0)\r\n        );\r\n\r\n        this.toneParts.push(\r\n            new Part_Part((time, event) => {\r\n\r\n                const {note} = event;\r\n                this.piano.keyUp({note, time});\r\n                if (onNoteUp) {\r\n                    onNoteUp(event);\r\n                }\r\n\r\n            }, noteOffNotes).start(0)\r\n        );\r\n    }\r\n\r\n    getNote(index) {\r\n        return this.notes[index];\r\n    }\r\n\r\n    startAudioContext() {\r\n        Global_start();\r\n    }\r\n\r\n    play() {\r\n        esm_Transport.start();\r\n    }\r\n\r\n    stop() {\r\n        esm_Transport.stop();\r\n\r\n        this.toneParts.forEach(tonePart => {\r\n            tonePart.clear();\r\n            tonePart.dispose();\r\n        });\r\n        this.toneParts = [];\r\n        this.notes = [];\r\n    }\r\n\r\n}\r\n\n// CONCATENATED MODULE: ./src/index.js\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nconst engine = new engine_Engine();\r\nconst music = new music_Music(getPiDigitArray());\r\nconst allPianoNotes = music.getPianoNotes();\r\nconst noteDurations = music.getNoteDurations();\r\nconst controlsView = new ControlsView(\r\n    music.getScalesByLength(),\r\n    noteDurations,\r\n    allPianoNotes\r\n);\r\nconst vis = new Visualization();\r\n\r\nlet timeoutIds = [];\r\nlet running = false;\r\n\r\ncontrolsView.addButtonClickListener(() => {\r\n    engine.startAudioContext();\r\n\r\n    running = ! running;\r\n\r\n    controlsView.setButtonLabel(running ? "STOP" : "PLAY");\r\n\r\n    setTimeout(() => toggle(running));\r\n});\r\n\r\nengine.init().then(() => {\r\n    controlsView.setLoadingComplete();\r\n    controlsView.activateButton();\r\n});\r\n\r\nfunction toggle(running) {\r\n    if (! running) {\r\n        engine.stop();\r\n        timeoutIds.forEach(id => clearTimeout(id));\r\n        timeoutIds = [];\r\n    }\r\n    else {\r\n        vis.clear();\r\n        src_build();\r\n        engine.play();\r\n    }\r\n}\r\n\r\nfunction src_build() {\r\n    const scale = controlsView.getScale();\r\n    const bpm = controlsView.getBpm();\r\n    const tonic = allPianoNotes[controlsView.getTonic()];\r\n    const noteDuration = parseFloat(controlsView.getNoteDuration());\r\n    const beatLengthSeconds = 60 / bpm;\r\n    const digitDisplayDelayMs = 0.25 * beatLengthSeconds * 1000;\r\n\r\n    const noteOptions = {\r\n        scale,\r\n        beatLengthSeconds,\r\n        tonic,\r\n        noteDuration\r\n    };\r\n\r\n    // queue small batch to get music started faster...\r\n    // then two more batches for the remainder of available digits\r\n\r\n    const total = 10000;\r\n    const firstBatchSize = 1800;\r\n    const secondaryBatchesSize = (total - firstBatchSize) / 2;\r\n    const secondBatchTimeoutMs = 1000 * ((firstBatchSize * beatLengthSeconds) - 15);\r\n    const thirdBatchTimeoutMs = secondBatchTimeoutMs + (1000 * ((secondaryBatchesSize * beatLengthSeconds) - 15));\r\n\r\n    const onNoteDown = (event) => timeoutIds.push(\r\n        setTimeout(\r\n            () => updateDigitDisplay(event),\r\n            digitDisplayDelayMs\r\n        )\r\n    );\r\n\r\n    engine.queueNotes(  // 1800 = 2 minutes at 900bpm\r\n        music.generateNotes(noteOptions, 0, firstBatchSize),\r\n        onNoteDown\r\n    );\r\n\r\n    timeoutIds.push(\r\n        setTimeout(\r\n            () => engine.queueNotes(\r\n                music.generateNotes(noteOptions, firstBatchSize, firstBatchSize + secondaryBatchesSize),\r\n                onNoteDown\r\n            ),\r\n            secondBatchTimeoutMs\r\n        )\r\n    );\r\n    timeoutIds.push(\r\n        setTimeout(\r\n            () => engine.queueNotes(\r\n                music.generateNotes(noteOptions, firstBatchSize + secondaryBatchesSize, total),\r\n                onNoteDown\r\n            ),\r\n            thirdBatchTimeoutMs\r\n        )\r\n    );\r\n\r\n}\r\n\r\nfunction updateDigitDisplay(note) {\r\n    vis.append(note.digit);\r\n\r\n    if (note.index % 10 === 0) {\r\n        vis.space();\r\n    }\r\n\r\n    if (note.index % 40 === 0 && note.index !== 0) {\r\n        vis.newline();\r\n    }\r\n}\r\n\n\n//# sourceURL=webpack:///./src/index.js_+_954_modules?')}]);